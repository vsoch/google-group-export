Date: Wed, 6 Jul 2016 07:20:36 -0700 (PDT)
From: Raimon Bosch <raimo...@gmail.com>
To: singularity <singu...@lbl.gov>
Cc: r...@open-mpi.org
Message-Id: <920d9dbb-1b7b-48cf-9d14-42cd149142e0@lbl.gov>
In-Reply-To: <CAN7etTy8-xQ5ATWaKxrUCi=AH+QHv8ddjTeG7P2XrikSAh2pug@mail.gmail.com>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
 <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
 <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
 <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
 <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov>
 <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
 <054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov>
 <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
 <007b47f4-0aea-42dc-b871-d653bb7a67a1@lbl.gov>
 <CAN7etTxOGqMfyg_C2AWisRWCCs2RKkM91s6SbWTcjYb3X5_Aew@mail.gmail.com>
 <15574850-11a7-4317-b784-26631fad4f29@lbl.gov>
 <e8eb3fb2-05cc-4f5e-bbab-91da877b6e1c@lbl.gov>
 <CAN7etTy8-xQ5ATWaKxrUCi=AH+QHv8ddjTeG7P2XrikSAh2pug@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_405_920590751.1467814836720"

------=_Part_405_920590751.1467814836720
Content-Type: multipart/alternative; 
	boundary="----=_Part_406_2011422175.1467814836722"

------=_Part_406_2011422175.1467814836722
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable


Hi Gregory,

> /run/user is associated with the Singularity container?

I guess it is. Because containers are 3G size and it matches with this=20
instances on /run/user/**. Unmounting them did not help.

> Can you show me the output of 'losetup -a' please?

"sudo losetup -a" returns empty

> Why are you are running it with sudo, you should not need to.

I execute with sudo because the container inside needs 'root'. This is an=
=20
old docker container that only has a unique user root with all the files=20
(probably i should change this in the future).

> It is weird, isn't -n a synonym for -np and if so, shouldn't it executing=
=20
1 process on the given node? It seems like it is doing more.=20

In my local machine the behaviour is correct. Tested it with -np and the=20
behaviour is the same.

> Lastly, what version of Singularity is this?

Is the master. I did "git clone=20
https://github.com/gmkurtzer/singularity.git" and followed the installation=
=20
steps.

As a side comment, If I deploy with a unique container I don't encounter=20
this problem. I think that when I want to mount extra containers that the=
=20
SO gets crazy or maybe singularity tries to assign containers to a=20
/dev/loop* that is busy and does not try to look for one that is available.=
=20
In my final test I will need at least 16 containers in one host. Is that=20
possible with singularity because I only see 8 loops?

Here you have the debug output:

> sudo mpirun -n 1 singularity -d exec=20
/mnt/glusterfs/singularity/nasmpi-singularity.img true
enabling debugging
ending argument loop
+ '[' -f /usr/local/etc/singularity/init ']'
+ . /usr/local/etc/singularity/init
++ unset module
++=20
PATH=3D/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/sbin:/usr/local/bin:/usr/sb=
in:/usr/bin:/sbin:/bin
++ HISTFILE=3D/dev/null
++ export PATH HISTFILE
++ '[' -n 1 ']'
++ SINGULARITY_NO_NAMESPACE_PID=3D1
++ export SINGULARITY_NO_NAMESPACE_PID
+ true
+ case $1 in
+ break
+ '[' -z /mnt/glusterfs/singularity/nasmpi-singularity.img ']'
+ SINGULARITY_IMAGE=3D/mnt/glusterfs/singularity/nasmpi-singularity.img
+ export SINGULARITY_IMAGE
+ shift
+ exec /usr/local/libexec/singularity/sexec true
VERBOSE [U=3D0,P=3D3944]       message.c:46:init()                        :=
=20
Setting messagelevel to: 5
DEBUG   [U=3D0,P=3D3944]       sexec.c:127:main()                         :=
=20
Gathering and caching user info.
DEBUG   [U=3D0,P=3D3944]       privilege.c:43:get_user_privs()            :=
=20
Called get_user_privs(struct s_privinfo *uinfo)
DEBUG   [U=3D0,P=3D3944]       privilege.c:54:get_user_privs()            :=
=20
Returning get_user_privs(struct s_privinfo *uinfo) =3D 0
DEBUG   [U=3D0,P=3D3944]       sexec.c:134:main()                         :=
=20
Checking if we can escalate privs properly.
DEBUG   [U=3D0,P=3D3944]       privilege.c:61:escalate_privs()            :=
=20
Called escalate_privs(void)
DEBUG   [U=3D0,P=3D3944]       privilege.c:73:escalate_privs()            :=
=20
Returning escalate_privs(void) =3D 0
DEBUG   [U=3D0,P=3D3944]       sexec.c:141:main()                         :=
=20
Setting privs to calling user
DEBUG   [U=3D0,P=3D3944]       privilege.c:79:drop_privs()                :=
=20
Called drop_privs(struct s_privinfo *uinfo)
DEBUG   [U=3D0,P=3D3944]       privilege.c:87:drop_privs()                :=
=20
Dropping privileges to GID =3D '0'
DEBUG   [U=3D0,P=3D3944]       privilege.c:93:drop_privs()                :=
=20
Dropping privileges to UID =3D '0'
DEBUG   [U=3D0,P=3D3944]       privilege.c:103:drop_privs()               :=
=20
Confirming we have correct GID
DEBUG   [U=3D0,P=3D3944]       privilege.c:109:drop_privs()               :=
=20
Confirming we have correct UID
DEBUG   [U=3D0,P=3D3944]       privilege.c:115:drop_privs()               :=
=20
Returning drop_privs(struct s_privinfo *uinfo) =3D 0
DEBUG   [U=3D0,P=3D3944]       sexec.c:146:main()                         :=
=20
Obtaining user's homedir
DEBUG   [U=3D0,P=3D3944]       sexec.c:150:main()                         :=
=20
Obtaining file descriptor to current directory
DEBUG   [U=3D0,P=3D3944]       sexec.c:155:main()                         :=
=20
Getting current working directory path string
DEBUG   [U=3D0,P=3D3944]       sexec.c:161:main()                         :=
=20
Obtaining SINGULARITY_COMMAND from environment
DEBUG   [U=3D0,P=3D3944]       sexec.c:168:main()                         :=
=20
Obtaining SINGULARITY_IMAGE from environment
DEBUG   [U=3D0,P=3D3944]       sexec.c:174:main()                         :=
=20
Checking container image is a file:=20
/mnt/glusterfs/singularity/nasmpi-singularity.img
DEBUG   [U=3D0,P=3D3944]       sexec.c:180:main()                         :=
=20
Building configuration file location
DEBUG   [U=3D0,P=3D3944]       sexec.c:183:main()                         :=
=20
Config location: /usr/local/etc/singularity/singularity.conf
DEBUG   [U=3D0,P=3D3944]       sexec.c:185:main()                         :=
=20
Checking Singularity configuration is a file:=20
/usr/local/etc/singularity/singularity.conf
DEBUG   [U=3D0,P=3D3944]       sexec.c:191:main()                         :=
=20
Checking Singularity configuration file is owned by root
DEBUG   [U=3D0,P=3D3944]       sexec.c:197:main()                         :=
=20
Opening Singularity configuration file
DEBUG   [U=3D0,P=3D3944]       sexec.c:210:main()                         :=
=20
Checking Singularity configuration for 'sessiondir prefix'
DEBUG   [U=3D0,P=3D3944]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, sessiondir prefix)
DEBUG   [U=3D0,P=3D3944]       config_parser.c:61:config_get_key_value()  :=
=20
Return config_get_key_value(fp, sessiondir prefix) =3D NULL
DEBUG   [U=3D0,P=3D3944]       file.c:48:file_id()                        :=
=20
Called file_id(/mnt/glusterfs/singularity/nasmpi-singularity.img)
VERBOSE [U=3D0,P=3D3944]       file.c:58:file_id()                        :=
=20
Generated file_id: 0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       file.c:60:file_id()                        :=
=20
Returning file_id(/mnt/glusterfs/singularity/nasmpi-singularity.img) =3D=20
0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       sexec.c:217:main()                         :=
 Set=20
sessiondir to: /tmp/.singularity-session-0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       sexec.c:221:main()                         :=
 Set=20
containername to: nasmpi-singularity.img
DEBUG   [U=3D0,P=3D3944]       sexec.c:223:main()                         :=
=20
Setting loop_dev_* paths
DEBUG   [U=3D0,P=3D3944]       sexec.c:229:main()                         :=
 Set=20
image mount path to: /usr/local/var/singularity/mnt
LOG     [U=3D0,P=3D3944]       sexec.c:231:main()                         :=
=20
Command=3Dexec, Container=3D/mnt/glusterfs/singularity/nasmpi-singularity.i=
mg,=20
CWD=3D/tmp/result, Arg1=3Dtrue
DEBUG   [U=3D0,P=3D3944]       sexec.c:236:main()                         :=
 Set=20
prompt to: Singularity/nasmpi-singularity.img>=20
DEBUG   [U=3D0,P=3D3944]       sexec.c:238:main()                         :=
=20
Checking if we are opening image as read/write
DEBUG   [U=3D0,P=3D3944]       sexec.c:240:main()                         :=
=20
Opening image as read only:=20
/mnt/glusterfs/singularity/nasmpi-singularity.img
DEBUG   [U=3D0,P=3D3944]       sexec.c:247:main()                         :=
=20
Setting shared lock on file descriptor: 6
DEBUG   [U=3D0,P=3D3944]       sexec.c:267:main()                         :=
=20
Checking for namespace daemon pidfile
DEBUG   [U=3D0,P=3D3944]       sexec.c:301:main()                         :=
=20
Escalating privledges
DEBUG   [U=3D0,P=3D3944]       privilege.c:61:escalate_privs()            :=
=20
Called escalate_privs(void)
DEBUG   [U=3D0,P=3D3944]       privilege.c:73:escalate_privs()            :=
=20
Returning escalate_privs(void) =3D 0
VERBOSE [U=3D0,P=3D3944]       sexec.c:306:main()                         :=
=20
Creating/Verifying session directory:=20
/tmp/.singularity-session-0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       file.c:196:s_mkpath()                      :=
=20
Creating directory: /tmp/.singularity-session-0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       sexec.c:320:main()                         :=
=20
Setting shared lock on session directory
DEBUG   [U=3D0,P=3D3944]       sexec.c:331:main()                         :=
=20
Caching info into sessiondir
DEBUG   [U=3D0,P=3D3944]       file.c:255:fileput()                       :=
=20
Called fileput(/tmp/.singularity-session-0.39.12911060245380037651/image,=
=20
nasmpi-singularity.img)
DEBUG   [U=3D0,P=3D3944]       sexec.c:337:main()                         :=
=20
Checking for set loop device
DEBUG   [U=3D0,P=3D3944]       loop-control.c:52:obtain_loop_dev()        :=
=20
Called obtain_loop_dev(void)
DEBUG   [U=3D0,P=3D3944]       loop-control.c:66:obtain_loop_dev()        :=
=20
Found available existing loop device number: 0
VERBOSE [U=3D0,P=3D3944]       loop-control.c:81:obtain_loop_dev()        :=
=20
Using loop device: /dev/loop0
DEBUG   [U=3D0,P=3D3944]       loop-control.c:95:obtain_loop_dev()        :=
=20
Returning obtain_loop_dev(void) =3D /dev/loop0
DEBUG   [U=3D0,P=3D3944]       loop-control.c:106:associate_loop()        :=
=20
Called associate_loop(image_fp, loop_fp, 1)
DEBUG   [U=3D0,P=3D3944]       loop-control.c:109:associate_loop()        :=
=20
Setting loop flags to LO_FLAGS_AUTOCLEAR
VERBOSE [U=3D0,P=3D3944]       image.c:39:image_offset()                  :=
=20
Calculating image offset
VERBOSE [U=3D0,P=3D3944]       image.c:48:image_offset()                  :=
=20
Found image at an offset of 31 bytes
DEBUG   [U=3D0,P=3D3944]       image.c:53:image_offset()                  :=
=20
Returning image_offset(image_fp) =3D 31
DEBUG   [U=3D0,P=3D3944]       loop-control.c:114:associate_loop()        :=
=20
Setting image offset to: 31
VERBOSE [U=3D0,P=3D3944]       loop-control.c:116:associate_loop()        :=
=20
Associating image to loop device
VERBOSE [U=3D0,P=3D3944]       loop-control.c:122:associate_loop()        :=
=20
Setting loop device flags
DEBUG   [U=3D0,P=3D3944]       loop-control.c:130:associate_loop()        :=
=20
Returning associate_loop(image_fp, loop_fp, 1) =3D 0
DEBUG   [U=3D0,P=3D3944]       file.c:255:fileput()                       :=
=20
Called=20
fileput(/tmp/.singularity-session-0.39.12911060245380037651/loop_dev,=20
/dev/loop0)
DEBUG   [U=3D0,P=3D3944]       sexec.c:375:main()                         :=
=20
Creating container image mount path: /usr/local/var/singularity/mnt
DEBUG   [U=3D0,P=3D3944]       sexec.c:441:main()                         :=
=20
Checking to see if we are joining an existing namespace
VERBOSE [U=3D0,P=3D3944]       sexec.c:444:main()                         :=
=20
Creating namespace process
DEBUG   [U=3D0,P=3D3944]       privilege.c:79:drop_privs()                :=
=20
Called drop_privs(struct s_privinfo *uinfo)
DEBUG   [U=3D0,P=3D3944]       privilege.c:87:drop_privs()                :=
=20
Dropping privileges to GID =3D '0'
DEBUG   [U=3D0,P=3D3944]       privilege.c:93:drop_privs()                :=
=20
Dropping privileges to UID =3D '0'
DEBUG   [U=3D0,P=3D3944]       privilege.c:103:drop_privs()               :=
=20
Confirming we have correct GID
DEBUG   [U=3D0,P=3D3944]       privilege.c:109:drop_privs()               :=
=20
Confirming we have correct UID
DEBUG   [U=3D0,P=3D3944]       privilege.c:115:drop_privs()               :=
=20
Returning drop_privs(struct s_privinfo *uinfo) =3D 0
DEBUG   [U=3D0,P=3D3949]       sexec.c:449:main()                         :=
=20
Hello from namespace child process
VERBOSE [U=3D0,P=3D3949]       sexec.c:461:main()                         :=
 Not=20
virtualizing PID namespace
DEBUG   [U=3D0,P=3D3949]       sexec.c:480:main()                         :=
=20
Virtualizing FS namespace
DEBUG   [U=3D0,P=3D3949]       sexec.c:488:main()                         :=
=20
Virtualizing mount namespace
DEBUG   [U=3D0,P=3D3949]       sexec.c:495:main()                         :=
=20
Making mounts private
DEBUG   [U=3D0,P=3D3949]       sexec.c:505:main()                         :=
=20
Mounting Singularity image file read/write
DEBUG   [U=3D0,P=3D3949]       mounts.c:48:mount_image()                  :=
=20
Called mount_image(/dev/loop0, /usr/local/var/singularity/mnt, 0)
DEBUG   [U=3D0,P=3D3949]       mounts.c:50:mount_image()                  :=
=20
Checking mount point is present
DEBUG   [U=3D0,P=3D3949]       mounts.c:56:mount_image()                  :=
=20
Checking loop is a block device
DEBUG   [U=3D0,P=3D3949]       mounts.c:75:mount_image()                  :=
=20
Trying to mount read only as ext4 with discard option
DEBUG   [U=3D0,P=3D3949]       mounts.c:88:mount_image()                  :=
=20
Returning mount_image(/dev/loop0, /usr/local/var/singularity/mnt, 0) =3D 0
DEBUG   [U=3D0,P=3D3949]       sexec.c:518:main()                         :=
=20
Checking if container has /bin/sh
DEBUG   [U=3D0,P=3D3949]       sexec.c:526:main()                         :=
=20
Checking to see if we should do bind mounts
DEBUG   [U=3D0,P=3D3949]       sexec.c:530:main()                         :=
=20
Checking configuration file for 'mount home'
DEBUG   [U=3D0,P=3D3949]       config_parser.c:69:config_get_key_bool()   :=
=20
Called config_get_key_bool(fp, mount home, 1)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, mount home)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, mount home) =3D yes
DEBUG   [U=3D0,P=3D3949]       config_parser.c:75:config_get_key_bool()   :=
=20
Return config_get_key_bool(fp, mount home, 1) =3D 1
VERBOSE [U=3D0,P=3D3949]       sexec.c:536:main()                         :=
=20
Mounting home directory base path: /root
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/root, 19992816, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/root, /usr/local/var/singularity/mnt//root, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/root, 19992816, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       sexec.c:551:main()                         :=
=20
Checking configuration file for 'bind path'
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D /etc/resolv.conf
VERBOSE [U=3D0,P=3D3949]       sexec.c:566:main()                         :=
=20
Found 'bind path' =3D /etc/resolv.conf, /etc/resolv.conf
VERBOSE [U=3D0,P=3D3949]       sexec.c:583:main()                         :=
=20
Binding '/etc/resolv.conf' to 'nasmpi-singularity.img:/etc/resolv.conf'
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/etc/resolv.conf, 19995920, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/etc/resolv.conf,=20
/usr/local/var/singularity/mnt//etc/resolv.conf, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/etc/resolv.conf, 19995920, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D /etc/hosts
VERBOSE [U=3D0,P=3D3949]       sexec.c:566:main()                         :=
=20
Found 'bind path' =3D /etc/hosts, /etc/hosts
VERBOSE [U=3D0,P=3D3949]       sexec.c:583:main()                         :=
=20
Binding '/etc/hosts' to 'nasmpi-singularity.img:/etc/hosts'
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/etc/hosts, 19998528, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/etc/hosts, /usr/local/var/singularity/mnt//etc/hosts, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/etc/hosts, 19998528, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D /dev
VERBOSE [U=3D0,P=3D3949]       sexec.c:566:main()                         :=
=20
Found 'bind path' =3D /dev, /dev
VERBOSE [U=3D0,P=3D3949]       sexec.c:583:main()                         :=
=20
Binding '/dev' to 'nasmpi-singularity.img:/dev'
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/dev, 20000832, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/dev, /usr/local/var/singularity/mnt//dev, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/dev, 20000832, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D /tmp
VERBOSE [U=3D0,P=3D3949]       sexec.c:566:main()                         :=
=20
Found 'bind path' =3D /tmp, /tmp
VERBOSE [U=3D0,P=3D3949]       sexec.c:583:main()                         :=
=20
Binding '/tmp' to 'nasmpi-singularity.img:/tmp'
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/tmp, 20003376, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/tmp, /usr/local/var/singularity/mnt//tmp, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/tmp, 20003376, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D /var/tmp
VERBOSE [U=3D0,P=3D3949]       sexec.c:566:main()                         :=
=20
Found 'bind path' =3D /var/tmp, /var/tmp
VERBOSE [U=3D0,P=3D3949]       sexec.c:583:main()                         :=
=20
Binding '/var/tmp' to 'nasmpi-singularity.img:/var/tmp'
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/var/tmp, 20005936, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/var/tmp, /usr/local/var/singularity/mnt//var/tmp, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/var/tmp, 20005936, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D /home
VERBOSE [U=3D0,P=3D3949]       sexec.c:566:main()                         :=
=20
Found 'bind path' =3D /home, /home
VERBOSE [U=3D0,P=3D3949]       sexec.c:583:main()                         :=
=20
Binding '/home' to 'nasmpi-singularity.img:/home'
DEBUG   [U=3D0,P=3D3949]       mounts.c:96:mount_bind()                   :=
=20
Called mount_bind(/home, 20008528, 1)
DEBUG   [U=3D0,P=3D3949]       mounts.c:98:mount_bind()                   :=
=20
Checking that source exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:104:mount_bind()                  :=
=20
Checking that destination exists and is a file or directory
DEBUG   [U=3D0,P=3D3949]       mounts.c:110:mount_bind()                  :=
=20
Calling mount(/home, /usr/local/var/singularity/mnt//home, ...)
DEBUG   [U=3D0,P=3D3949]       mounts.c:124:mount_bind()                  :=
=20
Returning mount_bind(/home, 20008528, 1) =3D 0
DEBUG   [U=3D0,P=3D3949]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, bind path)
DEBUG   [U=3D0,P=3D3949]       config_parser.c:61:config_get_key_value()  :=
=20
Return config_get_key_value(fp, bind path) =3D NULL
VERBOSE [U=3D0,P=3D3949]       sexec.c:633:main()                         :=
 Not=20
staging passwd or group (running as root)
VERBOSE [U=3D0,P=3D3949]       sexec.c:638:main()                         :=
=20
Forking exec process
DEBUG   [U=3D0,P=3D3949]       sexec.c:770:main()                         :=
=20
Dropping privs...
DEBUG   [U=3D0,P=3D3949]       privilege.c:79:drop_privs()                :=
=20
Called drop_privs(struct s_privinfo *uinfo)
DEBUG   [U=3D0,P=3D3949]       privilege.c:87:drop_privs()                :=
=20
Dropping privileges to GID =3D '0'
DEBUG   [U=3D0,P=3D3949]       privilege.c:93:drop_privs()                :=
=20
Dropping privileges to UID =3D '0'
DEBUG   [U=3D0,P=3D3949]       privilege.c:103:drop_privs()               :=
=20
Confirming we have correct GID
DEBUG   [U=3D0,P=3D3949]       privilege.c:109:drop_privs()               :=
=20
Confirming we have correct UID
DEBUG   [U=3D0,P=3D3949]       privilege.c:115:drop_privs()               :=
=20
Returning drop_privs(struct s_privinfo *uinfo) =3D 0
VERBOSE [U=3D0,P=3D3949]       sexec.c:776:main()                         :=
=20
Waiting for Exec process...
DEBUG   [U=3D0,P=3D3959]       sexec.c:642:main()                         :=
=20
Hello from exec child process
VERBOSE [U=3D0,P=3D3959]       sexec.c:644:main()                         :=
=20
Entering container file system space
DEBUG   [U=3D0,P=3D3959]       sexec.c:649:main()                         :=
=20
Changing dir to '/' within the new root
DEBUG   [U=3D0,P=3D3959]       sexec.c:657:main()                         :=
=20
Checking configuration file for 'mount proc'
DEBUG   [U=3D0,P=3D3959]       config_parser.c:69:config_get_key_bool()   :=
=20
Called config_get_key_bool(fp, mount proc, 1)
DEBUG   [U=3D0,P=3D3959]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, mount proc)
DEBUG   [U=3D0,P=3D3959]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, mount proc) =3D yes
DEBUG   [U=3D0,P=3D3959]       config_parser.c:75:config_get_key_bool()   :=
=20
Return config_get_key_bool(fp, mount proc, 1) =3D 1
VERBOSE [U=3D0,P=3D3959]       sexec.c:661:main()                         :=
=20
Mounting /proc
DEBUG   [U=3D0,P=3D3959]       sexec.c:674:main()                         :=
=20
Checking configuration file for 'mount sys'
DEBUG   [U=3D0,P=3D3959]       config_parser.c:69:config_get_key_bool()   :=
=20
Called config_get_key_bool(fp, mount sys, 1)
DEBUG   [U=3D0,P=3D3959]       config_parser.c:43:config_get_key_value()  :=
=20
Called config_get_key_value(fp, mount sys)
DEBUG   [U=3D0,P=3D3959]       config_parser.c:54:config_get_key_value()  :=
=20
Return config_get_key_value(fp, mount sys) =3D yes
DEBUG   [U=3D0,P=3D3959]       config_parser.c:75:config_get_key_bool()   :=
=20
Return config_get_key_bool(fp, mount sys, 1) =3D 1
VERBOSE [U=3D0,P=3D3959]       sexec.c:678:main()                         :=
=20
Mounting /sys
VERBOSE [U=3D0,P=3D3959]       sexec.c:692:main()                         :=
=20
Dropping all privileges
DEBUG   [U=3D0,P=3D3959]       privilege.c:121:drop_privs_perm()          :=
=20
Called drop_privs_perm(struct s_privinfo *uinfo)
DEBUG   [U=3D0,P=3D3959]       privilege.c:129:drop_privs_perm()          :=
=20
Resetting supplementary groups
DEBUG   [U=3D0,P=3D3959]       privilege.c:135:drop_privs_perm()          :=
=20
Dropping real and effective privileges to GID =3D '0'
DEBUG   [U=3D0,P=3D3959]       privilege.c:141:drop_privs_perm()          :=
=20
Dropping real and effective privileges to UID =3D '0'
DEBUG   [U=3D0,P=3D3959]       privilege.c:151:drop_privs_perm()          :=
=20
Confirming we have correct GID
DEBUG   [U=3D0,P=3D3959]       privilege.c:157:drop_privs_perm()          :=
=20
Confirming we have correct UID
DEBUG   [U=3D0,P=3D3959]       privilege.c:163:drop_privs_perm()          :=
=20
Returning drop_privs_perm(struct s_privinfo *uinfo) =3D 0
VERBOSE [U=3D0,P=3D3959]       sexec.c:699:main()                         :=
=20
Changing to correct working directory: /tmp/result
DEBUG   [U=3D0,P=3D3959]       sexec.c:713:main()                         :=
=20
Setting environment variable 'SINGULARITY_CONTAINER=3D1'
VERBOSE [U=3D0,P=3D3959]       sexec.c:732:main()                         :=
=20
COMMAND=3Dexec
DEBUG   [U=3D0,P=3D3959]       container_actions.c:59:container_exec()    :=
=20
Called container_exec(2, **argv)
VERBOSE [U=3D0,P=3D3959]       container_actions.c:65:container_exec()    :=
=20
Exec'ing program: true
VERBOSE [U=3D0,P=3D3949]       sexec.c:785:main()                         :=
=20
Exec parent process returned: 0
VERBOSE [U=3D0,P=3D3944]       sexec.c:804:main()                         :=
=20
Starting cleanup...
DEBUG   [U=3D0,P=3D3944]       sexec.c:955:main()                         :=
=20
Checking to see if we are the last process running in this sessiondir
DEBUG   [U=3D0,P=3D3944]       sexec.c:959:main()                         :=
=20
Escalating privs to clean session directory
DEBUG   [U=3D0,P=3D3944]       privilege.c:61:escalate_privs()            :=
=20
Called escalate_privs(void)
DEBUG   [U=3D0,P=3D3944]       privilege.c:73:escalate_privs()            :=
=20
Returning escalate_privs(void) =3D 0
VERBOSE [U=3D0,P=3D3944]       sexec.c:964:main()                         :=
=20
Cleaning sessiondir: /tmp/.singularity-session-0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       file.c:212:s_rmdir()                       :=
=20
Removing dirctory: /tmp/.singularity-session-0.39.12911060245380037651
DEBUG   [U=3D0,P=3D3944]       loop-control.c:138:disassociate_loop()     :=
=20
Called disassociate_loop(loop_fp)
VERBOSE [U=3D0,P=3D3944]       loop-control.c:140:disassociate_loop()     :=
=20
Disassociating image from loop device
DEBUG   [U=3D0,P=3D3944]       loop-control.c:146:disassociate_loop()     :=
=20
Returning disassociate_loop(loop_fp) =3D 0
DEBUG   [U=3D0,P=3D3944]       privilege.c:79:drop_privs()                :=
=20
Called drop_privs(struct s_privinfo *uinfo)
DEBUG   [U=3D0,P=3D3944]       privilege.c:87:drop_privs()                :=
=20
Dropping privileges to GID =3D '0'
DEBUG   [U=3D0,P=3D3944]       privilege.c:93:drop_privs()                :=
=20
Dropping privileges to UID =3D '0'
DEBUG   [U=3D0,P=3D3944]       privilege.c:103:drop_privs()               :=
=20
Confirming we have correct GID
DEBUG   [U=3D0,P=3D3944]       privilege.c:109:drop_privs()               :=
=20
Confirming we have correct UID
DEBUG   [U=3D0,P=3D3944]       privilege.c:115:drop_privs()               :=
=20
Returning drop_privs(struct s_privinfo *uinfo) =3D 0
VERBOSE [U=3D0,P=3D3944]       sexec.c:981:main()                         :=
=20
Cleaning up...

Thanks,

El mi=C3=A9rcoles, 6 de julio de 2016, 16:00:36 (UTC+2), Gregory M. Kurtzer=
=20
escribi=C3=B3:
>
> Hi,=20
>
> /run/user is associated with the Singularity container?
>
> Can you show me the output of 'losetup -a' please?
>
> Why are you are running it with sudo, you should not need to.
>
> It is weird, isn't -n a synonym for -np and if so, shouldn't it executing=
=20
> 1 process on the given node? It seems like it is doing more.=20
>
> Lastly, what version of Singularity is this? If from Git master when did=
=20
> you do the last pull? Can you try this in debug mode and with a=20
> simple binary for testing:
>
> $ mpirun -n 1 singularity -d exec /mnt/glusterfs/singularity/nasmpi-1.img=
=20
> true
>
> And send that output please.=20
>
>
> On Wednesday, July 6, 2016, Raimon Bosch <rai...@gmail.com=20
> <javascript:>> wrote:
>
>>
>> When I do "df -h" I see the singularity container still mounted. Maybe I=
=20
>> need to run a command to unmount it:
>>
>> > df -h
>> Filesystem                 Size  Used Avail Use% Mounted on
>> ****
>> tmpfs                      3.2G     0  3.2G   0% /run/user/1006
>> ****
>>
>> El mi=C3=A9rcoles, 6 de julio de 2016, 10:25:24 (UTC+2), Raimon Bosch es=
cribi=C3=B3:
>>>
>>>
>>> Hi Gregory,
>>>
>>> It fails depending on your environment. In my Ubuntu 14.04 it worked=20
>>> fine, but in this instance of Debian jessie I get the following:
>>>
>>> > ERROR: Failed to associate image to loop: Device or resource busy
>>>
>>> Maybe is because we are using a glusterfs shared disk to keep the=20
>>> containers?
>>>
>>> Here you have the entire output:
>>>
>>> > sudo mpirun -n 1 singularity exec=20
>>> /mnt/glusterfs/singularity/nasmpi-1.img /trace.sh=20
>>> /NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec=20
>>> /mnt/glusterfs/singularity/nasmpi-2.img /trace.sh=20
>>> /NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec=20
>>> /mnt/glusterfs/singularity/nasmpi-3.img /trace.sh=20
>>> /NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec=20
>>> /mnt/glusterfs/singularity/nasmpi-4.img /trace.sh /NPB/NPB3.3-MPI/bin/b=
t.C.4
>>> ERROR: Failed to associate image to loop: Device or resource busy
>>> ERROR: Failed to associate image to loop: Device or resource busy
>>> /bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-=
8)
>>> ERROR: Failed to associate image to loop: Device or resource busy
>>>
>>> -----------------------------------------------------------------------=
---
>>> mpirun has exited due to process rank 2 with PID 63416 on
>>> node bscgrid30 exiting improperly. There are two reasons this could=20
>>> occur:
>>>
>>> 1. this process did not call "init" before exiting, but others in
>>> the job did. This can cause a job to hang indefinitely while it waits
>>> for all processes to call "init". By rule, if one process calls "init",
>>> then ALL processes must call "init" prior to termination.
>>>
>>> 2. this process called "init", but exited without calling "finalize".
>>> By rule, all processes that call "init" MUST call "finalize" prior to
>>> exiting or it will be considered an "abnormal termination"
>>>
>>> This may have caused other processes in the application to be
>>> terminated by signals sent by mpirun (as reported here).
>>>
>>> -----------------------------------------------------------------------=
---
>>>
>>> Thanks in advance,
>>>
>>> El martes, 5 de julio de 2016, 18:21:48 (UTC+2), Gregory M. Kurtzer=20
>>> escribi=C3=B3:
>>>>
>>>> Hi Raimon,
>>>>
>>>> I am confused as to what the issue is that you are having. Singularity=
=20
>>>> supports running both across nodes as well as multiple processes per n=
ode=20
>>>> in any number of containers. Can you paste your command and the error =
you=20
>>>> are getting, maybe that will help.
>>>>
>>>> Thanks!
>>>>
>>>>
>>>>
>>>> On Tue, Jul 5, 2016 at 8:25 AM, Raimon Bosch <rai...@gmail.com>=20
>>>> wrote:
>>>>
>>>>>
>>>>> That solution does not work with nas/mpi benchmark. That's because=20
>>>>> bt.C.16 expects 16 processes. When you split processes it throws an=
=20
>>>>> exception because number of processes is lower than 16.=20
>>>>>
>>>>> I am still trying to figure out how to do this. Let me know if you=20
>>>>> have any suggestion.
>>>>>
>>>>> Cheers,
>>>>>
>>>>> El jueves, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Castain=20
>>>>> escribi=C3=B3:
>>>>>>
>>>>>> I think you are misunderstanding the basic nature of the Singularity=
=20
>>>>>> =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overlay=
. So =E2=80=9Csharing=E2=80=9D a container is=20
>>>>>> no different than running on a node where the procs all see the same=
 file=20
>>>>>> system. Thus, having multiple containers that are identical makes no=
 sense=20
>>>>>> - it=E2=80=99s all the same file system.
>>>>>>
>>>>>> Now if you want to run different containers (e.g., with different=20
>>>>>> libraries or OS in them), then you would use mpirun=E2=80=99s MPMD s=
yntax - for=20
>>>>>> example:
>>>>>>
>>>>>> mpirun -n 1 <container1> : -n 1 <container2>
>>>>>>
>>>>>> HTH
>>>>>> Ralph
>>>>>>
>>>>>> On Jun 23, 2016, at 1:53 AM, Raimon Bosch <rai...@gmail.com>=20
>>>>>> wrote:
>>>>>>
>>>>>>
>>>>>> One last question: What if I want to execute more than one container=
=20
>>>>>> in the same host? With this technique I am bounded always to the sam=
e=20
>>>>>> container. One of our experiments was based in measuring performance=
 of=20
>>>>>> several containers working in parallel in the same node. Also we had=
=20
>>>>>> experiments with N containers per host in a multihost environment.
>>>>>>
>>>>>> El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M.=
=20
>>>>>> Kurtzer escribi=C3=B3:
>>>>>>>
>>>>>>> Hi Raimon,
>>>>>>>
>>>>>>> Sorry I wasn't clear. I am not yet at my computer and thinking whil=
e=20
>>>>>>> typing on an iPhone hinders my mental processes. Lol
>>>>>>>
>>>>>>> If I understand your example properly, you have a docker or VM=20
>>>>>>> infrastructure already set up and you are invoking the mpirun comma=
nds from=20
>>>>>>> within the virtual environment. Singularity works on a very differe=
nt=20
>>>>>>> premis because integrating a virtual cluster into an existing clust=
er and=20
>>>>>>> scheduling system is a mess.
>>>>>>>
>>>>>>> So starting from the physical nodes, which already have access to=
=20
>>>>>>> all other nodes in the cluster, and already scheduled properly, and=
 in=20
>>>>>>> direct access to optimized hardware and file systems.... You call m=
pirun.=20
>>>>>>>
>>>>>>> The mpirun command will take the standard format as you illustrated=
=20
>>>>>>> with the following change to call Singularity inline:
>>>>>>>
>>>>>>> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.im=
g=20
>>>>>>> trace.sh bt.C.4
>>>>>>>
>>>>>>> This assumes the following:
>>>>>>>
>>>>>>> 1. The container image which contains the program's you want to run=
=20
>>>>>>> is at ~/container.img and accessible at this path on all nodes refe=
renced=20
>>>>>>> in hosts.txt
>>>>>>> 2. The hosts.txt references other physical nodes you want to run on
>>>>>>> 3. The executables trace.sh and bt.C.4 are both inside the containe=
r=20
>>>>>>> and in a standard path
>>>>>>>
>>>>>>> In this case we are performing one execution and MPI + singularity=
=20
>>>>>>> is managing all of the communication between processes, nodes and=
=20
>>>>>>> containers. Also it is now using any optimized hardware (eg.=20
>>>>>>> Infiniband) and existing high performance file systems (which shoul=
d not be=20
>>>>>>> accessible via a virtualized or Docker'ized cluster for security re=
asons).
>>>>>>>
>>>>>>> This way is actually MUCH simpler then what you are proposing=20
>>>>>>> because there is no need to manage any virtual nodes, virtual netwo=
rks, or=20
>>>>>>> resource manager hacks. It really is as easy as just running any ot=
her=20
>>>>>>> MPI process on an existing cluster.=20
>>>>>>>
>>>>>>> Hope that helps better!
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com>=20
>>>>>>> wrote:
>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Hi Gregory,
>>>>>>>>
>>>>>>>> I'm not sure if I would achieve the same with your commands. In an=
=20
>>>>>>>> environment based on dockers or virtual machines we would do somet=
hing like=20
>>>>>>>> this [non applicable to Singularity]:
>>>>>>>>
>>>>>>>> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh=
=20
>>>>>>>> ./bt.C.4
>>>>>>>>
>>>>>>>> where hosts.txt* is:
>>>>>>>>
>>>>>>>> >vm-ip-01-on-*host01* slots=3D2
>>>>>>>> >vm-ip-01-on-*host02* slots=3D2
>>>>>>>>
>>>>>>>> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>>>>>>>>
>>>>>>>> and trace.sh is:
>>>>>>>>
>>>>>>>> >#!/bin/bash
>>>>>>>> >
>>>>>>>> >export EXTRAE_HOME=3D/opt/extrae/
>>>>>>>> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
>>>>>>>> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>>>>>>>> >
>>>>>>>> >## Run the desired program
>>>>>>>> >$*
>>>>>>>>
>>>>>>>> As you see we only perform one execution and OpenMPI transparently=
=20
>>>>>>>> manages communication between containers or virtual machines. This=
 command=20
>>>>>>>> would work well rather VMs are on the same host or not.
>>>>>>>>
>>>>>>>> What I understand from your response is that now we should execute=
=20
>>>>>>>> OpenMPI on each host and then merge results manually. I don't know=
 yet how=20
>>>>>>>> to do this merge step or if it is any way to centralize everything=
 like I=20
>>>>>>>> would do with VMs.
>>>>>>>>
>>>>>>>> Thanks in advance,
>>>>>>>>
>>>>>>>> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory =
M.=20
>>>>>>>> Kurtzer escribi=C3=B3:
>>>>>>>>>
>>>>>>>>> Hi Raimon,
>>>>>>>>>
>>>>>>>>> The quick answer is you have mpirun handle that as you would=20
>>>>>>>>> normally where the container file lives on a shared file system:
>>>>>>>>>
>>>>>>>>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>>>>>>>>
>>>>>>>>> Let the MPI outside the container launch the singularity containe=
r=20
>>>>>>>>> on each host as it would normally launch any MPI program. Then it=
 will call=20
>>>>>>>>> Singulairty and Singularity will launch the MPI program inside th=
e=20
>>>>>>>>> container on each of your hosts/servers.=20
>>>>>>>>>
>>>>>>>>> Hope that helps!
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com>=20
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Hi Gregory,
>>>>>>>>>>
>>>>>>>>>> Thank you for your answer. One of our experiments needs to run=
=20
>>>>>>>>>> OpenMPI among several servers. This means that we should put one=
 of our=20
>>>>>>>>>> containers in host01, another in host02 and another in host03 an=
d collect=20
>>>>>>>>>> the results.=20
>>>>>>>>>>
>>>>>>>>>> How can I do this execution in parallel if I need to communicate=
=20
>>>>>>>>>> with more than one server?
>>>>>>>>>>
>>>>>>>>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M.=20
>>>>>>>>>> Kurtzer escribi=C3=B3:
>>>>>>>>>>>
>>>>>>>>>>> Hi Raimon,
>>>>>>>>>>>
>>>>>>>>>>> The communication model of a Singularity container is very=20
>>>>>>>>>>> different from that of a Docker implementation. This is because=
 Docker for=20
>>>>>>>>>>> all practical purposes emulates a virtual machine as each conta=
iner has=20
>>>>>>>>>>> it's own IP address and thus it's own ssh server. It also carri=
es its own=20
>>>>>>>>>>> set of complexities, for example networks need to be segregated=
/VLan'ed,=20
>>>>>>>>>>> DNS/host resolution needs to be dynamic and passed down to the =
containers=20
>>>>>>>>>>> (so they can reach each other), ssh daemons and other process r=
unning=20
>>>>>>>>>>> inside the containers, management via an existing scheduling sy=
stem, and=20
>>>>>>>>>>> the list goes on and on.
>>>>>>>>>>>
>>>>>>>>>>> Think of it this way, Singularity does not do any of that... It=
=20
>>>>>>>>>>> runs a program within the container as if it were running on th=
e host=20
>>>>>>>>>>> itself, so to communicate between containers is as easy as comm=
unicating=20
>>>>>>>>>>> between programs. So for MPI, it would happen with the MPI on t=
he physical=20
>>>>>>>>>>> host (outside the container) invoking the container subsystem w=
hich then=20
>>>>>>>>>>> invokes the MPI programs within the container and the MPI progr=
ams within=20
>>>>>>>>>>> the container communicate back to the MPI (orted) outside the c=
ontainer on=20
>>>>>>>>>>> the host to get access to the host resources. In this model all=
 available=20
>>>>>>>>>>> resources and infrastructure can be leveraged at full bandwidth=
 by the=20
>>>>>>>>>>> contained processes and all of the aforementioned complexities =
akin to=20
>>>>>>>>>>> running on a virtualized mini-cluster are circumvented.
>>>>>>>>>>>
>>>>>>>>>>> There is additional information I have written at:
>>>>>>>>>>>
>>>>>>>>>>> http://singularity.lbl.gov/#hpc
>>>>>>>>>>>
>>>>>>>>>>> That page is still coming along, and needs more information=20
>>>>>>>>>>> still but if you have any questions, comments or change proposa=
ls please=20
>>>>>>>>>>> let us know!
>>>>>>>>>>>
>>>>>>>>>>> Thanks and hope that helps!
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <
>>>>>>>>>>> rai...@gmail.com> wrote:
>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> Hi,
>>>>>>>>>>>>
>>>>>>>>>>>> We are trying to run experiments using singularity containers.=
=20
>>>>>>>>>>>> The idea is to run OpenMPI among several containers and check =
performance=20
>>>>>>>>>>>> results.=20
>>>>>>>>>>>>
>>>>>>>>>>>> How can I communicate with another container? In docker this i=
s=20
>>>>>>>>>>>> clear because every container gets an assigned IP and you can =
ping there,=20
>>>>>>>>>>>> but what is the situation in the case of singularity? Is it po=
ssible to=20
>>>>>>>>>>>> assign an IP to each container? Can I connect via ssh to them?
>>>>>>>>>>>>
>>>>>>>>>>>> Thanks in advance,
>>>>>>>>>>>>
>>>>>>>>>>>> --=20
>>>>>>>>>>>> You received this message because you are subscribed to the=20
>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from=
=20
>>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> --=20
>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>>> University of California
>>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> --=20
>>>>>>>>>> You received this message because you are subscribed to the=20
>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>> To unsubscribe from this group and stop receiving emails from it=
,=20
>>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --=20
>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>> University of California
>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>
>>>>>>>>>
>>>>>>>> --=20
>>>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,=
=20
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --=20
>>>>>>> Gregory M. Kurtzer
>>>>>>> High Performance Computing Services (HPCS)
>>>>>>> University of California
>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>
>>>>>>>
>>>>>> --=20
>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,=20
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>>
>>>>>> --=20
>>>>> You received this message because you are subscribed to the Google=20
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d=20
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --=20
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>> --=20
>> You received this message because you are subscribed to the Google Group=
s=20
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n=20
>> email to singu...@lbl.gov.
>>
>
>
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>
>
------=_Part_406_2011422175.1467814836722
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br>Hi Gregory,<br><br>&gt; /run/user is associated with t=
he Singularity container?<br><br>I guess it is. Because containers are 3G s=
ize and it matches with this instances on /run/user/**. Unmounting them did=
 not help.<br><br>&gt; Can you show me the output of &#39;losetup -a&#39; p=
lease?<br><br>&quot;sudo losetup -a&quot; returns empty<br><br>&gt; Why are=
=C2=A0<span></span>you are running it with sudo, you should not need to.<br=
><br>I execute with sudo because the container inside needs &#39;root&#39;.=
 This is an old docker container that only has a unique user root with all =
the files (probably i should change this in the future).<br><br>&gt; It is =
weird, isn&#39;t -n a synonym for -np and if so, shouldn&#39;t it=20
executing 1 process on the given node? It seems like it is doing more. <br>=
<br>In my local machine the behaviour is correct. Tested it with -np and th=
e behaviour is the same.<br><br>&gt; Lastly, what version of Singularity is=
 this?<br><br>Is the master. I did &quot;git clone https://github.com/gmkur=
tzer/singularity.git&quot; and followed the installation steps.<br><br>As a=
 side comment, If I deploy with a unique container I don&#39;t encounter th=
is problem. I think that when I want to mount extra containers that the SO =
gets crazy or maybe singularity tries to assign containers to a /dev/loop* =
that is busy and does not try to look for one that is available. In my fina=
l test I will need at least 16 containers in one host. Is that possible wit=
h singularity because I only see 8 loops?<br><br>Here you have the debug ou=
tput:<br><br>&gt; sudo mpirun -n 1 singularity -d exec /mnt/glusterfs/singu=
larity/nasmpi-singularity.img true<br>enabling debugging<br>ending argument=
 loop<br>+ &#39;[&#39; -f /usr/local/etc/singularity/init &#39;]&#39;<br>+ =
. /usr/local/etc/singularity/init<br>++ unset module<br>++ PATH=3D/bin:/sbi=
n:/usr/bin:/usr/sbin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbi=
n:/bin<br>++ HISTFILE=3D/dev/null<br>++ export PATH HISTFILE<br>++ &#39;[&#=
39; -n 1 &#39;]&#39;<br>++ SINGULARITY_NO_NAMESPACE_PID=3D1<br>++ export SI=
NGULARITY_NO_NAMESPACE_PID<br>+ true<br>+ case $1 in<br>+ break<br>+ &#39;[=
&#39; -z /mnt/glusterfs/singularity/nasmpi-singularity.img &#39;]&#39;<br>+=
 SINGULARITY_IMAGE=3D/mnt/glusterfs/singularity/nasmpi-singularity.img<br>+=
 export SINGULARITY_IMAGE<br>+ shift<br>+ exec /usr/local/libexec/singulari=
ty/sexec true<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 message.c:46:init()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Setting messagelevel to: 5<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3=
944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:127:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Gathering and cach=
ing user info.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 privilege.c:43:get_user_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called get_user_privs(struct s_pr=
ivinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 privilege.c:54:get_user_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning get_user_privs(struct s=
_privinfo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:134:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we can escalate privs=
 properly.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 privilege.c:61:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called escalate_privs(void)<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:=
73:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Returning escalate_privs(void) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:141:main()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting p=
rivs to calling user<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 privilege.c:79:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called=
 drop_privs(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_privs()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Dropping privileges to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:93:drop_priv=
s()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Dropping privileges to UID =3D &#39;0&#39;<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:=
103:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct GID<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:109=
:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:115:dr=
op_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Returning drop_privs(struct s_privinfo *uinfo) =3D =
0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 sexec.c:146:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Obtaining user&#39;s homedir<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:150:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining fi=
le descriptor to current directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:155:main()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Getting current working =
directory path string<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:161:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_COMMAND from e=
nvironment<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 sexec.c:168:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_IMAGE from environment<br>=
DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexe=
c.c:174:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Checking container image is a file: /mnt/glusterfs/singularity/=
nasmpi-singularity.img<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:180:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Building configuration file location=
<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
sexec.c:183:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Config location: /usr/local/etc/singularity/singularity.conf=
<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
sexec.c:185:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Checking Singularity configuration is a file: /usr/local/etc=
/singularity/singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:191:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity config=
uration file is owned by root<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:197:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening Singularity configu=
ration file<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:210:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity configuration for &#39;s=
essiondir prefix&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0 : Cal=
led config_get_key_value(fp, sessiondir prefix)<br>DEBUG=C2=A0=C2=A0 [U=3D0=
,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:61:config_ge=
t_key_value()=C2=A0 : Return config_get_key_value(fp, sessiondir prefix) =
=3D NULL<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 file.c:48:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Called file_id(/mnt/glusterfs/singularity/nasmpi-singu=
larity.img)<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 file.c:58:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Generated file_id: 0.39.12911060245380037651<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:60:file_id(=
)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Return=
ing file_id(/mnt/glusterfs/singularity/nasmpi-singularity.img) =3D 0.39.129=
11060245380037651<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:217:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set sessiondir to: /tmp/.singularity-se=
ssion-0.39.12911060245380037651<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:221:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set containername to: nasmp=
i-singularity.img<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:223:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting loop_dev_* paths<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:229:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Set image mount path to: /usr/local/var/singularity/mnt<br>LOG=C2=A0=C2=A0=
=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:2=
31:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Command=3Dexec, Container=3D/mnt/glusterfs/singularity/nasmpi-singula=
rity.img, CWD=3D/tmp/result, Arg1=3Dtrue<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D39=
44]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:236:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set prompt to: Sin=
gularity/nasmpi-singularity.img&gt; <br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:238:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we are op=
ening image as read/write<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:240:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening image as read only: /mnt/=
glusterfs/singularity/nasmpi-singularity.img<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:247:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting shared =
lock on file descriptor: 6<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:267:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking for namespace daemon =
pidfile<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:301:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Escalating privledges<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:61:escalate_privs(=
)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Calle=
d escalate_privs(void)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 privilege.c:73:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning escalate_privs(vo=
id) =3D 0<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 s=
exec.c:306:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Creating/Verifying session directory: /tmp/.singularity-sess=
ion-0.39.12911060245380037651<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:196:s_mkpath()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Creating directory: /tmp/.singularity-sess=
ion-0.39.12911060245380037651<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:320:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting shared lock on sess=
ion directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:331:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Caching info into sessiondir<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:255:file=
put()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called fi=
leput(/tmp/.singularity-session-0.39.12911060245380037651/image, nasmpi-sin=
gularity.img)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:337:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking for set loop device<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:=
52:obtain_loop_dev()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called obt=
ain_loop_dev(void)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 loop-control.c:66:obtain_loop_dev()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Found available existing loop device number: 0<br>V=
ERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:=
81:obtain_loop_dev()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Using loop=
 device: /dev/loop0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 loop-control.c:95:obtain_loop_dev()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Returning obtain_loop_dev(void) =3D /dev/loop0<br>D=
EBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-=
control.c:106:associate_loop()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Called associate_loop(image_fp, loop_fp, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:109:associate_l=
oop()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting loop flags to LO_=
FLAGS_AUTOCLEAR<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 image.c:39:image_offset()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Calculating =
image offset<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 image.c:48:image_offset()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Found image at =
an offset of 31 bytes<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 image.c:53:image_offset()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Returning image_offset(image_fp) =3D 31<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:114:associate_loop=
()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting image offset to: 31<=
br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-contro=
l.c:116:associate_loop()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Associ=
ating image to loop device<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 loop-control.c:122:associate_loop()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Setting loop device flags<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:130:associate_=
loop()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning associate_loop=
(image_fp, loop_fp, 1) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:255:fileput()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called fileput(/tmp/.singularity-session-0=
.39.12911060245380037651/loop_dev, /dev/loop0)<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:375:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Creating con=
tainer image mount path: /usr/local/var/singularity/mnt<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:441:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Che=
cking to see if we are joining an existing namespace<br>VERBOSE [U=3D0,P=3D=
3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:444:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Creating namespace=
 process<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 privilege.c:79:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called drop_privs(=
struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_privs()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dro=
pping privileges to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D394=
4]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:93:drop_privs()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Dropping privileges to UID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:103:drop_pri=
vs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Confirming we have correct GID<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:109:drop_privs(=
)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:115:drop_privs()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Returning drop_privs(struct s_privinfo *uinfo) =3D 0<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:4=
49:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Hello from namespace child process<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:461:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Not virtualizing PID namesp=
ace<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:480:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Virtualizing FS namespace<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:488:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Virtualizing=
 mount namespace<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:495:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Making mounts private<br>DEBUG=C2=A0=C2=A0=
 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:505:main()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Mounti=
ng Singularity image file read/write<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:48:mount_image()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Called mount_image(/dev/loop0, /usr/local/var/singularity=
/mnt, 0)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 mounts.c:50:mount_image()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking =
mount point is present<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 mounts.c:56:mount_image()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Checking loop is a block device<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:75:mount_image()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Trying to mount read only as ext4 with discard option<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.=
c:88:mount_image()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning mount_image(/dev/=
loop0, /usr/local/var/singularity/mnt, 0) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0=
,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:518:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if =
container has /bin/sh<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:526:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking to see if we should do bind=
 mounts<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:530:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Checking configuration file for &#39;mount home&#39=
;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 config_parser.c:69:config_get_key_bool()=C2=A0=C2=A0 : Called config_get_k=
ey_bool(fp, mount home, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0=
 : Called config_get_key_value(fp, mount home)<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get=
_key_value()=C2=A0 : Return config_get_key_value(fp, mount home) =3D yes<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 con=
fig_parser.c:75:config_get_key_bool()=C2=A0=C2=A0 : Return config_get_key_b=
ool(fp, mount home, 1) =3D 1<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:536:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Mounting home directory base path: /roo=
t<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 mounts.c:96:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called mount=
_bind(/root, 19992816, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:98:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Checking that source exists and is a file or directory<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:=
104:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking that destination e=
xists and is a file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:110:mount_bind()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Calling mount(/root, /usr/local/var/singularity/mnt//root, .=
..)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 mounts.c:124:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning mount=
_bind(/root, 19992816, 1) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:551:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking configuration file=
 for &#39;bind path&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0 : =
Called config_get_key_value(fp, bind path)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get_key=
_value()=C2=A0 : Return config_get_key_value(fp, bind path) =3D /etc/resolv=
.conf<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec=
.c:566:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Found &#39;bind path&#39; =3D /etc/resolv.conf, /etc/resolv.con=
f<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:5=
83:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Binding &#39;/etc/resolv.conf&#39; to &#39;nasmpi-singularity.img:/et=
c/resolv.conf&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 mounts.c:96:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Called mount_bind(/etc/resolv.conf, 19995920, 1)<br>DEBUG=C2=A0=C2=A0=
 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:98:mount_bin=
d()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking that source exists and is a=
 file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 mounts.c:104:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Checking that destination exists and is a file or directory<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:110:mo=
unt_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Calling mount(/etc/resolv.conf, /=
usr/local/var/singularity/mnt//etc/resolv.conf, ...)<br>DEBUG=C2=A0=C2=A0 [=
U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:124:mount_bind=
()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning mount_bind(/etc/resolv.conf, 199=
95920, 1) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0 : Called co=
nfig_get_key_value(fp, bind path)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get_key_value()=
=C2=A0 : Return config_get_key_value(fp, bind path) =3D /etc/hosts<br>VERBO=
SE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:566:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Fou=
nd &#39;bind path&#39; =3D /etc/hosts, /etc/hosts<br>VERBOSE [U=3D0,P=3D394=
9]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:583:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Binding &#39;/etc/hos=
ts&#39; to &#39;nasmpi-singularity.img:/etc/hosts&#39;<br>DEBUG=C2=A0=C2=A0=
 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:96:mount_bin=
d()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called mount_bind(/etc/hosts, 199985=
28, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 mounts.c:98:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checki=
ng that source exists and is a file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:104:mount_bind()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking that destination exists and is a fil=
e or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 mounts.c:110:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Cal=
ling mount(/etc/hosts, /usr/local/var/singularity/mnt//etc/hosts, ...)<br>D=
EBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mount=
s.c:124:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning mount_bind(/et=
c/hosts, 19998528, 1) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0=
 : Called config_get_key_value(fp, bind path)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get_=
key_value()=C2=A0 : Return config_get_key_value(fp, bind path) =3D /dev<br>=
VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:566:ma=
in()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Found &#39;bind path&#39; =3D /dev, /dev<br>VERBOSE [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:583:main()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Binding &#39;/dev&#39; t=
o &#39;nasmpi-singularity.img:/dev&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D394=
9]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:96:mount_bind()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Called mount_bind(/dev, 20000832, 1)<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:98:mou=
nt_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking that source exists an=
d is a file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:104:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Checking that destination exists and is a file or directory<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:=
110:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Calling mount(/dev, /usr/lo=
cal/var/singularity/mnt//dev, ...)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:124:mount_bind()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Returning mount_bind(/dev, 20000832, 1) =3D 0<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c=
:43:config_get_key_value()=C2=A0 : Called config_get_key_value(fp, bind pat=
h)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 config_parser.c:54:config_get_key_value()=C2=A0 : Return config_get_key=
_value(fp, bind path) =3D /tmp<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:566:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Found &#39;bind path&#39; =3D /tmp, =
/tmp<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.=
c:583:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Binding &#39;/tmp&#39; to &#39;nasmpi-singularity.img:/tmp&#39;<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mou=
nts.c:96:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called mount_bind(=
/tmp, 20003376, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 mounts.c:98:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Checking that source exists and is a file or directory<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:104:mo=
unt_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking that destination exists =
and is a file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:110:mount_bind()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Calling mount(/tmp, /usr/local/var/singularity/mnt//tmp, ...)<br>D=
EBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mount=
s.c:124:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning mount_bind(/tm=
p, 20003376, 1) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0 : Cal=
led config_get_key_value(fp, bind path)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D394=
9]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get_key_va=
lue()=C2=A0 : Return config_get_key_value(fp, bind path) =3D /var/tmp<br>VE=
RBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:566:main=
()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Found &#39;bind path&#39; =3D /var/tmp, /var/tmp<br>VERBOSE [U=3D0,P=3D3949=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:583:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Binding &#39;/var/tmp=
&#39; to &#39;nasmpi-singularity.img:/var/tmp&#39;<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:96:mount_bind()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called mount_bind(/var/tmp, 20005936, 1=
)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 mounts.c:98:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking tha=
t source exists and is a file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:104:mount_bind()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Checking that destination exists and is a file or d=
irectory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 mounts.c:110:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Calling m=
ount(/var/tmp, /usr/local/var/singularity/mnt//var/tmp, ...)<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:124:mo=
unt_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning mount_bind(/var/tmp, 20=
005936, 1) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0 : Called co=
nfig_get_key_value(fp, bind path)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get_key_value()=
=C2=A0 : Return config_get_key_value(fp, bind path) =3D /home<br>VERBOSE [U=
=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:566:main()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Found &#3=
9;bind path&#39; =3D /home, /home<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:583:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Binding &#39;/home&#39; to &#39;n=
asmpi-singularity.img:/home&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:96:mount_bind()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Called mount_bind(/home, 20008528, 1)<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:98:mou=
nt_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking that source exists an=
d is a file or directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:104:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Checking that destination exists and is a file or directory<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:=
110:mount_bind()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Calling mount(/home, /usr/l=
ocal/var/singularity/mnt//home, ...)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mounts.c:124:mount_bind()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Returning mount_bind(/home, 20008528, 1) =3D 0<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_pa=
rser.c:43:config_get_key_value()=C2=A0 : Called config_get_key_value(fp, bi=
nd path)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 config_parser.c:61:config_get_key_value()=C2=A0 : Return config_g=
et_key_value(fp, bind path) =3D NULL<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:633:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Not staging passwd or group (r=
unning as root)<br>VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:638:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Forking exec process<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:770:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping privs.=
..<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 privilege.c:79:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called drop_privs(struct=
 s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping =
privileges to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3949]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:93:drop_privs()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Dropping privileges to UID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:103:drop_privs(=
)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Confirming we have correct GID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:109:drop_privs()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:115:drop_privs()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Returning drop_privs(struct s_privinfo *uinfo) =3D 0<br>VERBOSE [U=
=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:776:main()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Waiting f=
or Exec process...<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:642:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Hello from exec child process<br>VERBOS=
E [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:644:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Ent=
ering container file system space<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:649:main()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Changing dir to &#39;/&#=
39; within the new root<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:657:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking configuration file for &=
#39;mount proc&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 config_parser.c:69:config_get_key_bool()=C2=A0=C2=A0 : C=
alled config_get_key_bool(fp, mount proc, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:43:config_get_=
key_value()=C2=A0 : Called config_get_key_value(fp, mount proc)<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parse=
r.c:54:config_get_key_value()=C2=A0 : Return config_get_key_value(fp, mount=
 proc) =3D yes<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 config_parser.c:75:config_get_key_bool()=C2=A0=C2=A0 : Retu=
rn config_get_key_bool(fp, mount proc, 1) =3D 1<br>VERBOSE [U=3D0,P=3D3959]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:661:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Mounting /proc<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c=
:674:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Checking configuration file for &#39;mount sys&#39;<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c=
:69:config_get_key_bool()=C2=A0=C2=A0 : Called config_get_key_bool(fp, moun=
t sys, 1)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 config_parser.c:43:config_get_key_value()=C2=A0 : Called config_g=
et_key_value(fp, mount sys)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:54:config_get_key_value()=C2=A0=
 : Return config_get_key_value(fp, mount sys) =3D yes<br>DEBUG=C2=A0=C2=A0 =
[U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:75:con=
fig_get_key_bool()=C2=A0=C2=A0 : Return config_get_key_bool(fp, mount sys, =
1) =3D 1<br>VERBOSE [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 se=
xec.c:678:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Mounting /sys<br>VERBOSE [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:692:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping all privileges<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:121=
:drop_privs_perm()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Called drop_privs_perm(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:129:drop_privs_=
perm()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Resetting su=
pplementary groups<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 privilege.c:135:drop_privs_perm()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping real and effective privileges =
to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 privilege.c:141:drop_privs_perm()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping real and effective privileg=
es to UID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:151:drop_privs_perm()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct GID<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 pri=
vilege.c:157:drop_privs_perm()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:163:drop_privs_perm()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning drop_pri=
vs_perm(struct s_privinfo *uinfo) =3D 0<br>VERBOSE [U=3D0,P=3D3959]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:699:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Changing to correct working=
 directory: /tmp/result<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3959]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:713:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting environment variable &#39=
;SINGULARITY_CONTAINER=3D1&#39;<br>VERBOSE [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:732:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : COMMAND=3Dexec<br>DEBUG=C2=A0=C2=A0 =
[U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 container_actions.c:59=
:container_exec()=C2=A0=C2=A0=C2=A0 : Called container_exec(2, **argv)<br>V=
ERBOSE [U=3D0,P=3D3959]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 container_actio=
ns.c:65:container_exec()=C2=A0=C2=A0=C2=A0 : Exec&#39;ing program: true<br>=
VERBOSE [U=3D0,P=3D3949]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:785:ma=
in()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Exec parent process returned: 0<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:804:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Starting cleanup...<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:955:=
main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Checking to see if we are the last process running in this sessiondir<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sex=
ec.c:959:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Escalating privs to clean session directory<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:61:esc=
alate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Called escalate_privs(void)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:73:escalate_privs()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning es=
calate_privs(void) =3D 0<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:964:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Cleaning sessiondir: /tmp/.singularity-ses=
sion-0.39.12911060245380037651<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:212:s_rmdir()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Removing dirctory: /tmp/.singularity-se=
ssion-0.39.12911060245380037651<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:138:disassociate_loop()=C2=A0=
=C2=A0=C2=A0=C2=A0 : Called disassociate_loop(loop_fp)<br>VERBOSE [U=3D0,P=
=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loop-control.c:140:disassociat=
e_loop()=C2=A0=C2=A0=C2=A0=C2=A0 : Disassociating image from loop device<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 loo=
p-control.c:146:disassociate_loop()=C2=A0=C2=A0=C2=A0=C2=A0 : Returning dis=
associate_loop(loop_fp) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:79:drop_privs()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Called drop_privs(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_privs()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Dropping privileges to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:93:=
drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping privileges to UID =3D &#39;0&#39;<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 pri=
vilege.c:103:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct GID<br>DE=
BUG=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privil=
ege.c:109:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege=
.c:115:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning drop_privs(struct s_privinfo *ui=
nfo) =3D 0<br>VERBOSE [U=3D0,P=3D3944]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
sexec.c:981:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Cleaning up...<br><br>Thanks,<br><br>El mi=C3=A9rcoles, 6 de=
 julio de 2016, 16:00:36 (UTC+2), Gregory M. Kurtzer  escribi=C3=B3:<blockq=
uote class=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;border-lef=
t: 1px #ccc solid;padding-left: 1ex;">Hi,=C2=A0<div><br></div><div>/run/use=
r is associated with the Singularity container?</div><div><br></div><div>Ca=
n you show me the output of &#39;losetup -a&#39; please?</div><div><br></di=
v><div>Why are=C2=A0<span></span>you are running it with sudo, you should n=
ot need to.</div><div><br></div><div>It is weird, isn&#39;t -n a synonym fo=
r -np and if so, shouldn&#39;t it executing 1 process on the given node? It=
 seems like it is doing more.=C2=A0</div><div><br></div><div>Lastly, what v=
ersion of Singularity is this? If from Git=C2=A0master when did you do the =
last pull? Can you try this in debug mode and with a simple=C2=A0binary for=
 testing:</div><div><br></div><div>$=C2=A0<font size=3D"2"><span style=3D"b=
ackground-color:rgba(255,255,255,0)">mpirun -n 1 singularity -d=C2=A0exec=
=C2=A0/mnt/glusterfs/<wbr>singularity/nasmpi-1.img=C2=A0</span></font><span=
 style=3D"background-color:rgba(255,255,255,0);font-size:small">true</span>=
</div><div><span style=3D"background-color:rgba(255,255,255,0);font-size:sm=
all"><br></span></div><div>And send that output please.=C2=A0</div><div><br=
></div><div><br>On Wednesday, July 6, 2016, Raimon Bosch &lt;<a href=3D"jav=
ascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"xCoT0XTmBgAJ" rel=3D"n=
ofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;return true;" onc=
lick=3D"this.href=3D&#39;javascript:&#39;;return true;">rai...@gmail.com</a=
>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>When I =
do &quot;df -h&quot; I see the singularity container still mounted. Maybe I=
 need to run a command to unmount it:<br><br>&gt; df -h<br>Filesystem=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 Size=C2=A0 Used Avail Use% Mounted on<br>****<br>tmpfs=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3.2G=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0 3.2G=C2=A0=C2=A0 0% /run/user/1006<br>****<br><br>El mi=C3=A9rc=
oles, 6 de julio de 2016, 10:25:24 (UTC+2), Raimon Bosch  escribi=C3=B3:<bl=
ockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-l=
eft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<br><b=
r>It fails depending on your environment. In my Ubuntu 14.04 it worked fine=
, but in this instance of Debian jessie I get the following:<br><br>&gt; ER=
ROR: Failed to associate image to loop: Device or resource busy<br><br>Mayb=
e is because we are using a glusterfs shared disk to keep the containers?<b=
r><br>Here you have the entire output:<br><br>&gt; sudo mpirun -n 1 singula=
rity exec /mnt/glusterfs/singularity/<wbr>nasmpi-1.img /trace.sh /NPB/NPB3.=
3-MPI/bin/bt.C.4 : -n 1 singularity exec /mnt/glusterfs/singularity/<wbr>na=
smpi-2.img /trace.sh /NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec /mn=
t/glusterfs/singularity/<wbr>nasmpi-3.img /trace.sh /NPB/NPB3.3-MPI/bin/bt.=
C.4 : -n 1 singularity exec /mnt/glusterfs/singularity/<wbr>nasmpi-4.img /t=
race.sh /NPB/NPB3.3-MPI/bin/bt.C.4<br>ERROR: Failed to associate image to l=
oop: Device or resource busy<br>ERROR: Failed to associate image to loop: D=
evice or resource busy<br>/bin/bash: warning: setlocale: LC_ALL: cannot cha=
nge locale (en_US.UTF-8)<br>ERROR: Failed to associate image to loop: Devic=
e or resource busy<br>------------------------------<wbr>------------------=
------------<wbr>--------------<br>mpirun has exited due to process rank 2 =
with PID 63416 on<br>node bscgrid30 exiting improperly. There are two reaso=
ns this could occur:<br><br>1. this process did not call &quot;init&quot; b=
efore exiting, but others in<br>the job did. This can cause a job to hang i=
ndefinitely while it waits<br>for all processes to call &quot;init&quot;. B=
y rule, if one process calls &quot;init&quot;,<br>then ALL processes must c=
all &quot;init&quot; prior to termination.<br><br>2. this process called &q=
uot;init&quot;, but exited without calling &quot;finalize&quot;.<br>By rule=
, all processes that call &quot;init&quot; MUST call &quot;finalize&quot; p=
rior to<br>exiting or it will be considered an &quot;abnormal termination&q=
uot;<br><br>This may have caused other processes in the application to be<b=
r>terminated by signals sent by mpirun (as reported here).<br>-------------=
-----------------<wbr>------------------------------<wbr>--------------<br>=
<br>Thanks in advance,<br><br>El martes, 5 de julio de 2016, 18:21:48 (UTC+=
2), Gregory M. Kurtzer  escribi=C3=B3:<blockquote class=3D"gmail_quote" sty=
le=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1e=
x"><div dir=3D"ltr">Hi Raimon,<div><br></div><div>I am confused as to what =
the issue is that you are having. Singularity supports running both across =
nodes as well as multiple processes per node in any number of containers. C=
an you paste your command and the error you are getting, maybe that will he=
lp.</div><div><br></div><div>Thanks!</div><div><br></div><div><br></div></d=
iv><div><br><div class=3D"gmail_quote">On Tue, Jul 5, 2016 at 8:25 AM, Raim=
on Bosch <span dir=3D"ltr">&lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt;=
</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>That =
solution does not work with nas/mpi benchmark. That&#39;s because bt.C.16 e=
xpects 16 processes. When you split processes it throws an exception becaus=
e number of processes is lower than 16. <br><br>I am still trying to figure=
 out how to do this. Let me know if you have any suggestion.<br><br>Cheers,=
<span><br><br>El jueves, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Casta=
in  escribi=C3=B3:</span><blockquote class=3D"gmail_quote" style=3D"margin:=
0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word"><span><div>I think you are misunderstanding the b=
asic nature of the Singularity =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s ju=
st a file system overlay. So =E2=80=9Csharing=E2=80=9D a container is no di=
fferent than running on a node where the procs all see the same file system=
. Thus, having multiple containers that are identical makes no sense - it=
=E2=80=99s all the same file system.</div><div><br></div><div>Now if you wa=
nt to run different containers (e.g., with different libraries or OS in the=
m), then you would use mpirun=E2=80=99s MPMD syntax - for example:</div><di=
v><br></div><div>mpirun -n 1 &lt;container1&gt; : -n 1 &lt;container2&gt;</=
div><div><br></div><div>HTH</div><div>Ralph</div></span><div><br><blockquot=
e type=3D"cite"><div><div><div>On Jun 23, 2016, at 1:53 AM, Raimon Bosch &l=
t;<a rel=3D"nofollow">rai...@gmail.com</a>&gt; wrote:</div><br></div></div>=
<div><div><div><div dir=3D"ltr" style=3D"font-family:Helvetica;font-size:12=
px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align:st=
art;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px=
"><br>One last question: What if I want to execute more than one container =
in the same host? With this technique I am bounded always to the same conta=
iner. One of our experiments was based in measuring performance of several =
containers working in parallel in the same node. Also we had experiments wi=
th N containers per host in a multihost environment.<br><br>El mi=C3=A9rcol=
es, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Kurtzer escribi=C3=B3=
:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border=
-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;=
padding-left:1ex">Hi Raimon,<div><br></div><div>Sorry I wasn&#39;t clear. I=
 am not yet at my computer and thinking while typing on an iPhone hinders m=
y mental processes. Lol</div><div><br></div><div>If I understand your examp=
le properly, you have a docker or VM infrastructure already set up and you =
are invoking the mpirun commands from within the virtual environment. Singu=
larity works on a very different premis because integrating a virtual clust=
er into an existing cluster and scheduling system is a mess.</div><div><br>=
</div><div>So starting from the physical nodes, which already have access t=
o all other nodes in the cluster, and already scheduled properly, and in di=
rect access to optimized hardware and file systems.... You call mpirun.=C2=
=A0</div><div><br></div><div>The mpirun command will take the standard form=
at as you illustrated with the following change to call Singularity inline:=
</div><div><br></div><div>$ mpirun -np 4 --hostfile hosts.txt singularity e=
xec ~/container.img trace.sh bt.C.4</div><div><br></div><div>This assumes t=
he following:</div><div><br></div><div>1. The container image which contain=
s the program&#39;s you want to run is at ~/container.img and accessible at=
 this path=C2=A0on all nodes referenced in hosts.txt</div><div>2. The hosts=
.txt references other physical nodes you want to run on</div><div>3. The ex=
ecutables trace.sh and bt.C.4 are both inside the container and in a standa=
rd path</div><br><div>In this case we are performing one execution and MPI =
+ singularity is managing all of the communication between processes, nodes=
 and containers. Also it is now using any optimized hardware (eg. Infiniban=
d)=C2=A0and existing high performance=C2=A0file systems (which should not b=
e accessible via a virtualized or Docker&#39;ized=C2=A0cluster for security=
 reasons).</div><div><br></div><div>This way is actually MUCH simpler then =
what you are proposing because there is no need to manage any virtual nodes=
, virtual networks, or resource manager hacks. It really is as easy as just=
 running any other MPI=C2=A0process on an existing cluster.=C2=A0</div><div=
><br></div><div>Hope that helps better!</div><div><br></div><div><span></sp=
an><br><br>On Wednesday, June 22, 2016, Raimon Bosch &lt;<a rel=3D"nofollow=
">raimon...@</a><a href=3D"http://gmail.com/" rel=3D"nofollow" target=3D"_b=
lank" onmousedown=3D"this.href=3D&#39;http://gmail.com/&#39;;return true;" =
onclick=3D"this.href=3D&#39;http://gmail.com/&#39;;return true;">gmail.com<=
/a>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px=
 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-=
left-style:solid;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi Gregory,<br>=
<br>I&#39;m not sure if I would achieve the same with your commands. In an =
environment based on dockers or virtual machines we would do something like=
 this [non applicable to Singularity]:<br><br>&gt; cd $OPEN_MPI/bin &amp;&a=
mp; mpirun -np 4 --hostfile hosts.txt ./trace.sh ./bt.C.4<br><br>where host=
s.txt* is:<br><br>&gt;vm-ip-01-on-<b>host01</b><span>=C2=A0</span>slots=3D2=
<br>&gt;vm-ip-01-on-<b>host02</b><span>=C2=A0</span>slots=3D2<br><br>* vm-i=
p-XX-on-hostXX<b><span>=C2=A0</span></b>are IPs i.e. 172.100.60.XX<br><br>a=
nd trace.sh is:<br><br>&gt;#!/bin/bash<br>&gt;<br>&gt;export EXTRAE_HOME=3D=
/opt/extrae/<br>&gt;export EXTRAE_CONFIG_FILE=3D/extrae.xml<br>&gt;export L=
D_PRELOAD=3D${EXTRAE_HOME}/lib/<wbr>libmpitrace.so<br>&gt;<br>&gt;## Run th=
e desired program<br>&gt;$*<br><br>As you see we only perform one execution=
 and OpenMPI transparently manages communication between containers or virt=
ual machines. This command would work well rather VMs are on the same host =
or not.<br><br>What I understand from your response is that now we should e=
xecute OpenMPI on each host and then merge results manually. I don&#39;t kn=
ow yet how to do this merge step or if it is any way to centralize everythi=
ng like I would do with VMs.<br><br>Thanks in advance,<br><br>El mi=C3=A9rc=
oles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Kurtzer escribi=C3=
=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;bor=
der-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:sol=
id;padding-left:1ex">Hi Raimon,<div><br></div><div>The quick answer is you =
have mpirun handle that as you would normally where the container file live=
s on a shared file system:</div><div><br></div><div>$ mpirun singularity ex=
ec ~/container.img mpi_prog_in_container</div><div><br></div><div>Let the M=
PI outside the container launch the singularity container on each host as i=
t would normally launch any MPI program. Then it will call Singulairty and =
Singularity will launch the MPI program inside the container on each of you=
r hosts/servers.=C2=A0</div><div><br></div><div>Hope that helps!</div><div>=
<br></div><div><span></span><br><br>On Wednesday, June 22, 2016, Raimon Bos=
ch &lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt; wrote:<br><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1p=
x;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1=
ex"><div dir=3D"ltr"><br>Hi Gregory,<br><br>Thank you for your answer. One =
of our experiments needs to run OpenMPI among several servers. This means t=
hat we should put one of our containers in host01, another in host02 and an=
other in host03 and collect the results.<span>=C2=A0</span><br><br>How can =
I do this execution in parallel if I need to communicate with more than one=
 server?<br><br>El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M=
. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0=
px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);b=
order-left-style:solid;padding-left:1ex"><div dir=3D"ltr">Hi Raimon,<div><b=
r></div><div>The communication model of a Singularity container is very dif=
ferent from that of a Docker implementation. This is because Docker for all=
 practical purposes emulates a virtual machine as each container has it&#39=
;s own IP address and thus it&#39;s own ssh server. It also carries its own=
 set of complexities, for example networks need to be segregated/VLan&#39;e=
d, DNS/host resolution needs to be dynamic and passed down to the container=
s (so they can reach each other), ssh daemons and other process running ins=
ide the containers, management via an existing scheduling system, and the l=
ist goes on and on.</div><div><br></div><div>Think of it this way, Singular=
ity does not do any of that... It runs a program within the container as if=
 it were running on the host itself, so to communicate between containers i=
s as easy as communicating between programs. So for MPI, it would happen wi=
th the MPI on the physical host (outside the container) invoking the contai=
ner subsystem which then invokes the MPI programs within the container and =
the MPI programs within the container communicate back to the MPI (orted) o=
utside the container on the host to get access to the host resources. In th=
is model all available resources and infrastructure can be leveraged at ful=
l bandwidth by the contained processes and all of the aforementioned comple=
xities akin to running on a virtualized mini-cluster are circumvented.</div=
><div><br></div><div>There is additional information I have written at:</di=
v><div><br></div><div><a href=3D"http://singularity.lbl.gov/#hpc" rel=3D"no=
follow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;http://www.google=
.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\=
x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;" onclic=
k=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingulari=
ty.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0ph=
cop4-SUwsYEjw&#39;;return true;">http://singularity.lbl.gov/#<wbr>hpc</a><b=
r></div><div><br></div><div>That page is still coming along, and needs more=
 information still but if you have any questions, comments or change propos=
als please let us know!</div><div><br></div><div>Thanks and hope that helps=
!</div><div><br></div><div><br></div></div><div><br><div class=3D"gmail_quo=
te">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch<span>=C2=A0</span><span d=
ir=3D"ltr">&lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt;</span><span>=C2=
=A0</span>wr<wbr>ote:<br><blockquote class=3D"gmail_quote" style=3D"margin:=
0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);=
border-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi,<br><=
br>We are trying to run experiments using singularity containers. The idea =
is to run OpenMPI among several containers and check performance results.<s=
pan>=C2=A0</span><br><br>How can I communicate with another container? In d=
ocker this is clear because every container gets an assigned IP and you can=
 ping there, but what is the situation in the case of singularity? Is it po=
ssible to assign an IP to each container? Can I connect via ssh to them?<br=
><br>Thanks in advance,<span><font color=3D"#888888"><br></font></span></di=
v><span><font color=3D"#888888"><div><br></div>--<span>=C2=A0</span><br>You=
 received this message because you are subscribed to the Google Groups &quo=
t;singularity&quot; group.<br>To unsubscribe from this group and stop recei=
ving emails from it, send an email to<span>=C2=A0</span><a rel=3D"nofollow"=
>singu...@lbl.gov</a>.<br></font></span></blockquote></div><br><br clear=3D=
"all"><div><br></div>--<span>=C2=A0</span><br><div><div dir=3D"ltr"><div>Gr=
egory M. Kurtzer<br>High Performance Computing Services (HPCS)<br>Universit=
y of California<br>Lawrence Berkeley National Laboratory<br>One Cyclotron R=
oad, Berkeley, CA 94720</div></div></div></div></blockquote></div><div><br>=
</div>--<span>=C2=A0</span><br>You received this message because you are su=
bscribed to the Google Groups &quot;singularity&quot; group.<br>To unsubscr=
ibe from this group and stop receiving emails from it, send an email to<spa=
n>=C2=A0</span><a>singularity+unsubscribe@<wbr>lbl.gov</a>.<br></blockquote=
></div><br><br>--<span>=C2=A0</span><br><div dir=3D"ltr"><div>Gregory M. Ku=
rtzer<br>High Performance Computing Services (HPCS)<br>University of Califo=
rnia<br>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkel=
ey, CA 94720</div></div><br></blockquote></div><div><br></div>--<span>=C2=
=A0</span><br>You received this message because you are subscribed to the G=
oogle Groups &quot;singularity&quot; group.<br>To unsubscribe from this gro=
up and stop receiving emails from it, send an email to<span>=C2=A0</span><a=
>singularity+unsubscribe@<wbr>lbl.gov</a>.<br></blockquote></div><br><br>--=
<span>=C2=A0</span><br><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Per=
formance Computing Services (HPCS)<br>University of California<br>Lawrence =
Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div=
></div><br></blockquote></div><div style=3D"font-family:Helvetica;font-size=
:12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align=
:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:=
0px"><br></div><span style=3D"font-family:Helvetica;font-size:12px;font-sty=
le:normal;font-weight:normal;letter-spacing:normal;text-align:start;text-in=
dent:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none=
;display:inline!important">--<span>=C2=A0</span></span><br style=3D"font-fa=
mily:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-s=
pacing:normal;text-align:start;text-indent:0px;text-transform:none;white-sp=
ace:normal;word-spacing:0px"><span style=3D"font-family:Helvetica;font-size=
:12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align=
:start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:=
0px;float:none;display:inline!important">You received this message because =
you are subscribed to the Google Groups &quot;singularity&quot; group.</spa=
n><br style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-=
weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-t=
ransform:none;white-space:normal;word-spacing:0px"></div></div><span style=
=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight:norm=
al;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:no=
ne;white-space:normal;word-spacing:0px;float:none;display:inline!important"=
>To unsubscribe from this group and stop receiving emails from it, send an =
email to<span>=C2=A0</span></span><a style=3D"font-family:Helvetica;font-si=
ze:12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-ali=
gn:start;text-indent:0px;text-transform:none;white-space:normal;word-spacin=
g:0px" rel=3D"nofollow">singu...@lbl.gov</a><span style=3D"font-family:Helv=
etica;font-size:12px;font-style:normal;font-weight:normal;letter-spacing:no=
rmal;text-align:start;text-indent:0px;text-transform:none;white-space:norma=
l;word-spacing:0px;float:none;display:inline!important">.</span></div></blo=
ckquote></div><br></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>
</blockquote></div>
------=_Part_406_2011422175.1467814836722--

------=_Part_405_920590751.1467814836720--
