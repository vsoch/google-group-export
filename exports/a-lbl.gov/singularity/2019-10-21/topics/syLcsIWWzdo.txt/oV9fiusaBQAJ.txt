X-Received: by 10.99.160.83 with SMTP id u19mr1564406pgn.46.1518806929150;
        Fri, 16 Feb 2018 10:48:49 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.101.90.200 with SMTP id d8ls381359pgt.33.gmail; Fri, 16 Feb
 2018 10:48:48 -0800 (PST)
X-Received: by 10.99.114.3 with SMTP id n3mr5872041pgc.225.1518806927931;
        Fri, 16 Feb 2018 10:48:47 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1518806927; cv=none;
        d=google.com; s=arc-20160816;
        b=yTNnP4H6ihPjtbwjKoUupCgLiEiFmPOrD06i9xw+v4rtrbbHRtmGr3GvQnKu2SeQuj
         u2Uxep/s/3Zu+vCD5O+k7VH4ayM4r3HoT4bSku0dOQOgxORPMsis5QN7gMSB27to4Au6
         WAqHRAWy2J+ZhbjclDxZtGikA0hqqW1zsH3FQw3tT9DQ7WyNDy/vEy2cdv6S9gNUG4K6
         4A9pr1+3nSOJC1xKIQOuI8mcWLTGkIGsQfGH3dQA7L6mQ+IkNIsiHTJFX/gW7RJ0Faot
         QFjxnYEWKf01jzLja7a44GIqPJx8FbtJKcU5XAaqMAuIuyfIYWUSIn++Altg19m4FfSw
         K5xg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=C8zWInnp8wbR2BzFhKgHk95DBFHJhh1UWFHOXhtNOpk=;
        b=OC7A6jThDOsDI9ghlR4VqIXNPN5Ok8R3YLd5ZW/rCe1PHuYDq7pQ5M6tkH3jn9POxh
         qYY0ERWb2bOj9Wl+VKZjToHNTsAf0xnL5MSF/phAmVWOsWX3ESdO8re9EmdtBJmxXyKc
         s0k6IOgNoMr334HlHPrSmC8yYs0oZUiKUE1Y2c3Nno6v9mK1RX3/liloP3Q/N8a3jNCj
         wTyNeCphBh5X4NTJAu8bOmrrs7qX0cUrtRcBVzavc2I68sTY+skZ6jJn/CLy5cEnQDPy
         5dS/uiEly70hcntBf+aj5Q2NkHs6t2nl+U0cPE6ogOWfjNHOU2wCXGhJkOLRN1HJVp0H
         7BNw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Q0rjvrqH;
       spf=pass (google.com: domain of jason...@gmail.com designates 209.85.220.196 as permitted sender) smtp.mailfrom=jason...@gmail.com
Return-Path: <jason...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id v4-v6si245829plb.529.2018.02.16.10.48.47
        for <singu...@lbl.gov>;
        Fri, 16 Feb 2018 10:48:47 -0800 (PST)
Received-SPF: pass (google.com: domain of jason...@gmail.com designates 209.85.220.196 as permitted sender) client-ip=209.85.220.196;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Q0rjvrqH;
       spf=pass (google.com: domain of jason...@gmail.com designates 209.85.220.196 as permitted sender) smtp.mailfrom=jason...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EqGwCgJodaf8TcVdFWAwMbAQEBAQMBA?=
 =?us-ascii?q?QEJAQEBgyJUAT5wKAqDQweYKoFeBR+BF4Jqk3OBIwMZQyINgVyCa08Cgj8HVxU?=
 =?us-ascii?q?BAgEBAQEBAQIBAhABAQkLCwgmMYI4BQIDAh4FBANIKQEvAQEBAQEBAQEBAR8CD?=
 =?us-ascii?q?B9AAQUjHQENDh4DCgIGBQsNAgImAgIbBgEBDgMBBQEcDgcEAQcTAgSILIE7AQM?=
 =?us-ascii?q?VBQugX0CMF4IFBQEcgwwFg2YKGScNWVmCEwEBAQEGAQEBAQEBGgIBBQUNfYN4g?=
 =?us-ascii?q?iiBV4QIgQ6CbEQBAYE+ARIBES8mglCCZQWSUoEWkBg1CYJNhVeIW4ULgiCKQ4d?=
 =?us-ascii?q?ljgZIiT0ZIIEXNVYucTMaI4EAghKCRQ8cgiUiNwEPi1hJgXUBAQE?=
X-IronPort-AV: E=Sophos;i="5.46,520,1511856000"; 
   d="scan'208";a="13922858"
Received: from mail-qk0-f196.google.com ([209.85.220.196])
  by fe4.lbl.gov with ESMTP; 16 Feb 2018 10:48:46 -0800
Received: by mail-qk0-f196.google.com with SMTP id b130so4983794qkg.9
        for <singu...@lbl.gov>; Fri, 16 Feb 2018 10:48:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=C8zWInnp8wbR2BzFhKgHk95DBFHJhh1UWFHOXhtNOpk=;
        b=Q0rjvrqHWU6TzbT9mN7WyFzWhTAVNNnJACukEbJnrrRSi/0d+lWAOe4iWE8rkcHkt8
         FptmdXeC4wmCtBmJmi8AvP2dgyfWyWz2RrXYi0Ds2Sw7p1AraM3Iow/Zw5qnaXqCtr8W
         rM9KNsFuylM+segs1XyqE0S2pfeYkHN5BBc6MyaUCSnCfDMQNm8E84wDdATu8T+ddj18
         6GNRljrvl0I81dxPve9GGbWF0q9PnrJoc9JtQgtuIIJVNBcZt4WORyp/68uyYDAytC/o
         3xvckzDAFD7WGvVQP6sNpQR0DdGq1T16Xn6Hl6mUTVPNmJMxJeNNs8o7WwZH1KTMlKsk
         JAqw==
X-Gm-Message-State: APf1xPDvrhQfg7XP0a+aOE8qSch/Ft8PfgO8osr7kezjIOvZ8qx1dlv1
	POJWZYnHyIt9QOdae3yuP2yT0Ey9kVYPOHUZYo5MzA==
X-Received: by 10.55.73.7 with SMTP id w7mr10553434qka.317.1518806925402; Fri,
 16 Feb 2018 10:48:45 -0800 (PST)
MIME-Version: 1.0
Received: by 10.200.3.88 with HTTP; Fri, 16 Feb 2018 10:48:44 -0800 (PST)
In-Reply-To: <CAGfAqt_JKaveudDCOrCBupc95jH_9kxVua2zhwh1pRg+SoV11w@mail.gmail.com>
References: <43b84047-926f-4ecd-a8da-fd7bbe3470ff@lbl.gov> <CADgKzdw9kpwwD46N0eKOqj=3mvf-_HqGqNeybem9ia0EZvNs1Q@mail.gmail.com>
 <CAFgQtxFN2+MTqcBqcu9M8qT4tvZfS2etgXpAN-diWNVUTEV0VA@mail.gmail.com>
 <b8ff849b-fdb7-4315-a28d-ebaa862ad671@lbl.gov> <044760f5-d18a-4090-88a3-0602ec7cc675@lbl.gov>
 <CAB2ovovPA--7NBTKbw6QpY22jAz9=sb8nB5XkQT5pDQcp2ARXQ@mail.gmail.com>
 <4b2b1672-7c5f-49b7-ab6f-3084eac067a7@lbl.gov> <CAGfAqt_JKaveudDCOrCBupc95jH_9kxVua2zhwh1pRg+SoV11w@mail.gmail.com>
From: Jason Stover <jason...@gmail.com>
Date: Fri, 16 Feb 2018 12:48:44 -0600
Message-ID: <CAGfAqt9Hb9u5qDNG6O5s2UQB0aJNyOJgmv6Aa48yumLeaYMaJg@mail.gmail.com>
Subject: Re: [Singularity] Submit additional jobs from within a container
To: singularity@lbl.gov
Content-Type: text/plain; charset="UTF-8"

As an addition ...

Having the scheduler in the container, installed or bind mount, should
"just work" if the nodes are set up to be able to submit jobs. Since
we don't have Network Namespaces, we're using the host network, so the
scheduler will see the traffic as coming from the host and _should_ be
allowed.

You'd also want to be sure the workdir is consistent between the path
in the container and outside. So if you're submitting from a directory
`/scratch/job_setup/` in the container ... that should exist outside
the container, or pass an option when submitting to the correct
location (i.e. `-d` in Torque).

-J


On Fri, Feb 16, 2018 at 12:33 PM, Jason Stover <jason...@gmail.com> wrote:
> I was able to with slurm ... I did an install of the slurm package
> into the container and pointed the configuration at the scheduler.
>
> If you have, for example, Torque installed somewhere like:
> /software/torque/[version]  ... you could bind mount that to the
> container. I think the only variable that needs set is PBS_SERVER or
> PBS_MASTER ...
>
> Once that's set, you should be able to talk to it ... though if you're
> bind mounting the install location, and the spool directory if needed,
> then you shouldn't need to set the variable as it'll be read from the
> config in the spool.
>
> -J
>
>
> On Fri, Feb 16, 2018 at 11:59 AM, Jonathan Greenberg <jgr...@gmail.com> wrote:
>> Basically, I wasn't able to figure out how to do that (sounds like neither
>> did Brian) -- how do I call an "external" sbatch with all the needed
>> environment variables?
>>
>> A straightforward example would be perfect, if someone has solved this!
>>
>> On Friday, February 16, 2018 at 9:43:33 AM UTC-8, Bennet Fauber wrote:
>>>
>>> Wouldn't you need to configure the container a slurm/torque what have
>>> you client?  If, internal to the container, it can run the slurm
>>> client commands, and it knows the correct scheduler node name,
>>> wouldn't that work?  That seems like it would be required to stick
>>> with the purpose of containing the application to the environment
>>> inside the container.  What am I missing?
>>>
>>>
>>>
>>> On Fri, Feb 16, 2018 at 12:36 PM, Jonathan Greenberg <j...@gmail.com>
>>> wrote:
>>> > I had a similar question about a month ago that we didn't quite get
>>> > figured
>>> > out:
>>> >
>>> >
>>> > https://groups.google.com/a/lbl.gov/d/topic/singularity/IGvUup1aGXE/discussion
>>> >
>>> >  -- a job running within a singularity container that passes an sbatch
>>> > command (similar to qsub) to the "global" scheduler.  It has to do with
>>> > the
>>> > container interacting with its "global" environment -- we can mount
>>> > files
>>> > within the container, but haven't figure out how to submit them.
>>> >
>>> > I think one of the issues that came up is how it might be bad practice
>>> > for
>>> > portable code but, in our case, we don't necessarily care about sharing
>>> > the
>>> > container with someone else -- our HPC REQUIRES we use singularity, but
>>> > we
>>> > need to have a singularity container create jobs and then submit them.
>>> > In
>>> > my case, what I end up doing is having the container build the jobs, but
>>> > then I have to manually (in the "outside" environment) submit them.
>>> >
>>> > I think the basic request is allowing a container to execute something
>>> > (anything) in the "containing" environment as if the user was typing it
>>> > on
>>> > the command line in that environment.
>>> >
>>> > On Wednesday, February 7, 2018 at 1:33:07 PM UTC-8, Kim Wong wrote:
>>> >>
>>> >> Hi Brian,
>>> >>
>>> >> Did you ever find a solution to this question?  This is a functionality
>>> >> we
>>> >> would like to use as well.  Thanks.
>>> >>
>>> >> On Monday, July 17, 2017 at 2:15:09 AM UTC-4, Brian Puchala wrote:
>>> >>>
>>> >>> Thank you for the response.  The purpose of a significant part of our
>>> >>> software package is to decide what jobs are necessary and submit them.
>>> >>> I
>>> >>> imagine this is not such an unusual potential use case.
>>> >>>
>>> >>> Cheers,
>>> >>> Brian
>>> >>>
>>> >>>
>>> >>> On Sun, Jul 16, 2017 at 6:46 PM, Paolo Di Tommaso
>>> >>> <pao...@gmail.com>
>>> >>> wrote:
>>> >>>>
>>> >>>> A better approach is to separate the application logic from the
>>> >>>> scheduling logic, by doing that you will be able to isole your job
>>> >>>> executions with singularity and submit them to SLURM or any other
>>> >>>> cluster.
>>> >>>>
>>> >>>> Hope it helps.
>>> >>>>
>>> >>>> p
>>> >>>>
>>> >>>> On Sun, Jul 16, 2017 at 8:44 PM, Brian Puchala <bp...@umich.edu>
>>> >>>> wrote:
>>> >>>>>
>>> >>>>> Hi,
>>> >>>>>
>>> >>>>> I'm trying to familiarize myself with how Singularity might work for
>>> >>>>> our application.  We have components that submit additional jobs
>>> >>>>> through the
>>> >>>>> host job manager (TORQUE or SLURM). Is it possible to run these
>>> >>>>> within their
>>> >>>>> own container?  Is there an example that shows how?
>>> >>>>>
>>> >>>>> Thanks!
>>> >>>>>
>>> >>>>> --
>>> >>>>> You received this message because you are subscribed to the Google
>>> >>>>> Groups "singularity" group.
>>> >>>>> To unsubscribe from this group and stop receiving emails from it,
>>> >>>>> send
>>> >>>>> an email to singu...@lbl.gov.
>>> >>>>
>>> >>>>
>>> >>>> --
>>> >>>> You received this message because you are subscribed to a topic in
>>> >>>> the
>>> >>>> Google Groups "singularity" group.
>>> >>>> To unsubscribe from this topic, visit
>>> >>>>
>>> >>>> https://groups.google.com/a/lbl.gov/d/topic/singularity/syLcsIWWzdo/unsubscribe.
>>> >>>> To unsubscribe from this group and all its topics, send an email to
>>> >>>> singu...@lbl.gov.
>>> >>>
>>> >>>
>>> >>>
>>> >>>
>>> >>> --
>>> >>> >>
>>> >>> >> whoami
>>> >>> Brian Puchala
>>> >>> Assistant Research Scientist
>>> >>> Materials Science and Engineering
>>> >>> University of Michigan
>>> >>> Phone: (734) 763-5282
>>> >>> Email: bp...@umich.edu
>>> >>> >>
>>> >
>>> > --
>>> > You received this message because you are subscribed to the Google
>>> > Groups
>>> > "singularity" group.
>>> > To unsubscribe from this group and stop receiving emails from it, send
>>> > an
>>> > email to singu...@lbl.gov.
>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
