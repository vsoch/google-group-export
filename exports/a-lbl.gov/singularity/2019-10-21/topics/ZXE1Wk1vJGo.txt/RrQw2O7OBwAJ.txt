Date: Fri, 16 Mar 2018 08:40:16 -0700 (PDT)
From: =?UTF-8?Q?Augusto_B=2E_Corr=C3=AAa?= <guto...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <55d317ac-bbae-4bfe-ac9a-652568c69fd7@lbl.gov>
Subject: Issue with Memory Cgroups
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_2015_1808526937.1521214816687"

------=_Part_2015_1808526937.1521214816687
Content-Type: multipart/alternative; 
	boundary="----=_Part_2016_1880933265.1521214816687"

------=_Part_2016_1880933265.1521214816687
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi everyone,


I've been trying to use Singularity together with Cgroups, but I'm having 
troubles with memory Cgroups for processes using Singularity.

More specifically, I have a given process for which I want to limit the 
memory usage -- let's say, to 1 GB. When this process tries to use or 
allocate for than 1 GB, I would like to kill this process immediately.

Whenever I run my process using Cgroup without Singularity, it works as 
expected. However, when I try to run it using Singularity, things get a bit 
"weird".
It seems that when the memory usage gets close to the limit, the program 
gets significantly slower, but it is never killed. Using a process monitor, 
we can see that the memory usage is close to the limit defined by the 
Cgroups, but it never really reaches the limit until the timeout (also 
specified by Cgroup).

I created a program that performs a Breadth-First Search (BFS) to debug the 
problem and I tested it using Cgroups for 1 GB memory limit and 30 minutes 
time limit. Running as a "normal" compiled program, without the Singularity 
image, the program is killed once the limit is reached (in about 3 or 4 
minutes). Using the singularity image, it gets close to this limit in the 
same time, but then it remains "almost frozen" until it reaches the time 
limit imposed by the Cgroups. For instance, it performs around 300.000 
nodes expansions in the BFS until it "freezes"; after that, it only expands 
6 nodes until the time limit is reached.

In another test, I wrote a C program to just fill memory in chunks and do 
some computation in between. Let's say that I defined a 100 MB memory limit 
this time. If I try to allocate chunks of 10 MB at each time, the program 
is killed once it goes over the limit. However, if I try smaller chunks,  
the program enters in this "frozen state".


I had this issue on an Ubuntu 16.04 and CentOS 7.4, both using the version 
2.4 of Singularity.


Would anyone have any idea on how to deal with it?


Please, apologize me if this is a known issue or a trivial problem, but I 
did not find any solution on the website or in any forum.

Kind regards,
Augusto.

------=_Part_2016_1880933265.1521214816687
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi everyone,<br><br><br>I&#39;ve been trying to use Singul=
arity together with Cgroups, but I&#39;m having troubles with memory Cgroup=
s for processes using Singularity.<br><br>More specifically, I have a given=
 process for which I want to limit the memory usage -- let&#39;s say, to 1 =
GB. When this process tries to use or allocate for than 1 GB, I would like =
to kill this process immediately.<br><br>Whenever I run my process using Cg=
roup without Singularity, it works as expected. However, when I try to run =
it using Singularity, things get a bit &quot;weird&quot;.<br>It seems that =
when the memory usage gets close to the limit, the program gets significant=
ly slower, but it is never killed. Using a process monitor, we can see that=
 the memory usage is close to the limit defined by the Cgroups, but it neve=
r really reaches the limit until the timeout (also specified by Cgroup).<br=
><br>I created a program that performs a Breadth-First Search (BFS) to debu=
g the problem and I tested it using Cgroups for 1 GB memory limit and 30 mi=
nutes time limit. Running as a &quot;normal&quot; compiled program, without=
 the Singularity image, the program is killed once the limit is reached (in=
 about 3 or 4 minutes). Using the singularity image, it gets close to this =
limit in the same time, but then it remains &quot;almost frozen&quot; until=
 it reaches the time limit imposed by the Cgroups. For instance, it perform=
s around 300.000 nodes expansions in the BFS until it &quot;freezes&quot;; =
after that, it only expands 6 nodes until the time limit is reached.<br><br=
>In another test, I wrote a C program to just fill memory in chunks and do =
some computation in between. Let&#39;s say that I defined a 100 MB memory l=
imit this time. If I try to allocate chunks of 10 MB at each time, the prog=
ram is killed once it goes over the limit. However, if I try smaller chunks=
,=C2=A0 the program enters in this &quot;frozen state&quot;.<br><br><br>I h=
ad this issue on an Ubuntu 16.04 and CentOS 7.4, both using the version 2.4=
 of Singularity.<br><br><br>Would anyone have any idea on how to deal with =
it?<br><br><br>Please, apologize me if this is a known issue or a trivial p=
roblem, but I did not find any solution on the website or in any forum.<br>=
<br>Kind regards,<br>Augusto.<br></div>
------=_Part_2016_1880933265.1521214816687--

------=_Part_2015_1808526937.1521214816687--
