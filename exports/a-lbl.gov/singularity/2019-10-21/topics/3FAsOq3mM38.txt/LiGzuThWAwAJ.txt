Date: Thu, 13 Oct 2016 16:11:03 -0700 (PDT)
From: Steve Mehlberg <sgmeh...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <c1907e4e-d873-4532-9e88-bccb5a57c04a@lbl.gov>
In-Reply-To: <CAN7etTwsFM1edg+ujDsKMktBD=i14qw0JzegWvzZeuZL7Zjqdw@mail.gmail.com>
References: <33de4201-8a0d-4012-a1fa-1d4e729926b7@lbl.gov> <1c1cf001-51aa-454e-8986-b64d010583f9@lbl.gov>
 <CAN7etTwsFM1edg+ujDsKMktBD=i14qw0JzegWvzZeuZL7Zjqdw@mail.gmail.com>
Subject: Re: [Singularity] Re: Does the singularity can run the HPL?
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_24_1722994980.1476400263874"

------=_Part_24_1722994980.1476400263874
Content-Type: multipart/alternative; 
	boundary="----=_Part_25_434658764.1476400263875"

------=_Part_25_434658764.1476400263875
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Gregory,

We were testing with 5 nodes, 56 tasks.  We were using an nfs system 
connected to an EXT4 fs.  Could the problem be all those processes trying 
to load the same image at the same time from the same place?  I just ran 
the test again and it takes about 3 seconds on a bare metal run for the 
output to start displaying and 25 seconds with a singularity run. This was 
using the IMB benchmark, but my experience was similar with HPL (High 
Performance Linpack) and others.

The commands I used were:

srun -N5 -n56 -w node[4-6,8-9] --mpi=pmix run.it
srun -N5 -n56 -w node[4-6,8-9] --mpi=pmix singularity exec c7a.img run.it

run.it just sources the environment and executes the benchmark.

-Steve

On Wednesday, October 12, 2016 at 3:37:20 PM UTC-6, Gregory M. Kurtzer 
wrote:
>
> Hi Steve,
>
> 30 seconds to startup? How many nodes were you testing on, and what kind 
> of file system is the image sitting on?
>
>
>
> On Tue, Oct 11, 2016 at 1:50 PM, Steve Mehlberg <sg...@gmail.com 
> <javascript:>> wrote:
>
>> I have run HPL on a cluster (3-8 nodes) using singularity.  I used slurm 
>> to start several processes/containers on several nodes in my cluster for 
>> testing singularity.  I configured some short runs, and one run that took 
>> 3.5 hours.  My task was to compare bare metal runs with singularity runs.  
>> I compared cputime and elapsed time from slurm sacct.  When starting the 
>> singularity runs there is a slight delay (up to 30 seconds, I'm assuming 
>> for the containers to load) - if that time is removed then the singularity 
>> and bare metal runs are nearly identical.  With the start up time, the 3.5 
>> hour singularity run took ~0.5% more time than bare metal.
>>
>>
>> On Tuesday, October 11, 2016 at 8:07:19 AM UTC-6, zhenjin zhang wrote:
>>>
>>> Does the singularity can run the HPL?
>>>
>> -- 
>> You received this message because you are subscribed to the Google Groups 
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an 
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
>
> -- 
> Gregory M. Kurtzer
> HPC Systems Architect and Technology Developer
> Lawrence Berkeley National Laboratory HPCS
> University of California Berkeley Research IT
> Singularity Linux Containers (http://singularity.lbl.gov/)
> Warewulf Cluster Management (http://warewulf.lbl.gov/)
> GitHub: https://github.com/gmkurtzer, Twitter: 
> https://twitter.com/gmkurtzer
>

------=_Part_25_434658764.1476400263875
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Gregory,<br><br>We were testing with 5 nodes, 56 tasks.=C2=
=A0 We were using an nfs system connected to an EXT4 fs.=C2=A0 Could the pr=
oblem be all those processes trying to load the same image at the same time=
 from the same place?=C2=A0 I just ran the test again and it takes about 3 =
seconds on a bare metal run for the output to start displaying and 25 secon=
ds with a singularity run. This was using the IMB benchmark, but my experie=
nce was similar with HPL (High Performance Linpack) and others.<br><br>The =
commands I used were:<br><br>srun -N5 -n56 -w node[4-6,8-9] --mpi=3Dpmix ru=
n.it<br>srun -N5 -n56 -w node[4-6,8-9] --mpi=3Dpmix singularity exec c7a.im=
g run.it<br><br>run.it just sources the environment and executes the benchm=
ark.<br><br>-Steve<br><br>On Wednesday, October 12, 2016 at 3:37:20 PM UTC-=
6, Gregory M. Kurtzer wrote:<blockquote class=3D"gmail_quote" style=3D"marg=
in: 0;margin-left: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><d=
iv dir=3D"ltr">Hi Steve,<div><br></div><div>30 seconds to startup? How many=
 nodes were you testing on, and what kind of file system is the image sitti=
ng on?<div><br></div><div><br></div></div></div><div><br><div class=3D"gmai=
l_quote">On Tue, Oct 11, 2016 at 1:50 PM, Steve Mehlberg <span dir=3D"ltr">=
&lt;<a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"nyIY=
0YYCAwAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;=
;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">s=
g...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div=
 dir=3D"ltr">I have run HPL on a cluster (3-8 nodes) using singularity.=C2=
=A0 I used slurm to start several processes/containers on several nodes in =
my cluster for testing singularity.=C2=A0 I configured some short runs, and=
 one run that took 3.5 hours.=C2=A0 My task was to compare bare metal runs =
with singularity runs.=C2=A0 I compared cputime and elapsed time from slurm=
 sacct.=C2=A0 When starting the singularity runs there is a slight delay (u=
p to 30 seconds, I&#39;m assuming for the containers to load) - if that tim=
e is removed then the singularity and bare metal runs are nearly identical.=
=C2=A0 With the start up time, the 3.5 hour singularity run took ~0.5% more=
 time than bare metal.<div><div><br><br>On Tuesday, October 11, 2016 at 8:0=
7:19 AM UTC-6, zhenjin zhang wrote:<blockquote class=3D"gmail_quote" style=
=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"=
><div dir=3D"ltr">Does the singularity can run the HPL?<br></div></blockquo=
te></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
nyIY0YYCAwAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;=
http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\x3=
dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return tr=
ue;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%=
2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-=
iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a>)=
</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lbl=
.gov/" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;h=
ttp://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD\x=
26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true;"=
 onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwa=
rewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcV=
gBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><div=
>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" target=3D"_blank" re=
l=3D"nofollow" onmousedown=3D"this.href=3D&#39;https://www.google.com/url?q=
\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3d=
AFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.href=
=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtze=
r\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;=
;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"f=
ont-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurt=
zer" style=3D"font-size:12.8px" target=3D"_blank" rel=3D"nofollow" onmoused=
own=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwitt=
er.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKs=
NsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.goo=
gle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x=
3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:/=
/<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div></=
div></div></div></div>
</div>
</blockquote></div>
------=_Part_25_434658764.1476400263875--

------=_Part_24_1722994980.1476400263874--
