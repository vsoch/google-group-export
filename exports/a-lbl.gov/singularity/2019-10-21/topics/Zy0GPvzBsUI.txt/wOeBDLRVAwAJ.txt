Date: Fri, 29 Sep 2017 10:02:09 -0700 (PDT)
From: Aaron <hol...@colorado.edu>
To: singularity <singu...@lbl.gov>
Message-Id: <47b9a516-6aa6-4134-ba3d-e0cbfbf066a5@lbl.gov>
Subject: MPI and compiled applications
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_3475_1111439737.1506704529501"

------=_Part_3475_1111439737.1506704529501
Content-Type: multipart/alternative; 
	boundary="----=_Part_3476_1441160297.1506704529501"

------=_Part_3476_1441160297.1506704529501
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi,

I'm running into problems using MPI on our cluster. I'm using Singularity 
2.3.2, have a centos7 container with OpenMPI 2.1.0 installed, and am 
running on a RHEL 7 cluster with OpenMPI 2.1.0 as well. My problem is 
things compiled on our cluster link to GPFS libraries (which aren't 
available for install by default in a container). Thus I get the following 
problem when trying to run a complied MPI ring program in C:

-bash-4.2$ mpirun -np 2 singularity exec openmpi.img  ./mpi_ring           
  
./mpi_ring: error while loading shared libraries: libgpfs.so: cannot open 
shared object file: No such file or directory
./mpi_ring: error while loading shared libraries: libgpfs.so: cannot open 
shared object file: No such file or directory

If I run without singularity I have no troubles:
-bash-4.2$ mpirun -np 2 ./mpi_ring
0 done.
1 done.

Has anyone else run into a similar issue? My first thought was to install 
GPFS libraries into the container, but then I feel it loses portability as 
GPFS isn't everywhere. Should I modify the mpicc wrapper so it doesn't link 
GPFS? Is there a way to link the GPFS library on our cluster (located at 
/usr/lib64/libgpfs.so) from inside the container? Other ideas?

Thanks,
-Aaron

------=_Part_3476_1441160297.1506704529501
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi,<div><br></div><div>I&#39;m running into problems using=
 MPI on our cluster. I&#39;m using Singularity 2.3.2, have a centos7 contai=
ner with OpenMPI 2.1.0 installed, and am running on a RHEL 7 cluster with O=
penMPI 2.1.0 as well. My problem is things compiled on our cluster link to =
GPFS libraries (which aren&#39;t available for install by default in a cont=
ainer). Thus I get the following problem when trying to run a complied MPI =
ring program in C:</div><div><br></div><div><div>-bash-4.2$ mpirun -np 2 si=
ngularity exec openmpi.img =C2=A0./mpi_ring =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0=C2=A0</div><div>./mpi_ring: error while loading shared libraries=
: libgpfs.so: cannot open shared object file: No such file or directory</di=
v><div>./mpi_ring: error while loading shared libraries: libgpfs.so: cannot=
 open shared object file: No such file or directory</div></div><div><br></d=
iv><div>If I run without singularity I have no troubles:</div><div><div>-ba=
sh-4.2$ mpirun -np 2 ./mpi_ring</div><div>0 done.</div><div>1 done.</div></=
div><div><br></div><div>Has anyone else run into a similar issue? My first =
thought was to install GPFS=C2=A0libraries into the container, but then I f=
eel it loses portability as GPFS=C2=A0isn&#39;t everywhere. Should I modify=
 the mpicc wrapper so it doesn&#39;t link GPFS? Is there a way to link the =
GPFS library on our cluster (located at /usr/lib64/libgpfs.so) from inside =
the container? Other ideas?</div><div><br></div><div>Thanks,</div><div>-Aar=
on</div></div>
------=_Part_3476_1441160297.1506704529501--

------=_Part_3475_1111439737.1506704529501--
