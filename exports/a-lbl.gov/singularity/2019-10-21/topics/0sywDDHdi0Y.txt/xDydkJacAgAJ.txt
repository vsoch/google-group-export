X-Received: by 10.237.48.131 with SMTP id 3mr18998251qtf.9.1466606518098;
        Wed, 22 Jun 2016 07:41:58 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.107.149 with SMTP id v143ls613381itc.29.gmail; Wed, 22 Jun
 2016 07:41:57 -0700 (PDT)
X-Received: by 10.36.84.68 with SMTP id t65mr14803973ita.7.1466606517496;
        Wed, 22 Jun 2016 07:41:57 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id z25si212185pff.218.2016.06.22.07.41.57
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 22 Jun 2016 07:41:57 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) client-ip=74.125.82.72;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2C2AABlompXgEhSfUpdhBRuDwaDNqYChHCHBIUBgXoXAQaFeQKBJQc4FAEBAQEBAQESAQEJDQkJHyYLhEwBAQEDARIRKxkXCwsLDSoCAiEBDwMBBQELEQYIBwQBHAQBh3QDDwgFpUuBMT4xizuMQA2DdQEBCAEBAQEBAR0ECwWKZIJDgU8RAQaDF4JaBYhyhXqEXoR/NAGGB4YrgXqBaU6EBYhniA2GMBIegQ8PD4RGHDIHiTWBNQEBAQ
X-IronPort-AV: E=Sophos;i="5.26,509,1459839600"; 
   d="scan'208,217";a="27759796"
Received: from mail-wm0-f72.google.com ([74.125.82.72])
  by fe3.lbl.gov with ESMTP; 22 Jun 2016 07:41:55 -0700
Received: by mail-wm0-f72.google.com with SMTP id r190so4482711wmr.0
        for <singu...@lbl.gov>; Wed, 22 Jun 2016 07:41:55 -0700 (PDT)
X-Gm-Message-State: ALyK8tKJVyoqNLk1Su1OVcdNAwt5pkbybA8v6OHeVLYCt2JEjEAbL5Z7y5QKa/RIE54EZtBzRUgnORRT81rNHqP6ToKeYtkTakJMJkgaakn/aJQ5cjAQy3OGc0obWGPDv4cVb4rj7JSoOQQtMI8EY2G/D3k=
X-Received: by 10.25.87.130 with SMTP id l124mr9919840lfb.170.1466606514643;
        Wed, 22 Jun 2016 07:41:54 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.25.87.130 with SMTP id l124mr9919832lfb.170.1466606514259;
 Wed, 22 Jun 2016 07:41:54 -0700 (PDT)
Received: by 10.25.214.158 with HTTP; Wed, 22 Jun 2016 07:41:54 -0700 (PDT)
In-Reply-To: <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
	<CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
	<3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
	<CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
	<1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov>
Date: Wed, 22 Jun 2016 07:41:54 -0700
Message-ID: <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: "singu...@lbl.gov" <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a1141f9ee4ed50b0535def00c

--001a1141f9ee4ed50b0535def00c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Raimon,

Sorry I wasn't clear. I am not yet at my computer and thinking while typing
on an iPhone hinders my mental processes. Lol

If I understand your example properly, you have a docker or VM
infrastructure already set up and you are invoking the mpirun commands from
within the virtual environment. Singularity works on a very different
premis because integrating a virtual cluster into an existing cluster and
scheduling system is a mess.

So starting from the physical nodes, which already have access to all other
nodes in the cluster, and already scheduled properly, and in direct access
to optimized hardware and file systems.... You call mpirun.

The mpirun command will take the standard format as you illustrated with
the following change to call Singularity inline:

$ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img
trace.sh bt.C.4

This assumes the following:

1. The container image which contains the program's you want to run is at
~/container.img and accessible at this path on all nodes referenced in
hosts.txt
2. The hosts.txt references other physical nodes you want to run on
3. The executables trace.sh and bt.C.4 are both inside the container and in
a standard path

In this case we are performing one execution and MPI + singularity is
managing all of the communication between processes, nodes and containers.
Also it is now using any optimized hardware (eg. Infiniband) and existing
high performance file systems (which should not be accessible via a
virtualized or Docker'ized cluster for security reasons).

This way is actually MUCH simpler then what you are proposing because there
is no need to manage any virtual nodes, virtual networks, or resource
manager hacks. It really is as easy as just running any other MPI process
on an existing cluster.

Hope that helps better!



On Wednesday, June 22, 2016, Raimon Bosch <raimo...@gmail.com> wrote:

>
>
> Hi Gregory,
>
> I'm not sure if I would achieve the same with your commands. In an
> environment based on dockers or virtual machines we would do something li=
ke
> this [non applicable to Singularity]:
>
> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh ./bt.C=
.4
>
> where hosts.txt* is:
>
> >vm-ip-01-on-*host01* slots=3D2
> >vm-ip-01-on-*host02* slots=3D2
>
> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>
> and trace.sh is:
>
> >#!/bin/bash
> >
> >export EXTRAE_HOME=3D/opt/extrae/
> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
> >
> >## Run the desired program
> >$*
>
> As you see we only perform one execution and OpenMPI transparently manage=
s
> communication between containers or virtual machines. This command would
> work well rather VMs are on the same host or not.
>
> What I understand from your response is that now we should execute OpenMP=
I
> on each host and then merge results manually. I don't know yet how to do
> this merge step or if it is any way to centralize everything like I would
> do with VMs.
>
> Thanks in advance,
>
> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Kurt=
zer
> escribi=C3=B3:
>>
>> Hi Raimon,
>>
>> The quick answer is you have mpirun handle that as you would normally
>> where the container file lives on a shared file system:
>>
>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>
>> Let the MPI outside the container launch the singularity container on
>> each host as it would normally launch any MPI program. Then it will call
>> Singulairty and Singularity will launch the MPI program inside the
>> container on each of your hosts/servers.
>>
>> Hope that helps!
>>
>>
>>
>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>
>>>
>>> Hi Gregory,
>>>
>>> Thank you for your answer. One of our experiments needs to run OpenMPI
>>> among several servers. This means that we should put one of our contain=
ers
>>> in host01, another in host02 and another in host03 and collect the resu=
lts.
>>>
>>> How can I do this execution in parallel if I need to communicate with
>>> more than one server?
>>>
>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer
>>> escribi=C3=B3:
>>>>
>>>> Hi Raimon,
>>>>
>>>> The communication model of a Singularity container is very different
>>>> from that of a Docker implementation. This is because Docker for all
>>>> practical purposes emulates a virtual machine as each container has it=
's
>>>> own IP address and thus it's own ssh server. It also carries its own s=
et of
>>>> complexities, for example networks need to be segregated/VLan'ed, DNS/=
host
>>>> resolution needs to be dynamic and passed down to the containers (so t=
hey
>>>> can reach each other), ssh daemons and other process running inside th=
e
>>>> containers, management via an existing scheduling system, and the list=
 goes
>>>> on and on.
>>>>
>>>> Think of it this way, Singularity does not do any of that... It runs a
>>>> program within the container as if it were running on the host itself,=
 so
>>>> to communicate between containers is as easy as communicating between
>>>> programs. So for MPI, it would happen with the MPI on the physical hos=
t
>>>> (outside the container) invoking the container subsystem which then in=
vokes
>>>> the MPI programs within the container and the MPI programs within the
>>>> container communicate back to the MPI (orted) outside the container on=
 the
>>>> host to get access to the host resources. In this model all available
>>>> resources and infrastructure can be leveraged at full bandwidth by the
>>>> contained processes and all of the aforementioned complexities akin to
>>>> running on a virtualized mini-cluster are circumvented.
>>>>
>>>> There is additional information I have written at:
>>>>
>>>> http://singularity.lbl.gov/#hpc
>>>>
>>>> That page is still coming along, and needs more information still but
>>>> if you have any questions, comments or change proposals please let us =
know!
>>>>
>>>> Thanks and hope that helps!
>>>>
>>>>
>>>>
>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>
>>>> wrote:
>>>>
>>>>>
>>>>>
>>>>> Hi,
>>>>>
>>>>> We are trying to run experiments using singularity containers. The
>>>>> idea is to run OpenMPI among several containers and check performance
>>>>> results.
>>>>>
>>>>> How can I communicate with another container? In docker this is clear
>>>>> because every container gets an assigned IP and you can ping there, b=
ut
>>>>> what is the situation in the case of singularity? Is it possible to a=
ssign
>>>>> an IP to each container? Can I connect via ssh to them?
>>>>>
>>>>> Thanks in advance,
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>


--=20
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a1141f9ee4ed50b0535def00c
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Raimon,<div><br></div><div>Sorry I wasn&#39;t clear. I am not yet at my =
computer and thinking while typing on an iPhone hinders my mental processes=
. Lol</div><div><br></div><div>If I understand your example properly, you h=
ave a docker or VM infrastructure already set up and you are invoking the m=
pirun commands from within the virtual environment. Singularity works on a =
very different premis because integrating a virtual cluster into an existin=
g cluster and scheduling system is a mess.</div><div><br></div><div>So star=
ting from the physical nodes, which already have access to all other nodes =
in the cluster, and already scheduled properly, and in direct access to opt=
imized hardware and file systems.... You call mpirun.=C2=A0</div><div><br><=
/div><div>The mpirun command will take the standard format as you illustrat=
ed with the following change to call Singularity inline:</div><div><br></di=
v><div>$ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img=
 trace.sh bt.C.4</div><div><br></div><div>This assumes the following:</div>=
<div><br></div><div>1. The container image which contains the program&#39;s=
 you want to run is at ~/container.img and accessible at this path=C2=A0on =
all nodes referenced in hosts.txt</div><div>2. The hosts.txt references oth=
er physical nodes you want to run on</div><div>3. The executables trace.sh =
and bt.C.4 are both inside the container and in a standard path</div><br><d=
iv>In this case we are performing one execution and MPI + singularity is ma=
naging all of the communication between processes, nodes and containers. Al=
so it is now using any optimized hardware (eg. Infiniband)=C2=A0and existin=
g high performance=C2=A0file systems (which should not be accessible via a =
virtualized or Docker&#39;ized=C2=A0cluster for security reasons).</div><di=
v><br></div><div>This way is actually MUCH simpler then what you are propos=
ing because there is no need to manage any virtual nodes, virtual networks,=
 or resource manager hacks. It really is as easy as just running any other =
MPI=C2=A0process on an existing cluster.=C2=A0</div><div><br></div><div>Hop=
e that helps better!</div><div><br></div><div><span></span><br><br>On Wedne=
sday, June 22, 2016, Raimon Bosch &lt;<a href=3D"mailto:raimo...@gmail.com"=
>raimo...@gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div d=
ir=3D"ltr"><br><br>Hi Gregory,<br><br>I&#39;m not sure if I would achieve t=
he same with your commands. In an environment based on dockers or virtual m=
achines we would do something like this [non applicable to Singularity]:<br=
><br>&gt; cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile hosts.txt ./t=
race.sh ./bt.C.4<br><br>where hosts.txt* is:<br><br>&gt;vm-ip-01-on-<b>host=
01</b> slots=3D2<br>&gt;vm-ip-01-on-<b>host02</b> slots=3D2<br><br>* vm-ip-=
XX-on-hostXX<b> </b>are IPs i.e. 172.100.60.XX<br><br>and trace.sh is:<br><=
br>&gt;#!/bin/bash<br>&gt;<br>&gt;export EXTRAE_HOME=3D/opt/extrae/<br>&gt;=
export EXTRAE_CONFIG_FILE=3D/extrae.xml<br>&gt;export LD_PRELOAD=3D${EXTRAE=
_HOME}/lib/libmpitrace.so<br>&gt;<br>&gt;## Run the desired program<br>&gt;=
$*<br><br>As you see we only perform one execution and OpenMPI transparentl=
y manages communication between containers or virtual machines. This comman=
d would work well rather VMs are on the same host or not.<br><br>What I und=
erstand from your response is that now we should execute OpenMPI on each ho=
st and then merge results manually. I don&#39;t know yet how to do this mer=
ge step or if it is any way to centralize everything like I would do with V=
Ms.<br><br>Thanks in advance,<br><br>El mi=C3=A9rcoles, 22 de junio de 2016=
, 14:42:54 (UTC+2), Gregory M. Kurtzer  escribi=C3=B3:<blockquote class=3D"=
gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid=
;padding-left:1ex">Hi Raimon,<div><br></div><div>The quick answer is you ha=
ve mpirun handle that as you would normally where the container file lives =
on a shared file system:</div><div><br></div><div>$ mpirun singularity exec=
 ~/container.img mpi_prog_in_container</div><div><br></div><div>Let the MPI=
 outside the container launch the singularity container on each host as it =
would normally launch any MPI program. Then it will call Singulairty and Si=
ngularity will launch the MPI program inside the container on each of your =
hosts/servers.=C2=A0</div><div><br></div><div>Hope that helps!</div><div><b=
r></div><div><span></span><br><br>On Wednesday, June 22, 2016, Raimon Bosch=
 &lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt; wrote:<br><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pa=
dding-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<br><br>Thank you for your =
answer. One of our experiments needs to run OpenMPI among several servers. =
This means that we should put one of our containers in host01, another in h=
ost02 and another in host03 and collect the results. <br><br>How can I do t=
his execution in parallel if I need to communicate with more than one serve=
r?<br><br>El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurt=
zer  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0;marg=
in-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
>Hi Raimon,<div><br></div><div>The communication model of a Singularity con=
tainer is very different from that of a Docker implementation. This is beca=
use Docker for all practical purposes emulates a virtual machine as each co=
ntainer has it&#39;s own IP address and thus it&#39;s own ssh server. It al=
so carries its own set of complexities, for example networks need to be seg=
regated/VLan&#39;ed, DNS/host resolution needs to be dynamic and passed dow=
n to the containers (so they can reach each other), ssh daemons and other p=
rocess running inside the containers, management via an existing scheduling=
 system, and the list goes on and on.</div><div><br></div><div>Think of it =
this way, Singularity does not do any of that... It runs a program within t=
he container as if it were running on the host itself, so to communicate be=
tween containers is as easy as communicating between programs. So for MPI, =
it would happen with the MPI on the physical host (outside the container) i=
nvoking the container subsystem which then invokes the MPI programs within =
the container and the MPI programs within the container communicate back to=
 the MPI (orted) outside the container on the host to get access to the hos=
t resources. In this model all available resources and infrastructure can b=
e leveraged at full bandwidth by the contained processes and all of the afo=
rementioned complexities akin to running on a virtualized mini-cluster are =
circumvented.</div><div><br></div><div>There is additional information I ha=
ve written at:</div><div><br></div><div><a href=3D"http://singularity.lbl.g=
ov/#hpc" rel=3D"nofollow" target=3D"_blank">http://singularity.lbl.gov/#hpc=
</a><br></div><div><br></div><div>That page is still coming along, and need=
s more information still but if you have any questions, comments or change =
proposals please let us know!</div><div><br></div><div>Thanks and hope that=
 helps!</div><div><br></div><div><br></div></div><div><br><div class=3D"gma=
il_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <span dir=3D"ltr">&=
lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt;</span> wrote:<br><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi,<br><br>We are trying to r=
un experiments using singularity containers. The idea is to run OpenMPI amo=
ng several containers and check performance results. <br><br>How can I comm=
unicate with another container? In docker this is clear because every conta=
iner gets an assigned IP and you can ping there, but what is the situation =
in the case of singularity? Is it possible to assign an IP to each containe=
r? Can I connect via ssh to them?<br><br>Thanks in advance,<span><font colo=
r=3D"#888888"><br></font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computi=
ng Services (HPCS)<br>University of California<br>Lawrence Berkeley Nationa=
l Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singu...@lbl.gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>

--001a1141f9ee4ed50b0535def00c--
