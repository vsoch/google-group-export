X-Received: by 10.31.238.196 with SMTP id m187mr16452908vkh.7.1468607302392;
        Fri, 15 Jul 2016 11:28:22 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.93.198 with SMTP id w189ls210746ita.0.canary; Fri, 15 Jul
 2016 11:28:22 -0700 (PDT)
X-Received: by 10.66.167.103 with SMTP id zn7mr33777037pab.149.1468607301861;
        Fri, 15 Jul 2016 11:28:21 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id b6si10334483pal.57.2016.07.15.11.28.21
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 15 Jul 2016 11:28:21 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.69 as permitted sender) client-ip=209.85.215.69;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.69 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EyAQBHKolXf0XXVdEZAzgIgnCBJHwGgzaBDKgSjBqBe4YaAoEoBzgUAQEBAQEBAQMPAQEJCwsJHzGEXAEBBAESCAEIKzALCQILDRYBCQEJAgIhAQ8DAQUBCxEGCAcEARwEAYd0Aw8IBZQBKI9CgTE+MYs7iXoNhBoBAQgBAQEBASIQimeCQ4FPDAUBZAGCOII9HQWGCYIFCAdfhQx1P4QkhQ00AYtxD0OCF4FrF4dxhUKGXYFFhjoSHoEPHoJBHIFsHDIHhQoNF4EeAQEB
X-IronPort-AV: E=Sophos;i="5.28,369,1464678000"; 
   d="scan'208,217";a="29787129"
Received: from mail-lf0-f69.google.com ([209.85.215.69])
  by fe4.lbl.gov with ESMTP; 15 Jul 2016 11:28:17 -0700
Received: by mail-lf0-f69.google.com with SMTP id f199so13075502lfg.2
        for <singu...@lbl.gov>; Fri, 15 Jul 2016 11:28:18 -0700 (PDT)
X-Gm-Message-State: ALyK8tKi5mkU6voVIziP4ai8zN9fZTIQzKFRoutcVWN9ZWDjVcUK40DcECa+0IE5bcb6q/BRHx3j3nhagwa7CoDkQDhUcZ6PmVHY7KQbU8SS5WS8jFNDOB+bBBFdrEAN3wMtSn7C3nS+MDBz4J2nCeLk58A=
X-Received: by 10.25.87.130 with SMTP id l124mr11401046lfb.170.1468607297026;
        Fri, 15 Jul 2016 11:28:17 -0700 (PDT)
X-Received: by 10.25.87.130 with SMTP id l124mr11401032lfb.170.1468607296621;
 Fri, 15 Jul 2016 11:28:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.158 with HTTP; Fri, 15 Jul 2016 11:28:15 -0700 (PDT)
In-Reply-To: <8d23e9ce-1ce5-4ddb-ab81-f3aafdec4577@lbl.gov>
References: <03a19fb0-27ce-43c4-9400-8e58cf726500@lbl.gov> <CAN7etTwRbSe1MMh9wdQAMYoKVJTb_SGJeHto+WrZ=aU7NoBmhQ@mail.gmail.com>
 <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov> <CAN7etTyqGkWy1P57-cVJgyru5BT_DvnhwDzLe1p38BV8z_PPww@mail.gmail.com>
 <66b82c74-2778-44f2-ae5c-87e01ec8885d@lbl.gov> <CAN7etTz09uzfP-NpZ3-+HnijfrC+u+=pOuBQDTShXG+unxgOVg@mail.gmail.com>
 <c4e5864c-cbaa-4269-9522-db61d63d7cff@lbl.gov> <CAN7etTztBSE1YXY3etq9ipMNnPSKh2Eatz5i-QQOH=ecdNDVCg@mail.gmail.com>
 <0686e644-e7d6-45d7-a371-bf17bead57a4@lbl.gov> <CAN7etTy7cLFaG_NK8-izwc4V-WiU6tsGqNwyVY79TPND9eZnrw@mail.gmail.com>
 <f942d15c-1e7f-401a-8f6b-01ecfec1c7e9@lbl.gov> <CAN7etTz8Sj3CvHsn6ywyLVxfk3g5+kPy021sBbvXCZTtjsyi4w@mail.gmail.com>
 <8d23e9ce-1ce5-4ddb-ab81-f3aafdec4577@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Fri, 15 Jul 2016 11:28:15 -0700
Message-ID: <CAN7etTxyzBC_VSa8Zv6RVEe1C5gETyaMJM=Dkzej94jWaMnbwA@mail.gmail.com>
Subject: Re: [Singularity] Image is locked by another process
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a1141f9ee3ae7f70537b0c831

--001a1141f9ee3ae7f70537b0c831
Content-Type: text/plain; charset=UTF-8

Hi Steve,

I think I have fixed the initial error you got regarding being unable to
create the sessiondir. The second issue regarding file locks, I am
debugging now. I'll let you know shortly when to retest.

Thanks!



On Fri, Jul 15, 2016 at 11:26 AM, Steve Mehlberg <sgmeh...@gmail.com>
wrote:

> The problem only arises when there is an error.
> The tmp directory is different on each machine, it is not shared or an NFS
> mount.  Is that a problem?
>
> Steve
>
> On Friday, July 15, 2016 at 9:51:55 AM UTC-6, Gregory M. Kurtzer wrote:
>
>>
>> On Fri, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <sg...@gmail.com>
>> wrote:
>>
>>> Hello,
>>>
>>> I went back and recreated everything from scratch using v2.1.  When I
>>> moved the image file to the mach1 and mach2 nodes I used tar with the -S
>>> option.  I used /tmp for the image files.  After everything was set I ran
>>> mpirun on each individual system (only starting jobs on that system) with
>>> no errors.  For some reason when I ran mpirun from mach0 and asked for jobs
>>> to be run on mach0 and another system I get the error.  The mpirun
>>> complains about not being able to create a file (already exists) and
>>> hangs.  The singularity image file on mach0 and the other system selected
>>> are locked with pids that are no longer running.
>>>
>>> Saying all that, I decided to go back and put --debug on the singularity
>>> command in the mpirun to get you better diagnostics.  I rebooted mach0 and
>>> mach2, then reran the command and it worked correctly.  I then removed the
>>> --debug command and it worked.  I added mach1 to the mix and everything
>>> worked fine.  What? I guess the bottom line is that if the mpirun aborts
>>> (and it seems to hang) then you can get this condition.
>>>
>>
>> So does the problem seem to arise only when there is a failure on one of
>> the processes, and MPI kills the remaining processes?
>>
>> Ralph, do you know how OMPI kills the leftover processes in a job abort
>> and what signal it uses?
>>
>> (additional comments inline)
>>
>>
>>> Steve
>>>
>>>
>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 2 -H mach2
>>> singularity exec /tmp/c7new.img /usr/bin/ring
>>> Process 1 exiting
>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>> Process 0 sent to 1
>>> Process 0 decremented value: 9
>>> Process 0 decremented value: 8
>>> Process 0 decremented value: 7
>>> Process 0 decremented value: 6
>>> Process 0 decremented value: 5
>>> Process 0 decremented value: 4
>>> Process 0 decremented value: 3
>>> Process 0 decremented value: 2
>>> Process 0 decremented value: 1
>>> Process 0 decremented value: 0
>>> Process 0 exiting
>>>
>>
>> Is /tmp NFS mounted or shared?
>>
>>
>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 4 -H mach0,mach2
>>> singularity exec /tmp/c7new.img /usr/bin/ring
>>> ERROR  : Could not create directory
>>> /tmp/.singularity-session-0.64768.158221: File exists
>>> ERROR  : Failed creating session directory:
>>> /tmp/.singularity-session-0.64768.158221
>>> -------------------------------------------------------
>>> Primary job  terminated normally, but 1 process returned
>>> a non-zero exit code. Per user-direction, the job has been aborted.
>>> -------------------------------------------------------
>>> ^CKilled by signal 2.
>>> mpirun: abort is already in progress...hit ctrl-c again to forcibly
>>> terminate
>>>
>>
>> Ahhhhh, I think you may have found a race condition in Singularity which
>> killed one of the processes. Then the MPI killed the rest.
>>
>>
>>> [root@mach0 ompi]# singularity shell -w /tmp/c7new.img
>>> ERROR  : Image is locked by another process
>>> [root@mach0 ompi]# lslocks
>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>> crond             606 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>> master            820 FLOCK  33B WRITE 0     0   0
>>> /var/spool/postfix/pid/master.pid
>>> master            820 FLOCK  33B WRITE 0     0   0
>>> /var/lib/postfix/master.lock
>>> slurmctld         924 POSIX   4B WRITE 0     0   0 /run/slurmctld.pid
>>> slurmd           1103 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
>>> (unknown)        2477 FLOCK   0B READ  0     0   0 /
>>> [root@mach0 ompi]# ls /proc/2477
>>> ls: cannot access /proc/2477: No such file or directory
>>>
>>
>> OK, that is weird. If process 2477 is dead, there should be no active
>> lock. I would guess this might be a kernel bug? What distribution and
>> kernel are you running?
>>
>>
>>> [root@mach0 ompi]# ssh mach2
>>> Last login: Fri Jul 15 07:52:56 2016 from mach0
>>> [root@mach2 ~]# singularity shell -w /tmp/c7new.img
>>> ERROR  : Image is locked by another process
>>> [root@mach2 ~]# lslocks
>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>> master            851 FLOCK  33B WRITE 0     0   0
>>> /var/spool/postfix/pid/master.pid
>>> master            851 FLOCK  33B WRITE 0     0   0
>>> /var/lib/postfix/master.lock
>>> (unknown)       10110 FLOCK   0B READ  0     0   0 /
>>> crond             668 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>> slurmd           1610 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
>>> [root@mach2 ~]# ls /proc/10110
>>> ls: cannot access /proc/10110: No such file or directory
>>>
>>
>> Very weird. Seems both nodes have the same issue..?
>>
>> I wonder if I can replicate this by kill -9 the outer parent of a
>> Singularity run. I will test when I get to my development box.
>>
>>
>>>
>>> Good Run:
>>>
>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 6 -H
>>> mach0,mach1,mach2 singularity exec /tmp/c7new.img /usr/bin/ring
>>> Process 0 sending 10 to 1, tag 201 (6 processes in ring)
>>> Process 0 sent to 1
>>> Process 1 exiting
>>> Process 0 decremented value: 9
>>> Process 0 decremented value: 8
>>> Process 0 decremented value: 7
>>> Process 0 decremented value: 6
>>> Process 0 decremented value: 5
>>> Process 0 decremented value: 4
>>> Process 0 decremented value: 3
>>> Process 0 decremented value: 2
>>> Process 0 decremented value: 1
>>> Process 0 decremented value: 0
>>> Process 0 exiting
>>> Process 2 exiting
>>> Process 5 exiting
>>> Process 3 exiting
>>> Process 4 exiting
>>>
>>
>> So as long as we don't crash (or get killed), all seems fine...
>> Interesting.
>>
>> I will try and replicate the error condition.
>>
>> Greg
>>
>>
>>>
>>>
>>>
>>> On Thursday, July 14, 2016 at 3:56:48 PM UTC-6, Gregory M. Kurtzer wrote:
>>>
>>>> I am wondering if some aspect of the sparse file has caused this.
>>>> Sparse files are weird, and do require some attention when copying around.
>>>> With that said, I do not think it should ever corrupt an image, but it
>>>> might take along garbage data with it. One way to retest this is to go back
>>>> to the original image, and tar it before sending using the -S/--sparse tar
>>>> option when packaging it up. Then bring it to the other test system and see
>>>> if it still does that.
>>>>
>>>> Aside from that, I'm a bit confused as to what is happening. There
>>>> should be no issue with 2.0 vs. 2.1, but it might be worth checking with
>>>> some images also created with 2.1 on the same target system where you are
>>>> seeing the problem.
>>>>
>>>> On Thu, Jul 14, 2016 at 2:46 PM, Steve Mehlberg <sg...@gmail.com>
>>>> wrote:
>>>>
>>>>> It is very possible that I did copy it from another system - maybe
>>>>> created it with singularity 2.0 and now running it with 2.1??  I will start
>>>>> over and recreate from scratch and see if I still have the problem.
>>>>>
>>>>> Sorry, I checked there is no /proc/2299
>>>>>
>>>>> The only way I've found to get the file unlocked is to reboot.  Not an
>>>>> issue with the little VM.
>>>>>
>>>>> Steve
>>>>>
>>>>> On Thursday, July 14, 2016 at 3:30:12 PM UTC-6, Gregory M. Kurtzer
>>>>> wrote:
>>>>>
>>>>>> Very odd...
>>>>>>
>>>>>> I'm concerned about the Buffer I/O errors. It is almost like the
>>>>>> image itself has a problem with it. Did you copy this image from another
>>>>>> system? I wonder if it is the sparseness...
>>>>>>
>>>>>> Does the directory exist for /proc/2299/ ?
>>>>>>
>>>>>> Can you kill it with a -9?
>>>>>>
>>>>>> On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlberg <sg...@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Seems that process isn't running any more.  I did find this in the
>>>>>>> /var/log/messages file:
>>>>>>>
>>>>>>> [root@mach0 ~]# cat /var/log/messages | grep 2299
>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> Command=exec,
>>>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1,
>>>>>>> logical block 22299
>>>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1,
>>>>>>> logical block 22990
>>>>>>> ...
>>>>>>>
>>>>>>> [root@mach0 ~]# ps -ef |grep 2299
>>>>>>> root     10545  2002  0 14:15 pts/0    00:00:00 grep --color=auto
>>>>>>> 2299
>>>>>>> [root@mach0 ~]# ps |grep 2299
>>>>>>>
>>>>>>> On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer
>>>>>>> wrote:
>>>>>>>
>>>>>>>> What is running at PID 2299?
>>>>>>>>
>>>>>>>> On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <sg...@gmail.com
>>>>>>>> > wrote:
>>>>>>>>
>>>>>>>>> Ok, I see my error with shell vs exec.  I'm running on a VM with
>>>>>>>>> my own space, no NFS involved.  Here is the debug command run and lsofl
>>>>>>>>>
>>>>>>>>> [root@mach0 ~]# df -h
>>>>>>>>> Filesystem             Size  Used Avail Use% Mounted on
>>>>>>>>> /dev/mapper/vg1-lv001  5.7G  4.3G  1.1G  80% /
>>>>>>>>> devtmpfs               911M     0  911M   0% /dev
>>>>>>>>> tmpfs                  920M     0  920M   0% /dev/shm
>>>>>>>>> tmpfs                  920M   41M  880M   5% /run
>>>>>>>>> tmpfs                  920M     0  920M   0% /sys/fs/cgroup
>>>>>>>>> /dev/vda1              190M  110M   67M  63% /boot
>>>>>>>>> /dev/mapper/vg1-lv002   12G  203M   11G   2% /var
>>>>>>>>> tmpfs                  184M     0  184M   0% /run/user/0
>>>>>>>>> [root@mach0 ~]# singularity --debug shell -w c7
>>>>>>>>> enabling debugging
>>>>>>>>> ending argument loop
>>>>>>>>> Exec'ing: /usr/local/libexec/singularity/cli/shell.exec -w+ '[' -f
>>>>>>>>> /usr/local/etc/singularity/init ']'
>>>>>>>>> + . /usr/local/etc/singularity/init
>>>>>>>>> ++ unset module
>>>>>>>>> ++
>>>>>>>>> PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin
>>>>>>>>> ++ HISTFILE=/dev/null
>>>>>>>>> ++ export PATH HISTFILE
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>> + true
>>>>>>>>> + case ${1:-} in
>>>>>>>>> + shift
>>>>>>>>> + SINGULARITY_WRITABLE=1
>>>>>>>>> + export SINGULARITY_WRITABLE
>>>>>>>>> + true
>>>>>>>>> + case ${1:-} in
>>>>>>>>> + break
>>>>>>>>> + '[' -z c7 ']'
>>>>>>>>> + SINGULARITY_IMAGE=c7
>>>>>>>>> + export SINGULARITY_IMAGE
>>>>>>>>> + shift
>>>>>>>>> + exec /usr/local/libexec/singularity/sexec
>>>>>>>>> VERBOSE [U=0,P=8045]
>>>>>>>>> message.c:52:init()                        : Set messagelevel to: 5
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:127:main()                         : Gathering and caching user
>>>>>>>>> info.
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:43:get_user_privs()            : Called get_user_privs(struct
>>>>>>>>> s_privinfo *uinfo)
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:54:get_user_privs()            : Returning
>>>>>>>>> get_user_privs(struct s_privinfo *uinfo) = 0
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:134:main()                         : Checking if we can escalate
>>>>>>>>> privs properly.
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:61:escalate_privs()            : Called escalate_privs(void)
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:73:escalate_privs()            : Returning escalate_privs(void)
>>>>>>>>> = 0
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:141:main()                         : Setting privs to calling user
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:79:drop_privs()                : Called drop_privs(struct
>>>>>>>>> s_privinfo *uinfo)
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:87:drop_privs()                : Dropping privileges to GID =
>>>>>>>>> '0'
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:93:drop_privs()                : Dropping privileges to UID =
>>>>>>>>> '0'
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:103:drop_privs()               : Confirming we have correct GID
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:109:drop_privs()               : Confirming we have correct UID
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> privilege.c:115:drop_privs()               : Returning drop_privs(struct
>>>>>>>>> s_privinfo *uinfo) = 0
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:146:main()                         : Obtaining user's homedir
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:150:main()                         : Obtaining file descriptor to
>>>>>>>>> current directory
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:155:main()                         : Getting current working
>>>>>>>>> directory path string
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:161:main()                         : Obtaining SINGULARITY_COMMAND
>>>>>>>>> from environment
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:168:main()                         : Obtaining SINGULARITY_IMAGE
>>>>>>>>> from environment
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:174:main()                         : Checking container image is a
>>>>>>>>> file: c7
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:180:main()                         : Building configuration file
>>>>>>>>> location
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:183:main()                         : Config location:
>>>>>>>>> /usr/local/etc/singularity/singularity.conf
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:185:main()                         : Checking Singularity
>>>>>>>>> configuration is a file: /usr/local/etc/singularity/singularity.conf
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:191:main()                         : Checking Singularity
>>>>>>>>> configuration file is owned by root
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:197:main()                         : Opening Singularity
>>>>>>>>> configuration file
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:210:main()                         : Checking Singularity
>>>>>>>>> configuration for 'sessiondir prefix'
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> config_parser.c:47:config_get_key_value()  : Called
>>>>>>>>> config_get_key_value(fp, sessiondir prefix)
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> config_parser.c:66:config_get_key_value()  : Return
>>>>>>>>> config_get_key_value(fp, sessiondir prefix) = NULL
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> file.c:48:file_id()                        : Called file_id(c7)
>>>>>>>>> VERBOSE [U=0,P=8045]
>>>>>>>>> file.c:58:file_id()                        : Generated file_id:
>>>>>>>>> 0.64768.26052
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> file.c:60:file_id()                        : Returning file_id(c7) =
>>>>>>>>> 0.64768.26052
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:217:main()                         : Set sessiondir to:
>>>>>>>>> /tmp/.singularity-session-0.64768.26052
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:221:main()                         : Set containername to: c7
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:223:main()                         : Setting loop_dev_* paths
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> config_parser.c:47:config_get_key_value()  : Called
>>>>>>>>> config_get_key_value(fp, container dir)
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> config_parser.c:58:config_get_key_value()  : Return
>>>>>>>>> config_get_key_value(fp, container dir) = /var/singularity/mnt
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:232:main()                         : Set image mount path to:
>>>>>>>>> /var/singularity/mnt
>>>>>>>>> LOG     [U=0,P=8045]
>>>>>>>>> sexec.c:234:main()                         : Command=shell, Container=c7,
>>>>>>>>> CWD=/root, Arg1=(null)
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:247:main()                         : Set prompt to: Singularity.c7>
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:249:main()                         : Checking if we are opening
>>>>>>>>> image as read/write
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:264:main()                         : Opening image as read/write
>>>>>>>>> only: c7
>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>> sexec.c:271:main()                         : Setting exclusive lock on file
>>>>>>>>> descriptor: 6
>>>>>>>>> ERROR   [U=0,P=8045]
>>>>>>>>> sexec.c:273:main()                         : Image is locked by another
>>>>>>>>> process
>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>> [root@mach0 ~]# lslocks
>>>>>>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>>>>>>> crond             601 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>>>>>>> master            826 FLOCK  33B WRITE 0     0   0
>>>>>>>>> /var/spool/postfix/pid/master.pid
>>>>>>>>> master            826 FLOCK  33B WRITE 0     0   0
>>>>>>>>> /var/lib/postfix/master.lock
>>>>>>>>> lvmetad           478 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>>>>>>>> slurmctld         940 POSIX   4B WRITE 0     0   0
>>>>>>>>> /run/slurmctld.pid
>>>>>>>>> slurmd            966 POSIX   4B WRITE 0     0   0 /run/slurmd.pid
>>>>>>>>> (unknown)        2299 FLOCK   0B READ  0     0   0 /
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gregory M. Kurtzer
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <
>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>>> Gregory,
>>>>>>>>>>>
>>>>>>>>>>> Thanks for the quick suggestions.  There don't seem to be any
>>>>>>>>>>> processes attached to the container and I can't seem to run other commands
>>>>>>>>>>> (in read mode). I'm not sure what's going on.
>>>>>>>>>>>
>>>>>>>>>>> Just lazy with root right now, I need to create a normal uid.
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Ahh, ok. Been there, done that! lol
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> Steve
>>>>>>>>>>>
>>>>>>>>>>> [root@mach0 ~]# ls -la c7
>>>>>>>>>>> -rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7
>>>>>>>>>>> [root@mach0 ~]# singularity shell -w c7
>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Can you run this command again in --debug mode (singularity
>>>>>>>>>> --debug ....)
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> What about the command "lslocks"?
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>> [root@mach0 ~]# singularity shell c7 whoami
>>>>>>>>>>> /usr/bin/whoami: /usr/bin/whoami: cannot execute binary file
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Ahh, this is normal. You are asking the shell script to read in
>>>>>>>>>> /usr/bin/whoami. If you want to use shell to run whoami, you must prefix it
>>>>>>>>>> with the -c (e.g. -c "whoami [args]"), or use the 'exec' Singularity
>>>>>>>>>> subcommand: "singularity exec c7 whoami"
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>>> [root@mach0 ~]# ps -ef |grep c7
>>>>>>>>>>> root      7479  2002  0 11:49 pts/0    00:00:00 grep
>>>>>>>>>>> --color=auto c7
>>>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> What kind of file system does your image exist on? Is it NFS by
>>>>>>>>>> chance? I wonder if there is a host issue with a locking daemon or
>>>>>>>>>> something else weird going on where it is not giving the exclusive lock
>>>>>>>>>> properly. If this is NFS or other non local file system, can you copy the
>>>>>>>>>> image to /tmp, rerun the MPI command to get it to fail again, and try
>>>>>>>>>> again?
>>>>>>>>>>
>>>>>>>>>> Thanks!
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M.
>>>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>>>
>>>>>>>>>>>> Hi Steve,
>>>>>>>>>>>>
>>>>>>>>>>>> That means there is an active file descriptor/process still
>>>>>>>>>>>> running and attached to the container maintaining a shared lock. You can
>>>>>>>>>>>> run other commands against the container as long as long as the container
>>>>>>>>>>>> is not being requested as --writable(-w), because that will try and obtain
>>>>>>>>>>>> an exclusive lock and it will fail if there are any active shared locks.
>>>>>>>>>>>> Try an "lsof /path/to/c7" to see what processes are attached to it. You may
>>>>>>>>>>>> see a list like:
>>>>>>>>>>>>
>>>>>>>>>>>> # lsof /tmp/Demo-2.img
>>>>>>>>>>>> COMMAND    PID USER   FD   TYPE DEVICE   SIZE/OFF      NODE NAME
>>>>>>>>>>>> sexec   107975 root    6rR  REG  253,0 1073741856 202112247
>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>> sexec   107977 root    6r   REG  253,0 1073741856 202112247
>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>> bash    107982 root    6r   REG  253,0 1073741856 202112247
>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>
>>>>>>>>>>>> Notice the two top ones are 'sexec' which are part of the
>>>>>>>>>>>> Singularity process stack. Kill the bottom one, and those should go away
>>>>>>>>>>>> naturally.
>>>>>>>>>>>>
>>>>>>>>>>>> BTW, as long as you have installed Singularity as root, there
>>>>>>>>>>>> is no need to run Singularity commands as root (unless you want to make
>>>>>>>>>>>> system changes within the container).
>>>>>>>>>>>>
>>>>>>>>>>>> Hope that helps!
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <
>>>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>>>
>>>>>>>>>>>>> Running mpirun tests, when an abort occurs, my image ends up
>>>>>>>>>>>>> locked.  Is there a way to clear the lock without rebooting?  I looked for
>>>>>>>>>>>>> processes that I could kill, but didn't see anything worthy.
>>>>>>>>>>>>>
>>>>>>>>>>>>> I'm using singularity v2.1 on Centos 7.2 (both host and
>>>>>>>>>>>>> container).
>>>>>>>>>>>>>
>>>>>>>>>>>>> Regards,
>>>>>>>>>>>>>
>>>>>>>>>>>>> Steve
>>>>>>>>>>>>>
>>>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H
>>>>>>>>>>>>> mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>>>>>>>>>>> Process 0 sent to 1
>>>>>>>>>>>>> Process 0 decremented value: 9
>>>>>>>>>>>>> Process 0 decremented value: 8
>>>>>>>>>>>>> Process 0 decremented value: 7
>>>>>>>>>>>>> Process 0 decremented value: 6
>>>>>>>>>>>>> Process 0 decremented value: 5
>>>>>>>>>>>>> Process 0 decremented value: 4
>>>>>>>>>>>>> Process 0 decremented value: 3
>>>>>>>>>>>>> Process 0 decremented value: 2
>>>>>>>>>>>>> Process 0 decremented value: 1
>>>>>>>>>>>>> Process 0 decremented value: 0
>>>>>>>>>>>>> Process 0 exiting
>>>>>>>>>>>>> Process 1 exiting
>>>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H
>>>>>>>>>>>>> mach0,mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>>>
>>>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>>>> It appears as if there is not enough space for
>>>>>>>>>>>>> /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory
>>>>>>>>>>>>> backing
>>>>>>>>>>>>> file). It is likely that your MPI job will now either abort or
>>>>>>>>>>>>> experience
>>>>>>>>>>>>> performance degradation.
>>>>>>>>>>>>>
>>>>>>>>>>>>>   Local host:  mach0
>>>>>>>>>>>>>   Space Requested: 4194312 B
>>>>>>>>>>>>>   Space Available: 0 B
>>>>>>>>>>>>>
>>>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>>>> [mach0:02308] create_and_attach: unable to create shared
>>>>>>>>>>>>> memory BTL coordinating structure :: size 134217728
>>>>>>>>>>>>> [mach0:02291] 2 more processes have sent help message
>>>>>>>>>>>>> help-opal-shmem-mmap.txt / target full
>>>>>>>>>>>>> [mach0:02291] Set MCA parameter "orte_base_help_aggregate" to
>>>>>>>>>>>>> 0 to see all help / error messages
>>>>>>>>>>>>> ^CKilled by signal 2.
>>>>>>>>>>>>> Killed by signal 2.
>>>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2308
>>>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2309
>>>>>>>>>>>>> [warn] Epoll ADD(4) on fd 31 failed.  Old events were 0; read
>>>>>>>>>>>>> change was 0 (none); write change was 1 (add): Bad file descriptor
>>>>>>>>>>>>> ^C[root@mach0 ~]singularity shell -w c7
>>>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>>>> [root@mach0 ~]# tail -30 /var/log/messages
>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon
>>>>>>>>>>>>> management.
>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Reached target Multi-User
>>>>>>>>>>>>> System.
>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.
>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about
>>>>>>>>>>>>> System Runlevel Changes...
>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data
>>>>>>>>>>>>> Collection 10s After Completed Startup.
>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Update UTMP about
>>>>>>>>>>>>> System Runlevel Changes.
>>>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel
>>>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]
>>>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel
>>>>>>>>>>>>> arming.
>>>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms
>>>>>>>>>>>>> (kernel) + 1.100s (initrd) + 4.931s (userspace) = 6.446s.
>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.
>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting user-0.slice.
>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user
>>>>>>>>>>>>> root.
>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Started Session 1 of user root.
>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user root.
>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2023)>
>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)>
>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Image
>>>>>>>>>>>>> is locked by another process
>>>>>>>>>>>>> Jul 14 10:42:36 mach0 kernel: loop: module loaded
>>>>>>>>>>>>> Jul 14 10:43:38 mach0 Singularity: sexec (U=0,P=2050)>
>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>> Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>> Jul 14 10:49:17 mach0 Singularity: sexec (U=0,P=2203)>
>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>> Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>> Jul 14 10:50:39 mach0 Singularity: sexec (U=0,P=2244)>
>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>> Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)>
>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2300)>
>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>> Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)>
>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Image
>>>>>>>>>>>>> is locked by another process
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> --
>>>>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from
>>>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> --
>>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>>>> University of California
>>>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>>>
>>>>>>>>>>> --
>>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from
>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> --
>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>> University of California
>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>>> Groups "singularity" group.
>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> Gregory M. Kurtzer
>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>> University of California
>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>
>>>>>>> --
>>>>>>> You received this message because you are subscribed to the Google
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a1141f9ee3ae7f70537b0c831
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Steve,<div><br></div><div>I think I have fixed the init=
ial error you got regarding being unable to create the sessiondir. The seco=
nd issue regarding file locks, I am debugging now. I&#39;ll let you know sh=
ortly when to retest.</div><div><br></div><div>Thanks!</div><div><br></div>=
<div><br><div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On =
Fri, Jul 15, 2016 at 11:26 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a href=
=3D"mailto:sgmeh...@gmail.com" target=3D"_blank">sgmeh...@gmail.com</a>&gt;=
</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>The =
problem only arises when there is an error.</div><div>The tmp directory is =
different on each machine, it is not shared or an NFS mount.=C2=A0 Is that =
a problem?</div><div><br></div><div>Steve<br><br>On Friday, July 15, 2016 a=
t 9:51:55 AM UTC-6, Gregory M. Kurtzer wrote:</div><div><div class=3D"h5"><=
blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-=
left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-le=
ft-style:solid"><div dir=3D"ltr"><div><br><div class=3D"gmail_quote">On Fri=
, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"n=
ofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmai=
l_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-col=
or:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><div dir=
=3D"ltr"><div>Hello,</div><div><br></div><div>I went back and recreated eve=
rything from scratch using v2.1.=C2=A0 When I moved the image file to the m=
ach1 and mach2 nodes I used tar with the -S option.=C2=A0 I used /tmp for t=
he image files.=C2=A0 After everything was set I ran mpirun on each individ=
ual system (only starting jobs on that system)=C2=A0with no errors.=C2=A0 F=
or some reason when I ran mpirun from mach0 and asked for jobs to be run on=
 mach0 and another system I get the error.=C2=A0 The mpirun complains about=
 not being able to create a file (already exists) and hangs.=C2=A0 The sing=
ularity image file on mach0 and the other system selected are locked with p=
ids that are no longer running.</div><div><br></div><div>Saying all that, I=
 decided to go back and put --debug on the singularity command in the mpiru=
n to get you better diagnostics.=C2=A0 I rebooted mach0 and mach2, then rer=
an the command and it worked correctly.=C2=A0 I then removed the --debug co=
mmand and it worked.=C2=A0 I added mach1 to the mix and everything worked f=
ine.=C2=A0 What? I guess the bottom line is that if the mpirun aborts (and =
it seems to hang) then you can get this condition.</div></div></blockquote>=
<div><br></div><div>So does the problem seem to arise only when there is a =
failure on one of the processes, and MPI kills the remaining processes?<br>=
</div><div><br></div><div>Ralph, do you know how OMPI kills the leftover pr=
ocesses in a job abort and what signal it uses?</div><div><br></div><div>(a=
dditional comments inline)</div><div><br></div><blockquote class=3D"gmail_q=
uote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:=
rgb(204,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D=
"ltr"><div><br>Steve</div><div><br></div><div><br></div><div><font face=3D"=
courier new,monospace">[root@mach0 ompi]# mpirun --allow-run-as-root -np 2 =
-H mach2 singularity exec /tmp/c7new.img /usr/bin/ring<br>Process 1 exiting=
<span><br>Process 0 sending 10 to 1, tag 201 (2 processes in ring)<br>Proce=
ss 0 sent to 1<br>Process 0 decremented value: 9<br>Process 0 decremented v=
alue: 8<br>Process 0 decremented value: 7<br>Process 0 decremented value: 6=
<br>Process 0 decremented value: 5<br>Process 0 decremented value: 4<br>Pro=
cess 0 decremented value: 3<br>Process 0 decremented value: 2<br>Process 0 =
decremented value: 1<br>Process 0 decremented value: 0<br>Process 0 exiting=
<br></span></font></div></div></blockquote><div><br></div><div>Is /tmp NFS =
mounted or shared?</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(20=
4,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">=
<div><font face=3D"courier new,monospace"><span></span>[root@mach0 ompi]# m=
pirun --allow-run-as-root -np 4 -H mach0,mach2 singularity exec /tmp/c7new.=
img /usr/bin/ring<br><font color=3D"#ff0000">ERROR=C2=A0 : Could not create=
 directory /tmp/.singularity-session-0.64768.158221: File exists<br>ERROR=
=C2=A0 : Failed creating session directory: /tmp/.singularity-session-0.647=
68.158221</font><br>-------------------------------------------------------=
<br>Primary job=C2=A0 terminated normally, but 1 process returned<br>a non-=
zero exit code. Per user-direction, the job has been aborted.<br>----------=
---------------------------------------------<br><font color=3D"#ff0000">^C=
</font>Killed by signal 2.<br>mpirun: abort is already in progress...hit ct=
rl-c again to forcibly terminate</font></div></div></blockquote><div><br></=
div><div>Ahhhhh, I think you may have found a race condition in Singularity=
 which killed one of the processes. Then the MPI killed the rest.</div><div=
>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width=
:1px;border-left-style:solid"><div dir=3D"ltr"><div><font face=3D"courier n=
ew,monospace">[root@mach0 ompi]# singularity shell -w /tmp/c7new.img<span><=
br>ERROR=C2=A0 : Image is locked by another process<br></span>[root@mach0 o=
mpi]# lslocks<span><br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH<br></span>l=
vmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 473 POSI=
X=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/lvmet=
ad.pid<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 606 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=
=A0=C2=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 820 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.pid<br>master=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 820 FLOCK=C2=A0 3=
3B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix/master.=
lock<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 924 POSIX=
=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmc=
tld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 1103 POSIX=C2=A0=C2=A0 5B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=
 0 /run/slurmd.pid<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <=
font color=3D"#ff0000">2477</font> FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=
=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>[root@mach0 ompi]# ls /proc/2477<br=
>ls: cannot access /proc/2477: No such file or directory<br></font></div></=
div></blockquote><div><br></div><div>OK, that is weird. If process 2477 is =
dead, there should be no active lock. I would guess this might be a kernel =
bug? What distribution and kernel are you running?</div><div>=C2=A0</div><b=
lockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-l=
eft:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-lef=
t-style:solid"><div dir=3D"ltr"><div><font face=3D"courier new,monospace">[=
root@mach0 ompi]# ssh mach2<br>Last login: Fri Jul 15 07:52:56 2016 from ma=
ch0<br>[root@mach2 ~]# singularity shell -w /tmp/c7new.img<span><br>ERROR=
=C2=A0 : Image is locked by another process<br></span>[root@mach2 ~]# lsloc=
ks<span><br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH<br></span>lvmetad=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 473 POSIX=C2=A0=
=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/lvmetad.pid<=
br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 851 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/s=
pool/postfix/pid/master.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 851 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=
=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix/master.lock<br>(unknown)=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <font color=3D"#ff0000">10110</font> FLOCK=
=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>c=
rond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 668 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 =
0 /run/crond.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 1610 POSIX=C2=A0=C2=A0 5B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=
=A0=C2=A0 0 /run/slurmd.pid<br>[root@mach2 ~]# ls /proc/10110<br>ls: cannot=
 access /proc/10110: No such file or directory</font></div></div></blockquo=
te><div><br></div><div>Very weird. Seems both nodes have the same issue..?<=
/div><div><br></div><div>I wonder if I can replicate this by kill -9 the ou=
ter parent of a Singularity run. I will test when I get to my development b=
ox.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin=
:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);bord=
er-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div><br></div>=
<div>Good Run:</div><div><br></div><div>[<font face=3D"courier new,monospac=
e">root@mach0 ompi]# mpirun --allow-run-as-root -np 6 -H mach0,mach1,mach2 =
singularity exec /tmp/c7new.img /usr/bin/ring<br>Process 0 sending 10 to 1,=
 tag 201 (6 processes in ring)<span><br>Process 0 sent to 1<br></span>Proce=
ss 1 exiting<span><br>Process 0 decremented value: 9<br>Process 0 decrement=
ed value: 8<br>Process 0 decremented value: 7<br>Process 0 decremented valu=
e: 6<br>Process 0 decremented value: 5<br>Process 0 decremented value: 4<br=
>Process 0 decremented value: 3<br>Process 0 decremented value: 2<br>Proces=
s 0 decremented value: 1<br>Process 0 decremented value: 0<br>Process 0 exi=
ting<br></span>Process 2 exiting<br>Process 5 exiting<br>Process 3 exiting<=
br>Process 4 exiting</font></div></div></blockquote><div><br></div><div>So =
as long as we don&#39;t crash (or get killed), all seems fine... Interestin=
g.</div><div><br></div><div>I will try and replicate the error condition.</=
div><div><br></div><div>Greg</div><div>=C2=A0</div><blockquote class=3D"gma=
il_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-co=
lor:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><div di=
r=3D"ltr"><div><br></div><span><div><br></div><div><br>On Thursday, July 14=
, 2016 at 3:56:48 PM UTC-6, Gregory M. Kurtzer wrote:</div></span><blockquo=
te class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex=
;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style=
:solid"><span><div dir=3D"ltr">I am wondering if some aspect of the sparse =
file has caused this. Sparse files are weird, and do require some attention=
 when copying around. With that said, I do not think it should ever corrupt=
 an image, but it might take along garbage data with it. One way to retest =
this is to go back to the original image, and tar it before sending using t=
he -S/--sparse tar option when packaging it up. Then bring it to the other =
test system and see if it still does that.<div><br></div><div>Aside from th=
at, I&#39;m a bit confused as to what is happening. There should be no issu=
e with 2.0 vs. 2.1, but it might be worth checking with some images also cr=
eated with 2.1 on the same target system where you are seeing the problem.<=
/div></div></span><div><div><div><br><div class=3D"gmail_quote">On Thu, Jul=
 14, 2016 at 2:46 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofoll=
ow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quo=
te" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rg=
b(204,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"l=
tr"><div>It is very possible that I did copy it from another system - maybe=
 created it with singularity 2.0 and now running it with 2.1??=C2=A0 I will=
 start over and recreate from scratch and see if I still have the problem.<=
/div><div><br></div><div>Sorry, I checked there is no /proc/2299</div><div>=
<br></div><div>The only way I&#39;ve found to get the file unlocked is to r=
eboot.=C2=A0 Not an issue with the little VM.</div><span><font color=3D"#88=
8888"><div><br></div></font></span><div><span><font color=3D"#888888">Steve=
</font></span><span><br><br>On Thursday, July 14, 2016 at 3:30:12 PM UTC-6,=
 Gregory M. Kurtzer wrote:</span></div><blockquote class=3D"gmail_quote" st=
yle=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,=
204,204);border-left-width:1px;border-left-style:solid"><span><div dir=3D"l=
tr">Very odd...<div><br></div><div>I&#39;m concerned about the Buffer I/O e=
rrors. It is almost like the image itself has a problem with it. Did you co=
py this image from another system? I wonder if it is the sparseness...</div=
><div><br></div><div>Does the directory exist for /proc/2299/ ?</div><div><=
br></div><div>Can you kill it with a -9?</div></div></span><div><div><div><=
br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlbe=
rg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span>=
 wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.=
8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1=
px;border-left-style:solid"><div dir=3D"ltr"><div>Seems that process isn&#3=
9;t running any more.=C2=A0 I did find this in the /var/log/messages file:<=
/div><div><br></div><div><font face=3D"courier new,monospace">[root@mach0 ~=
]# cat /var/log/messages | grep 2299<span><br>Jul 14 10:51:34 mach0 Singula=
rity: sexec (U=3D0,P=3D2299)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/roo=
t, Arg1=3D/usr/bin/ring<br></span>Jul 14 11:37:17 mach0 kernel: Buffer I/O =
error on device loop1, logical block 22299<br>Jul 14 11:37:17 mach0 kernel:=
 Buffer I/O error on device loop1, logical block 22990<br>...</font></div><=
div><font face=3D"courier new,monospace"><br></font></div><div><font face=
=3D"courier new,monospace">[root@mach0 ~]# ps -ef |grep 2299<br>root=C2=A0=
=C2=A0=C2=A0=C2=A0 10545=C2=A0 2002=C2=A0 0 14:15 pts/0=C2=A0=C2=A0=C2=A0 0=
0:00:00 grep --color=3Dauto 2299<br>[root@mach0 ~]# ps |grep 2299</font><br=
></div><span><div><br>On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Grego=
ry M. Kurtzer wrote:</div></span><blockquote class=3D"gmail_quote" style=3D=
"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,20=
4);border-left-width:1px;border-left-style:solid"><span><div dir=3D"ltr">Wh=
at is running at PID=C2=A0<span style=3D"font-size:12.8px">2299?</span></di=
v></span><div><div><div><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016=
 at 1:35 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...=
@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204=
,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">Ok, I=
 see my error with shell vs exec.=C2=A0 I&#39;m running on a VM with my own=
 space, no NFS involved.=C2=A0 Here is the debug command run and lsofl<br><=
br>[root@mach0 ~]# df -h<br>Filesystem=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 Size=C2=A0 Used Avail Use% Mounted on<=
br>/dev/mapper/vg1-lv001=C2=A0 5.7G=C2=A0 4.3G=C2=A0 1.1G=C2=A0 80% /<br>de=
vtmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 911M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 911M=C2=A0=C2=A0 0% /d=
ev<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=
=A0 920M=C2=A0=C2=A0 0% /dev/shm<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=
=C2=A0=C2=A0 41M=C2=A0 880M=C2=A0=C2=A0 5% /run<br>tmpfs=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=C2=A0 0% /sys/fs/=
cgroup<br>/dev/vda1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 190M=C2=A0 110M=C2=A0=C2=A0 67M=C2=A0 63% /boot<br=
>/dev/mapper/vg1-lv002=C2=A0=C2=A0 12G=C2=A0 203M=C2=A0=C2=A0 11G=C2=A0=C2=
=A0 2% /var<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 184M=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0 184M=C2=A0=C2=A0 0% /run/user/0<br>[root@mach0 ~]# singularity =
--debug shell -w c7<br>enabling debugging<br>ending argument loop<br>Exec&#=
39;ing: /usr/local/libexec/singularity/cli/shell.exec -w+ &#39;[&#39; -f /u=
sr/local/etc/singularity/init &#39;]&#39;<br>+ . /usr/local/etc/singularity=
/init<br>++ unset module<br>++ PATH=3D/usr/local/sbin:/usr/local/bin:/usr/s=
bin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin<br>++ HISTFILE=3D/dev/=
null<br>++ export PATH HISTFILE<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;=
<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39=
; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39;=
 -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>+=
+ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#3=
9;]&#39;<br>+ true<br>+ case ${1:-} in<br>+ shift<br>+ SINGULARITY_WRITABLE=
=3D1<br>+ export SINGULARITY_WRITABLE<br>+ true<br>+ case ${1:-} in<br>+ br=
eak<br>+ &#39;[&#39; -z c7 &#39;]&#39;<br>+ SINGULARITY_IMAGE=3Dc7<br>+ exp=
ort SINGULARITY_IMAGE<br>+ shift<br>+ exec /usr/local/libexec/singularity/s=
exec<br>VERBOSE [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 messag=
e.c:52:init()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Set messagelevel to: 5<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:127:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Gathering and caching user inf=
o.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 privilege.c:43:get_user_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called get_user_privs(struct s_privinfo *uinf=
o)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 privilege.c:54:get_user_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning get_user_privs(struct s_privinfo *u=
info) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:134:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we can escalate privs properly.<b=
r>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 pr=
ivilege.c:61:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Called escalate_privs(void)<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:73:escalate_=
privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 :=
 Returning escalate_privs(void) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:141:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting privs to call=
ing user<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 privilege.c:79:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called drop_privs(=
struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_privs()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dro=
pping privileges to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D804=
5]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:93:drop_privs()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Dropping privileges to UID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:103:drop_pri=
vs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Confirming we have correct GID<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:109:drop_privs(=
)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:115:drop_privs()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Returning drop_privs(struct s_privinfo *uinfo) =3D 0<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:1=
46:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Obtaining user&#39;s homedir<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:150:main()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining file descripto=
r to current directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:155:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Getting current working directory pa=
th string<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 sexec.c:161:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_COMMAND from environment<b=
r>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 se=
xec.c:168:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Obtaining SINGULARITY_IMAGE from environment<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:174:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Checking container image is a file: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:180:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Building configuratio=
n file location<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:183:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Config location: /usr/local/etc/singularit=
y/singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:185:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity configuration is a=
 file: /usr/local/etc/singularity/singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:191:main()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking =
Singularity configuration file is owned by root<br>DEBUG=C2=A0=C2=A0 [U=3D0=
,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:197:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening Sing=
ularity configuration file<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:210:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity configura=
tion for &#39;sessiondir prefix&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:47:config_get_key_valu=
e()=C2=A0 : Called config_get_key_value(fp, sessiondir prefix)<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parse=
r.c:66:config_get_key_value()=C2=A0 : Return config_get_key_value(fp, sessi=
ondir prefix) =3D NULL<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 file.c:48:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called file_id(c7)<br>VERBOSE [U=3D0,P=3D8=
045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:58:file_id()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Generated file_id: 0.647=
68.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 file.c:60:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Returning file_id(c7) =3D 0.64768.26052<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:217:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Set sessiondir to: /tmp/.singularity-session-0.64768.26052<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:221:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Set containername to: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:223:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting loop_dev_* paths<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_pa=
rser.c:47:config_get_key_value()=C2=A0 : Called config_get_key_value(fp, co=
ntainer dir)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 config_parser.c:58:config_get_key_value()=C2=A0 : Return confi=
g_get_key_value(fp, container dir) =3D /var/singularity/mnt<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:232:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Set image mount path to: /var/singularity/mnt<br>LOG=C2=A0=C2=A0=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:234:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Com=
mand=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:247:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set=
 prompt to: Singularity.c7&gt;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:249:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we are opening =
image as read/write<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:264:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening image as read/write only: c7<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sex=
ec.c:271:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Setting exclusive lock on file descriptor: 6<br>ERROR=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:273:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Ima=
ge is locked by another process<span><br>[root@mach0 ~]# lsof c7<br></span>=
[root@mach0 ~]# lslocks<br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH<br>cr=
ond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 601 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /=
run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=
=C2=A0 0 /var/spool/postfix/pid/master.pid<br>master=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 33B WRITE 0=
=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix/master.lock<br>lv=
metad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 478 POSIX=
=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/lvmeta=
d.pid<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 940 POSI=
X=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurm=
ctld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 966 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=
=C2=A0 0 /run/slurmd.pid<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 2299 FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=
=A0=C2=A0 0 /<br><br><br>On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gr=
egory M. Kurtzer wrote:<div><div><blockquote class=3D"gmail_quote" style=3D=
"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,20=
4);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><br><div=
><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 11:50 AM, Steve Meh=
lberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</sp=
an> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px=
 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-widt=
h:1px;border-left-style:solid"><div dir=3D"ltr">Gregory,<br><br>Thanks for =
the quick suggestions.=C2=A0 There don&#39;t seem to be any processes attac=
hed to the container and I can&#39;t seem to run other commands (in read mo=
de). I&#39;m not sure what&#39;s going on.<br><br>Just lazy with root right=
 now, I need to create a normal uid.<br></div></blockquote><div><br></div><=
div>Ahh, ok. Been there, done that! lol</div><div>=C2=A0</div><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;bor=
der-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:sol=
id"><div dir=3D"ltr"><br>Steve<br><br><span style=3D"font-family:&quot;cour=
ier new&quot;,monospace">[root@mach0 ~]# ls -la c7<br>-rwxr-xr-x 1 root roo=
t 1610612769 Jul 14 10:49 c7<br>[root@mach0 ~]# singularity shell -w c7<spa=
n><br>ERROR=C2=A0 : Image is locked by another process<br></span></span></d=
iv></blockquote><div><br></div><div>Can you run this command again in --deb=
ug mode (singularity --debug ....)</div><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-=
left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid">=
<div dir=3D"ltr"><span style=3D"font-family:&quot;courier new&quot;,monospa=
ce"><span></span>[root@mach0 ~]# lsof c7<br></span></div></blockquote><div>=
<br></div><div>What about the command &quot;lslocks&quot;?</div><div>=C2=A0=
</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;p=
adding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;bo=
rder-left-style:solid"><div dir=3D"ltr"><span style=3D"font-family:&quot;co=
urier new&quot;,monospace">[root@mach0 ~]# singularity shell c7 whoami<br>/=
usr/bin/whoami: /usr/bin/whoami: cannot execute binary file<br></span></div=
></blockquote><div><br></div><div>Ahh, this is normal. You are asking the s=
hell script to read in /usr/bin/whoami. If you want to use shell to run who=
ami, you must prefix it with the -c (e.g. -c &quot;whoami [args]&quot;), or=
 use the &#39;exec&#39; Singularity subcommand: &quot;singularity exec c7 w=
hoami&quot;</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204=
,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><span=
 style=3D"font-family:&quot;courier new&quot;,monospace">[root@mach0 ~]#<br=
>[root@mach0 ~]# ps -ef |grep c7<br>root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 7479=
=C2=A0 2002=C2=A0 0 11:49 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --color=3Da=
uto c7<br>[root@mach0 ~]#</span></div></blockquote><div><br></div><div>What=
 kind of file system does your image exist on? Is it NFS by chance? I wonde=
r if there is a host issue with a locking daemon or something else weird go=
ing on where it is not giving the exclusive lock properly. If this is NFS o=
r other non local file system, can you copy the image to /tmp, rerun the MP=
I command to get it to fail again, and try again?=C2=A0</div><div><br></div=
><div>Thanks!</div><div><br></div><div><br></div><div>=C2=A0</div><blockquo=
te class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex=
;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style=
:solid"><div dir=3D"ltr"><span><br><br><br><br>On Thursday, July 14, 2016 a=
t 12:30:01 PM UTC-6, Gregory M. Kurtzer wrote:</span><blockquote class=3D"g=
mail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-=
color:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><span=
><div dir=3D"ltr">Hi Steve,<div><br></div><div>That means there is an activ=
e file descriptor/process still running and attached to the container maint=
aining a shared lock. You can run other commands against the container as l=
ong as long as the container is not being requested as --writable(-w), beca=
use that will try and obtain an exclusive lock and it will fail if there ar=
e any active shared locks. Try an &quot;lsof /path/to/c7&quot; to see what =
processes are attached to it. You may see a list like:</div><div><br></div>=
<div><div># lsof /tmp/Demo-2.img=C2=A0</div><div>COMMAND =C2=A0 =C2=A0PID U=
SER =C2=A0 FD =C2=A0 TYPE DEVICE =C2=A0 SIZE/OFF =C2=A0 =C2=A0 =C2=A0NODE N=
AME</div><div>sexec =C2=A0 107975 root =C2=A0 =C2=A06rR =C2=A0REG =C2=A0253=
,0 1073741856 202112247 /tmp/Demo-2.img</div><div>sexec =C2=A0 107977 root =
=C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 1073741856 202112247 /tmp/Demo-2.img=
</div><div>bash =C2=A0 =C2=A0107982 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A02=
53,0 1073741856 202112247 /tmp/Demo-2.img</div></div><div><br></div><div>No=
tice the two top ones are &#39;sexec&#39; which are part of the Singularity=
 process stack. Kill the bottom one, and those should go away naturally.</d=
iv><div><br></div><div>BTW, as long as you have installed Singularity as ro=
ot, there is no need to run Singularity commands as root (unless you want t=
o make system changes within the container).</div><div><br></div><div>Hope =
that helps!</div><div><br></div></div></span><div><br><div class=3D"gmail_q=
uote"><div><div>On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <span dir=
=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><=
/div></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.=
8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1=
px;border-left-style:solid"><div><div><div dir=3D"ltr">Running mpirun tests=
, when an abort occurs, my image ends up locked.=C2=A0 Is there a way to cl=
ear the lock without rebooting?=C2=A0 I looked for processes that I could k=
ill, but didn&#39;t see anything worthy.<br><br>I&#39;m using singularity v=
2.1 on Centos 7.2 (both host and container).<br><br>Regards,<br><br>Steve<b=
r><br><span style=3D"font-family:&quot;courier new&quot;,monospace">[root@m=
ach0 ~]# mpirun --allow-run-as-root -n 2 -H mach1,mach2 singularity exec c7=
 /usr/bin/ring<br>Process 0 sending 10 to 1, tag 201 (2 processes in ring)<=
br>Process 0 sent to 1<br>Process 0 decremented value: 9<br>Process 0 decre=
mented value: 8<br>Process 0 decremented value: 7<br>Process 0 decremented =
value: 6<br>Process 0 decremented value: 5<br>Process 0 decremented value: =
4<br>Process 0 decremented value: 3<br>Process 0 decremented value: 2<br>Pr=
ocess 0 decremented value: 1<br>Process 0 decremented value: 0<br>Process 0=
 exiting<br>Process 1 exiting<br>[root@mach0 ~]# mpirun --allow-run-as-root=
 -n 3 -H mach0,mach1,mach2 singularity exec c7 /usr/bin/ring<br>-----------=
---------------------------------------------------------------<br>It appea=
rs as if there is not enough space for /tmp/ompi.mach0.2291/54935/1/0/vader=
_segment.mach0.0 (the shared-memory backing<br>file). It is likely that you=
r MPI job will now either abort or experience<br>performance degradation.<b=
r><br>=C2=A0 Local host:=C2=A0 mach0<br>=C2=A0 Space Requested: 4194312 B<b=
r>=C2=A0 Space Available: 0 B<br>------------------------------------------=
--------------------------------<br>[mach0:02308] create_and_attach: unable=
 to create shared memory BTL coordinating structure :: size 134217728<br>[m=
ach0:02291] 2 more processes have sent help message help-opal-shmem-mmap.tx=
t / target full<br>[mach0:02291] Set MCA parameter &quot;orte_base_help_agg=
regate&quot; to 0 to see all help / error messages<br>^CKilled by signal 2.=
<br>Killed by signal 2.<br>Singularity is sending SIGKILL to child pid: 230=
8<br>Singularity is sending SIGKILL to child pid: 2309<br>[warn] Epoll ADD(=
4) on fd 31 failed.=C2=A0 Old events were 0; read change was 0 (none); writ=
e change was 1 (add): Bad file descriptor<br>^C[root@mach0 ~]singularity sh=
ell -w c7<br>ERROR=C2=A0 : Image is locked by another process<br>[root@mach=
0 ~]# tail -30 /var/log/messages<br>Jul 14 10:42:17 mach0 systemd: Started =
LSB: slurm daemon management.<br>Jul 14 10:42:17 mach0 systemd: Reached tar=
get Multi-User System.<br>Jul 14 10:42:17 mach0 systemd: Starting Multi-Use=
r System.<br>Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about Syst=
em Runlevel Changes...<br>Jul 14 10:42:17 mach0 systemd: Started Stop Read-=
Ahead Data Collection 10s After Completed Startup.<br>Jul 14 10:42:17 mach0=
 systemd: Started Update UTMP about System Runlevel Changes.<br>Jul 14 10:4=
2:18 mach0 kdumpctl: kexec: loaded kdump kernel<br>Jul 14 10:42:18 mach0 kd=
umpctl: Starting kdump: [OK]<br>Jul 14 10:42:18 mach0 systemd: Started Cras=
h recovery kernel arming.<br>Jul 14 10:42:18 mach0 systemd: Startup finishe=
d in 415ms (kernel) + 1.100s (initrd) + 4.931s (userspace) =3D 6.446s.<br>J=
ul 14 10:42:34 mach0 systemd: Created slice user-0.slice.<br>Jul 14 10:42:3=
4 mach0 systemd: Starting user-0.slice.<br>Jul 14 10:42:34 mach0 systemd-lo=
gind: New session 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: Started=
 Session 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: Starting Session=
 1 of user root.<br>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D202=
3)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br=
>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D2024)&gt; Command=3Dex=
ec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:36 ma=
ch0 Singularity: sexec (U=3D0,P=3D2024)&gt; Image is locked by another proc=
ess<br>Jul 14 10:42:36 mach0 kernel: loop: module loaded<br>Jul 14 10:43:38=
 mach0 Singularity: sexec (U=3D0,P=3D2050)&gt; Command=3Dshell, Container=
=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:43:38 mach0 kernel: EXT4-fs =
(loop0): mounted filesystem with ordered data mode. Opts: discard<br>Jul 14=
 10:49:17 mach0 Singularity: sexec (U=3D0,P=3D2203)&gt; Command=3Dshell, Co=
ntainer=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:49:17 mach0 kernel: E=
XT4-fs (loop0): mounted filesystem with ordered data mode. Opts: discard<br=
>Jul 14 10:50:39 mach0 Singularity: sexec (U=3D0,P=3D2244)&gt; Command=3Dex=
ec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:50:39 ma=
ch0 kernel: EXT4-fs (loop0): mounted filesystem with ordered data mode. Opt=
s: discard<br>Jul 14 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2299)&gt;=
 Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 1=
4 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2300)&gt; Command=3Dexec, Co=
ntainer=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mach0 ke=
rnel: EXT4-fs (loop0): mounted filesystem with ordered data mode. Opts: dis=
card<br>Jul 14 10:51:57 mach0 Singularity: sexec (U=3D0,P=3D2322)&gt; Comma=
nd=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:51:57 m=
ach0 Singularity: sexec (U=3D0,P=3D2322)&gt; Image is locked by another pro=
cess</span><span><font color=3D"#888888"><br><br><br></font></span></div></=
div></div><span><font color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance C=
omputing Services (HPCS)<br>University of California<br>Lawrence Berkeley N=
ational Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></d=
iv>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div></div></div>

--001a1141f9ee3ae7f70537b0c831--
