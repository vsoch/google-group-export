X-Received: by 10.99.7.9 with SMTP id 9mr1039998pgh.21.1518749381139;
        Thu, 15 Feb 2018 18:49:41 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:903:: with SMTP id 3-v6ls452661plm.7.gmail; Thu, 15
 Feb 2018 18:49:40 -0800 (PST)
X-Received: by 2002:a17:902:b081:: with SMTP id p1-v6mr4377344plr.314.1518749379814;
        Thu, 15 Feb 2018 18:49:39 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1518749379; cv=none;
        d=google.com; s=arc-20160816;
        b=U5Hh2tkR9xwQ75p5qy3bySwUdoQWaViot8LdAHgTLG0v+fHkxvyE3JxI4jiL3y0Ryv
         zYY2tpTg13HVurv7mk/o3DLr9D0PBpewDctw/DLfZPT46XOGt9c9xHBErsubHbSXc/9b
         MwTc9kvMhkZlVNGn2sdEop4gYt674K7UPL3ae01BaI1BFAvA3Ejzc7gOlt0bAhM5jZIF
         xwg9OUg+BPl6MmUjMJj+qTtyF+ENkiB6hcj/aJkzqyYKQoPQ3XNL8eYB2H9TNWNMADXw
         BghUD+sogh+F3hdlAi06rIH+6mfW61036LYblhZ0xUzeY46amBihqK8HhyUKS7N8lgXj
         gMAg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:references:in-reply-to
         :mime-version:dkim-signature:arc-authentication-results;
        bh=fzm/ivfqqPOwiXBQFwEF09lpjs/zxOc1Gfd86DpTfcA=;
        b=FS/gLSGcDV5asJpITsJGGX2RIRZi4IzSGre/acjB47M+nk4UNXDoMiT55+FP+f2pDH
         8wo9HkAqh5EPqjEhMCTiHzqrfufcptn/XBVbBru59wIs0C8+ledP2e8Mw0rv+nxbAdl2
         aXv7J4W9fdMsAko9A+2yiv2KDnLhbrT5+g+htq5bWgKFJ3BC0jTj55ROj52yABj6BgSA
         sLW2ci5SC35eJ1itOEA4GjPc+UXZUtXyj5BmfMg403V1fr4EZNBT6Lbc1q82OAnUXfEG
         fNi+uFpFStmlqARy7dv7Y5BzPerusZRYjLAwNSgikzYhjHzpcmw9v49V8eLR3h/AReWD
         tXjA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=UrjQoPwQ;
       spf=pass (google.com: domain of smahm...@gmail.com designates 209.85.214.41 as permitted sender) smtp.mailfrom=smahm...@gmail.com
Return-Path: <smahm...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id z8-v6si2314993pll.381.2018.02.15.18.49.39
        for <singu...@lbl.gov>;
        Thu, 15 Feb 2018 18:49:39 -0800 (PST)
Received-SPF: pass (google.com: domain of smahm...@gmail.com designates 209.85.214.41 as permitted sender) client-ip=209.85.214.41;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=UrjQoPwQ;
       spf=pass (google.com: domain of smahm...@gmail.com designates 209.85.214.41 as permitted sender) smtp.mailfrom=smahm...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EoAwBqRoZafynWVdFcHAEBAQQBAQoBA?=
 =?us-ascii?q?YMlVAGBLigKg1QHgTmIbI4CggKBF4JqhQ2FZ4MLhVwVgUBDIgGFIgKCOwdUGAE?=
 =?us-ascii?q?CAQEBAQEBAgECEAEBCQsLCCYxgjgFAgMaBglLKi8BAQEBAQEBAQEBHwIrJQIZA?=
 =?us-ascii?q?QQBIx0BDQ4JFAEDAQsGBQs3AgIhAQEOAwEFARwOBwQBHASIQIE7AQMNCAWiJUC?=
 =?us-ascii?q?MF4IFBQEcgwwFg2EKGScNWVmCEwEBAQEBAQQBAQEBAQEBARgCBhKEcYIngz+DL?=
 =?us-ascii?q?oJrggQBEgGDNoJlBYEtAQEBiUOIcpAYNAEHAQGBcgqOfoULhCSQIo5NhhiDIxk?=
 =?us-ascii?q?ggRcfbC5xMxojUjJugSaCRoJPIzcQi0dJgXUBAQE?=
X-IronPort-AV: E=Sophos;i="5.46,519,1511856000"; 
   d="scan'208,217";a="13856189"
Received: from mail-it0-f41.google.com ([209.85.214.41])
  by fe4.lbl.gov with ESMTP; 15 Feb 2018 18:49:17 -0800
Received: by mail-it0-f41.google.com with SMTP id 193so501241iti.1
        for <singu...@lbl.gov>; Thu, 15 Feb 2018 18:49:17 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc;
        bh=fzm/ivfqqPOwiXBQFwEF09lpjs/zxOc1Gfd86DpTfcA=;
        b=UrjQoPwQAHw61UoATv5jx1xpx0UfEZTtExPMXLnpbXqPsXLUjVbQwnTdO7723ZIM3/
         +mhoWN2AiObRX7iG6OrfQ+vHgm76OR1HjA5W4tsXh5Mvv5N7HYqIL5kzyrGv+EnPf2fO
         I3PbyA9DcQPXWwe5mXNPaxBRhGuyUuqmXFmsUOm/EecRYH5lCLM6Rx5c9doTI2TGYjfk
         QAcg9eyRd6pdLmfdP/ppakbrDgWQ7fmSCeNXD28V4gX9xQXFb3hQd3wUU+mhPHDtJaIP
         8waSq4DRvGtb4pBTgZOojW7vOgwRKxlCZ7sy+irAbduPMoGC7mJ9NeDAMhWcd+IzZeue
         0Xcw==
X-Gm-Message-State: APf1xPChLM34PCGEEri07SPrmOBq4AJ+7XpCCZI7SdFknRRB57xmm/a0
	HT3/UptkWppfsirp2Ybx38QogJVVw8gYnuULIXMnDg==
X-Received: by 10.36.182.2 with SMTP id g2mr6044403itf.19.1518749356976; Thu,
 15 Feb 2018 18:49:16 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.47.34 with HTTP; Thu, 15 Feb 2018 18:48:36 -0800 (PST)
In-Reply-To: <CA+Wz_Fwcs-zqn5=G_8A2jAa4P7wGb=vvqSp6xCojYWFOdmSQhg@mail.gmail.com>
References: <CAHqiYpfh5BO7h8MZM09+7wmWpz5jKQtzE7wAPMBf9Qbnk=ANoQ@mail.gmail.com>
 <CA+Wz_Fwcs-zqn5=G_8A2jAa4P7wGb=vvqSp6xCojYWFOdmSQhg@mail.gmail.com>
From: Haseeb Mahmud <smahm...@gmail.com>
Date: Thu, 15 Feb 2018 21:48:36 -0500
Message-ID: <CAHqiYpc4mrffsA3kQ+q=1jttq-OKuHDPcZtAv+02mUqUNYbSbg@mail.gmail.com>
Subject: Re: [Singularity] singularity container on HPC
To: singularity@lbl.gov
Cc: vict...@gmail.com
Content-Type: multipart/alternative; boundary="089e08200cdcecef8305654b63dc"

--089e08200cdcecef8305654b63dc
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hello,

Since my email I rebuilt my container with NAMD linked against OpenMPI
version 2.10. I then ran my container on my host which also uses OpenMPI
version 2.10.. And I get new errors. Looks like now now it is using both
nodes but I get the below errors.


[compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received: 255
[compute-0.cluster:64549] unknown address family for tcp: 0
[compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received: 255
[compute-0.cluster:64549] unknown address family for tcp: 0
*** Error in `/NAMD_2.12_Source/Linux-x86_64-g++/namd2': munmap_chunk():
invalid pointer: 0x00000000098c5cb0 ***
*** Error in `/NAMD_2.12_Source/Linux-x86_64-g++/namd2': corrupted
double-linked list: 0x0000000008b4b540 ***

Charm++> Running on MPI version: 3.1
Charm++> level of thread support used: MPI_THREAD_SINGLE (desired:
MPI_THREAD_SINGLE)
Charm++> Running in non-SMP mode: numPes 88
Charm++> Using recursive bisection (scheme 3) for topology aware partitions
Converse/Charm++ Commit ID:
v6.7.1-0-gbdf6a1b-namd-charm-6.7.1-build-2016-Nov-07-136676
Warning> Randomization of stack pointer is turned on in kernel, thread
migration may not work! Run 'echo 0 > /proc/sys/kernel/randomize_va_space'
as root to disable it, or try run with '+isomalloc_sync'.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 2 unique compute nodes (88-way SMP).
Charm++> cpu topology info is gathered in 0.019 seconds.

Info: Startup phase 9 took 0.00478208 s, 1237.92 MB of memory in use
Info: CREATING 44375 COMPUTE OBJECTS
Info: Startup phase 10 took 0.0075435 s, 1237.92 MB of memory in use
Info: Startup phase 11 took 0.000689846 s, 1237.92 MB of memory in use
Info: Startup phase 12 took 4.81852e-05 s, 1237.92 MB of memory in use
Info: Finished startup at 13.4259 s, 1237.92 MB of memory in use

Info: useSync: 0 useProxySync: 0
[compute-0:64562] *** Process received signal ***
[compute-0:64562] Signal: Segmentation fault (11)
[compute-0:64562] Signal code: Address not mapped (1)
[compute-0:64562] Failing at address: 0x7
[compute-0:64699] *** Process received signal ***
[compute-0:64699] Signal: Segmentation fault (11)
[compute-0:64699] Signal code: Address not mapped (1)
[compute-0:64699] Failing at address: 0xffffffffffffffff
[compute-0:64737] *** Process received signal ***
[compute-0:64737] Signal: Segmentation fault (11)



Haseeb



On Thu, Feb 15, 2018 at 10:07 AM, victor sv <vict...@gmail.com> wrote:

> HI Haseeb,
>
> first of all I would like to understand with MPI family and version is
> running in and out the containers.
>
> NAMD is linked agains OpenMPI or MPICH?
>
> Which MPI family and version is running in the host? it should be enough
> if you show the output of "mpirun --version".
>
> BR,
> V=C3=ADctor.
>
> 2018-02-15 14:33 GMT+01:00 Haseeb Mahmud <smahm...@gmail.com>:
>
>> Hello,
>>
>> I have built a namd container that uses the MPI build of NAMD 2.12 that =
I
>> built from source. I am trying to execute  this container  on my HPC usi=
ng
>> mpirun on my host slurm script using multiple nodes , however, although =
the
>> job runs , it looks to be only using 1 processer and 1 node.
>>
>> I built my container from a script  with an Ubuntu operating syytem and
>> In my %post section, I first configure and make openmpi-2.1.0  then i
>> install mpich using "apt install mpich"  because my MPI build of NAMD wo=
nt
>> compile without it.  Then in my %post section I also build the namd 2.12
>> MPI build itself as all the NAMD source files are in my container as wel=
l.
>>
>> In my slurm script when I run " mpirun -np #P singularity exec
>> namdimage.simg /path_to namd_executable_in_container/namd2 inputfile" ,I
>> get the problem  of no scaling.
>>
>>
>>
>>
>> My sample out looks like :
>> Charm++> Running on MPI version: 3.0
>> Charm++> level of thread support used: MPI_THREAD_SINGLE (desired:
>> MPI_THREAD_SINGLE)
>> Charm++> Running in non-SMP mode: numPes 1
>> Charm++> Using recursive bisection (scheme 3) for topology aware
>> partitions
>> ..........
>>
>> Info: Running on 1 processors, 1 nodes, 1 physical nodes.
>>
>>
>> Any ideas what I may be doing wrong.
>>
>> Thanks,
>>
>> Haseeb
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--089e08200cdcecef8305654b63dc
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello,<div><br></div><div>Since my email I rebuilt my cont=
ainer with NAMD linked against OpenMPI version 2.10. I then ran my containe=
r on my host which also uses OpenMPI version 2.10.. And I get new errors. L=
ooks like now now it is using both nodes but I get the below errors.</div><=
div><br></div><div><br></div><div><div>[compute-0.cluster:64549] mca_btl_tc=
p_proc: unknown af_family received: 255</div><div>[compute-0.cluster:64549]=
 unknown address family for tcp: 0</div><div>[compute-0.cluster:64549] mca_=
btl_tcp_proc: unknown af_family received: 255</div><div>[compute-0.cluster:=
64549] unknown address family for tcp: 0</div><div>*** Error in `/NAMD_2.12=
_Source/Linux-x86_64-g++/namd2&#39;: munmap_chunk(): invalid pointer: 0x000=
00000098c5cb0 ***</div><div>*** Error in `/NAMD_2.12_Source/Linux-x86_64-g+=
+/namd2&#39;: corrupted double-linked list: 0x0000000008b4b540 ***</div></d=
iv><div><br></div><div><div>Charm++&gt; Running on MPI version: 3.1</div><d=
iv>Charm++&gt; level of thread support used: MPI_THREAD_SINGLE (desired: MP=
I_THREAD_SINGLE)</div><div>Charm++&gt; Running in non-SMP mode: numPes 88</=
div><div>Charm++&gt; Using recursive bisection (scheme 3) for topology awar=
e partitions</div><div>Converse/Charm++ Commit ID: v6.7.1-0-gbdf6a1b-namd-c=
harm-6.7.1-build-2016-Nov-07-136676</div><div>Warning&gt; Randomization of =
stack pointer is turned on in kernel, thread migration may not work! Run &#=
39;echo 0 &gt; /proc/sys/kernel/randomize_va_space&#39; as root to disable =
it, or try run with &#39;+isomalloc_sync&#39;.</div><div>CharmLB&gt; Load b=
alancer assumes all CPUs are same.</div><div>Charm++&gt; Running on 2 uniqu=
e compute nodes (88-way SMP).</div><div>Charm++&gt; cpu topology info is ga=
thered in 0.019 seconds.</div></div><div><br></div><div><div>Info: Startup =
phase 9 took 0.00478208 s, 1237.92 MB of memory in use</div><div>Info: CREA=
TING 44375 COMPUTE OBJECTS</div><div>Info: Startup phase 10 took 0.0075435 =
s, 1237.92 MB of memory in use</div><div>Info: Startup phase 11 took 0.0006=
89846 s, 1237.92 MB of memory in use</div><div>Info: Startup phase 12 took =
4.81852e-05 s, 1237.92 MB of memory in use</div><div>Info: Finished startup=
 at 13.4259 s, 1237.92 MB of memory in use</div><div><br></div><div>Info: u=
seSync: 0 useProxySync: 0</div><div>[compute-0:64562] *** Process received =
signal ***</div><div>[compute-0:64562] Signal: Segmentation fault (11)</div=
><div>[compute-0:64562] Signal code: Address not mapped (1)</div><div>[comp=
ute-0:64562] Failing at address: 0x7</div><div>[compute-0:64699] *** Proces=
s received signal ***</div><div>[compute-0:64699] Signal: Segmentation faul=
t (11)</div><div>[compute-0:64699] Signal code: Address not mapped (1)</div=
><div>[compute-0:64699] Failing at address: 0xffffffffffffffff</div><div>[c=
ompute-0:64737] *** Process received signal ***</div><div>[compute-0:64737]=
 Signal: Segmentation fault (11)</div></div><div><br></div><div><br></div><=
div><br></div><div>Haseeb=C2=A0</div><div><br></div><div><br></div></div><d=
iv class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Thu, Feb 15, 201=
8 at 10:07 AM, victor sv <span dir=3D"ltr">&lt;<a href=3D"mailto:vict...@gm=
ail.com" target=3D"_blank">vict...@gmail.com</a>&gt;</span> wrote:<br><bloc=
kquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #cc=
c solid;padding-left:1ex"><div dir=3D"ltr"><div><div><div><div><div>HI Hase=
eb,<br><br></div>first of all I would like to understand with MPI family an=
d version is running in and out the containers.<br><br></div>NAMD is linked=
 agains OpenMPI or MPICH?<br><br></div>Which MPI family and version is runn=
ing in the host? it should be enough if you show the output of &quot;mpirun=
 --version&quot;.<br><br></div>BR,<br></div>V=C3=ADctor.<br></div><div clas=
s=3D"gmail_extra"><br><div class=3D"gmail_quote"><div><div class=3D"h5">201=
8-02-15 14:33 GMT+01:00 Haseeb Mahmud <span dir=3D"ltr">&lt;<a href=3D"mail=
to:smahm...@gmail.com" target=3D"_blank">smahm...@gmail.com</a>&gt;</span>:=
<br></div></div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div><div class=3D"h5"><div =
dir=3D"ltr">Hello,<div><br></div><div>I have built a namd container that us=
es the MPI build of NAMD 2.12 that I built from source. I am trying to exec=
ute=C2=A0 this container=C2=A0 on my HPC using mpirun on my host slurm scri=
pt using multiple nodes , however, although the job runs , it looks to be o=
nly using 1 processer and 1 node.=C2=A0</div><div><br></div><div>I built my=
 container from a script=C2=A0 with an Ubuntu operating syytem and In my %p=
ost section, I first configure and make openmpi-2.1.0=C2=A0 then i install =
mpich using &quot;apt install mpich&quot;=C2=A0 because my MPI build of NAM=
D wont compile without it.=C2=A0 Then in my %post section I also build the =
namd 2.12 MPI build itself as all the NAMD source files are in my container=
 as well.</div><div><br></div><div>In my slurm script when I run &quot; mpi=
run -np #P singularity exec=C2=A0 namdimage.simg /path_to namd_executable_i=
n_container/n<wbr>amd2 inputfile&quot; ,I get the problem=C2=A0 of no scali=
ng.</div><div><br></div><div><br></div><div><br></div><div><br></div><div>M=
y sample out looks like :</div><div><div>Charm++&gt; Running on MPI version=
: 3.0</div><div>Charm++&gt; level of thread support used: MPI_THREAD_SINGLE=
 (desired: MPI_THREAD_SINGLE)</div><div>Charm++&gt; Running in non-SMP mode=
: numPes 1</div><div>Charm++&gt; Using recursive bisection (scheme 3) for t=
opology aware partitions</div></div><div>..........</div><div><br></div><di=
v><div>Info: Running on 1 processors, 1 nodes, 1 physical nodes.</div></div=
><div><br></div><div><br></div><div>Any ideas what I may be doing wrong.</d=
iv><div><br></div><div>Thanks,</div><div><br></div><div>Haseeb</div></div><=
/div></div><span class=3D"HOEnZb"><font color=3D"#888888"><span class=3D"m_=
-6963216037349168429HOEnZb"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</font></span></font></span></blockquote></div><span class=3D"HOEnZb"><font=
 color=3D"#888888"><br></font></span></div><span class=3D"HOEnZb"><font col=
or=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br></div>

--089e08200cdcecef8305654b63dc--
