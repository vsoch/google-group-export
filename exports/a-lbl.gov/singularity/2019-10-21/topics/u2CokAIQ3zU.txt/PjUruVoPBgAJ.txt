X-Received: by 10.84.191.165 with SMTP id a34mr134837pld.12.1509427391653;
        Mon, 30 Oct 2017 22:23:11 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.98.192.79 with SMTP id x76ls3170048pff.10.gmail; Mon, 30 Oct
 2017 22:23:10 -0700 (PDT)
X-Received: by 10.99.180.3 with SMTP id s3mr774865pgf.174.1509427390802;
        Mon, 30 Oct 2017 22:23:10 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1509427390; cv=none;
        d=google.com; s=arc-20160816;
        b=zJoCG0YQBJ7t6b7uOGAxRzxe0Z6Mxc5UBmXjpSi14T0jhx02p1N6rsxWBn4sW5H8Wu
         7XmWwtz0CXEicq7whBEJ7BqjwtNqCp2iXP20KPYPWaqggT6ZkUnu7anTMsv0WFN38avS
         YITROLhJU8ZNaly+AMJoDAYYcciAagDWGPGSxDmaSC+AWDiLJWBBIl5QBu22t4SqwhoF
         XJhc562BGGrOiUFFBM5gwhhqePbPomnWnCSmvA5z6+7E235MYkSzfZgy2GMnArC72xWK
         6rYnCjUuhZO5O/QI2PMmOVs8zbdOXO6ipFoVtALApDxevkU47RMu/wbSx9tfMyikhO9k
         SHjw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=+Sk2Cqff0S1M37KAwDa+rZWvdnZdT5vdMDIcRioC2m8=;
        b=AEZl1TCMb56fhYKm6CHCwzNsfebHI/4hJtxW25gmkRYMlnzYxUEXtRcvf18BL9qcDL
         3m1AHMIWhSvLcKbsuyT9w6MwlU+RPt9ktA37aaL3E1JtusgDpoA3xQvKzZHwj/6+Z+gj
         f2RZxswthpC4XYWBegvEZQOo4t2R3tCKEk0C3LlvR1IzKfFeAvlwyeo3zp1jMpe9lXub
         KzaaX92t4Y2/2w7iY0mEX5MRP/UnHoNQHqH98CTJ9rXXORx+7cnXlwUVfzhWPcANbLWA
         beg8hd/9nWLm1JJlkzQ6ujetETBoVc2yUqUm4VyQMR31kdqo5ke+F9917EsBM1HgdMmu
         35fg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@googlemail.com header.s=20161025 header.b=BckXvR8R;
       spf=pass (google.com: domain of hea...@googlemail.com designates 209.85.216.175 as permitted sender) smtp.mailfrom=hea...@googlemail.com
Return-Path: <hea...@googlemail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id a2si676439pfa.560.2017.10.30.22.23.10
        for <singu...@lbl.gov>;
        Mon, 30 Oct 2017 22:23:10 -0700 (PDT)
Received-SPF: pass (google.com: domain of hea...@googlemail.com designates 209.85.216.175 as permitted sender) client-ip=209.85.216.175;
Authentication-Results: mx.google.com;
       dkim=pass head...@googlemail.com header.s=20161025 header.b=BckXvR8R;
       spf=pass (google.com: domain of hea...@googlemail.com designates 209.85.216.175 as permitted sender) smtp.mailfrom=hea...@googlemail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2F/AQB5CPhZf6/YVdFdHgYMGAEFAQsBg?=
 =?us-ascii?q?wYCgRBuJweDbQiBNpd+gXx8h1OILoVFEIEiAxlDIgEMgQlTgmtPAoRYBz8YAQE?=
 =?us-ascii?q?BAQEBAQEBAQECEAEBCQsLCCYxgjgFAgECAh4EBEYnMAEBAQEBAQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEBAQEBAQEBAQkCDB8EOwEBAQECASMdAQ0sAwELBgULDSoCAiE?=
 =?us-ascii?q?BDwMBBQEcDgcEARwEiDGBOAEDDQgFC5t8QIwMgieDCQWDYwpADYNAAQEBBwEBA?=
 =?us-ascii?q?QEBAQEZAgYSgxyCB4M8gnU1gmqBdAEMBgELQYJogmEFiimOWYhFPIdmh01QhHm?=
 =?us-ascii?q?CMZB8jF85SoRCAYNXGR+BFR+BCDRlNAEgCB0VfoIvCYIaKg8cgWhANgEHiXIOG?=
 =?us-ascii?q?DAsgUEBAQE?=
X-IronPort-AV: E=Sophos;i="5.44,322,1505804400"; 
   d="scan'208,217";a="2461238"
Received: from mail-qt0-f175.google.com ([209.85.216.175])
  by fe4.lbl.gov with ESMTP; 30 Oct 2017 22:22:58 -0700
Received: by mail-qt0-f175.google.com with SMTP id k31so19302582qta.6
        for <singu...@lbl.gov>; Mon, 30 Oct 2017 22:22:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=googlemail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=+Sk2Cqff0S1M37KAwDa+rZWvdnZdT5vdMDIcRioC2m8=;
        b=BckXvR8Rg+WsPnfnmMV+bfUdGp3jXik4iGwy6394mtmeitZJ2bRZiTbWgXz5kHIwP7
         Y86+TYtmT2EdIlokWCN7fF9arlxrgDrVQb3os654W4ARdpYi2c/pIOS2W9bPpG5Ur8mB
         0AH2h7ANfNBFesCkJQz9HGF4WR5PDQHA/tJySnYjbuoHGw5d14mqRU/EeJJwfw7XWebv
         lGT6d2p70VELO7Pk+E5YNtAnyTOZWsm2gS4GtY3xB/1x6SKCUv5erN0B3hTagfuyLP5q
         QQ6y8/1L6hS8uUtMZoEppoXoeIVXakBiqo9QPfdDpl6aTjYAfC4+nzJmJSmoIB4JMziU
         jq0Q==
X-Gm-Message-State: AMCzsaWb6w2fO+FUURuzWdBv13ZGlYIY/7wvanyG0MxJThBvCLSZTaCK
	2untuU5RihS6nra/ISW+59QQk/iW4SwAKiNJROkNHA==
X-Received: by 10.200.15.204 with SMTP id f12mr1082102qtk.161.1509427378502;
 Mon, 30 Oct 2017 22:22:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.12.130.134 with HTTP; Mon, 30 Oct 2017 22:22:28 -0700 (PDT)
In-Reply-To: <CAJuraoi5RmNJ5rh1UJGXQ_TrCuWSxB1290PgsDfthADx1_bdhQ@mail.gmail.com>
References: <dc630f79-37fc-4c83-a358-b9e8e9b39d1f@lbl.gov> <CAJm6r9_BDGUV5Pq9E5ZC6rkX22qmY8O-GEZW3vjrZmL7sq9nBQ@mail.gmail.com>
 <CAJuraoi5RmNJ5rh1UJGXQ_TrCuWSxB1290PgsDfthADx1_bdhQ@mail.gmail.com>
From: John Hearns <hea...@googlemail.com>
Date: Tue, 31 Oct 2017 06:22:28 +0100
Message-ID: <CAPqNE2Wi66dQJea5gcvEOC+=_4G3-k1pXA4z41PEP-LjeO=MQQ@mail.gmail.com>
Subject: Re: [Singularity] Panasas on Singularity
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="94eb2c043826b5c1b5055cd0f263"

--94eb2c043826b5c1b5055cd0f263
Content-Type: text/plain; charset="UTF-8"

Nick, I don't have much advice for you.
I have done a lot of work with Panasas int he past, and have a lto of
respect for those guys. I put in an early installation at the University of
Nottingham in the UK.
One thing to consider - Panasas is software defined storage, which runs on
BSD on the storage blades/director blades (metadata servers). The protocol
on te wire is really iSCSI from what I gather.
So if anyone here has experience of iSCSI with Singularity then that will
be relevant.
Also Panasas will build drivers for you for the particular Linux kernel you
are running, but that won't be relevant in your case.

Admitting defeat slightly, you could get started by using NFS mounts?  Yes
I know that is not using a high performance filesystem properly...




On 30 October 2017 at 22:31, Nick Eggleston <nicke...@gmail.com> wrote:

> I've solved one (stupid) problem and exchanged it for another. The reason
> that it didn't seem to matter what settings I changed is while I was
> messing with the correct config file, I had written the LMOD module to look
> at the wrong version of Singularity. Now that I got that cleared up I'm
> getting actual errors about "Could not mount Singularity overlay: No such
> device" which I expect is more of a system problem than a Singularity
> problem. I'll keep digging into this, I appreciate anyone who even thought
> about giving me advice, some Mondays are even more Monday than others.
>
> On Mon, Oct 30, 2017 at 2:36 PM, Krishna Muriki <kmu...@lbl.gov> wrote:
>
>> Just wanted to throw a suggestion and see if that helps. Try setting
>> 'mount home = no' and explicitly bind mount each of the filesystems that
>> you need to access inside the container. I did notice the 'mount home =
>> yes' is doing few things which is causing issues. I did not get a chance
>> yet to further investigate this and submit a proper github issue here. This
>> might not have anything to do with Panasas. Try this and see if it helps.
>>
>> Good luck,
>> --Krishna.
>>
>> On Mon, Oct 30, 2017 at 11:16 AM, Nick Eggleston <nicke...@gmail.com>
>> wrote:
>>
>>> Hey All,
>>>
>>> I'm one of the HPC Sysadmins for University of Kansas. We use Panasas
>>> for our cluster file storage, and I'm having a bit of an issue getting
>>> Singularity to play nice with some of our Panasas. As is standard with a
>>> lot of compute clusters, we give users a home, work, and scratch directory
>>> to play in. Home is accessible under /home/<username> as would be expected,
>>> and we do this with a symlink to the home volume under our Panasas mount.
>>> Like I said before we also give a work and scratch directory to every user
>>> and these directories have higher space limitations than the home directory
>>> so that's where most everyone puts their files. When I start a Singularity
>>> container, I don't have any issues with /home, that just seems to work.
>>> What doesn't seem to work is anything else that needs to reach out to the
>>> Panasas. Users all have a symlink in their homedir that points to their
>>> work and scratch and those links appear to be broken, and that Panasas
>>> doesn't show up at all, even though I've enabled what I think would be the
>>> relevant settings in my singularity.conf file (which I'll paste a condensed
>>> version of below in case you all see something I don't). Does anyone have
>>> any experience with using Panasas in this context with Singularity? So far
>>> the people I know that use both only use the homedir as the only link to
>>> the outside world so to speak, but that's not enough for our users. Also,
>>> just for a relevant info include, we run RHEL 6.4 with a 2.6 kernel.
>>>
>>> Thanks!
>>>
>>>
>>> # SINGULARITY.CONF
>>> # This is the global configuration file for Singularity. This file
>>> controls
>>> # what the container is allowed to do on a particular host, and as a
>>> result
>>> # this file must be owned by root.
>>>
>>> allow setuid = yes
>>>
>>> max loop devices = 256
>>>
>>> allow pid ns = yes
>>>
>>> config passwd = yes
>>>
>>> config group = yes
>>>
>>> config resolv_conf = yes
>>>
>>> mount proc = yes
>>>
>>> mount sys = yes
>>>
>>> mount dev = yes
>>>
>>> mount devpts = yes
>>>
>>> mount home = yes
>>>
>>> mount tmp = yes
>>>
>>> #mount hostfs = no
>>> mount hostfs = yes
>>>
>>> #bind path = /etc/singularity/default-nsswitch.conf:/etc/nsswitch.conf
>>> #bind path = /opt
>>> #bind path = /scratch
>>> #bind path = /etc/localtime
>>> bind path = /etc/hosts
>>> bind path = /panfs/pfs.local
>>>
>>> user bind control = yes
>>>
>>> #enable overlay = no
>>> enable overlay = yes
>>>
>>> sessiondir max size = 16
>>>
>>> #limit container owners = gmk, singularity, nobody
>>>
>>> #limit container paths = /scratch, /tmp, /global
>>>
>>> allow container squashfs = yes
>>> allow container extfs = yes
>>> allow container dir = yes
>>>
>>> #autofs bug path = /nfs
>>> #autofs bug path = /cifs-share
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to a topic in the
>> Google Groups "singularity" group.
>> To unsubscribe from this topic, visit https://groups.google.com/a/lb
>> l.gov/d/topic/singularity/u2CokAIQ3zU/unsubscribe.
>> To unsubscribe from this group and all its topics, send an email to
>> singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--94eb2c043826b5c1b5055cd0f263
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>Nick, I don&#39;t have much advice for you.</div><div=
>I have done a lot of work with Panasas int he past, and have a lto of resp=
ect for those guys. I put in an early installation at the University of Not=
tingham in the UK.</div><div>One thing to consider - Panasas is software de=
fined storage, which runs on BSD on the storage blades/director blades (met=
adata servers). The protocol on te wire is really iSCSI from what I gather.=
 =C2=A0</div><div>So if anyone here has experience of iSCSI with Singularit=
y then that will be relevant.</div><div>Also Panasas will build drivers for=
 you for the particular Linux kernel you are running, but that won&#39;t be=
 relevant in your case.</div><div><br></div><div>Admitting defeat slightly,=
 you could get started by using NFS mounts?=C2=A0 Yes I know that is not us=
ing a high performance filesystem properly...</div><div><br></div><div><br>=
</div><div><br></div></div><div class=3D"gmail_extra"><br><div class=3D"gma=
il_quote">On 30 October 2017 at 22:31, Nick Eggleston <span dir=3D"ltr">&lt=
;<a href=3D"mailto:nicke...@gmail.com" target=3D"_blank">nicke...@gmail.com=
</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin=
:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">I=
&#39;ve solved one (stupid) problem and exchanged it for another. The reaso=
n that it didn&#39;t seem to matter what settings I changed is while I was =
messing with the correct config file, I had written the LMOD module to look=
 at the wrong version of Singularity. Now that I got that cleared up I&#39;=
m getting actual errors about &quot;Could not mount Singularity overlay: No=
 such device&quot; which I expect is more of a system problem than a Singul=
arity problem. I&#39;ll keep digging into this, I appreciate anyone who eve=
n thought about giving me advice, some Mondays are even more Monday than ot=
hers.</div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote"><div><=
div class=3D"h5">On Mon, Oct 30, 2017 at 2:36 PM, Krishna Muriki <span dir=
=3D"ltr">&lt;<a href=3D"mailto:kmu...@lbl.gov" target=3D"_blank">kmu...@lbl=
.gov</a>&gt;</span> wrote:<br></div></div><blockquote class=3D"gmail_quote"=
 style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><d=
iv><div class=3D"h5"><div dir=3D"ltr">Just wanted to throw a suggestion and=
 see if that helps. Try setting &#39;mount home =3D no&#39; and explicitly =
bind mount each of the filesystems that you need to access inside the conta=
iner. I did notice the &#39;mount home =3D yes&#39; is doing few things whi=
ch is causing issues. I did not get a chance yet to further investigate thi=
s and submit a proper github issue here. This might not have anything to do=
 with Panasas. Try this and see if it helps.=C2=A0<div><br></div><div>Good =
luck,</div><div>--Krishna.</div></div><div class=3D"gmail_extra"><br><div c=
lass=3D"gmail_quote">On Mon, Oct 30, 2017 at 11:16 AM, Nick Eggleston <span=
 dir=3D"ltr">&lt;<a href=3D"mailto:nicke...@gmail.com" target=3D"_blank">ni=
cke...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote"=
 style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><d=
iv dir=3D"ltr">Hey All,<div><br></div><div>I&#39;m one of the HPC Sysadmins=
 for University of Kansas. We use Panasas for our cluster file storage, and=
 I&#39;m having a bit of an issue getting Singularity to play nice with som=
e of our Panasas. As is standard with a lot of compute clusters, we give us=
ers a home, work, and scratch directory to play in. Home is accessible unde=
r /home/&lt;username&gt; as would be expected, and we do this with a symlin=
k to the home volume under our Panasas mount. Like I said before we also gi=
ve a work and scratch directory to every user and these directories have hi=
gher space limitations than the home directory so that&#39;s where most eve=
ryone puts their files. When I start a Singularity container, I don&#39;t h=
ave any issues with /home, that just seems to work. What doesn&#39;t seem t=
o work is anything else that needs to reach out to the Panasas. Users all h=
ave a symlink in their homedir that points to their work and scratch and th=
ose links appear to be broken, and that Panasas doesn&#39;t show up at all,=
 even though I&#39;ve enabled what I think would be the relevant settings i=
n my singularity.conf file (which I&#39;ll paste a condensed version of bel=
ow in case you all see something I don&#39;t). Does anyone have any experie=
nce with using Panasas in this context with Singularity? So far the people =
I know that use both only use the homedir as the only link to the outside w=
orld so to speak, but that&#39;s not enough for our users. Also, just for a=
 relevant info include, we run RHEL 6.4 with a 2.6 kernel.</div><div><br></=
div><div>Thanks!</div><div><br></div><div><br></div><div><div># SINGULARITY=
.CONF</div><div># This is the global configuration file for Singularity. Th=
is file controls</div><div># what the container is allowed to do on a parti=
cular host, and as a result</div><div># this file must be owned by root.</d=
iv><div><br></div><div>allow setuid =3D yes<br></div><div><br></div><div>ma=
x loop devices =3D 256</div><div><br></div><div>allow pid ns =3D yes<br></d=
iv><div><br></div><div>config passwd =3D yes<br></div><div><br></div><div>c=
onfig group =3D yes<br></div><div><br></div><div>config resolv_conf =3D yes=
<br></div><div><br></div><div>mount proc =3D yes<br></div><div><br></div><d=
iv>mount sys =3D yes<br></div><div><br></div><div>mount dev =3D yes<br></di=
v><div><br></div><div>mount devpts =3D yes<br></div><div><br></div><div>mou=
nt home =3D yes</div><div><br></div><div>mount tmp =3D yes</div><div><br></=
div><div>#mount hostfs =3D no</div><div>mount hostfs =3D yes</div><div><br>=
</div><div>#bind path =3D /etc/singularity/default-nsswi<wbr>tch.conf:/etc/=
nsswitch.conf<br></div><div>#bind path =3D /opt</div><div>#bind path =3D /s=
cratch</div><div>#bind path =3D /etc/localtime</div><div>bind path =3D /etc=
/hosts</div><div>bind path =3D /panfs/pfs.local</div><div><br></div><div>us=
er bind control =3D yes</div><div><br></div><div>#enable overlay =3D no<br>=
</div><div>enable overlay =3D yes</div><div><br></div><div>sessiondir max s=
ize =3D 16<br></div><div><br></div><div>#limit container owners =3D gmk, si=
ngularity, nobody</div><div><br></div><div>#limit container paths =3D /scra=
tch, /tmp, /global</div><div><br></div><div>allow container squashfs =3D ye=
s<br></div><div>allow container extfs =3D yes</div><div>allow container dir=
 =3D yes</div><div><br></div><div>#autofs bug path =3D /nfs</div><div>#auto=
fs bug path =3D /cifs-share</div></div><span class=3D"m_3420685998535107441=
HOEnZb"><font color=3D"#888888"><span class=3D"m_3420685998535107441m_98798=
3729816602886HOEnZb"><font color=3D"#888888"><div><br></div></font></span><=
/font></span></div><span class=3D"m_3420685998535107441HOEnZb"><font color=
=3D"#888888"><span class=3D"m_3420685998535107441m_987983729816602886HOEnZb=
"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</font></span></font></span></blockquote></div><span class=3D"m_34206859985=
35107441HOEnZb"><font color=3D"#888888"><br></font></span></div></div></div=
><span class=3D"HOEnZb"><font color=3D"#888888"><span class=3D"m_3420685998=
535107441HOEnZb"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/u2CokAIQ3zU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lb<wbr>l.gov/d/topic/singularity/u2Co<wbr>kAIQ3zU=
/unsubscribe</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscribe@lbl.=
go<wbr>v</a>.<br>
</font></span></font></span></blockquote></div><br></div><div class=3D"HOEn=
Zb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--94eb2c043826b5c1b5055cd0f263--
