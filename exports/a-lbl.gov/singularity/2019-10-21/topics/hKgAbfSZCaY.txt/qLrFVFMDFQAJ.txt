X-Received: by 2002:a62:1915:: with SMTP id 21mr3803984pfz.0.1549463616220;
        Wed, 06 Feb 2019 06:33:36 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a62:4393:: with SMTP id l19ls11449294pfi.6.gmail; Wed, 06
 Feb 2019 06:33:34 -0800 (PST)
X-Received: by 2002:a63:ce0e:: with SMTP id y14mr9968412pgf.145.1549463614817;
        Wed, 06 Feb 2019 06:33:34 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1549463614; cv=none;
        d=google.com; s=arc-20160816;
        b=oZdlomkYzl+yuFGsu0jikvTTAiQRs1NCHMSZo09h5vT9oFmXF9ce/sB9y3ds97Z8Tm
         eT14J3Iem/RRYUIAMsd6J8FC+IOtvc9C4hpvCp3J/tUZ+tSl2X/XQwkynvvX3iExeDbn
         HJ0H3CBpGPWOAZ/Kd+03PrySJR/aY2mcat6l7YSyPvvYmHY348Y/MdKr/e16y3T5t3/i
         XFX38XgFOgLBUCn3Kuzs4HGfdpW53Z2y9FC5oLw3GIhTZvLpNfHF5CwFuwBAeDswzU1P
         TmFowI9qf99z1z0zmOFgvaRSLoim5Tziut8u46rpFVEmSDrcKJbHp2BMbeW76BuDbGJe
         8aPw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:in-reply-to:references
         :mime-version:dkim-signature;
        bh=6HGpApvndXcqmdDN0mKCfdUltIht0T0Jl5Hxc1xY6Uk=;
        b=RRhEj294B1Nu3s8Vbx3pWn5Ey6MjstxS+PoVSwF5di3yqO73JrbqozbsyMx7De6wWv
         OGAAIxkZOMVdaT2C8d7iqO8DpBwLsyywWHeaezp8qB+KoV+GxHA5SxtqzOk7JxPK/hXU
         GXvIZLbWBtNu3cm1JXgixQFAZ2SVek0EwlF8Apw0IuS47P+aEINnTAMxJkT0c8Tso+vP
         hqLXJjk3ZCBXjhXU17qfjQY7k8GqSZjnIYECHjmTOGIV3axiqFyNWvS2ywPfT2pczLNC
         UUCM37e0qE1qNtpjkX/fsZwKpMLEJfRn/txHpw2+Cbyznr2BfYQbG4TJZv9NdEJE269T
         fPpg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=fcebTH8W;
       spf=pass (google.com: domain of fert...@gmail.com designates 209.85.210.47 as permitted sender) smtp.mailfrom=fert...@gmail.com
Return-Path: <fert...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [131.243.228.52])
        by mx.google.com with ESMTP id f9si1406022pgc.85.2019.02.06.06.33.34
        for <singu...@lbl.gov>;
        Wed, 06 Feb 2019 06:33:34 -0800 (PST)
Received-SPF: pass (google.com: domain of fert...@gmail.com designates 209.85.210.47 as permitted sender) client-ip=209.85.210.47;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=fcebTH8W;
       spf=pass (google.com: domain of fert...@gmail.com designates 209.85.210.47 as permitted sender) smtp.mailfrom=fert...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EeAgDV7lpchi/SVdFlHAEBAQQBAQcEA?=
 =?us-ascii?q?QGBZYFbgRBQMyeDfQaBHYJekAqCDZIghgOBKzwlARWBSYJ1AoMbIjgSAQMBAQI?=
 =?us-ascii?q?BAQIBAQIQAQEBCAsLCBsOIwyCOgUCAx8HDk07MAEBAQEBAQEBAQEfAi0pAQEYA?=
 =?us-ascii?q?QEBAQIBIwQZAQ0OHQEDAQsGBQsNKgICIQEBDgMBBQELEQ4HBAEHEwIEgwEBJwG?=
 =?us-ascii?q?BQAEDDQgFoSg8ixt8FgUBF4J5BYRJChknDV+BNwIGEowxF4FAP4ERgmQuhEkFA?=
 =?us-ascii?q?RIBgyiCVwKJdQqBAIY7WJBDMwmMfYVGGZJOh0aURTCBPFcwcTMaCBsVOzGCOwm?=
 =?us-ascii?q?CExqBCQEJgkGKcSQwEIwsR4F3AQE?=
X-IronPort-AV: E=Sophos;i="5.58,340,1544515200"; 
   d="scan'208,217";a="142887549"
Received: from mail-ot1-f47.google.com ([209.85.210.47])
  by fe3.lbl.gov with ESMTP; 06 Feb 2019 06:33:23 -0800
Received: by mail-ot1-f47.google.com with SMTP id e12so12157612otl.5
        for <singu...@lbl.gov>; Wed, 06 Feb 2019 06:33:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc;
        bh=6HGpApvndXcqmdDN0mKCfdUltIht0T0Jl5Hxc1xY6Uk=;
        b=fcebTH8W9fmqip2HM2dNs6cyxVPzRoO23cyTPRqNoXHaiuSSs+BHIE4r3Mezn1MsUl
         /3qQyFwkH1QrV8fxfgpFgFnUC9B+Yioi4r9agfUuR+SNYN7dw8UO8VbFqacK+I5l7D5Y
         u1za41BemTvr3n8V1v1yNBfq/XMXbXHG27jloMWeNW3wUosAtWEqRkaDl4N7+LHyDcK7
         LwjZFl0UYjNNlA0G1Ytm12WZONna7mdIjgzsqZrO5aTlN6z3fxYSGMYiqM130wy/FyrS
         o8lMs4GZiViVOJj4Ui0tvB3Cku5cV5qEDklCQ9gWHPat4TTwAvq2z9ZG5worDhDbIo+9
         I3Lw==
X-Gm-Message-State: AHQUAuauqMb/rL+DlFjcqFzH01NiulRAZlK8lHXxlMbpn+2JLbdmau2l
	WXmHljH5h9nWYYiqmJhEtptkFcf48tA1CvaPVTic0w==
X-Received: by 2002:a9d:10f:: with SMTP id 15mr264245otu.84.1549463601886;
 Wed, 06 Feb 2019 06:33:21 -0800 (PST)
MIME-Version: 1.0
References: <71fec772-2642-4b6e-98a3-8801b2ae0cf7@lbl.gov> <59709108-447E-491F-BB5A-C04D3F5AB654@gmail.com>
 <7A55B410-CD7D-4360-A1F3-44E1EF1ECA1B@gmail.com> <cbd878fc-f1a6-4548-8dd2-3ac380da2ecd@lbl.gov>
In-Reply-To: <cbd878fc-f1a6-4548-8dd2-3ac380da2ecd@lbl.gov>
From: Fatih Ertinaz <fert...@gmail.com>
Date: Wed, 6 Feb 2019 09:32:45 -0500
Message-ID: <CAJk3+YVedLNPT6=hxrQtAR6xJG3NtERLRFRBCO3M4JL6YXS9Yg@mail.gmail.com>
Subject: Re: [Singularity] Error when running OpenFoam over Singularity using Slurm
To: singularity@lbl.gov
Cc: wangs...@gmail.com
Content-Type: multipart/alternative; boundary="000000000000953c5f05813a9b13"

--000000000000953c5f05813a9b13
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hello Carmen

To me this looks like an OpenMPI & Slurm issue rather than an OF & Slurm
problem.

Few things you can check;
xx Try to execute simple jobs using Slurm, e.g. printing hostnames or mpi
ping-pong stuff.
xx Do you know how OpenMPI is installed in the host? Maybe it is built with
some other underlying libraries regarding IB that you don't have in your
container.

I'd say if the first one works with hostnames then I'd say focus on the
OpenMPI installation.

On Wed, Feb 6, 2019 at 4:44 AM 'ccvera' via singularity <singu...@lbl.gov>
wrote:

> Thanks a lot for your quickly reply :)
>
> This solution doesn't work for me. I tried to unset all SLURM environment
> variables (first SLURM_JOBID, then SLURM_NODELIST and finally all as you
> told me) and i obtain the same MPI error.
>
> Carmen
>
> El martes, 5 de febrero de 2019, 17:56:27 (UTC+1), Shenglong Wang escribi=
=C3=B3:
>>
>> It seems
>>
>> unset SLURM_JOBID
>>
>> is enough to cheat mpiexec
>>
>> Shenglong
>>
>> On Feb 5, 2019, at 11:37 AM, Shenglong Wang <wa...@gmail.com> wrote:
>>
>> Can you try to unset all SLURM environment variables?
>>
>> for e in $(env | egrep ^SLURM_ | cut -d=3D -f1); do unset $e; done
>>
>> or
>>
>> unset SLURM_NODELIST
>>
>> But you=E2=80=99ll have to manually generate host file.
>>
>> Best,
>> Shenglong
>>
>>
>> On Feb 5, 2019, at 11:30 AM, 'ccvera' via singularity <si...@lbl.gov>
>> wrote:
>>
>> Dear all,
>>
>> I'm experiencing some issues running OpenFOAM over singularity with
>> slurm.
>>
>> I've several images based on Ubuntu and within several versions of
>> OpenMPI and PMIx and i'm able to run OpenFOAM correctly without use slur=
m
>> (directly on the node) using next command:
>>
>> $ mpirun -n 16 singularity exec -B /home ../../of6/openfoam6.x.img
>> simpleFoam -parallel -case
>> /home/carmen/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pruebaOF6=
_16cores
>>
>> My problem comes when I run my program with slurm. Whether I make salloc
>> or execute a script with sbatch, it shows me the following error:
>>
>> It looks like MPI_INIT failed for some reason; your parallel process is
>>
>> likely to abort.  There are many reasons that a parallel process can
>>
>> fail during MPI_INIT; some of which are due to configuration or
>> environment
>>
>> problems.  This failure appears to be an internal failure; here's some
>>
>> additional information (which may only be relevant to an Open MPI
>>
>> developer):
>>
>>
>>   ompi_mpi_init: ompi_rte_init failed
>>
>>   --> Returned "(null)" (-43) instead of "Success" (0)
>>
>>
>> and
>>
>> *** An error occurred in MPI_Init_thread
>>
>> *** on a NULL communicator
>>
>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
>>
>> ***    and potentially your MPI job)
>>
>> [cn3045:369] Local abort before MPI_INIT completed completed
>> successfully, but am not able to aggregate error messages, and not able =
to
>> guarantee that all other processes were killed!
>>
>>
>> I know I must have the same openMPI versions on both (host and container=
)
>> and I have also tried other versions of OpenMPI (2.X.X) and in all cases
>> OpenFOAM works correctly, but at the moment I want to run it with slurm =
it
>> show me the errors.
>>
>> I have also tried other ways to run the program with srun using the
>> option --mpi=3Dpmi2 (among others) but I always find the same problem.
>>
>> I use the following script to run OpenFoam:
>>
>> ----------------------------
>>
>> #!/bin/bash
>>
>>
>> #SBATCH -N 1
>>
>> #SBATCH -p haswell
>>
>> #SBATCH -J test_OpenFOAM
>>
>> #SBATCH --output=3D"singularity.%j.o"
>>
>> #SBATCH --error=3D"singularity.%j.e"
>>
>>
>> module load haswell/singularity_2.6.0
>>
>> module load haswell/openmpi_3.1.2_gcc8.2.0_pmix
>>
>>
>> ulimit -s unlimited
>>
>>
>> mpirun -n 16 singularity exec ../../of6/openfoam6.x.img simpleFoam
>> -parallel -case
>> /home/software/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pruebaO=
F6_16cores
>>
>> ----------------------------
>>
>>
>> The versions that I'm using are:
>>
>> *Host: *
>>
>> OS: CentOS7.5
>>
>> OpenMPI: 3.1.2
>>
>> PMIx: 2.1.4
>>
>>
>> *Container:*
>>
>> OS: Ubuntu16.04
>>
>> OpenMPI: 3.1.2
>>
>> PMIx: 2.1.4
>>
>>
>> Can it be a configuration problem of SLURM? Is there any limitation of
>> SLURM that is affecting OpenFOAM?
>>
>> Some info about slurm:
>>
>> # srun --version
>> slurm 18.08.3
>> # srun --mpi=3Dlist
>> srun: MPI types are...
>> srun: pmi2
>> srun: openmpi
>> srun: none
>>
>>
>> I'm a little bit lost with this issue :(
>> Can someone give me some lights?
>>
>> Thanks a lot in advance,
>> Carmen
>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>>
>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--000000000000953c5f05813a9b13
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello Carmen<div><br></div><div>To me this looks like an O=
penMPI &amp; Slurm issue rather than an OF &amp; Slurm problem.=C2=A0</div>=
<div><br></div><div>Few things you can check;</div><div>xx Try to execute s=
imple jobs using Slurm, e.g. printing hostnames or mpi ping-pong stuff.=C2=
=A0</div><div>xx Do you know how OpenMPI is installed in the host? Maybe it=
 is built with some other underlying libraries regarding IB that you don&#3=
9;t have in your container.</div><div><br></div><div>I&#39;d say if the fir=
st one works with hostnames then I&#39;d say focus on the OpenMPI installat=
ion.</div></div><br><div class=3D"gmail_quote"><div dir=3D"ltr" class=3D"gm=
ail_attr">On Wed, Feb 6, 2019 at 4:44 AM &#39;ccvera&#39; via singularity &=
lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</=
a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0p=
x 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><d=
iv dir=3D"ltr"><div>Thanks a lot for your quickly reply :)</div><div><br></=
div><div>This solution doesn&#39;t work for me. I tried to unset all SLURM =
environment variables (first SLURM_JOBID, then SLURM_NODELIST and finally a=
ll as you told me) and i obtain the same MPI error.=C2=A0</div><div><br></d=
iv><div>Carmen</div><br>El martes, 5 de febrero de 2019, 17:56:27 (UTC+1), =
Shenglong Wang  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:=
1ex"><div>It seems=C2=A0<div><br></div><div>unset SLURM_JOBID</div><div><br=
></div><div>is enough to cheat mpiexec</div><div><br></div><div>Shenglong<b=
r><div><br><blockquote type=3D"cite"><div>On Feb 5, 2019, at 11:37 AM, Shen=
glong Wang &lt;<a rel=3D"nofollow">wa...@gmail.com</a>&gt; wrote:</div><br>=
<div><div>Can you try to unset all SLURM environment variables?<div><br></d=
iv><div><div>for e in $(env | egrep ^SLURM_ | cut -d=3D -f1); do unset $e; =
done</div><div><br></div><div>or</div><div><br></div><div>unset SLURM_NODEL=
IST</div><div><br></div><div>But you=E2=80=99ll have to manually generate h=
ost file.</div><div><br></div><div>Best,</div><div>Shenglong</div><div><br>=
<div><br><blockquote type=3D"cite"><div>On Feb 5, 2019, at 11:30 AM, &#39;c=
cvera&#39; via singularity &lt;<a rel=3D"nofollow">si...@lbl.gov</a>&gt; wr=
ote:</div><br><div><div dir=3D"ltr"><div><div>Dear all,</div><div><br></div=
><div>I&#39;m experiencing some issues running OpenFOAM over singularity wi=
th slurm.=C2=A0</div><div><br></div><div>I&#39;ve several images based on U=
buntu and within several versions of OpenMPI and PMIx and i&#39;m able to r=
un OpenFOAM correctly without use slurm (directly on the node) using next c=
ommand:</div><div><br></div><div>$ mpirun -n 16 singularity exec -B /home .=
./../of6/openfoam6.x.img simpleFoam -parallel -case /home/carmen/test_singu=
larity/OpenFOAM/pruebaOF6_16cores_SLURM/pruebaOF6_16cores</div><div><br></d=
iv><div>My problem comes when I run my program with slurm. Whether I make s=
alloc or execute a script with sbatch, it shows me the following error:</di=
v><div><br></div></div><blockquote style=3D"margin:0px 0px 0px 40px;border:=
none;padding:0px"><blockquote style=3D"margin:0px 0px 0px 40px;border:none;=
padding:0px"><div><div><font color=3D"#666666">It looks like MPI_INIT faile=
d for some reason; your parallel process is</font></div></div></blockquote>=
<blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div>=
<div><font color=3D"#666666">likely to abort.=C2=A0 There are many reasons =
that a parallel process can</font></div></div></blockquote><blockquote styl=
e=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><font color=
=3D"#666666">fail during MPI_INIT; some of which are due to configuration o=
r environment</font></div></div></blockquote><blockquote style=3D"margin:0p=
x 0px 0px 40px;border:none;padding:0px"><div><div><font color=3D"#666666">p=
roblems.=C2=A0 This failure appears to be an internal failure; here&#39;s s=
ome</font></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px =
40px;border:none;padding:0px"><div><div><font color=3D"#666666">additional =
information (which may only be relevant to an Open MPI</font></div></div></=
blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding=
:0px"><div><div><font color=3D"#666666">developer):</font></div></div></blo=
ckquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0p=
x"><div><div><font color=3D"#666666"><br></font></div></div></blockquote><b=
lockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><d=
iv><font color=3D"#666666">=C2=A0 ompi_mpi_init: ompi_rte_init failed</font=
></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bord=
er:none;padding:0px"><div><div><font color=3D"#666666">=C2=A0 --&gt; Return=
ed &quot;(null)&quot; (-43) instead of &quot;Success&quot; (0)</font></div>=
</div></blockquote></blockquote><div><div><br></div><div>and</div><div><br>=
</div></div><blockquote style=3D"margin:0px 0px 0px 40px;border:none;paddin=
g:0px"><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px=
"><div><div><font color=3D"#666666">*** An error occurred in MPI_Init_threa=
d</font></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40=
px;border:none;padding:0px"><div><div><font color=3D"#666666">*** on a NULL=
 communicator</font></div></div></blockquote><blockquote style=3D"margin:0p=
x 0px 0px 40px;border:none;padding:0px"><div><div><font color=3D"#666666">*=
** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,</fo=
nt></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bo=
rder:none;padding:0px"><div><div><font color=3D"#666666">***=C2=A0 =C2=A0 a=
nd potentially your MPI job)</font></div></div></blockquote><blockquote sty=
le=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><font colo=
r=3D"#666666">[cn3045:369] Local abort before MPI_INIT completed completed =
successfully, but am not able to aggregate error messages, and not able to =
guarantee that all other processes were killed!</font></div></div></blockqu=
ote></blockquote><div><div><br></div><div>I know I must have the same openM=
PI versions on both (host and container) and I have also tried other versio=
ns of OpenMPI (2.X.X) and in all cases OpenFOAM works correctly, but at the=
 moment I want to run it with slurm it show me the errors.</div><div><br></=
div><div>I have also tried other ways to run the program with srun using th=
e option --mpi=3Dpmi2 (among others) but I always find the same problem.</d=
iv><div><br></div><div>I use the following script to run OpenFoam:</div></d=
iv><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><d=
iv><div>----------------------------</div></div></blockquote><blockquote st=
yle=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><blockquote style=
=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>#!/bin/bash<=
/div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border=
:none;padding:0px"><div><div><br></div></div></blockquote><blockquote style=
=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>#SBATCH -N 1=
</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;borde=
r:none;padding:0px"><div><div>#SBATCH -p haswell=C2=A0</div></div></blockqu=
ote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><=
div><div>#SBATCH -J test_OpenFOAM</div></div></blockquote><blockquote style=
=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>#SBATCH --ou=
tput=3D&quot;singularity.%j.o&quot;=C2=A0</div></div></blockquote><blockquo=
te style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>#SBA=
TCH --error=3D&quot;singularity.%j.e&quot;</div></div></blockquote><blockqu=
ote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><br=
></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bord=
er:none;padding:0px"><div><div>module load haswell/singularity_2.6.0</div><=
/div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;=
padding:0px"><div><div>module load haswell/openmpi_3.1.2_gcc8.2.0_pmix</div=
></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:non=
e;padding:0px"><div><div><br></div></div></blockquote><blockquote style=3D"=
margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>ulimit -s unlimi=
ted</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bo=
rder:none;padding:0px"><div><div><br></div></div></blockquote><blockquote s=
tyle=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>mpirun -=
n 16=C2=A0singularity exec ../../of6/openfoam6.x.img simpleFoam -parallel -=
case /home/software/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/prueb=
aOF6_16cores</div></div></blockquote></blockquote><blockquote style=3D"marg=
in:0px 0px 0px 40px;border:none;padding:0px"><div><div>--------------------=
--------</div></div></blockquote><div><div><br></div><div>The versions that=
 I&#39;m using are:</div></div><blockquote style=3D"margin:0px 0px 0px 40px=
;border:none;padding:0px"><div><div><b>Host:=C2=A0</b></div></div></blockqu=
ote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><=
blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><=
div>OS: CentOS7.5</div></div></blockquote><blockquote style=3D"margin:0px 0=
px 0px 40px;border:none;padding:0px"><div><div>OpenMPI: 3.1.2</div></div></=
blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding=
:0px"><div><div>PMIx: 2.1.4</div></div></blockquote></blockquote><blockquot=
e style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><br><=
/div></div><div><div><b>Container:</b></div></div></blockquote><blockquote =
style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><blockquote style=
=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>OS: Ubuntu16=
.04</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bo=
rder:none;padding:0px"><div><div>OpenMPI: 3.1.2</div></div></blockquote><bl=
ockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><di=
v>PMIx: 2.1.4</div></div></blockquote></blockquote><div><div><br></div><div=
>Can it be a configuration problem of SLURM? Is there any limitation of SLU=
RM that is affecting OpenFOAM?</div><div><br></div><div>Some info about slu=
rm:</div></div><blockquote style=3D"margin:0px 0px 0px 40px;border:none;pad=
ding:0px"><div><div># srun --version</div></div><div><div>slurm 18.08.3</di=
v></div><div><div># srun --mpi=3Dlist</div></div><div><div>srun: MPI types =
are...</div></div><div><div>srun: pmi2</div></div><div><div>srun: openmpi</=
div></div><div><div>srun: none</div></div></blockquote><div><div><br></div>=
<div>I&#39;m a little bit lost with this issue :(</div><div>Can someone giv=
e me some lights?</div><div><br></div><div>Thanks a lot in advance,</div><d=
iv>Carmen</div></div><div><br></div></div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></blockquote></div><br></div></div></div></div></blockquote></div><br=
></div></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div>

--000000000000953c5f05813a9b13--
