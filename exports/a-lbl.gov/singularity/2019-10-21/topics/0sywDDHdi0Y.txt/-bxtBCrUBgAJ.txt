Date: Wed, 6 Jul 2016 01:25:23 -0700 (PDT)
From: Raimon Bosch <raimo...@gmail.com>
To: singularity <singu...@lbl.gov>
Cc: r...@open-mpi.org
Message-Id: <15574850-11a7-4317-b784-26631fad4f29@lbl.gov>
In-Reply-To: <CAN7etTxOGqMfyg_C2AWisRWCCs2RKkM91s6SbWTcjYb3X5_Aew@mail.gmail.com>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov> <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
 <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov> <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
 <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov> <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
 <054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov> <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
 <007b47f4-0aea-42dc-b871-d653bb7a67a1@lbl.gov>
 <CAN7etTxOGqMfyg_C2AWisRWCCs2RKkM91s6SbWTcjYb3X5_Aew@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_2191_417424227.1467793523787"

------=_Part_2191_417424227.1467793523787
Content-Type: multipart/alternative; 
	boundary="----=_Part_2192_247351025.1467793523794"

------=_Part_2192_247351025.1467793523794
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable


Hi Gregory,

It fails depending on your environment. In my Ubuntu 14.04 it worked fine,=
=20
but in this instance of Debian jessie I get the following:

> ERROR: Failed to associate image to loop: Device or resource busy

Maybe is because we are using a glusterfs shared disk to keep the=20
containers?

Here you have the entire output:

> sudo mpirun -n 1 singularity exec /mnt/glusterfs/singularity/nasmpi-1.img=
=20
/trace.sh /NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec=20
/mnt/glusterfs/singularity/nasmpi-2.img /trace.sh=20
/NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec=20
/mnt/glusterfs/singularity/nasmpi-3.img /trace.sh=20
/NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec=20
/mnt/glusterfs/singularity/nasmpi-4.img /trace.sh /NPB/NPB3.3-MPI/bin/bt.C.=
4
ERROR: Failed to associate image to loop: Device or resource busy
ERROR: Failed to associate image to loop: Device or resource busy
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
ERROR: Failed to associate image to loop: Device or resource busy
--------------------------------------------------------------------------
mpirun has exited due to process rank 2 with PID 63416 on
node bscgrid30 exiting improperly. There are two reasons this could occur:

1. this process did not call "init" before exiting, but others in
the job did. This can cause a job to hang indefinitely while it waits
for all processes to call "init". By rule, if one process calls "init",
then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).
--------------------------------------------------------------------------

Thanks in advance,

El martes, 5 de julio de 2016, 18:21:48 (UTC+2), Gregory M. Kurtzer=20
escribi=C3=B3:
>
> Hi Raimon,
>
> I am confused as to what the issue is that you are having. Singularity=20
> supports running both across nodes as well as multiple processes per node=
=20
> in any number of containers. Can you paste your command and the error you=
=20
> are getting, maybe that will help.
>
> Thanks!
>
>
>
> On Tue, Jul 5, 2016 at 8:25 AM, Raimon Bosch <rai...@gmail.com=20
> <javascript:>> wrote:
>
>>
>> That solution does not work with nas/mpi benchmark. That's because=20
>> bt.C.16 expects 16 processes. When you split processes it throws an=20
>> exception because number of processes is lower than 16.=20
>>
>> I am still trying to figure out how to do this. Let me know if you have=
=20
>> any suggestion.
>>
>> Cheers,
>>
>> El jueves, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Castain escribi=
=C3=B3:
>>>
>>> I think you are misunderstanding the basic nature of the Singularity=20
>>> =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overlay. S=
o =E2=80=9Csharing=E2=80=9D a container is=20
>>> no different than running on a node where the procs all see the same fi=
le=20
>>> system. Thus, having multiple containers that are identical makes no se=
nse=20
>>> - it=E2=80=99s all the same file system.
>>>
>>> Now if you want to run different containers (e.g., with different=20
>>> libraries or OS in them), then you would use mpirun=E2=80=99s MPMD synt=
ax - for=20
>>> example:
>>>
>>> mpirun -n 1 <container1> : -n 1 <container2>
>>>
>>> HTH
>>> Ralph
>>>
>>> On Jun 23, 2016, at 1:53 AM, Raimon Bosch <rai...@gmail.com> wrote:
>>>
>>>
>>> One last question: What if I want to execute more than one container in=
=20
>>> the same host? With this technique I am bounded always to the same=20
>>> container. One of our experiments was based in measuring performance of=
=20
>>> several containers working in parallel in the same node. Also we had=20
>>> experiments with N containers per host in a multihost environment.
>>>
>>> El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Ku=
rtzer=20
>>> escribi=C3=B3:
>>>>
>>>> Hi Raimon,
>>>>
>>>> Sorry I wasn't clear. I am not yet at my computer and thinking while=
=20
>>>> typing on an iPhone hinders my mental processes. Lol
>>>>
>>>> If I understand your example properly, you have a docker or VM=20
>>>> infrastructure already set up and you are invoking the mpirun commands=
 from=20
>>>> within the virtual environment. Singularity works on a very different=
=20
>>>> premis because integrating a virtual cluster into an existing cluster =
and=20
>>>> scheduling system is a mess.
>>>>
>>>> So starting from the physical nodes, which already have access to all=
=20
>>>> other nodes in the cluster, and already scheduled properly, and in dir=
ect=20
>>>> access to optimized hardware and file systems.... You call mpirun.=20
>>>>
>>>> The mpirun command will take the standard format as you illustrated=20
>>>> with the following change to call Singularity inline:
>>>>
>>>> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img=
=20
>>>> trace.sh bt.C.4
>>>>
>>>> This assumes the following:
>>>>
>>>> 1. The container image which contains the program's you want to run is=
=20
>>>> at ~/container.img and accessible at this path on all nodes referenced=
 in=20
>>>> hosts.txt
>>>> 2. The hosts.txt references other physical nodes you want to run on
>>>> 3. The executables trace.sh and bt.C.4 are both inside the container=
=20
>>>> and in a standard path
>>>>
>>>> In this case we are performing one execution and MPI + singularity is=
=20
>>>> managing all of the communication between processes, nodes and contain=
ers.=20
>>>> Also it is now using any optimized hardware (eg. Infiniband) and exist=
ing=20
>>>> high performance file systems (which should not be accessible via a=20
>>>> virtualized or Docker'ized cluster for security reasons).
>>>>
>>>> This way is actually MUCH simpler then what you are proposing because=
=20
>>>> there is no need to manage any virtual nodes, virtual networks, or res=
ource=20
>>>> manager hacks. It really is as easy as just running any other MPI proc=
ess=20
>>>> on an existing cluster.=20
>>>>
>>>> Hope that helps better!
>>>>
>>>>
>>>>
>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>>>
>>>>>
>>>>>
>>>>> Hi Gregory,
>>>>>
>>>>> I'm not sure if I would achieve the same with your commands. In an=20
>>>>> environment based on dockers or virtual machines we would do somethin=
g like=20
>>>>> this [non applicable to Singularity]:
>>>>>
>>>>> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh=20
>>>>> ./bt.C.4
>>>>>
>>>>> where hosts.txt* is:
>>>>>
>>>>> >vm-ip-01-on-*host01* slots=3D2
>>>>> >vm-ip-01-on-*host02* slots=3D2
>>>>>
>>>>> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>>>>>
>>>>> and trace.sh is:
>>>>>
>>>>> >#!/bin/bash
>>>>> >
>>>>> >export EXTRAE_HOME=3D/opt/extrae/
>>>>> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
>>>>> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>>>>> >
>>>>> >## Run the desired program
>>>>> >$*
>>>>>
>>>>> As you see we only perform one execution and OpenMPI transparently=20
>>>>> manages communication between containers or virtual machines. This co=
mmand=20
>>>>> would work well rather VMs are on the same host or not.
>>>>>
>>>>> What I understand from your response is that now we should execute=20
>>>>> OpenMPI on each host and then merge results manually. I don't know ye=
t how=20
>>>>> to do this merge step or if it is any way to centralize everything li=
ke I=20
>>>>> would do with VMs.
>>>>>
>>>>> Thanks in advance,
>>>>>
>>>>> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M.=
=20
>>>>> Kurtzer escribi=C3=B3:
>>>>>>
>>>>>> Hi Raimon,
>>>>>>
>>>>>> The quick answer is you have mpirun handle that as you would normall=
y=20
>>>>>> where the container file lives on a shared file system:
>>>>>>
>>>>>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>>>>>
>>>>>> Let the MPI outside the container launch the singularity container o=
n=20
>>>>>> each host as it would normally launch any MPI program. Then it will =
call=20
>>>>>> Singulairty and Singularity will launch the MPI program inside the=
=20
>>>>>> container on each of your hosts/servers.=20
>>>>>>
>>>>>> Hope that helps!
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com>=20
>>>>>> wrote:
>>>>>>
>>>>>>>
>>>>>>> Hi Gregory,
>>>>>>>
>>>>>>> Thank you for your answer. One of our experiments needs to run=20
>>>>>>> OpenMPI among several servers. This means that we should put one of=
 our=20
>>>>>>> containers in host01, another in host02 and another in host03 and c=
ollect=20
>>>>>>> the results.=20
>>>>>>>
>>>>>>> How can I do this execution in parallel if I need to communicate=20
>>>>>>> with more than one server?
>>>>>>>
>>>>>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtze=
r=20
>>>>>>> escribi=C3=B3:
>>>>>>>>
>>>>>>>> Hi Raimon,
>>>>>>>>
>>>>>>>> The communication model of a Singularity container is very=20
>>>>>>>> different from that of a Docker implementation. This is because Do=
cker for=20
>>>>>>>> all practical purposes emulates a virtual machine as each containe=
r has=20
>>>>>>>> it's own IP address and thus it's own ssh server. It also carries =
its own=20
>>>>>>>> set of complexities, for example networks need to be segregated/VL=
an'ed,=20
>>>>>>>> DNS/host resolution needs to be dynamic and passed down to the con=
tainers=20
>>>>>>>> (so they can reach each other), ssh daemons and other process runn=
ing=20
>>>>>>>> inside the containers, management via an existing scheduling syste=
m, and=20
>>>>>>>> the list goes on and on.
>>>>>>>>
>>>>>>>> Think of it this way, Singularity does not do any of that... It=20
>>>>>>>> runs a program within the container as if it were running on the h=
ost=20
>>>>>>>> itself, so to communicate between containers is as easy as communi=
cating=20
>>>>>>>> between programs. So for MPI, it would happen with the MPI on the =
physical=20
>>>>>>>> host (outside the container) invoking the container subsystem whic=
h then=20
>>>>>>>> invokes the MPI programs within the container and the MPI programs=
 within=20
>>>>>>>> the container communicate back to the MPI (orted) outside the cont=
ainer on=20
>>>>>>>> the host to get access to the host resources. In this model all av=
ailable=20
>>>>>>>> resources and infrastructure can be leveraged at full bandwidth by=
 the=20
>>>>>>>> contained processes and all of the aforementioned complexities aki=
n to=20
>>>>>>>> running on a virtualized mini-cluster are circumvented.
>>>>>>>>
>>>>>>>> There is additional information I have written at:
>>>>>>>>
>>>>>>>> http://singularity.lbl.gov/#hpc
>>>>>>>>
>>>>>>>> That page is still coming along, and needs more information still=
=20
>>>>>>>> but if you have any questions, comments or change proposals please=
 let us=20
>>>>>>>> know!
>>>>>>>>
>>>>>>>> Thanks and hope that helps!
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>
>>>>>>>>  wrote:
>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Hi,
>>>>>>>>>
>>>>>>>>> We are trying to run experiments using singularity containers. Th=
e=20
>>>>>>>>> idea is to run OpenMPI among several containers and check perform=
ance=20
>>>>>>>>> results.=20
>>>>>>>>>
>>>>>>>>> How can I communicate with another container? In docker this is=
=20
>>>>>>>>> clear because every container gets an assigned IP and you can pin=
g there,=20
>>>>>>>>> but what is the situation in the case of singularity? Is it possi=
ble to=20
>>>>>>>>> assign an IP to each container? Can I connect via ssh to them?
>>>>>>>>>
>>>>>>>>> Thanks in advance,
>>>>>>>>>
>>>>>>>>> --=20
>>>>>>>>> You received this message because you are subscribed to the Googl=
e=20
>>>>>>>>> Groups "singularity" group.
>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,=
=20
>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --=20
>>>>>>>> Gregory M. Kurtzer
>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>> University of California
>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>
>>>>>>>
>>>>>>> --=20
>>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,=
=20
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>> --=20
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>>>
>>>>> --=20
>>>>> You received this message because you are subscribed to the Google=20
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d=20
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>> --=20
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>>>
>>> --=20
>>> You received this message because you are subscribed to the Google=20
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>> an email to singu...@lbl.gov.
>>>
>>>
>>> --=20
>> You received this message because you are subscribed to the Google Group=
s=20
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n=20
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
>
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>

------=_Part_2192_247351025.1467793523794
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br>Hi Gregory,<br><br>It fails depending on your environm=
ent. In my Ubuntu 14.04 it worked fine, but in this instance of Debian jess=
ie I get the following:<br><br>&gt; ERROR: Failed to associate image to loo=
p: Device or resource busy<br><br>Maybe is because we are using a glusterfs=
 shared disk to keep the containers?<br><br>Here you have the entire output=
:<br><br>&gt; sudo mpirun -n 1 singularity exec /mnt/glusterfs/singularity/=
nasmpi-1.img /trace.sh /NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec /=
mnt/glusterfs/singularity/nasmpi-2.img /trace.sh /NPB/NPB3.3-MPI/bin/bt.C.4=
 : -n 1 singularity exec /mnt/glusterfs/singularity/nasmpi-3.img /trace.sh =
/NPB/NPB3.3-MPI/bin/bt.C.4 : -n 1 singularity exec /mnt/glusterfs/singulari=
ty/nasmpi-4.img /trace.sh /NPB/NPB3.3-MPI/bin/bt.C.4<br>ERROR: Failed to as=
sociate image to loop: Device or resource busy<br>ERROR: Failed to associat=
e image to loop: Device or resource busy<br>/bin/bash: warning: setlocale: =
LC_ALL: cannot change locale (en_US.UTF-8)<br>ERROR: Failed to associate im=
age to loop: Device or resource busy<br>-----------------------------------=
---------------------------------------<br>mpirun has exited due to process=
 rank 2 with PID 63416 on<br>node bscgrid30 exiting improperly. There are t=
wo reasons this could occur:<br><br>1. this process did not call &quot;init=
&quot; before exiting, but others in<br>the job did. This can cause a job t=
o hang indefinitely while it waits<br>for all processes to call &quot;init&=
quot;. By rule, if one process calls &quot;init&quot;,<br>then ALL processe=
s must call &quot;init&quot; prior to termination.<br><br>2. this process c=
alled &quot;init&quot;, but exited without calling &quot;finalize&quot;.<br=
>By rule, all processes that call &quot;init&quot; MUST call &quot;finalize=
&quot; prior to<br>exiting or it will be considered an &quot;abnormal termi=
nation&quot;<br><br>This may have caused other processes in the application=
 to be<br>terminated by signals sent by mpirun (as reported here).<br>-----=
---------------------------------------------------------------------<br><b=
r>Thanks in advance,<br><br>El martes, 5 de julio de 2016, 18:21:48 (UTC+2)=
, Gregory M. Kurtzer  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=
=3D"margin: 0;margin-left: 0.8ex;border-left: 1px #ccc solid;padding-left: =
1ex;"><div dir=3D"ltr">Hi Raimon,<div><br></div><div>I am confused as to wh=
at the issue is that you are having. Singularity supports running both acro=
ss nodes as well as multiple processes per node in any number of containers=
. Can you paste your command and the error you are getting, maybe that will=
 help.</div><div><br></div><div>Thanks!</div><div><br></div><div><br></div>=
</div><div><br><div class=3D"gmail_quote">On Tue, Jul 5, 2016 at 8:25 AM, R=
aimon Bosch <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_blank"=
 gdf-obfuscated-mailto=3D"JBPkvJSfBgAJ" rel=3D"nofollow" onmousedown=3D"thi=
s.href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D&#39;ja=
vascript:&#39;;return true;">rai...@gmail.com</a>&gt;</span> wrote:<br><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #c=
cc solid;padding-left:1ex"><div dir=3D"ltr"><br>That solution does not work=
 with nas/mpi benchmark. That&#39;s because bt.C.16 expects 16 processes. W=
hen you split processes it throws an exception because number of processes =
is lower than 16. <br><br>I am still trying to figure out how to do this. L=
et me know if you have any suggestion.<br><br>Cheers,<span><br><br>El jueve=
s, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Castain  escribi=C3=B3:</sp=
an><blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-wo=
rd"><span><div>I think you are misunderstanding the basic nature of the Sin=
gularity =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overl=
ay. So =E2=80=9Csharing=E2=80=9D a container is no different than running o=
n a node where the procs all see the same file system. Thus, having multipl=
e containers that are identical makes no sense - it=E2=80=99s all the same =
file system.</div><div><br></div><div>Now if you want to run different cont=
ainers (e.g., with different libraries or OS in them), then you would use m=
pirun=E2=80=99s MPMD syntax - for example:</div><div><br></div><div>mpirun =
-n 1 &lt;container1&gt; : -n 1 &lt;container2&gt;</div><div><br></div><div>=
HTH</div><div>Ralph</div></span><div><br><blockquote type=3D"cite"><div><di=
v><div>On Jun 23, 2016, at 1:53 AM, Raimon Bosch &lt;<a rel=3D"nofollow">ra=
i...@gmail.com</a>&gt; wrote:</div><br></div></div><div><div><div><div dir=
=3D"ltr" style=3D"font-family:Helvetica;font-size:12px;font-style:normal;fo=
nt-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;tex=
t-transform:none;white-space:normal;word-spacing:0px"><br>One last question=
: What if I want to execute more than one container in the same host? With =
this technique I am bounded always to the same container. One of our experi=
ments was based in measuring performance of several containers working in p=
arallel in the same node. Also we had experiments with N containers per hos=
t in a multihost environment.<br><br>El mi=C3=A9rcoles, 22 de junio de 2016=
, 16:41:58 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=3D"g=
mail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-=
left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">Hi Ra=
imon,<div><br></div><div>Sorry I wasn&#39;t clear. I am not yet at my compu=
ter and thinking while typing on an iPhone hinders my mental processes. Lol=
</div><div><br></div><div>If I understand your example properly, you have a=
 docker or VM infrastructure already set up and you are invoking the mpirun=
 commands from within the virtual environment. Singularity works on a very =
different premis because integrating a virtual cluster into an existing clu=
ster and scheduling system is a mess.</div><div><br></div><div>So starting =
from the physical nodes, which already have access to all other nodes in th=
e cluster, and already scheduled properly, and in direct access to optimize=
d hardware and file systems.... You call mpirun.=C2=A0</div><div><br></div>=
<div>The mpirun command will take the standard format as you illustrated wi=
th the following change to call Singularity inline:</div><div><br></div><di=
v>$ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img trac=
e.sh bt.C.4</div><div><br></div><div>This assumes the following:</div><div>=
<br></div><div>1. The container image which contains the program&#39;s you =
want to run is at ~/container.img and accessible at this path=C2=A0on all n=
odes referenced in hosts.txt</div><div>2. The hosts.txt references other ph=
ysical nodes you want to run on</div><div>3. The executables trace.sh and b=
t.C.4 are both inside the container and in a standard path</div><br><div>In=
 this case we are performing one execution and MPI + singularity is managin=
g all of the communication between processes, nodes and containers. Also it=
 is now using any optimized hardware (eg. Infiniband)=C2=A0and existing hig=
h performance=C2=A0file systems (which should not be accessible via a virtu=
alized or Docker&#39;ized=C2=A0cluster for security reasons).</div><div><br=
></div><div>This way is actually MUCH simpler then what you are proposing b=
ecause there is no need to manage any virtual nodes, virtual networks, or r=
esource manager hacks. It really is as easy as just running any other MPI=
=C2=A0process on an existing cluster.=C2=A0</div><div><br></div><div>Hope t=
hat helps better!</div><div><br></div><div><span></span><br><br>On Wednesda=
y, June 22, 2016, Raimon Bosch &lt;<a rel=3D"nofollow">raimon...@</a><a hre=
f=3D"http://gmail.com/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"t=
his.href=3D&#39;http://gmail.com/&#39;;return true;" onclick=3D"this.href=
=3D&#39;http://gmail.com/&#39;;return true;">gmail.com</a>&gt; wrote:<br><b=
lockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-le=
ft-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;pad=
ding-left:1ex"><div dir=3D"ltr"><br><br>Hi Gregory,<br><br>I&#39;m not sure=
 if I would achieve the same with your commands. In an environment based on=
 dockers or virtual machines we would do something like this [non applicabl=
e to Singularity]:<br><br>&gt; cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --h=
ostfile hosts.txt ./trace.sh ./bt.C.4<br><br>where hosts.txt* is:<br><br>&g=
t;vm-ip-01-on-<b>host01</b><span>=C2=A0</span>slots=3D2<br>&gt;vm-ip-01-on-=
<b>host02</b><span>=C2=A0</span>slots=3D2<br><br>* vm-ip-XX-on-hostXX<b><sp=
an>=C2=A0</span></b>are IPs i.e. 172.100.60.XX<br><br>and trace.sh is:<br><=
br>&gt;#!/bin/bash<br>&gt;<br>&gt;export EXTRAE_HOME=3D/opt/extrae/<br>&gt;=
export EXTRAE_CONFIG_FILE=3D/extrae.xml<br>&gt;export LD_PRELOAD=3D${EXTRAE=
_HOME}/lib/<wbr>libmpitrace.so<br>&gt;<br>&gt;## Run the desired program<br=
>&gt;$*<br><br>As you see we only perform one execution and OpenMPI transpa=
rently manages communication between containers or virtual machines. This c=
ommand would work well rather VMs are on the same host or not.<br><br>What =
I understand from your response is that now we should execute OpenMPI on ea=
ch host and then merge results manually. I don&#39;t know yet how to do thi=
s merge step or if it is any way to centralize everything like I would do w=
ith VMs.<br><br>Thanks in advance,<br><br>El mi=C3=A9rcoles, 22 de junio de=
 2016, 14:42:54 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">=
Hi Raimon,<div><br></div><div>The quick answer is you have mpirun handle th=
at as you would normally where the container file lives on a shared file sy=
stem:</div><div><br></div><div>$ mpirun singularity exec ~/container.img mp=
i_prog_in_container</div><div><br></div><div>Let the MPI outside the contai=
ner launch the singularity container on each host as it would normally laun=
ch any MPI program. Then it will call Singulairty and Singularity will laun=
ch the MPI program inside the container on each of your hosts/servers.=C2=
=A0</div><div><br></div><div>Hope that helps!</div><div><br></div><div><spa=
n></span><br><br>On Wednesday, June 22, 2016, Raimon Bosch &lt;<a rel=3D"no=
follow">rai...@gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote=
" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color=
:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir=3D"ltr=
"><br>Hi Gregory,<br><br>Thank you for your answer. One of our experiments =
needs to run OpenMPI among several servers. This means that we should put o=
ne of our containers in host01, another in host02 and another in host03 and=
 collect the results.<span>=C2=A0</span><br><br>How can I do this execution=
 in parallel if I need to communicate with more than one server?<br><br>El =
martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer escribi=
=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;=
border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:=
solid;padding-left:1ex"><div dir=3D"ltr">Hi Raimon,<div><br></div><div>The =
communication model of a Singularity container is very different from that =
of a Docker implementation. This is because Docker for all practical purpos=
es emulates a virtual machine as each container has it&#39;s own IP address=
 and thus it&#39;s own ssh server. It also carries its own set of complexit=
ies, for example networks need to be segregated/VLan&#39;ed, DNS/host resol=
ution needs to be dynamic and passed down to the containers (so they can re=
ach each other), ssh daemons and other process running inside the container=
s, management via an existing scheduling system, and the list goes on and o=
n.</div><div><br></div><div>Think of it this way, Singularity does not do a=
ny of that... It runs a program within the container as if it were running =
on the host itself, so to communicate between containers is as easy as comm=
unicating between programs. So for MPI, it would happen with the MPI on the=
 physical host (outside the container) invoking the container subsystem whi=
ch then invokes the MPI programs within the container and the MPI programs =
within the container communicate back to the MPI (orted) outside the contai=
ner on the host to get access to the host resources. In this model all avai=
lable resources and infrastructure can be leveraged at full bandwidth by th=
e contained processes and all of the aforementioned complexities akin to ru=
nning on a virtualized mini-cluster are circumvented.</div><div><br></div><=
div>There is additional information I have written at:</div><div><br></div>=
<div><a href=3D"http://singularity.lbl.gov/#hpc" rel=3D"nofollow" target=3D=
"_blank" onmousedown=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhtt=
p%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAF=
QjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;" onclick=3D"this.href=3D=
&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23h=
pc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39=
;;return true;">http://singularity.lbl.gov/#<wbr>hpc</a><br></div><div><br>=
</div><div>That page is still coming along, and needs more information stil=
l but if you have any questions, comments or change proposals please let us=
 know!</div><div><br></div><div>Thanks and hope that helps!</div><div><br><=
/div><div><br></div></div><div><br><div class=3D"gmail_quote">On Tue, Jun 2=
1, 2016 at 7:37 AM, Raimon Bosch<span>=C2=A0</span><span dir=3D"ltr">&lt;<a=
 rel=3D"nofollow">rai...@gmail.com</a>&gt;</span><span>=C2=A0</span>wr<wbr>=
ote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex=
;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style=
:solid;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi,<br><br>We are trying =
to run experiments using singularity containers. The idea is to run OpenMPI=
 among several containers and check performance results.<span>=C2=A0</span>=
<br><br>How can I communicate with another container? In docker this is cle=
ar because every container gets an assigned IP and you can ping there, but =
what is the situation in the case of singularity? Is it possible to assign =
an IP to each container? Can I connect via ssh to them?<br><br>Thanks in ad=
vance,<span><font color=3D"#888888"><br></font></span></div><span><font col=
or=3D"#888888"><div><br></div>--<span>=C2=A0</span><br>You received this me=
ssage because you are subscribed to the Google Groups &quot;singularity&quo=
t; group.<br>To unsubscribe from this group and stop receiving emails from =
it, send an email to<span>=C2=A0</span><a rel=3D"nofollow">singu...@lbl.gov=
</a>.<br></font></span></blockquote></div><br><br clear=3D"all"><div><br></=
div>--<span>=C2=A0</span><br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div></div></div></blockquote></div><div><br></div>--<span>=C2=
=A0</span><br>You received this message because you are subscribed to the G=
oogle Groups &quot;singularity&quot; group.<br>To unsubscribe from this gro=
up and stop receiving emails from it, send an email to<span>=C2=A0</span><a=
>singularity+unsubscribe@<wbr>lbl.gov</a>.<br></blockquote></div><br><br>--=
<span>=C2=A0</span><br><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Per=
formance Computing Services (HPCS)<br>University of California<br>Lawrence =
Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div=
></div><br></blockquote></div><div><br></div>--<span>=C2=A0</span><br>You r=
eceived this message because you are subscribed to the Google Groups &quot;=
singularity&quot; group.<br>To unsubscribe from this group and stop receivi=
ng emails from it, send an email to<span>=C2=A0</span><a>singularity+unsubs=
cribe@<wbr>lbl.gov</a>.<br></blockquote></div><br><br>--<span>=C2=A0</span>=
<br><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing =
Services (HPCS)<br>University of California<br>Lawrence Berkeley National L=
aboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div><br></blockq=
uote></div><div style=3D"font-family:Helvetica;font-size:12px;font-style:no=
rmal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:=
0px;text-transform:none;white-space:normal;word-spacing:0px"><br></div><spa=
n style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weig=
ht:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-trans=
form:none;white-space:normal;word-spacing:0px;float:none;display:inline!imp=
ortant">--<span>=C2=A0</span></span><br style=3D"font-family:Helvetica;font=
-size:12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-=
align:start;text-indent:0px;text-transform:none;white-space:normal;word-spa=
cing:0px"><span style=3D"font-family:Helvetica;font-size:12px;font-style:no=
rmal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:=
0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;disp=
lay:inline!important">You received this message because you are subscribed =
to the Google Groups &quot;singularity&quot; group.</span><br style=3D"font=
-family:Helvetica;font-size:12px;font-style:normal;font-weight:normal;lette=
r-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white=
-space:normal;word-spacing:0px"></div></div><span style=3D"font-family:Helv=
etica;font-size:12px;font-style:normal;font-weight:normal;letter-spacing:no=
rmal;text-align:start;text-indent:0px;text-transform:none;white-space:norma=
l;word-spacing:0px;float:none;display:inline!important">To unsubscribe from=
 this group and stop receiving emails from it, send an email to<span>=C2=A0=
</span></span><a style=3D"font-family:Helvetica;font-size:12px;font-style:n=
ormal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent=
:0px;text-transform:none;white-space:normal;word-spacing:0px" rel=3D"nofoll=
ow">singu...@lbl.gov</a><span style=3D"font-family:Helvetica;font-size:12px=
;font-style:normal;font-weight:normal;letter-spacing:normal;text-align:star=
t;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;f=
loat:none;display:inline!important">.</span></div></blockquote></div><br></=
div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
JBPkvJSfBgAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>
------=_Part_2192_247351025.1467793523794--

------=_Part_2191_417424227.1467793523787--
