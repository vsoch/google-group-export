X-Received: by 10.50.40.39 with SMTP id u7mr11799678igk.9.1463386669918;
        Mon, 16 May 2016 01:17:49 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.18.198 with SMTP id 67ls1354601ios.76.gmail; Mon, 16 May
 2016 01:17:49 -0700 (PDT)
X-Received: by 10.98.100.71 with SMTP id y68mr43579312pfb.84.1463386669374;
        Mon, 16 May 2016 01:17:49 -0700 (PDT)
Return-Path: <shapov...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id 25si44157926pfi.242.2016.05.16.01.17.49
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 May 2016 01:17:49 -0700 (PDT)
Received-SPF: pass (google.com: domain of shapov...@gmail.com designates 74.125.82.52 as permitted sender) client-ip=74.125.82.52;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of shapov...@gmail.com designates 74.125.82.52 as permitted sender) smtp.mailfrom=shapov...@gmail.com
X-Ironport-SBRS: 3.6
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2H3AADagDlXiDRSfUpehQoGgzSxNYUFgXYXAYV5AoEcBzkTAQEBAQEBAQMPAQEBCAsLCR8xgjc5EFUCK0ABAQEDARIRHQENDh4DDAYFCw0qAgIhAQEOAwEFARwOBwQBGQMEAYdyAQMPCAWeXoExPjGLO4FqglgFhwsKGScNUoNhARwCBhCJX4EDgkOBThEBBoMWglkFjk+EYYRGMYh2gy+BeYFphE+IYYdchicSHoEOIQGCOQ0RCoFOOTIHhkqBNQEBAQ
X-IronPort-AV: E=Sophos;i="5.24,626,1455004800"; 
   d="scan'208,217";a="24099510"
Received: from mail-wm0-f52.google.com ([74.125.82.52])
  by fe3.lbl.gov with ESMTP; 16 May 2016 01:17:47 -0700
Received: by mail-wm0-f52.google.com with SMTP id g17so123175615wme.1
        for <singu...@lbl.gov>; Mon, 16 May 2016 01:17:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=K2XATAgULqr2cDfC9/gS4yQTqnGZYkRReFOtdAh1DAI=;
        b=Dkg5T7hL8QKMxPumsGr5zgdRY9u2jRajx5EDlbirrsYsWRujb3fYvcybXChYsdDSCi
         cHdapJnaWtv+dquByl7SyANflWFWW8H9IzgZ453B5cIgi6MAv7rHKZ4BOGgbnzD01U95
         yQF/ByYdE7wMLuPPRo+u1f58KQbrzSHc4HuQt0j2NlG985AkZ9KNnGoFmIfM4IXQu8RZ
         LiwVCyyOEBK5TohSDQa2enZd1F4pA2rKPFR5ff0j5cz9sqAGowhyrCPc1R1/FnG4Oj0B
         9y91Qhv4DOf46FMGkZ3/9IMw1CBMjEGlaoVv2knf70ZPJAATW8cdJFEBsdTNLOruybd1
         wz4g==
X-Gm-Message-State: AOPr4FXmxwGKJviKb83NgZqaHjziF93BON/Z2xBdAqcv6Mz83Mjr4wG0O/spm7ZQN0OlWKMuAtTEb/TbnqYCdw==
X-Received: by 10.194.216.33 with SMTP id on1mr28327299wjc.120.1463386666451;
 Mon, 16 May 2016 01:17:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.37.230 with HTTP; Mon, 16 May 2016 01:17:06 -0700 (PDT)
In-Reply-To: <CAN7etTxQkXe6uEDZgt+kkAex0Hzt9UWYE6+V4qS4vnKJAMgFzQ@mail.gmail.com>
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov> <CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com>
 <CAAS-_CBb3sy393W1Bga5Wnr9-EvSHC6NPNaOx58Mpdw-LiFq8g@mail.gmail.com>
 <414C0039-42A3-4C2E-89F6-3D97D082C742@open-mpi.org> <CAN7etTxQkXe6uEDZgt+kkAex0Hzt9UWYE6+V4qS4vnKJAMgFzQ@mail.gmail.com>
From: Taras Shapovalov <shapov...@gmail.com>
Date: Mon, 16 May 2016 11:17:06 +0300
Message-ID: <CAAS-_CBsKM3=d_OhVknqcF2De29UiQ-cjOv5imCyR=jv=4Rh7g@mail.gmail.com>
Subject: Re: [Singularity] SIngularity and MPI implementations
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=089e0149359e6c4a8d0532f14261

--089e0149359e6c4a8d0532f14261
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi guys,

Thanks for the great answers! Now it is more or less clear how it works. To
be absolutely sure, can you please confirm also these statements (got from
your answers):

1. Ralph's answer mentions mpiexec, but Gregory's answer is about mpirun.
So, all the discussed here can be applied to the both utilities included in
Open MPI distribution.

2. Running Open MPI processes in a single container is impleneted only in
Singularity v2. In v1 each Open MPI process still will be executed in
different containers.

3. Lets compare these 2 scenarios: Singularity runs child processes in a
single container agains scenario when each child runs in a separate
container each. The optimization with dlopen call happens in the first
scenario, because the opened library is loaded into the memory per
Singularity container, then dlopen magically returns the same handler for
each child process inside the container, which should be faster. Or there
is some other low level optimization occurs in the first scenario regarding
dlopen?

Best regards,

Taras


On Fri, May 13, 2016 at 9:18 PM, Gregory M. Kurtzer <gmku...@lbl.gov>
wrote:

>
>
> On Fri, May 13, 2016 at 10:10 AM, Ralph Castain <r...@open-mpi.org> wrote=
:
>
>>
>> On May 13, 2016, at 9:52 AM, Taras Shapovalov <shapov...@gmail.com>
>> wrote:
>>
>> Hi Ralph and Gregory,
>>
>> Thank you the both for the so detailed answers! I see your replies
>> complement each other. Although I am a bit confused now with the whole
>> picture, so could you confirm that I get the ideas correctly:
>>
>> 1. All implementations of MPI by default should work with Singularity
>> containers (maybe not as optimal as could be, but should start and finis=
h
>> correctly always). Actually I've tested recently MPICH+Singularity with
>> several workload managers, worked fine (did not benchmark it comparing w=
ith
>> Open MPI). I did not manage to make Singularity+MPI work in LSF, but thi=
s
>> is a different story that deserves a separate thread.
>>
>>
> I wouldn't necessarily say they would all work by default. For example,
> some namespaces may necessitate being disabled in order to get proper
> shared memory IO performance. But ... If you have tested this, that is
> great news and I'd love to hear more about your findings!
>
>
>>
>> Correct - the LSF issue is likely a problem of getting the required setu=
p
>> info passed by LSF
>>
>>
>> 2. MPI process calls dl_open, thus the more MPI processes starts on a
>> node, the more times dl_open will be called. Open MPI 2.0.1 somehow solv=
es
>> this magically (I don't get how) and dl_open is called only once per nod=
e.
>> Other implementations of MPI and older Open MPI are not Singularity awar=
e,
>> thus they still will call dl_open each time when MPI process spawns.
>>
>>
>> Not exactly. Singularity will solve the dl_open problem by itself. What
>> the container does is wrap all the dl_open libraries into the container,
>> and so all dl_open calls by the app are locally resolved. Thus, you
>> automatically resolve the IO node bottleneck scaling issue.
>>
>> What OMPI adds is that it pulls the container only once/node. Other
>> mpiexec implementations will pull the container again for every local
>> process. So if you have 100 procs/node, OMPI will result in 100x fewer
>> =E2=80=9Cpulls=E2=80=9D thru that IO node.
>>
>
> Yep, and additionally I want to make sure we keep Singularity v1 and v2
> features separate. Version 2 has several huge benefits (including this)
> over v1, but it is a departure from using SAPPs (and now uses images).
>
>
>>
>>
>> 3. dl_open issue affects only process start time and does not effect the
>> process execution, so on small scale with long running processes there i=
s
>> no difference between Open MPI 2.0.1 and older Open MPI versions (as wel=
l
>> as other MPI implementations).
>>
>>
>> Correct
>>
>
> Correct, just keep in mind start times at massive scale have been stated
> by several large centers to approach 30 minutes. During that 30 minutes, =
it
> basically looks like a distributed denial of service attack to the file
> system metadata server killing file system performance to the rest of the
> system.
>
>
>>
>>
>> 4. When sapp is built then Singularity detects Open MPI (even older then
>> 2.0.1, right?) and resolves all dependencies automatically adding all fi=
les
>> to the sapp. But with, say, MVAPICH2 the dependencies are not resolved
>> automatically, so user should add some stuff manually.
>>
>>
>> Correct
>>
>
> And in v2, this will get handled either by an RPM installation of Open
> MPI, or the 'singularity exec --writable /path/to/Container.img make
> install'.
>
>>
>>
>> 5. Apart of solving dl_open issue Open MPI 2.0.1 does some splitting
>> between the host and the container, which allows user/admin to not optim=
ize
>> Open MPI for a target platform. I really don't get how Singularity does
>> this, but I get the problem. Could you explain what Singularity or Open =
MPI
>> 2.0.1 does for that specificaly?
>>
>>
>> When running under mpiexec with Singularity, OMPI=E2=80=99s local daemon=
 on each
>> node actually runs outside of the containers. We then fork/exec the
>> container itself, and the container is defined so it auto-executes the
>> application process. This allows us to minimize the services overhead,
>> keeping all services outside of your container (and thus shared across a=
ll
>> containers.
>>
>> Other approaches have the daemon -inside- the container, and you get one
>> daemon for each container - and thus, one daemon for each local
>> application. So you get a higher overhead and therefore lower performanc=
e.
>>
>
> Maybe this will help to articulate it... I have described this via the
> MPI/Singularity invocation pathway as follows (hopefully it is reasonably
> correct and doesn't cause Ralph to kick me). Considering the command:
>
> $ mpirun -np X singularity exec ~/Centos-7.img mpi_program
>
> 1. From shell (or resource manager) mpirun gets called
> 2. mpirun forks and exec orte daemon
> 3. Orted process creates PMI
> 4. Orted forks =3D=3D to the number of process per node requested
> 5. Orted children exec to original command passed to mpirun (Singularity)
> 6. Each Singularity execs the command passed inside the given container
> 7. Each MPI program links in the dynamic Open MPI libraries (ldd)
> 8. Open MPI libraries continue to open the non-ldd shared libraries
> (dlopen)
> 9. Open MPI libraries connect back to original orted via PMI
> 10. All non-shared memory communication occurs through the PMI and then t=
o
> local interfaces (e.g. InfiniBand)
>
> While the workflow from the MPI perspective seems simpler with the daemon
> inside the container, it is much more complicated from the system
> perspective because each orted process must also know about the other
> hosts, be able to communicate with them and mitigate performance factors =
of
> host/resource/interconnect tuning.
>
> Hope that helps!
>
>
>
>
>>
>> HTH
>> Ralph
>>
>>
>>
>> Best regards,
>>
>> Taras
>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
/T

--089e0149359e6c4a8d0532f14261
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div><div><div>Hi guys,<br><br></div>Thanks for =
the great answers! Now it is more or less clear how it works. To be absolut=
ely sure, can you please confirm also these statements (got from your answe=
rs):<br><br></div><div>1. Ralph&#39;s answer mentions mpiexec, but Gregory&=
#39;s answer is about mpirun. So, all the discussed here can be applied to =
the both utilities included in Open MPI distribution.<br></div><div><br></d=
iv>2. Running Open MPI processes in a single container is impleneted only i=
n Singularity v2. In v1 each Open MPI process still will be executed in dif=
ferent containers.<br><br></div>3. Lets compare these 2 scenarios: Singular=
ity runs child processes in a single container agains scenario when each ch=
ild runs in a separate container each. The optimization with dlopen call ha=
ppens in the first scenario, because the opened library is loaded into the =
memory per Singularity container, then dlopen magically returns the same ha=
ndler for each child process inside the container, which should be faster. =
Or there is some other low level optimization occurs in the first scenario =
regarding dlopen?<br><br></div>Best regards,<br><br></div>Taras<br><div><di=
v><br></div></div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_=
quote">On Fri, May 13, 2016 at 9:18 PM, Gregory M. Kurtzer <span dir=3D"ltr=
">&lt;<a href=3D"mailto:gmku...@lbl.gov" target=3D"_blank">gmku...@lbl.gov<=
/a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:=
0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><b=
r><div class=3D"gmail_extra"><br><div class=3D"gmail_quote"><span class=3D"=
">On Fri, May 13, 2016 at 10:10 AM, Ralph Castain <span dir=3D"ltr">&lt;<a =
href=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>&g=
t;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0=
 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:=
break-word"><br><div><span><blockquote type=3D"cite"><div>On May 13, 2016, =
at 9:52 AM, Taras Shapovalov &lt;<a href=3D"mailto:shapov...@gmail.com" tar=
get=3D"_blank">shapov...@gmail.com</a>&gt; wrote:</div><br><div><div dir=3D=
"ltr"><div><div><div><div><div><div>Hi Ralph and Gregory,<br><br></div>Than=
k you the both for the so detailed answers! I see your replies complement e=
ach other. Although I am a bit confused now with the whole picture, so coul=
d you confirm that I get the ideas correctly:<br><br></div>1. All implement=
ations of MPI by default should work with Singularity containers (maybe not=
 as optimal as could be, but should start and finish correctly always). Act=
ually I&#39;ve tested recently MPICH+Singularity with several workload mana=
gers, worked fine (did not benchmark it comparing with Open MPI). I did not=
 manage to make Singularity+MPI work in LSF, but this is a different story =
that deserves a separate thread.<br></div></div></div></div></div></div></b=
lockquote></span></div></div></blockquote><div><br></div></span><div>I woul=
dn&#39;t necessarily say they would all work by default. For example, some =
namespaces may necessitate being disabled in order to get proper shared mem=
ory IO performance. But ... If you have tested this, that is great news and=
 I&#39;d love to hear more about your findings!</div><span class=3D""><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-wo=
rd"><div><span><blockquote type=3D"cite"><div><div dir=3D"ltr"><div><div><d=
iv><div></div></div></div></div></div></div></blockquote><div><br></div></s=
pan>Correct - the LSF issue is likely a problem of getting the required set=
up info passed by LSF</div><div><span><br><blockquote type=3D"cite"><div><d=
iv dir=3D"ltr"><div><div><div><div><br></div>2. MPI process calls dl_open, =
thus the more MPI processes starts on a node, the more times dl_open will b=
e called. Open MPI 2.0.1 somehow solves this magically (I don&#39;t get how=
) and dl_open is called only once per node. Other implementations of MPI an=
d older Open MPI are not Singularity aware, thus they still will call dl_op=
en each time when MPI process spawns. <br></div></div></div></div></div></b=
lockquote><div><br></div></span>Not exactly. Singularity will solve the dl_=
open problem by itself. What the container does is wrap all the dl_open lib=
raries into the container, and so all dl_open calls by the app are locally =
resolved. Thus, you automatically resolve the IO node bottleneck scaling is=
sue.</div><div><br></div><div>What OMPI adds is that it pulls the container=
 only once/node. Other mpiexec implementations will pull the container agai=
n for every local process. So if you have 100 procs/node, OMPI will result =
in 100x fewer =E2=80=9Cpulls=E2=80=9D thru that IO node.</div></div></block=
quote><div><br></div></span><div>Yep, and additionally I want to make sure =
we keep Singularity v1 and v2 features separate. Version 2 has several huge=
 benefits (including this) over v1, but it is a departure from using SAPPs =
(and now uses images).</div><span class=3D""><div>=C2=A0</div><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;=
padding-left:1ex"><div style=3D"word-wrap:break-word"><div><span><br><block=
quote type=3D"cite"><div><div dir=3D"ltr"><div><div><div><br>3. dl_open iss=
ue affects only process start time and does not effect the process executio=
n, so on small scale with long running processes there is no difference bet=
ween Open MPI 2.0.1 and older Open MPI versions (as well as other MPI imple=
mentations).<br></div></div></div></div></div></blockquote><div><br></div><=
/span>Correct</div></div></blockquote><div><br></div></span><div>Correct, j=
ust keep in mind start times at massive scale have been stated by several l=
arge centers to approach 30 minutes. During that 30 minutes, it basically l=
ooks like a distributed denial of service attack to the file system metadat=
a server killing file system performance to the rest of the system.</div><s=
pan class=3D""><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"=
margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word"><div><span><br><blockquote type=3D"cite"><div><di=
v dir=3D"ltr"><div><div><div><br></div><div>4. When sapp is built then Sing=
ularity detects Open MPI (even older then 2.0.1, right?) and resolves all d=
ependencies automatically adding all files to the sapp. But with, say, MVAP=
ICH2 the dependencies are not resolved automatically, so user should add so=
me stuff manually.<br></div></div></div></div></div></blockquote><div><br><=
/div></span>Correct</div></div></blockquote><div><br></div></span><div>And =
in v2, this will get handled either by an RPM installation of Open MPI, or =
the &#39;singularity exec --writable /path/to/Container.img make install&#3=
9;.=C2=A0</div><span class=3D""><blockquote class=3D"gmail_quote" style=3D"=
margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word"><div><span><br><blockquote type=3D"cite"><div><di=
v dir=3D"ltr"><div><div><div><br></div>5. Apart of solving dl_open issue Op=
en MPI 2.0.1 does some splitting between the host and the container, which =
allows user/admin to not optimize Open MPI for a target platform. I really =
don&#39;t get how Singularity does this, but I get the problem. Could you e=
xplain what Singularity or Open MPI 2.0.1 does for that specificaly?<br></d=
iv></div></div></div></blockquote><div><br></div></span><div>When running u=
nder mpiexec with Singularity, OMPI=E2=80=99s local daemon on each node act=
ually runs outside of the containers. We then fork/exec the container itsel=
f, and the container is defined so it auto-executes the application process=
. This allows us to minimize the services overhead, keeping all services ou=
tside of your container (and thus shared across all containers.</div><div><=
br></div><div>Other approaches have the daemon -inside- the container, and =
you get one daemon for each container - and thus, one daemon for each local=
 application. So you get a higher overhead and therefore lower performance.=
</div></div></div></blockquote><div><br></div></span><div>Maybe this will h=
elp to articulate it... I have described this via the MPI/Singularity invoc=
ation pathway as follows (hopefully it is reasonably correct and doesn&#39;=
t cause Ralph to kick me). Considering the command:</div><div><br></div><di=
v>$ mpirun -np X singularity exec ~/Centos-7.img mpi_program</div><div><br>=
</div><div>1. From shell (or resource manager) mpirun gets called</div><div=
>2. mpirun forks and exec orte daemon</div><div>3. Orted process creates PM=
I</div><div>4. Orted forks =3D=3D to the number of process per node request=
ed</div><div>5. Orted children exec to original command passed to mpirun (S=
ingularity)</div><div>6. Each Singularity execs the command passed inside t=
he given container</div><div>7. Each MPI program links in the dynamic Open =
MPI libraries (ldd)</div><div>8. Open MPI libraries continue to open the no=
n-ldd shared libraries (dlopen)</div><div>9. Open MPI libraries connect bac=
k to original orted via PMI</div><div>10. All non-shared memory communicati=
on occurs through the PMI and then to local interfaces (e.g. InfiniBand)</d=
iv><div><br></div><div>While the workflow from the MPI perspective seems si=
mpler with the daemon inside the container, it is much more complicated fro=
m the system perspective because each orted process must also know about th=
e other hosts, be able to communicate with them and mitigate performance fa=
ctors of host/resource/interconnect tuning.</div><div><br></div><div>Hope t=
hat helps!</div><span class=3D""><div><br></div><div><br></div><div>=C2=A0<=
/div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><di=
v><div><br></div><div>HTH</div><span><font color=3D"#888888"><div>Ralph</di=
v></font></span><span><div><br></div><br><blockquote type=3D"cite"><div><di=
v dir=3D"ltr"><div><div><br></div>Best regards,<br><br></div>Taras<br><br><=
/div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></blockquote></span></div><br></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></span></div><br><br clear=3D"all"><span class=3D"=
"><div><br></div>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>Hi=
gh Performance Computing Services (HPCS)<br>University of California<br>Law=
rence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 9472=
0</div></div></div>
</span></div></div>

<p></p>

-- <br><div class=3D"HOEnZb"><div class=3D"h5">
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><br>-- <br><div class=
=3D"gmail_signature"><div dir=3D"ltr">/T<br></div></div>
</div>

--089e0149359e6c4a8d0532f14261--
