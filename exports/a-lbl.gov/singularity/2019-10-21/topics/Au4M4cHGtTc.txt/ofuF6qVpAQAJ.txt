Date: Thu, 12 Oct 2017 12:02:33 -0700 (PDT)
From: Matt Kijowski <matthew...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <9bfc89b3-b4ca-40df-bfcf-a1c7909380e7@lbl.gov>
In-Reply-To: <942d5c3f-57a6-4d81-a4d9-5755b03fdc60@lbl.gov>
References: <43e70f0f-e4c3-4876-aff5-ed8255ebbae4@lbl.gov>
 <CAHGBjRwXGr50wCTgknTD6TAhxoFc-hOvLr07qPw7YHeW3eqwdA@mail.gmail.com>
 <942d5c3f-57a6-4d81-a4d9-5755b03fdc60@lbl.gov>
Subject: Re: [Singularity] Beginner Singularity question (StarCCM++)
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_14623_1528050238.1507834953567"

------=_Part_14623_1528050238.1507834953567
Content-Type: multipart/alternative; 
	boundary="----=_Part_14624_2044114879.1507834953567"

------=_Part_14624_2044114879.1507834953567
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

So my current parallel command is as follows:

singularity exec starccm.img starccm+ -power -rsh ssh -machinefile 
nodelist.txt -batch run -load simfile

This command wants to use ssh to launch starccm+ (and other related 
commands) on all compute nodes that my scheduler gives it (via 
nodelist.txt).  The problem is that starccm+ doesn't know to prepend 
"singularity exec starccm.img" to the command so I just get command not 
found on the other compute nodes.  I guess I'm trying to figure out a way 
to remotely launch and remotely connect to a singularity container.

On Thursday, October 12, 2017 at 2:53:07 PM UTC-4, Matt Kijowski wrote:
>
> yes (ish).  I dont really care whether mpi runs inside the container or 
> outside.  But given that starccm+ seems to only work with its own built in 
> mpi I think I need it running within the container itself.
>
>
> On Thursday, October 12, 2017 at 2:48:04 PM UTC-4, Andrew Rokitka wrote:
>>
>> Just to clarify - you want mpi to run within the container itself?
>>
>> On Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski <mat...@gmail.com> 
>> wrote:
>>
>>> Hello all, we're just starting to use Singularity on our HPC system in 
>>> conjunction with Slurm and I am having some difficulties understanding how 
>>> to set up one of our containers (but I might be setting things up wrong).
>>>
>>> So I understand why we would use mpirun outside of a container, but what 
>>> is a good configuration for a program (StarCCM++) that wraps its own MPI 
>>> and remote commands into its execution?
>>>
>>> Example.  I have a 64 node cluster and starccm++ installed into a 
>>> container.  If I want to run my job inside the container I execute: 
>>> *srun singularity exec starccm.img starccm+ options simfile*where srun 
>>> requests a compute node from slurm and executes the rest of the command on 
>>> that compute node.  The problem I have is with parallel jobs.  starccm+ 
>>> wants to start its processes on the compute nodes itself (I can specify 
>>> what sort of remote shell to use and which compute nodes I have requested 
>>> via a machine list file in the options) but starccm+ is not aware that it 
>>> needs to launch a singularity container on the compute nodes first.
>>>
>>> My first thought is I need to launch the containers on the compute nodes 
>>> before executing the starccm++ parallel job so that when it uses SSH to log 
>>> in to the compute nodes the container is already running but I'm not sure 
>>> on how to accomplish this.  Any thoughts?
>>>
>>> -- 
>>> You received this message because you are subscribed to the Google 
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send 
>>> an email to singu...@lbl.gov.
>>>
>>
>>
------=_Part_14624_2044114879.1507834953567
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">So my current parallel command is as follows:<br><br>singu=
larity exec starccm.img starccm+ -power -rsh ssh -machinefile nodelist.txt =
-batch run -load simfile<br><br>This command wants to use ssh to launch sta=
rccm+ (and other related commands) on all compute nodes that my scheduler g=
ives it (via nodelist.txt).=C2=A0 The problem is that starccm+ doesn&#39;t =
know to prepend &quot;singularity exec starccm.img&quot; to the command so =
I just get command not found on the other compute nodes.=C2=A0 I guess I&#3=
9;m trying to figure out a way to remotely launch and remotely connect to a=
 singularity container.<br><br>On Thursday, October 12, 2017 at 2:53:07 PM =
UTC-4, Matt Kijowski wrote:<blockquote class=3D"gmail_quote" style=3D"margi=
n: 0;margin-left: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><di=
v dir=3D"ltr">yes (ish).=C2=A0 I dont really care whether mpi runs inside t=
he container or outside.=C2=A0 But given that starccm+ seems to only work w=
ith its own built in mpi I think I need it running within the container its=
elf.<br><br><br>On Thursday, October 12, 2017 at 2:48:04 PM UTC-4, Andrew R=
okitka wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-lef=
t:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Just =
to clarify - you want mpi to run within the container itself?</div><div><br=
><div class=3D"gmail_quote">On Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski =
<span dir=3D"ltr">&lt;<a rel=3D"nofollow">mat...@gmail.com</a>&gt;</span> w=
rote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hello all, we&#39;=
re just starting to use Singularity on our HPC system in conjunction with S=
lurm and I am having some difficulties understanding how to set up one of o=
ur containers (but I might be setting things up wrong).<br><br>So I underst=
and why we would use mpirun outside of a container, but what is a good conf=
iguration for a program (StarCCM++) that wraps its own MPI and remote comma=
nds into its execution?<br><br>Example.=C2=A0 I have a 64 node cluster and =
starccm++ installed into a container.=C2=A0 If I want to run my job inside =
the container I execute: <i>srun singularity exec starccm.img starccm+ opti=
ons simfile<br></i>where srun requests a compute node from slurm and execut=
es the rest of the command on that compute node.=C2=A0 The problem I have i=
s with parallel jobs.=C2=A0 starccm+ wants to start its processes on the co=
mpute nodes itself (I can specify what sort of remote shell to use and whic=
h compute nodes I have requested via a machine list file in the options) bu=
t starccm+ is not aware that it needs to launch a singularity container on =
the compute nodes first.<br><br>My first thought is I need to launch the co=
ntainers on the compute nodes before executing the starccm++ parallel job s=
o that when it uses SSH to log in to the compute nodes the container is alr=
eady running but I&#39;m not sure on how to accomplish this.=C2=A0 Any thou=
ghts?<span><font color=3D"#888888"><br><i></i></font></span></div><span><fo=
nt color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br></div>
</blockquote></div></blockquote></div>
------=_Part_14624_2044114879.1507834953567--

------=_Part_14623_1528050238.1507834953567--
