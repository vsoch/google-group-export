Date: Wed, 22 Jun 2016 07:46:19 -0700 (PDT)
From: Raimon Bosch <raimo...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <10fb53ef-981b-4b12-88ae-5e64f42b6246@lbl.gov>
In-Reply-To: <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
 <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
 <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
 <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
 <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov>
 <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_2612_745715365.1466606779123"

------=_Part_2612_745715365.1466606779123
Content-Type: multipart/alternative; 
	boundary="----=_Part_2613_1376922640.1466606779124"

------=_Part_2613_1376922640.1466606779124
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable


I think it is more clear now. Thanks for your answer! I will post if I get=
=20
interesting results.

El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Kurtze=
r=20
escribi=C3=B3:
>
> Hi Raimon,
>
> Sorry I wasn't clear. I am not yet at my computer and thinking while=20
> typing on an iPhone hinders my mental processes. Lol
>
> If I understand your example properly, you have a docker or VM=20
> infrastructure already set up and you are invoking the mpirun commands fr=
om=20
> within the virtual environment. Singularity works on a very different=20
> premis because integrating a virtual cluster into an existing cluster and=
=20
> scheduling system is a mess.
>
> So starting from the physical nodes, which already have access to all=20
> other nodes in the cluster, and already scheduled properly, and in direct=
=20
> access to optimized hardware and file systems.... You call mpirun.=20
>
> The mpirun command will take the standard format as you illustrated with=
=20
> the following change to call Singularity inline:
>
> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img=20
> trace.sh bt.C.4
>
> This assumes the following:
>
> 1. The container image which contains the program's you want to run is at=
=20
> ~/container.img and accessible at this path on all nodes referenced in=20
> hosts.txt
> 2. The hosts.txt references other physical nodes you want to run on
> 3. The executables trace.sh and bt.C.4 are both inside the container and=
=20
> in a standard path
>
> In this case we are performing one execution and MPI + singularity is=20
> managing all of the communication between processes, nodes and containers=
.=20
> Also it is now using any optimized hardware (eg. Infiniband) and existing=
=20
> high performance file systems (which should not be accessible via a=20
> virtualized or Docker'ized cluster for security reasons).
>
> This way is actually MUCH simpler then what you are proposing because=20
> there is no need to manage any virtual nodes, virtual networks, or resour=
ce=20
> manager hacks. It really is as easy as just running any other MPI process=
=20
> on an existing cluster.=20
>
> Hope that helps better!
>
>
>
> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com=20
> <javascript:>> wrote:
>
>>
>>
>> Hi Gregory,
>>
>> I'm not sure if I would achieve the same with your commands. In an=20
>> environment based on dockers or virtual machines we would do something l=
ike=20
>> this [non applicable to Singularity]:
>>
>> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh=20
>> ./bt.C.4
>>
>> where hosts.txt* is:
>>
>> >vm-ip-01-on-*host01* slots=3D2
>> >vm-ip-01-on-*host02* slots=3D2
>>
>> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>>
>> and trace.sh is:
>>
>> >#!/bin/bash
>> >
>> >export EXTRAE_HOME=3D/opt/extrae/
>> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
>> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>> >
>> >## Run the desired program
>> >$*
>>
>> As you see we only perform one execution and OpenMPI transparently=20
>> manages communication between containers or virtual machines. This comma=
nd=20
>> would work well rather VMs are on the same host or not.
>>
>> What I understand from your response is that now we should execute=20
>> OpenMPI on each host and then merge results manually. I don't know yet h=
ow=20
>> to do this merge step or if it is any way to centralize everything like =
I=20
>> would do with VMs.
>>
>> Thanks in advance,
>>
>> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Kur=
tzer=20
>> escribi=C3=B3:
>>>
>>> Hi Raimon,
>>>
>>> The quick answer is you have mpirun handle that as you would normally=
=20
>>> where the container file lives on a shared file system:
>>>
>>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>>
>>> Let the MPI outside the container launch the singularity container on=
=20
>>> each host as it would normally launch any MPI program. Then it will cal=
l=20
>>> Singulairty and Singularity will launch the MPI program inside the=20
>>> container on each of your hosts/servers.=20
>>>
>>> Hope that helps!
>>>
>>>
>>>
>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>>
>>>>
>>>> Hi Gregory,
>>>>
>>>> Thank you for your answer. One of our experiments needs to run OpenMPI=
=20
>>>> among several servers. This means that we should put one of our contai=
ners=20
>>>> in host01, another in host02 and another in host03 and collect the res=
ults.=20
>>>>
>>>> How can I do this execution in parallel if I need to communicate with=
=20
>>>> more than one server?
>>>>
>>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer=
=20
>>>> escribi=C3=B3:
>>>>>
>>>>> Hi Raimon,
>>>>>
>>>>> The communication model of a Singularity container is very different=
=20
>>>>> from that of a Docker implementation. This is because Docker for all=
=20
>>>>> practical purposes emulates a virtual machine as each container has i=
t's=20
>>>>> own IP address and thus it's own ssh server. It also carries its own =
set of=20
>>>>> complexities, for example networks need to be segregated/VLan'ed, DNS=
/host=20
>>>>> resolution needs to be dynamic and passed down to the containers (so =
they=20
>>>>> can reach each other), ssh daemons and other process running inside t=
he=20
>>>>> containers, management via an existing scheduling system, and the lis=
t goes=20
>>>>> on and on.
>>>>>
>>>>> Think of it this way, Singularity does not do any of that... It runs =
a=20
>>>>> program within the container as if it were running on the host itself=
, so=20
>>>>> to communicate between containers is as easy as communicating between=
=20
>>>>> programs. So for MPI, it would happen with the MPI on the physical ho=
st=20
>>>>> (outside the container) invoking the container subsystem which then i=
nvokes=20
>>>>> the MPI programs within the container and the MPI programs within the=
=20
>>>>> container communicate back to the MPI (orted) outside the container o=
n the=20
>>>>> host to get access to the host resources. In this model all available=
=20
>>>>> resources and infrastructure can be leveraged at full bandwidth by th=
e=20
>>>>> contained processes and all of the aforementioned complexities akin t=
o=20
>>>>> running on a virtualized mini-cluster are circumvented.
>>>>>
>>>>> There is additional information I have written at:
>>>>>
>>>>> http://singularity.lbl.gov/#hpc
>>>>>
>>>>> That page is still coming along, and needs more information still but=
=20
>>>>> if you have any questions, comments or change proposals please let us=
 know!
>>>>>
>>>>> Thanks and hope that helps!
>>>>>
>>>>>
>>>>>
>>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>=20
>>>>> wrote:
>>>>>
>>>>>>
>>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> We are trying to run experiments using singularity containers. The=
=20
>>>>>> idea is to run OpenMPI among several containers and check performanc=
e=20
>>>>>> results.=20
>>>>>>
>>>>>> How can I communicate with another container? In docker this is clea=
r=20
>>>>>> because every container gets an assigned IP and you can ping there, =
but=20
>>>>>> what is the situation in the case of singularity? Is it possible to =
assign=20
>>>>>> an IP to each container? Can I connect via ssh to them?
>>>>>>
>>>>>> Thanks in advance,
>>>>>>
>>>>>> --=20
>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,=20
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --=20
>>>>> Gregory M. Kurtzer
>>>>> High Performance Computing Services (HPCS)
>>>>> University of California
>>>>> Lawrence Berkeley National Laboratory
>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>
>>>> --=20
>>>> You received this message because you are subscribed to the Google=20
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>> --=20
>>> Gregory M. Kurtzer
>>> High Performance Computing Services (HPCS)
>>> University of California
>>> Lawrence Berkeley National Laboratory
>>> One Cyclotron Road, Berkeley, CA 94720
>>>
>>> --=20
>> You received this message because you are subscribed to the Google Group=
s=20
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n=20
>> email to singu...@lbl.gov.
>>
>
>
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>
>
------=_Part_2613_1376922640.1466606779124
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br>I think it is more clear now. Thanks for your answer! =
I will post if I get interesting results.<br><br>El mi=C3=A9rcoles, 22 de j=
unio de 2016, 16:41:58 (UTC+2), Gregory M. Kurtzer  escribi=C3=B3:<blockquo=
te class=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left:=
 1px #ccc solid;padding-left: 1ex;">Hi Raimon,<div><br></div><div>Sorry I w=
asn&#39;t clear. I am not yet at my computer and thinking while typing on a=
n iPhone hinders my mental processes. Lol</div><div><br></div><div>If I und=
erstand your example properly, you have a docker or VM infrastructure alrea=
dy set up and you are invoking the mpirun commands from within the virtual =
environment. Singularity works on a very different premis because integrati=
ng a virtual cluster into an existing cluster and scheduling system is a me=
ss.</div><div><br></div><div>So starting from the physical nodes, which alr=
eady have access to all other nodes in the cluster, and already scheduled p=
roperly, and in direct access to optimized hardware and file systems.... Yo=
u call mpirun.=C2=A0</div><div><br></div><div>The mpirun command will take =
the standard format as you illustrated with the following change to call Si=
ngularity inline:</div><div><br></div><div>$ mpirun -np 4 --hostfile hosts.=
txt singularity exec ~/container.img trace.sh bt.C.4</div><div><br></div><d=
iv>This assumes the following:</div><div><br></div><div>1. The container im=
age which contains the program&#39;s you want to run is at ~/container.img =
and accessible at this path=C2=A0on all nodes referenced in hosts.txt</div>=
<div>2. The hosts.txt references other physical nodes you want to run on</d=
iv><div>3. The executables trace.sh and bt.C.4 are both inside the containe=
r and in a standard path</div><br><div>In this case we are performing one e=
xecution and MPI + singularity is managing all of the communication between=
 processes, nodes and containers. Also it is now using any optimized hardwa=
re (eg. Infiniband)=C2=A0and existing high performance=C2=A0file systems (w=
hich should not be accessible via a virtualized or Docker&#39;ized=C2=A0clu=
ster for security reasons).</div><div><br></div><div>This way is actually M=
UCH simpler then what you are proposing because there is no need to manage =
any virtual nodes, virtual networks, or resource manager hacks. It really i=
s as easy as just running any other MPI=C2=A0process on an existing cluster=
.=C2=A0</div><div><br></div><div>Hope that helps better!</div><div><br></di=
v><div><span></span><br><br>On Wednesday, June 22, 2016, Raimon Bosch &lt;<=
a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"xDydkJacA=
gAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;retu=
rn true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">rai...=
@gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"mar=
gin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr=
"><br><br>Hi Gregory,<br><br>I&#39;m not sure if I would achieve the same w=
ith your commands. In an environment based on dockers or virtual machines w=
e would do something like this [non applicable to Singularity]:<br><br>&gt;=
 cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile hosts.txt ./trace.sh .=
/bt.C.4<br><br>where hosts.txt* is:<br><br>&gt;vm-ip-01-on-<b>host01</b> sl=
ots=3D2<br>&gt;vm-ip-01-on-<b>host02</b> slots=3D2<br><br>* vm-ip-XX-on-hos=
tXX<b> </b>are IPs i.e. 172.100.60.XX<br><br>and trace.sh is:<br><br>&gt;#!=
/bin/bash<br>&gt;<br>&gt;export EXTRAE_HOME=3D/opt/extrae/<br>&gt;export EX=
TRAE_CONFIG_FILE=3D/extrae.xml<br>&gt;export LD_PRELOAD=3D${EXTRAE_HOME}/li=
b/<wbr>libmpitrace.so<br>&gt;<br>&gt;## Run the desired program<br>&gt;$*<b=
r><br>As you see we only perform one execution and OpenMPI transparently ma=
nages communication between containers or virtual machines. This command wo=
uld work well rather VMs are on the same host or not.<br><br>What I underst=
and from your response is that now we should execute OpenMPI on each host a=
nd then merge results manually. I don&#39;t know yet how to do this merge s=
tep or if it is any way to centralize everything like I would do with VMs.<=
br><br>Thanks in advance,<br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 14=
:42:54 (UTC+2), Gregory M. Kurtzer  escribi=C3=B3:<blockquote class=3D"gmai=
l_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;pad=
ding-left:1ex">Hi Raimon,<div><br></div><div>The quick answer is you have m=
pirun handle that as you would normally where the container file lives on a=
 shared file system:</div><div><br></div><div>$ mpirun singularity exec ~/c=
ontainer.img mpi_prog_in_container</div><div><br></div><div>Let the MPI out=
side the container launch the singularity container on each host as it woul=
d normally launch any MPI program. Then it will call Singulairty and Singul=
arity will launch the MPI program inside the container on each of your host=
s/servers.=C2=A0</div><div><br></div><div>Hope that helps!</div><div><br></=
div><div><span></span><br><br>On Wednesday, June 22, 2016, Raimon Bosch &lt=
;<a rel=3D"nofollow">rai...@gmail.com</a>&gt; wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<br><br>Thank you for your an=
swer. One of our experiments needs to run OpenMPI among several servers. Th=
is means that we should put one of our containers in host01, another in hos=
t02 and another in host03 and collect the results. <br><br>How can I do thi=
s execution in parallel if I need to communicate with more than one server?=
<br><br>El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtze=
r  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin=
-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">H=
i Raimon,<div><br></div><div>The communication model of a Singularity conta=
iner is very different from that of a Docker implementation. This is becaus=
e Docker for all practical purposes emulates a virtual machine as each cont=
ainer has it&#39;s own IP address and thus it&#39;s own ssh server. It also=
 carries its own set of complexities, for example networks need to be segre=
gated/VLan&#39;ed, DNS/host resolution needs to be dynamic and passed down =
to the containers (so they can reach each other), ssh daemons and other pro=
cess running inside the containers, management via an existing scheduling s=
ystem, and the list goes on and on.</div><div><br></div><div>Think of it th=
is way, Singularity does not do any of that... It runs a program within the=
 container as if it were running on the host itself, so to communicate betw=
een containers is as easy as communicating between programs. So for MPI, it=
 would happen with the MPI on the physical host (outside the container) inv=
oking the container subsystem which then invokes the MPI programs within th=
e container and the MPI programs within the container communicate back to t=
he MPI (orted) outside the container on the host to get access to the host =
resources. In this model all available resources and infrastructure can be =
leveraged at full bandwidth by the contained processes and all of the afore=
mentioned complexities akin to running on a virtualized mini-cluster are ci=
rcumvented.</div><div><br></div><div>There is additional information I have=
 written at:</div><div><br></div><div><a href=3D"http://singularity.lbl.gov=
/#hpc" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;h=
ttp://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26=
sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;retu=
rn true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3=
A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjC=
NEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;">http://singularity.lbl.gov=
/#<wbr>hpc</a><br></div><div><br></div><div>That page is still coming along=
, and needs more information still but if you have any questions, comments =
or change proposals please let us know!</div><div><br></div><div>Thanks and=
 hope that helps!</div><div><br></div><div><br></div></div><div><br><div cl=
ass=3D"gmail_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <span dir=
=3D"ltr">&lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt;</span> wrote:<br>=
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi,<br><br>We are t=
rying to run experiments using singularity containers. The idea is to run O=
penMPI among several containers and check performance results. <br><br>How =
can I communicate with another container? In docker this is clear because e=
very container gets an assigned IP and you can ping there, but what is the =
situation in the case of singularity? Is it possible to assign an IP to eac=
h container? Can I connect via ssh to them?<br><br>Thanks in advance,<span>=
<font color=3D"#888888"><br></font></span></div><span><font color=3D"#88888=
8">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computi=
ng Services (HPCS)<br>University of California<br>Lawrence Berkeley Nationa=
l Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>
</blockquote></div>
------=_Part_2613_1376922640.1466606779124--

------=_Part_2612_745715365.1466606779123--
