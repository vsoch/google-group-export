Date: Fri, 24 Feb 2017 05:52:59 -0800 (PST)
From: Geert Jan BEX <geert...@uhasselt.be>
To: singularity <singu...@lbl.gov>
Message-Id: <104158ea-3718-400a-a536-b0c64ccad1fa@lbl.gov>
In-Reply-To: <CAN7etTyX+_RYBb8m4eNFORisfa848Bksm6s5vWZZjMgsdD1WHg@mail.gmail.com>
References: <4a675b1d-5116-4702-a9e4-cc0024dcb436@lbl.gov> <CAN7etTzUMksoSB_H1ZgA3+-fFvM2pO+XzeD3E-pLqLiiL69c4w@mail.gmail.com>
 <5c39ff15-22d6-4b4f-8566-a670d2910e6a@lbl.gov> <2e6d299f-12d6-42ea-aacb-55f0d8924e36@lbl.gov>
 <CAN7etTyX+_RYBb8m4eNFORisfa848Bksm6s5vWZZjMgsdD1WHg@mail.gmail.com>
Subject: Re: [Singularity] job with multiple singularity calls (mixed
 MPI/serial) fails
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_190_2014464807.1487944379741"

------=_Part_190_2014464807.1487944379741
Content-Type: multipart/alternative; 
	boundary="----=_Part_191_1851475176.1487944379742"

------=_Part_191_1851475176.1487944379742
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Dear Greg,

Thanks for the suggestion.

Some update: my initial impression that this problem had to do with 
multiple singularity calls was wrong.  I've reproduced this in isolation as 
well.

When I try your suggestion with Singularity 2.2, I get the following error:
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
A system call failed during shared memory initialization that should
not have.  It is likely that your MPI job will now either abort or
experience performance degradation.

  Local host:  r04i15n1
  System call: open(2) 
  Error:       No such file or directory (errno 2)

[r04i15n1:29725] 9 more processes have sent help message 
help-opal-shmem-mmap.txt / sys call fail

This is the same type of error I get with Singularity 2.2.1, without 
setting SINGULARITY_NOSESSIONCLEANUP..  That would indeed imply that the 
session directory cleanup is fixed in 2.2.1, but now there seems to be an 
issue with the shared memory file of OpenMPI.

Any ideas?

Thanks in advance, best regards, Geert Jan Bex



On Thursday, February 23, 2017 at 4:23:08 PM UTC+1, Gregory M. Kurtzer 
wrote:
>
> While I think you are hitting some locking bug in Singularity, that I 
> think/hope is already fixed in one of our development branches, I do think 
> there is a workaround you can use. Modify your MPI line to include:
>
> SINGULARITY_NOSESSIONCLEANUP=1
>
> I think you can do it like:
>
> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" SINGULARITY_NOSESSIONCLEANUP=1 singularity 
> --debug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img 
> bash -c 'cd /wrf/run/wrfwkdir; real.exe'
>
> Or you can make sure that the variable is set on every node running the 
> process. A better fix is included (or rather will be included) in the 
> latest version of Open MPI that will automatically set that variable, and 
> allow Open MPI to clean up the Singularity sessionfiles directly after the 
> job runs. Presently those optimizations exist in Open MPI's master branch. 
> So if you can build that yourself and test, that is also an option.
>
> Sorry for the delay, and hope that helps!
>
> Greg
>
> On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BEX <gee...@uhasselt.be 
> <javascript:>> wrote:
>
>> Dear Gregory,
>>
>> Any feedback you can give me?
>>
>> Thanks in advance, best regards, -gjb-
>>
>>
>> On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:
>>>
>>> Dear Gregory,
>>>
>>> Thanks for looking into this.
>>>
>>> The /tmp is on the local disks of the compute nodes, so no NFS mounts 
>>> involved.
>>>
>>> The /tmp is also used for the MCA orte_tmpdir_base for mpirun.
>>>
>>> I've put the output of a run with --debug on Google Drive, you can 
>>> access it via:
>>>
>>> https://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=sharing
>>>
>>> The "offending" statement in the job script is:
>>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" singularity --debug exec -B 
>>> ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img bash -c 'cd 
>>> /wrf/run/wrfwkdir; real.exe'
>>>
>>> So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH and 
>>> SIMDIR refer to a GPFS file system, and that is also where the working 
>>> directory for the job is on.
>>>
>>> Singularity's config file is unaltered from default.
>>>
>>> Thanks in advance, -gjb-
>>>
>>> On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. Kurtzer 
>>> wrote:
>>>>
>>>> Hi Geert,
>>>>
>>>> The directory is only supposed to be removed when the flock()s have all 
>>>> been released. I just did some fixups for this in one of the development 
>>>> branches, but 2.2* should be working in the same way. Is `/tmp` a local 
>>>> directory, or is it on some form of shared NFS? Does it have any mount 
>>>> options to disable locking?
>>>>
>>>> Can you also try with --debug enabled and send all of the output? 
>>>>
>>>> Thanks!
>>>>
>>>> On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <gee...@uhasselt.be> 
>>>> wrote:
>>>>
>>>>> Dear,
>>>>>
>>>>> I've run into a problem with singularity, and out of ideas by now.
>>>>>
>>>>> When running a job that involves multiple calls to singularity (same 
>>>>> image each time), some of them serial, some MPI, the MPI ones 
>>>>> intermittently fail with:
>>>>> ERROR  : Could not remove session directory 
>>>>> /tmp/.singularity-session-2530140.42.465829344: Device or resource busy
>>>>>
>>>>> This is for OpenMPI 1.10.2 on host and in image.  The host OS is 
>>>>> CentOS 7, the image OS Ubuntu 16.04,  Singularity 2.2.  The job runs on 4 
>>>>> nodes, 27 processes each.
>>>>>
>>>>> I've tried several approaches (sleep between calls, setting a session 
>>>>> directory per singularity call, ...).
>>>>>
>>>>> Any ideas?
>>>>>
>>>>> Thanks in advance, Geert Jan Bex
>>>>>
>>>>>
>>>>>
>>>>> -- 
>>>>> You received this message because you are subscribed to the Google 
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send 
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> -- 
>>>> Gregory M. Kurtzer
>>>> HPC Systems Architect and Technology Developer
>>>> Lawrence Berkeley National Laboratory HPCS
>>>> University of California Berkeley Research IT
>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>> GitHub: https://github.com/gmkurtzer, Twitter: 
>>>> https://twitter.com/gmkurtzer
>>>>
>>> -- 
>> You received this message because you are subscribed to the Google Groups 
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an 
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
>
> -- 
> Gregory M. Kurtzer
> HPC Systems Architect and Technology Developer
> Lawrence Berkeley National Laboratory HPCS
> University of California Berkeley Research IT
> Singularity Linux Containers (http://singularity.lbl.gov/)
> Warewulf Cluster Management (http://warewulf.lbl.gov/)
> GitHub: https://github.com/gmkurtzer, Twitter: 
> https://twitter.com/gmkurtzer
>

------=_Part_191_1851475176.1487944379742
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Dear Greg,<div><br></div><div>Thanks for the suggestion.</=
div><div><br></div><div>Some update: my initial impression that this proble=
m had to do with multiple singularity calls was wrong. =C2=A0I&#39;ve repro=
duced this in isolation as well.</div><div><br></div><div>When I try your s=
uggestion with Singularity 2.2, I get the following error:</div><div><div>P=
rimary job =C2=A0terminated normally, but 1 process returned</div><div>a no=
n-zero exit code.. Per user-direction, the job has been aborted.</div><div>=
-------------------------------------------------------</div><div>---------=
-----------------------------------------------------------------</div><div=
>A system call failed during shared memory initialization that should</div>=
<div>not have. =C2=A0It is likely that your MPI job will now either abort o=
r</div><div>experience performance degradation.</div><div><br></div><div>=
=C2=A0 Local host: =C2=A0r04i15n1</div><div>=C2=A0 System call: open(2)=C2=
=A0</div><div>=C2=A0 Error: =C2=A0 =C2=A0 =C2=A0 No such file or directory =
(errno 2)</div><div><div><br></div><div>[r04i15n1:29725] 9 more processes h=
ave sent help message help-opal-shmem-mmap.txt / sys call fail</div></div><=
div><br></div><div>This is the same type of error I get with Singularity 2.=
2.1, without setting=C2=A0<span style=3D"font-size: small;">SINGULARITY_</s=
pan><wbr style=3D"font-size: small;"><span style=3D"font-size: small;">NOSE=
SSIONCLEANUP.. =C2=A0That would indeed imply that the session directory cle=
anup is fixed in 2.2.1, but now there seems to be an issue with the shared =
memory file of OpenMPI.</span></div><div><span style=3D"font-size: small;">=
<br></span></div><div><span style=3D"font-size: small;">Any ideas?</span></=
div><div><span style=3D"font-size: small;"><br></span></div><div><span styl=
e=3D"font-size: small;">Thanks in advance, best regards, Geert Jan Bex</spa=
n></div><div><span style=3D"font-size: small;"><br></span></div><div><span =
style=3D"font-size: small;"><br></span></div><br>On Thursday, February 23, =
2017 at 4:23:08 PM UTC+1, Gregory M. Kurtzer wrote:<blockquote class=3D"gma=
il_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left: 1px #ccc solid=
;padding-left: 1ex;"><div dir=3D"ltr"><div>While I think you are hitting so=
me locking bug in Singularity, that I think/hope is already fixed in one of=
 our development branches, I do think there is a workaround you can use. Mo=
dify your MPI line to include:</div><div><br></div><div>SINGULARITY_NOSESSI=
ONCLEANUP=3D1</div><div><br></div><div>I think you can do it like:</div><di=
v><br></div><div><div style=3D"font-size:12.8px">mpirun -mca orte_tmpdir_ba=
se &quot;${ORTE_TMP}&quot;=C2=A0<span style=3D"font-size:small">SINGULARITY=
_<wbr>NOSESSIONCLEANUP=3D1=C2=A0</span><span style=3D"font-size:12.8px">sin=
gularity --debug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/</span>=
<span style=3D"font-size:12.8px">wrfrun<wbr>-latest.img bash -c &#39;cd /wr=
f/run/wrfwkdir; real.exe&#39;</span></div></div><div><br></div><div>Or you =
can make sure that the variable is set on every node running the process. A=
 better fix is included (or rather will be included) in the latest version =
of Open MPI that will automatically set that variable, and allow Open MPI t=
o clean up the Singularity sessionfiles directly after the job runs. Presen=
tly those optimizations exist in Open MPI&#39;s master branch. So if you ca=
n build that yourself and test, that is also an option.</div><div><br></div=
>Sorry for the delay, and hope that helps!<div><br></div><div>Greg</div></d=
iv><div><br><div class=3D"gmail_quote">On Thu, Feb 23, 2017 at 5:11 AM, Gee=
rt Jan BEX <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_blank" =
gdf-obfuscated-mailto=3D"xG9ch81oDAAJ" rel=3D"nofollow" onmousedown=3D"this=
.href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D&#39;jav=
ascript:&#39;;return true;">gee...@uhasselt.be</a>&gt;</span> wrote:<br><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #=
ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div><br></div><d=
iv>Any feedback you can give me?</div><div><br></div><div>Thanks in advance=
, best regards, -gjb-</div><div><div><div><br><br>On Tuesday, February 21, =
2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:<blockquote class=3D"gmail_qu=
ote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding=
-left:1ex"><div dir=3D"ltr">Dear Gregory,<div><br></div><div>Thanks for loo=
king into this.</div><div><br></div><div>The /tmp is on the local disks of =
the compute nodes, so no NFS mounts involved.</div><div><br></div><div>The =
/tmp is also used for the MCA orte_tmpdir_base for mpirun.</div><div><br></=
div><div>I&#39;ve put the output of a run with --debug on Google Drive, you=
 can access it via:</div><div><a href=3D"https://drive.google.com/file/d/0B=
xlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=3Dsharing" rel=3D"nofollow" target=3D"_=
blank" onmousedown=3D"this.href=3D&#39;https://drive.google.com/file/d/0Bxl=
Seep1Z7B-MzRsd2pwV2dUTG8/view?usp\x3dsharing&#39;;return true;" onclick=3D"=
this.href=3D&#39;https://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUT=
G8/view?usp\x3dsharing&#39;;return true;">https://drive.google.com/file/<wb=
r>d/0BxlSeep1Z7B-<wbr>MzRsd2pwV2dUTG8/view?usp=3D<wbr>sharing</a></div><div=
><br></div><div>The &quot;offending&quot; statement in the job script is:</=
div><div><div>mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP}&quot; singular=
ity --debug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/<wbr>wrfrun-=
latest.img bash -c &#39;cd /wrf/run/wrfwkdir; real.exe&#39;</div></div><div=
><br></div><div>So ORTE_TMP points to a directory in /tmp (local disk), VSC=
_SCRATCH and SIMDIR refer to a GPFS file system, and that is also where the=
 working directory for the job is on.</div><div><br></div><div>Singularity&=
#39;s config file is unaltered from default.</div><div><br></div><div>Thank=
s in advance, -gjb-<br><br>On Monday, February 20, 2017 at 5:56:56 PM UTC+1=
, Gregory M. Kurtzer wrote:<blockquote class=3D"gmail_quote" style=3D"margi=
n:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr">Hi Geert,<div><br></div><div>The directory is only supposed to be =
removed when the flock()s have all been released. I just did some fixups fo=
r this in one of the development branches, but 2.2* should be working in th=
e same way. Is `/tmp` a local directory, or is it on some form of shared NF=
S? Does it have any mount options to disable locking?</div><div><br></div><=
div>Can you also try with --debug enabled and send all of the output?=C2=A0=
</div><div><br></div><div>Thanks!</div></div><div><br><div class=3D"gmail_q=
uote">On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <span dir=3D"ltr">&lt;=
<a rel=3D"nofollow">gee...@uhasselt.be</a>&gt;</span> wrote:<br><blockquote=
 class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc soli=
d;padding-left:1ex"><div dir=3D"ltr">Dear,<div><br></div><div>I&#39;ve run =
into a problem with singularity, and out of ideas by now.</div><div><br></d=
iv><div>When running a job that involves multiple calls to singularity (sam=
e image each time), some of them serial, some MPI, the MPI ones intermitten=
tly fail with:</div><div><div>ERROR =C2=A0: Could not remove session direct=
ory /tmp/.singularity-session-<wbr>2530140.42.465829344: Device or resource=
 busy</div></div><div><br></div><div>This is for OpenMPI 1.10.2 on host and=
 in image.=C2=A0 The host OS is CentOS 7, the image OS Ubuntu 16.04, =C2=A0=
Singularity 2.2.=C2=A0 The job runs on 4 nodes, 27 processes each.</div><di=
v><br></div><div>I&#39;ve tried several approaches (sleep between calls, se=
tting a session directory per singularity call, ...).</div><div><br></div><=
div>Any ideas?</div><div><br></div><div>Thanks in advance, Geert Jan Bex</d=
iv><span><font color=3D"#888888"><div><br></div><div><br></div><div><br></d=
iv></font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#3=
9;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\=
x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return =
true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2=
F%2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjd=
e-iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a=
>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.l=
bl.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39=
;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD=
\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true=
;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2F=
warewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BK=
cVgBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><d=
iv>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"nofollow" t=
arget=3D"_blank" onmousedown=3D"this.href=3D&#39;https://www.google.com/url=
?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x=
3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.hre=
f=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtz=
er\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39=
;;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"=
font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkur=
tzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=3D"_blank" onmouse=
down=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwit=
ter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsK=
sNsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.go=
ogle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\=
x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:=
//<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div><=
/div></div></div></div>
</div>
</blockquote></div></div></blockquote></div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
xG9ch81oDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;=
http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\x3=
dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return tr=
ue;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%=
2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-=
iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a>)=
</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lbl=
.gov/" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;h=
ttp://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD\x=
26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true;"=
 onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwa=
rewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcV=
gBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><div=
>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" target=3D"_blank" re=
l=3D"nofollow" onmousedown=3D"this.href=3D&#39;https://www.google.com/url?q=
\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3d=
AFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.href=
=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtze=
r\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;=
;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"f=
ont-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurt=
zer" style=3D"font-size:12.8px" target=3D"_blank" rel=3D"nofollow" onmoused=
own=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwitt=
er.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKs=
NsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.goo=
gle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x=
3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:/=
/<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div></=
div></div></div></div>
</div>
</blockquote></div></div>
------=_Part_191_1851475176.1487944379742--

------=_Part_190_2014464807.1487944379741--
