X-Received: by 10.98.149.142 with SMTP id c14mr2393490pfk.38.1476398889192;
        Thu, 13 Oct 2016 15:48:09 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.178.80 with SMTP id b77ls2436318iof.47.gmail; Thu, 13 Oct
 2016 15:48:08 -0700 (PDT)
X-Received: by 10.98.106.202 with SMTP id f193mr13570840pfc.168.1476398888506;
        Thu, 13 Oct 2016 15:48:08 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id t18si13434553pag.147.2016.10.13.15.48.08
        for <singu...@lbl.gov>;
        Thu, 13 Oct 2016 15:48:08 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.71 as permitted sender) client-ip=209.85.215.71;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.71 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
IronPort-PHdr: 9a23:LGrPax3pkyVlGAQosmDT+DRfVm0co7zxezQtwd8ZsekSKfad9pjvdHbS+e9qxAeQG96KsbQY1qGP7ejJYi8p2d65qncMcZhBBVcuqP49uEgeOvODElDxN/XwbiY3T4xoXV5h+GynYwAOQJ6tL2PbrnD61zMOABK3bVMzfbWvXNKPxJ3pn8mJuLTrKz1SgzS8Zb4gZD6Xli728vcsvI15N6wqwQHIqHYbM85fxGdvOE7B102kvpT4wYRnuxh0l7phspABAu3Heb8lR+ldBTUiL2dn/8ztugTHRBGO+mpfT2MNjxBTCBLE5hzSWp319CT9qLlB33yBPMv5ULQ9QzW+/u8/Ux7uhzoDPiQ47HD/jsZ0yq1cvkTl7yZ2x5bUKKqcL+Z3f6WVKckaTkJGRstXEilZA8W7dYRZXMQbOuMN5abnqlQJtwr2IE/kJ/711i1FnDW+iaI/0Pk7DRPLxiQkFdZIv3PK+oamfJwOWPy4mfGbhQ7IaOlbjHKj5Q==
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FQAACCDgBYhkfXVdFcGgEBAQECAQEBAQgBAQEBFwEBBAEBCgEBglw1AQEBAQF0fAeDOIl1lwWCVoUFjFmBSBskAxwBBoF0gjCBWgKCBAc4FAEBAQEBAQEBAQEBAhABAQEICwsJGS+CMgQCAQIRBQQBOQoxAQEBAQEBAQEBAQEBAQEBARoCCAUiDwMPAhkBAQEDARIIAQgrHRMLCQILDSABCQICIQEPAwEFAQsRBgEHBwQBHAQBh0xKAw8IBZkrj02BMj4yi0KJDg2DcgEBAQcBAQEBAQEBASAQiwKCR4FSEQGDIIJbBYg8ZIYVhHOFJTUBhiaGS4MLgW5OhBmDN4VpiGWEFII9Ex6BEQ8PW4JlOxELgXMeNAeFXUcxgSgBAQE
X-IronPort-AV: E=Sophos;i="5.31,490,1473145200"; 
   d="scan'208,217";a="43737546"
Received: from mail-lf0-f71.google.com ([209.85.215.71])
  by fe3.lbl.gov with ESMTP; 13 Oct 2016 15:48:03 -0700
Received: by mail-lf0-f71.google.com with SMTP id i187so57981911lfe.4
        for <singu...@lbl.gov>; Thu, 13 Oct 2016 15:48:03 -0700 (PDT)
X-Gm-Message-State: AA6/9Rk22h6hUsWri+gHQPii5lKtsDNkQkVdxNGS/tDDKYIFsLOPrOymDcDLjbZ+GEm+F0APccgQ+0G8eqLpfjdP8y0lyyjQfwWurDFLP8HKGsHvNnVVp4pQ7E3KVLSQ+JMFgW0g+6B9vdVQ9o3Hk5IAny4=
X-Received: by 10.25.20.228 with SMTP id 97mr49101lfu.93.1476398882445;
        Thu, 13 Oct 2016 15:48:02 -0700 (PDT)
X-Received: by 10.25.20.228 with SMTP id 97mr49073lfu.93.1476398881917; Thu,
 13 Oct 2016 15:48:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.167.67 with HTTP; Thu, 13 Oct 2016 15:48:00 -0700 (PDT)
In-Reply-To: <dc6c4870-0f65-4f97-b38c-180052ea1020@lbl.gov>
References: <04cccf38-72fb-49ef-b010-15fb80e71e8e@lbl.gov> <CAN7etTyY_+ytGXpY4Te-=xED4zHNzz4Pg7xkB4ULj2sZ=gk2WA@mail.gmail.com>
 <6e2a6338-f64e-4f2a-894b-d40f1f646113@lbl.gov> <CAN7etTzk47mf9bCo2S6Wp-9sYpS6u3JfwVgN2koRpKcamDtRWw@mail.gmail.com>
 <8689886f-9da7-4f40-8769-06dbfc0a547f@lbl.gov> <CAN7etTz0wBEjGweZXC_pcOzEULW7hmeBr7zY5ndtsXOL88jSdA@mail.gmail.com>
 <5cd8d8d2-44f5-47ab-8e02-838942890090@lbl.gov> <CAN7etTyWS+UGZa_EHc-J5at6ULUtFe=Jh3qPXksJNs4-dS5dVg@mail.gmail.com>
 <CAN7etTwDa1oYx6K7sRxhATxT-4w+Z5A5rmpZrO7MPSFmdv2GPA@mail.gmail.com> <dc6c4870-0f65-4f97-b38c-180052ea1020@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 13 Oct 2016 15:48:00 -0700
Message-ID: <CAN7etTyR-NKepOEGf3YAASPTZERTLNC-RP4vpHO+QZxpsT7HGQ@mail.gmail.com>
Subject: Re: [Singularity] Using singularity with MPI jobs
To: singularity <singu...@lbl.gov>, Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary=001a113fa848e75c33053ec6e636

--001a113fa848e75c33053ec6e636
Content-Type: text/plain; charset=UTF-8

On Thu, Oct 13, 2016 at 8:56 AM, Steve Mehlberg <sgmeh...@gmail.com>
wrote:

> Gregory,
>
> I didn't set anything concerning /dev/shm, so I'm not sure why the openmpi
> stuff gets there.
>

I did a bit of checking, and I think openmpi conditionally uses /dev/shm/
based on local configuration of /tmp.


>
> Our group (Atos/Bull) is doing development on the slurm product so that is
> why we are interested in sbatch/srun vs mpirun.  We haven't found anything
> amiss with the invocation using slurm - but something is different from
> mpirun that is causing this issue.
>

I am not an expert on PMIx, but as I understand it, if you are invoking
using PMIx via `srun`, you need to have the SLURM PMIx implementation also
installed within the container, or that the OMPI build itself has to
include the PMIx support.

Just to reiterate, does it work as expected when executing via mpirun?


>
> I'm interested in your comment about singularity support for openmpi.  Are
> you saying there are changes in openmpi for singularity that are not in the
> stable released versions but are in the master branch?  Are any of these
> changes specific to pmi2 or pmix?
>

Yes, there are but I'm not sure if those changes are critical to the
failure you are seeing now.


>
> How can I make sure I'm running with an openmpi that has the "required"
> singularity changes?
>

I believe currently, my schizo/personality fixes are only in the master
branch of OMPI on GitHub at present, and it will be included in the next
release... But, again, I don't think this is the cause of what you are
seeing. I think it is a PMIx issue in that the PMI support is lacking
inside the container. I am CC'ing Ralph with the hope that he can chime in.

Greg




>
> -Steve
>
> On Wednesday, October 12, 2016 at 8:46:29 PM UTC-6, Gregory M. Kurtzer
> wrote:
>>
>> Hi Steve,
>>
>> Did you mention that it works if you call it via mpirun? If so, why don't
>> you just launch with mpirun/mpiexec? I'm not sure the startup invocation is
>> the same for srun even via pmi.
>>
>> Additionally, you may need to use OMPI from the master branch from
>> GitHub. I just heard from Ralph that proper Singularity support has not
>> been part of an OMPI release yet.
>>
>> Thanks and hope that helps!
>>
>> On Wed, Oct 12, 2016 at 2:37 PM, Gregory M. Kurtzer <gm...@lbl.gov>
>> wrote:
>>
>>> Weird how openmpi is actually throwing the session directory in
>>> /dev/shm. I thought it usually uses /tmp.
>>>
>>> Did you set that somewhere or am I confused?
>>>
>>>
>>>
>>>
>>> On Wednesday, October 12, 2016, Steve Mehlberg <sg...@gmail.com>
>>> wrote:
>>>
>>>> Gregory,
>>>>
>>>> Yes, I was able to create a file on the host (non-root uid) in /dev/shm/
>>>> test.it and then view it in the singularity shell.
>>>>
>>>> And, there is some stuff there too, is that normal?
>>>>
>>>> bash-4.2$ ls /dev/shm -la
>>>> total 4
>>>> drwxrwxrwt   4 root      root   100 Oct 12 17:34 .
>>>> drwxr-xr-x  20 root      root  3580 Oct  7 22:04 ..
>>>> -rwxr-xr-x   1 mehlberg  user   880 Oct 12 17:34 test.it
>>>> drwx------  47 root      root   940 Sep 30 17:19
>>>> openmpi-sessions-0@node9_0
>>>> drwx------ 561 mehlberg  user 11220 Oct 12 15:39
>>>> openmpi-sessions-50342@node9_0
>>>>
>>>>
>>>> Steve
>>>>
>>>> On Wednesday, October 12, 2016 at 11:18:20 AM UTC-6, Gregory M. Kurtzer
>>>> wrote:
>>>>>
>>>>> Can you create a file in /dev/shm/... on the host, and then start a
>>>>> Singularity container and confirm that you can see that file from within
>>>>> the container please?
>>>>>
>>>>> Thanks!
>>>>>
>>>>> On Wed, Oct 12, 2016 at 9:45 AM, Steve Mehlberg <sg...@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> Wow, that was very interesting.  I indeed get the same problem with
>>>>>> the singularity -n1 (srun - one task).  I created the strace, then wanted
>>>>>> to compare the output to a non-singularity run.  But when I change the
>>>>>> non-singularity run to use anything other than the required number of tasks
>>>>>> I get the same error!  That seems to indicate that in the singularity run
>>>>>> (srun with the correct number of tasks) for some reason the MPI processes
>>>>>> can't communicate with one another.
>>>>>>
>>>>>> The strace doesn't show much - or at least not much that means
>>>>>> something to me.  The program seems to be going along outputting data then
>>>>>> aborts with exit 6:
>>>>>>
>>>>>> [pid 13573] write(27, "                    suppress iso"..., 56) = 56
>>>>>> [pid 13573] write(27, "                    ------------"..., 56) = 56
>>>>>> [pid 13573] write(27, "      no isolated ocean grid poi"..., 36) = 36
>>>>>> [pid 13573] open("/opt/mpi/openmpi-icc/2.0
>>>>>> .0/share/openmpi/help-mpi-errors.txt", O_RDONLY) = 28
>>>>>> [pid 13573] ioctl(28, SNDCTL_TMR_TIMEBASE or
>>>>>> SNDRV_TIMER_IOCTL_NEXT_DEVICE or TCGETS, 0x7ffec31d4d80) = -1 ENOTTY
>>>>>> (Inappropriate ioctl for device)
>>>>>> [pid 13573] fstat(28, {st_mode=S_IFREG|0644, st_size=1506, ...}) = 0
>>>>>> [pid 13573] mmap(NULL, 4096, PROT_READ|PROT_WRITE,
>>>>>> MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f4b65f45000
>>>>>> [pid 13573] read(28, "# -*- text -*-\n#\n# Copyright (c)"..., 8192) =
>>>>>> 1506
>>>>>> [pid 13573] read(28, "", 4096)          = 0
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] munmap(0x7f4b65f45000, 4096) = 0
>>>>>> [pid 13573] write(1, "[node9:13573] *** An error occ"..., 361) = 361
>>>>>> [pid 13573] stat("/dev/shm/openmpi-sessions-50342@node9_0/37255/1/0",
>>>>>> {st_mode=S_IFDIR|0700, st_size=40, ...}) = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0/37255/1/0",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 28
>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) = 48
>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) = 0
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0/37255/1/0",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 28
>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) = 48
>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) = 0
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] rmdir("/dev/shm/openmpi-sessions-50342@node9_0/37255/1/0")
>>>>>> = 0
>>>>>> [pid 13573] stat("/dev/shm/openmpi-sessions-50342@node9_0/37255/1",
>>>>>> {st_mode=S_IFDIR|0700, st_size=40, ...}) = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0/37255/1",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 28
>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) = 48
>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) = 0
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0/37255/1",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 28
>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) = 48
>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) = 0
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] rmdir("/dev/shm/openmpi-sessions-50342@node9_0/37255/1")
>>>>>> = 0
>>>>>> [pid 13573] stat("/dev/shm/openmpi-sessions-50342@node9_0",
>>>>>> {st_mode=S_IFDIR|0700, st_size=11080, ...}) = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 28
>>>>>> [pid 13573] getdents(28, /* 554 entries */, 32768) = 17424
>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) = 0
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 28
>>>>>> [pid 13573] getdents(28, /* 554 entries */, 32768) = 17424
>>>>>> [pid 13573] close(28)                   = 0
>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-50342@node9_0/37255/1/0",
>>>>>> O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = -1 ENOENT (No such file
>>>>>> or directory)
>>>>>> [pid 13573] exit_group(6)               = ?
>>>>>> [pid 13574] +++ exited with 6 +++
>>>>>> +++ exited with 6 +++
>>>>>> srun: error: node9: task 0: Exited with exit code 6
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Wednesday, October 12, 2016 at 8:53:12 AM UTC-6, Gregory M.
>>>>>> Kurtzer wrote:
>>>>>>>
>>>>>>> Can you replicate the problem with a -np 1? If so can you strace it
>>>>>>> from within the container:
>>>>>>>
>>>>>>> mpirun -np 1 singularity exec container.img strace -ff
>>>>>>> /path/to/mpi.exe (opts)
>>>>>>>
>>>>>>> Yes you can try Singularity 2.2. Please install it to a different
>>>>>>> path so we can test side by side if you don't mind (if really like to debug
>>>>>>> this).
>>>>>>>
>>>>>>> Thanks!
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Wednesday, October 12, 2016, Steve Mehlberg <sg...@gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Greg,
>>>>>>>>
>>>>>>>> I put a bind to /opt in the singularity.conf file so that
>>>>>>>> /opt/intel is available in the container.
>>>>>>>>
>>>>>>>> All the tasks (16) immediately exit code 6.  The job exits after
>>>>>>>> about 4 seconds.  It normally takes about 16 minutes to run with the
>>>>>>>> configuration I'm using and I do see the start of some output.
>>>>>>>>
>>>>>>>> I am using openmpi 2.0.0.
>>>>>>>>
>>>>>>>> I tried an "export SINGULARITY_NO_NAMESPACE_PID=1" in the bash
>>>>>>>> script that runs all of this for each process and I still get the problem.
>>>>>>>>
>>>>>>>> [node12:9779] *** An error occurred in MPI_Isend
>>>>>>>> [node12:9779] *** reported by process [3025076225,0]
>>>>>>>> [node12:9779] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
>>>>>>>> [node12:9779] *** MPI_ERR_RANK: invalid rank
>>>>>>>> [node12:9779] *** MPI_ERRORS_ARE_FATAL (processes in this
>>>>>>>> communicator will now abort,
>>>>>>>> [node12:9779] ***    and potentially your MPI job)
>>>>>>>>
>>>>>>>> I can try 2.2 - do you think it might behave differently?
>>>>>>>>
>>>>>>>> Thanks for the ideas and help.
>>>>>>>>
>>>>>>>> Regards,
>>>>>>>>
>>>>>>>> Steve
>>>>>>>>
>>>>>>>> On Tuesday, October 11, 2016 at 8:19:47 PM UTC-6, Gregory M.
>>>>>>>> Kurtzer wrote:
>>>>>>>>>
>>>>>>>>> Hi Steve,
>>>>>>>>>
>>>>>>>>> I'm not sure at first glance, but just to touch on the basics...
>>>>>>>>> Is /opt/intel available from within the container? Do all tasks exit code
>>>>>>>>> 6, or just some of them?
>>>>>>>>>
>>>>>>>>> What version of OMPI are you using?
>>>>>>>>>
>>>>>>>>> I wonder if the PID namespace is causing a problem here... I'm not
>>>>>>>>> sure it gets effectively disabled when running via srun and pmi. Can you
>>>>>>>>> export the environment variable "SINGULARITY_NO_NAMESPACE_PID=1"
>>>>>>>>> in a place where Singularity will pick it up definitively by all ranks?
>>>>>>>>> That will ensure that the PID namespace is not being exported.
>>>>>>>>>
>>>>>>>>> Additionally, you could try version 2.2. I just released it, and
>>>>>>>>> by default it does not unshare() out the PID namespace. But... It is the
>>>>>>>>> first release in the 2.2 series so it may bring with it other issues that
>>>>>>>>> still need resolving.... But we should debug those too! :)
>>>>>>>>>
>>>>>>>>> Greg
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Tue, Oct 11, 2016 at 2:40 PM, Steve Mehlberg <
>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> Does singularity support MPI PMI-2 jobs?  I've had mixed success
>>>>>>>>>> testing benchmark applications using a singularity container.
>>>>>>>>>>
>>>>>>>>>> Currently I'm struggling to get the NEMO benchmark to run using
>>>>>>>>>> slurm 16.05 and pmi2.  I can run the exact same executable on bare metal
>>>>>>>>>> with the same slurm, but I get Rank errors when I run using "srun
>>>>>>>>>> --mpi=pmi2 singularity...".  The application aborts with an exit code 6.
>>>>>>>>>>
>>>>>>>>>> I tried pmix too, but that gets mpi aborts for both bare metal
>>>>>>>>>> and singularity.
>>>>>>>>>>
>>>>>>>>>> The only way I could get the NEMO application to compile was to
>>>>>>>>>> use the intel compilers and mpi:
>>>>>>>>>>
>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/compilervars.sh
>>>>>>>>>> intel64
>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/ifortvars.sh
>>>>>>>>>> intel64
>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/iccvars.sh
>>>>>>>>>> intel64
>>>>>>>>>> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>>>>>>>>>>
>>>>>>>>>> It runs fine when I use mpirun with or without singularity.
>>>>>>>>>>
>>>>>>>>>> Example run/error:
>>>>>>>>>>
>>>>>>>>>> sbatch ...
>>>>>>>>>> srun --mpi=pmi2 -n16 singularity exec c7.img run.it > out_now
>>>>>>>>>>
>>>>>>>>>> .......
>>>>>>>>>> srun: error: node11: tasks 0-7: Exited with exit code 6
>>>>>>>>>> srun: error: node12: tasks 8-15: Exited with exit code 6
>>>>>>>>>>
>>>>>>>>>> $ cat run.it
>>>>>>>>>> #!/bin/sh
>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/compilervars.sh
>>>>>>>>>> intel64
>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/ifortvars.sh
>>>>>>>>>> intel64
>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/iccvars.sh
>>>>>>>>>> intel64
>>>>>>>>>> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>>>>>>>>>> source env_bench
>>>>>>>>>> export PATH=/opt/mpi/openmpi-icc/2.0.
>>>>>>>>>> 0/bin:/opt/pmix/1.1.5/bin:$PATH
>>>>>>>>>> export LD_LIBRARY_PATH=/opt/openmpi-i
>>>>>>>>>> cc/2.0.0/lib:/opt/pmix/1.1.5/lib:$LD_LIBRARY_PATH
>>>>>>>>>> export OMPI_MCA_btl=self,sm,openib
>>>>>>>>>>
>>>>>>>>>> ./opa_8_2 namelist >out_now
>>>>>>>>>>
>>>>>>>>>> $ cat out_now
>>>>>>>>>> [node12:29725] *** An error occurred in MPI_Isend
>>>>>>>>>> [node12:29725] *** reported by process [3865116673,0]
>>>>>>>>>> [node12:29725] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
>>>>>>>>>> [node12:29725] *** MPI_ERR_RANK: invalid rank
>>>>>>>>>> [node12:29725] *** MPI_ERRORS_ARE_FATAL (processes in this
>>>>>>>>>> communicator will now abort,
>>>>>>>>>> [node12:29725] ***    and potentially your MPI job)
>>>>>>>>>>
>>>>>>>>>> I am running singularity 2.1 - any ideas?
>>>>>>>>>>
>>>>>>>>>> -Steve
>>>>>>>>>>
>>>>>>>>>> --
>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>>>> University of California Berkeley Research IT
>>>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>>>> er.com/gmkurtzer
>>>>>>>>>
>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Gregory M. Kurtzer
>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>> University of California Berkeley Research IT
>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>> er.com/gmkurtzer
>>>>>>>
>>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gregory M. Kurtzer
>>>>> HPC Systems Architect and Technology Developer
>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>> University of California Berkeley Research IT
>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>> er.com/gmkurtzer
>>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--001a113fa848e75c33053ec6e636
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Thu, Oct 13, 2016 at 8:56 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<=
a href=3D"mailto:sgmeh...@gmail.com" target=3D"_blank">sgmeh...@gmail.com</=
a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Gre=
gory,<br><br>I didn&#39;t set anything concerning /dev/shm, so I&#39;m not =
sure why the openmpi stuff gets there.<br></div></blockquote><div><br></div=
><div>I did a bit of checking, and I think openmpi conditionally uses /dev/=
shm/ based on local configuration of /tmp.=C2=A0</div><div>=C2=A0</div><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #c=
cc solid;padding-left:1ex"><div dir=3D"ltr"><br>Our group (Atos/Bull) is do=
ing development on the slurm product so that is why we are interested in sb=
atch/srun vs mpirun.=C2=A0 We haven&#39;t found anything amiss with the inv=
ocation using slurm - but something is different from mpirun that is causin=
g this issue.<br></div></blockquote><div><br></div><div>I am not an expert =
on PMIx, but as I understand it, if you are invoking using PMIx via `srun`,=
 you need to have the SLURM PMIx implementation also installed within the c=
ontainer, or that the OMPI build itself has to include the PMIx support.</d=
iv><div><br></div><div>Just to reiterate, does it work as expected when exe=
cuting via mpirun?</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" =
style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><di=
v dir=3D"ltr"><br>I&#39;m interested in your comment about singularity supp=
ort for openmpi.=C2=A0 Are you saying there are changes in openmpi for sing=
ularity that are not in the stable released versions but are in the master =
branch?=C2=A0 Are any of these changes specific to pmi2 or pmix?<br></div><=
/blockquote><div><br></div><div>Yes, there are but I&#39;m not sure if thos=
e changes are critical to the failure you are seeing now.</div><div>=C2=A0<=
/div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>How can I make sur=
e I&#39;m running with an openmpi that has the &quot;required&quot; singula=
rity changes?<br></div></blockquote><div><br></div><div>I believe currently=
, my schizo/personality fixes are only in the master branch of OMPI on GitH=
ub at present, and it will be included in the next release... But, again, I=
 don&#39;t think this is the cause of what you are seeing. I think it is a =
PMIx issue in that the PMI support is lacking inside the container. I am CC=
&#39;ing Ralph with the hope that he can chime in.</div><div><br></div><div=
>Greg</div><div><br></div><div><br></div><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div dir=3D"ltr"><br>-Steve<span class=3D""><br><br>On Wednes=
day, October 12, 2016 at 8:46:29 PM UTC-6, Gregory M. Kurtzer wrote:</span>=
<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Steve,<div><br>=
</div><span class=3D""><div>Did you mention that it works if you call it vi=
a mpirun? If so, why don&#39;t you just launch with mpirun/mpiexec? I&#39;m=
 not sure the startup invocation is the same for srun even via pmi.</div><d=
iv><br></div><div>Additionally, you may need to use OMPI from the master br=
anch from GitHub. I just heard from Ralph that proper Singularity support h=
as not been part of an OMPI release yet.</div><div><br></div><div>Thanks an=
d hope that helps!</div></span></div><div><br><div class=3D"gmail_quote"><s=
pan class=3D"">On Wed, Oct 12, 2016 at 2:37 PM, Gregory M. Kurtzer <span di=
r=3D"ltr">&lt;<a rel=3D"nofollow">gm...@lbl.gov</a>&gt;</span> wrote:<br></=
span><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><span class=3D"">Weird=
 how openmpi is actually throwing the session directory in /dev/shm. I thou=
ght it usually uses /tmp.=C2=A0<div><br></div>Did you set that somewhere or=
 am I confused?</span><div><div class=3D"h5"><div><div><br><div><br></div><=
div><br><br>On Wednesday, October 12, 2016, Steve Mehlberg &lt;<a rel=3D"no=
follow">sg...@gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote"=
 style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><d=
iv dir=3D"ltr">Gregory,<br><br>Yes, I was able to create a file on the host=
 (non-root uid) in /dev/shm/<a href=3D"http://test.it" rel=3D"nofollow" tar=
get=3D"_blank">test.it</a> and then view it in the singularity shell.<br><b=
r>And, there is some stuff there too, is that normal?<br><br><span style=3D=
"font-family:courier new,monospace">bash-4.2$ ls /dev/shm -la<br>total 4<br=
>drwxrwxrwt=C2=A0=C2=A0 4 root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 root=C2=A0=C2=
=A0 100 Oct 12 17:34 .<br>drwxr-xr-x=C2=A0 20 root=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 root=C2=A0 3580 Oct=C2=A0 7 22:04 ..<br>-rwxr-xr-x=C2=A0=C2=A0 1 meh=
lberg=C2=A0 user =C2=A0 880 Oct 12 17:34 <a href=3D"http://test.it" rel=3D"=
nofollow" target=3D"_blank">test.it</a><br>drwx------=C2=A0 47 root=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 root=C2=A0=C2=A0 940 Sep 30 17:19 openmpi-sessions=
-0@node9_0<br>drwx------ 561 mehlberg=C2=A0 user 11220 Oct 12 15:39 openmpi=
-sessions-50342@node9_0</span><br><br><br>Steve<br><br>On Wednesday, Octobe=
r 12, 2016 at 11:18:20 AM UTC-6, Gregory M. Kurtzer wrote:<blockquote class=
=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div dir=3D"ltr">Can you create a file in /dev/shm/.=
.. on the host, and then start a Singularity container and confirm that you=
 can see that file from within the container please?<div><br></div><div>Tha=
nks!</div></div><div><br><div class=3D"gmail_quote">On Wed, Oct 12, 2016 at=
 9:45 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gm=
ail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D=
"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D=
"ltr">Wow, that was very interesting.=C2=A0 I indeed get the same problem w=
ith the singularity -n1 (srun - one task).=C2=A0 I created the strace, then=
 wanted to compare the output to a non-singularity run.=C2=A0 But when I ch=
ange the non-singularity run to use anything other than the required number=
 of tasks I get the same error!=C2=A0 That seems to indicate that in the si=
ngularity run (srun with the correct number of tasks) for some reason the M=
PI processes can&#39;t communicate with one another. <br><br>The strace doe=
sn&#39;t show much - or at least not much that means something to me.=C2=A0=
 The program seems to be going along outputting data then aborts with exit =
6:<br><br><span style=3D"font-family:courier new,monospace">[pid 13573] wri=
te(27, &quot;=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 suppress iso&quot;..=
., 56) =3D 56<br>[pid 13573] write(27, &quot;=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 ------------&quot;..., 56) =3D 56<br>[pid 13573] write(27, &quot;=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 no isolated ocean grid poi&quot;..., 36) =3D=
 36<br>[pid 13573] open(&quot;/opt/mpi/openmpi-icc/2.0<wbr>.0/share/openmpi=
/help-mpi-erro<wbr>rs.txt&quot;, O_RDONLY) =3D 28<br>[pid 13573] ioctl(28, =
SNDCTL_TMR_TIMEBASE or SNDRV_TIMER_IOCTL_NEXT_DEVICE or TCGETS, 0x7ffec31d4=
d80) =3D -1 ENOTTY (Inappropriate ioctl for device)<br>[pid 13573] fstat(28=
, {st_mode=3DS_IFREG|0644, st_size=3D1506, ...}) =3D 0<br>[pid 13573] mmap(=
NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) =3D 0x7=
f4b65f45000<br>[pid 13573] read(28, &quot;# -*- text -*-\n#\n# Copyright (c=
)&quot;..., 8192) =3D 1506<br>[pid 13573] read(28, &quot;&quot;, 4096)=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] cl=
ose(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] munmap(0x7f=
4b65f45000, 4096) =3D 0<br>[pid 13573] write(1, &quot;[node9:13573] *** An =
error occ&quot;..., 361) =3D 361<br>[pid 13573] stat(&quot;/dev/shm/openmpi=
-session<wbr>s-50342@node9_0/37255/1/0&quot;, {st_mode=3DS_IFDIR|0700, st_s=
ize=3D40, ...}) =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/openmp=
i-sessions-503<wbr>42@node9_0/37255/1/0&quot;, O_RDONLY|O_NONBLOCK|O_DIRECT=
OR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 13573] getdents(28, /* 2 entries */, 327=
68) =3D 48<br>[pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0<br>[pi=
d 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] op=
enat(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0/37255/1/0=
&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 1357=
3] getdents(28, /* 2 entries */, 32768) =3D 48<br>[pid 13573] getdents(28, =
/* 0 entries */, 32768) =3D 0<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 =3D 0<br>[pid 13573] rmdir(&quot;/dev/shm/openmpi-sessio<wbr>n=
s-50342@node9_0/37255/1/0&quot;) =3D 0<br>[pid 13573] stat(&quot;/dev/shm/o=
penmpi-session<wbr>s-50342@node9_0/37255/1&quot;, {st_mode=3DS_IFDIR|0700, =
st_size=3D40, ...}) =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/op=
enmpi-sessions-503<wbr>42@node9_0/37255/1&quot;, O_RDONLY|O_NONBLOCK|O_DIRE=
CTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 13573] getdents(28, /* 2 entries */, 3=
2768) =3D 48<br>[pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0<br>[=
pid 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573]=
 openat(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0/37255/=
1&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 135=
73] getdents(28, /* 2 entries */, 32768) =3D 48<br>[pid 13573] getdents(28,=
 /* 0 entries */, 32768) =3D 0<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] rmdir(&quot;/dev/shm/openmpi-sessio<wb=
r>ns-50342@node9_0/37255/1&quot;) =3D 0<br>[pid 13573] stat(&quot;/dev/shm/=
openmpi-session<wbr>s-50342@node9_0&quot;, {st_mode=3DS_IFDIR|0700, st_size=
=3D11080, ...}) =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/openmp=
i-sessions-503<wbr>42@node9_0&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O=
_CLOEXEC) =3D 28<br>[pid 13573] getdents(28, /* 554 entries */, 32768) =3D =
17424<br>[pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0<br>[pid 135=
73] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] opena=
t(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0&quot;, O_RDO=
NLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 13573] getdents(2=
8, /* 554 entries */, 32768) =3D 17424<br>[pid 13573] close(28)=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/=
openmpi-sessions-503<wbr>42@node9_0/37255/1/0&quot;, O_RDONLY|O_NONBLOCK|O_=
DIRECTOR<wbr>Y|O_CLOEXEC) =3D -1 ENOENT (No such file or directory)<br>[pid=
 13573] exit_group(6)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D ?<br>[pid 13574] +++ exited with 6 +++<b=
r>+++ exited with 6 +++<br>srun: error: node9: task 0: Exited with exit cod=
e 6</span><span><br><br><br><br>On Wednesday, October 12, 2016 at 8:53:12 A=
M UTC-6, Gregory M. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" =
style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left=
:1ex"><span>Can you replicate the problem with a -np 1? If so can you strac=
e it from within the container:<div><br></div><div>mpirun -np 1 singularity=
 exec container.img strace -ff /path/to/mpi.exe (opts)<span></span></div><d=
iv><br></div><div>Yes you can try Singularity 2.2. Please install it to a d=
ifferent path so we can test side by side if you don&#39;t mind (if really =
like to debug this).=C2=A0</div><div><br></div><div>Thanks!</div><div><br><=
/div></span><div><div><div><br><br>On Wednesday, October 12, 2016, Steve Me=
hlberg &lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt; wrote:<br><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div dir=3D"ltr">Greg,<br><br>I put a bind to /opt in =
the singularity.conf file so that /opt/intel is available in the container.=
<br><br>All the tasks (16) immediately exit code 6.=C2=A0 The job exits aft=
er about 4 seconds.=C2=A0 It normally takes about 16 minutes to run with th=
e configuration I&#39;m using and I do see the start of some output.<br><br=
>I am using openmpi 2.0.0.<br><br>I tried an &quot;export <span>SINGULARITY=
_NO_NAMESPACE_</span><span>PID=3D1<wbr>&quot; in the bash script that runs =
all of this for each process and I still get the problem.</span><br><br>[no=
de12:9779] *** An error occurred in MPI_Isend<br>[node12:9779] *** reported=
 by process <a value=3D"+13025076225">[3025076225</a>,0]<br>[node12:9779] *=
** on communicator MPI COMMUNICATOR 3 DUP FROM 0<br>[node12:9779] *** MPI_E=
RR_RANK: invalid rank<br>[node12:9779] *** MPI_ERRORS_ARE_FATAL (processes =
in this communicator will now abort,<br>[node12:9779] ***=C2=A0=C2=A0=C2=A0=
 and potentially your MPI job)<br><br>I can try 2.2 - do you think it might=
 behave differently?<br><br>Thanks for the ideas and help.<br><br>Regards,<=
br><br>Steve<br><br>On Tuesday, October 11, 2016 at 8:19:47 PM UTC-6, Grego=
ry M. Kurtzer wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0;mar=
gin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr=
">Hi Steve,<div><br></div><div>I&#39;m not sure at first glance, but just t=
o touch on the basics... Is /opt/intel available from within the container?=
 Do all tasks exit code 6, or just some of them?</div><div><br></div><div>W=
hat version of OMPI are you using?</div><div><br></div><div>I wonder if the=
 PID namespace is causing a problem here... I&#39;m not sure it gets effect=
ively disabled when running via srun and pmi. Can you export the environmen=
t variable &quot;<span>SINGULARITY_NO_NAMESPACE_</span><span>PID=3D<wbr>1&q=
uot; in a place where Singularity will pick it up definitively by all ranks=
? That will ensure that the PID namespace is not being exported.</span></di=
v><div><span><br></span></div><div><span>Additionally, you could try versio=
n 2.2. I just released it, and by default it does not unshare() out the PID=
 namespace. But... It is the first release in the 2.2 series so it may brin=
g with it other issues that still need resolving.... But we should debug th=
ose too! :)</span></div><div><span><br></span></div><div>Greg</div><div><br=
></div><div><br></div>







</div><div><br><div class=3D"gmail_quote">On Tue, Oct 11, 2016 at 2:40 PM, =
Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a=
>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 =
0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Does=
 singularity support MPI PMI-2 jobs?=C2=A0 I&#39;ve had mixed success testi=
ng benchmark applications using a singularity container.=C2=A0 <br><br>Curr=
ently I&#39;m struggling to get the NEMO benchmark to run using slurm 16.05=
 and pmi2.=C2=A0 I can run the exact same executable on bare metal with the=
 same slurm, but I get Rank errors when I run using &quot;srun --mpi=3Dpmi2=
 singularity...&quot;.=C2=A0 The application aborts with an exit code 6.<br=
><br>I tried pmix too, but that gets mpi aborts for both bare metal and sin=
gularity.<br><br>The only way I could get the NEMO application to compile w=
as to use the intel compilers and mpi:<br><br><span style=3D"font-size:12.0=
pt;font-family:&quot;Times New Roman&quot;,&quot;serif&quot;;color:black">s=
ource
/opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>compilerv=
ars.sh intel64<br>
source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>if=
ortvars.sh
intel64<br>
source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>ic=
cvars.sh
intel64<br>
source /opt/mpi/openmpi-icc/2.0.0/bin<wbr>/mpivars.sh<br><br></span>It runs=
 fine when I use mpirun with or without singularity.<br><br>Example run/err=
or:<br><br>sbatch ...<br>srun --mpi=3Dpmi2 -n16 singularity exec c7.img <a =
href=3D"http://run.it" rel=3D"nofollow" target=3D"_blank">run.it</a> &gt; o=
ut_now<br><br>.......<br>srun: error: node11: tasks 0-7: Exited with exit c=
ode 6<br>srun: error: node12: tasks 8-15: Exited with exit code 6<br><br>$ =
cat <a href=3D"http://run.it" rel=3D"nofollow" target=3D"_blank">run.it</a>=
<br>#!/bin/sh<br>source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/=
linux/bin/<wbr>compilervars.sh intel64<br>source /opt/intel/compilers_and_l=
ibra<wbr>ries_2016.3.210/linux/bin/<wbr>ifortvars.sh intel64<br>source /opt=
/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>iccvars.sh in=
tel64<br>source /opt/mpi/openmpi-icc/2.0.0/bin<wbr>/mpivars.sh<br>source en=
v_bench<br>export PATH=3D/opt/mpi/openmpi-icc/2.0.<wbr>0/bin:/opt/pmix/1.1.=
5/bin:$PAT<wbr>H<br>export LD_LIBRARY_PATH=3D/opt/openmpi-i<wbr>cc/2.0.0/li=
b:/opt/pmix/1.1.5/l<wbr>ib:$LD_LIBRARY_PATH<br>export OMPI_MCA_btl=3Dself,s=
m,openib<br><br>./opa_8_2 namelist &gt;out_now<br><br>$ cat out_now<br>[nod=
e12:29725] *** An error occurred in MPI_Isend<br>[node12:29725] *** reporte=
d by process <a value=3D"+13865116673">[3865116673</a>,0]<br>[node12:29725]=
 *** on communicator MPI COMMUNICATOR 3 DUP FROM 0<br>[node12:29725] *** MP=
I_ERR_RANK: invalid rank<br>[node12:29725] *** MPI_ERRORS_ARE_FATAL (proces=
ses in this communicator will now abort,<br>[node12:29725] ***=C2=A0=C2=A0=
=C2=A0 and potentially your MPI job)<br><br>I am running singularity 2.1 - =
any ideas?<span><font color=3D"#888888"><br><br>-Steve<br><br></font></span=
></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.go=
v/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warew=
ulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.g=
ov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" re=
l=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div><div dir=3D"ltr"><d=
iv><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"=
><div>Gregory M. Kurtzer</div><div>HPC Systems Architect and Technology Dev=
eloper</div><div>Lawrence Berkeley National Laboratory HPCS<br>University o=
f California Berkeley Research IT<br>Singularity Linux Containers=C2=A0(<a =
href=3D"http://singularity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">htt=
p://singularity<wbr>.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=
=A0(<a href=3D"http://warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank"=
>http://warewulf.lb<wbr>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https=
://github.com/gmkurtzer" rel=3D"nofollow" target=3D"_blank">https://github.=
com/gmk<wbr>urtzer</a>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=
=A0</span><a href=3D"https://twitter.com/gmkurtzer" style=3D"font-size:12.8=
px" rel=3D"nofollow" target=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</=
a></div></div></div></div></div></div></div></div></div></div><br>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.gov/=
</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewul=
f.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.gov=
/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=
=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
</blockquote></div>
</div></div></div></div></div>
</blockquote></div><div><div class=3D"h5"><br><br clear=3D"all"><div><br></=
div>-- <br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr=
"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. K=
urtzer</div><div>HPC Systems Architect and Technology Developer</div><div>L=
awrence Berkeley National Laboratory HPCS<br>University of California Berke=
ley Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://sin=
gularity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wb=
r>.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"htt=
p://warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.l=
b<wbr>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmku=
rtzer" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer=
</a>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D=
"https://twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow"=
 target=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div>=
</div></div></div></div></div></div></div></div>
</div></div></div>
</blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr">=
<div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sys=
tems Architect and Technology Developer</div><div>Lawrence Berkeley Nationa=
l Laboratory HPCS<br>University of California Berkeley Research IT<br>Singu=
larity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targe=
t=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster Ma=
nagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http:=
//warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.c=
om/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<spa=
n style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitte=
r.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twitt=
er.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div><=
/div></div>
</div></div>

--001a113fa848e75c33053ec6e636--
