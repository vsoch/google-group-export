Date: Fri, 31 May 2019 19:19:22 -0700 (PDT)
From: Dianne Patterson <dian...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <a269a864-9088-4c67-b844-447d2941dbaa@lbl.gov>
In-Reply-To: <CAM=pu+KisyYoy18tL_L847D1S5BbXj4AE310e+fr2w1bkZxf0Q@mail.gmail.com>
References: <c77fd727-4d70-4554-8eb2-ed2bf902ca6a@lbl.gov> <0ef0f32e-0db6-44e2-a98f-cea211da06ba@lbl.gov>
 <CAM=pu+KisyYoy18tL_L847D1S5BbXj4AE310e+fr2w1bkZxf0Q@mail.gmail.com>
Subject: Re: [Singularity] Re: build exceeded max build time
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_1049_675250097.1559355562347"

------=_Part_1049_675250097.1559355562347
Content-Type: multipart/alternative; 
	boundary="----=_Part_1050_1810584487.1559355562348"

------=_Part_1050_1810584487.1559355562348
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Thanks!  We don't have slurm, but I got it working with PBS and qsub.  This 
is very cool.
-Dianne

On Friday, May 31, 2019 at 4:04:15 PM UTC-7, vanessa wrote:
>
> Yep, looks like it was killed because it took too much memory to squash. 
> If you are on HPC, make sure you are on an interactive node (and not the 
> login node). If you need to get an interactive node with more memory, you 
> can usually ask for one with srun:
>
> srun --mem 32000 --time 8:00:00 --pty bash
>
>
> You did everything right, you just need more memory :) 
>
> On Fri, May 31, 2019 at 6:50 PM Dianne Patterson <di...@gmail.com 
> <javascript:>> wrote:
>
>> Thanks, before trying the remote option, I tried building directly on the 
>> HPC, with exactly the command you suggested:
>> singularity build fmriprep.sif docker://poldracklab/fmriprep
>>
>> but got this message every time:
>>
>> FATAL:   While performing build: While running mksquashfs: signal: 
>> killed: 
>>
>>
>> I could build lolcow directly on the HPC, but neither fmriprep not 
>> bids/mrtrix3_connectome (both got almost all the way through and then 
>> failed with the highlighted message above). 
>>
>> I have built successfully on our linux box at home and transferred the 
>> resulting sif files...but I'd LOVE to be able to build directly on the HPC.
>>
>> Thanks for your quick reply. I'll try to move the cache and see if it'll 
>> run.
>>
>> -Dianne
>>
>> On Friday, May 31, 2019 at 2:08:53 PM UTC-7, Dianne Patterson wrote:
>>>
>>> Dear experts,
>>>
>>> From our HPC, I tried this: 
>>>
>>> singularity build --remote fmriprep.sif docker://poldracklab/fmriprep
>>>
>>> However, it eventually timed out and thus failed: 
>>>
>>> 5cf17cfd46f1aa23bc9fcd5b build exceeded max build time FATAL:   While 
>>> performing build: build has not completed 
>>>
>>> Can you tell me whether the problem is on the HPC side or the remote 
>>> builder at Sylabs side?
>>>
>>> Thanks so much,
>>>
>>>
>>> Dianne
>>>
>> -- 
>> You received this message because you are subscribed to the Google Groups 
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an 
>> email to si...@lbl.gov <javascript:>.
>> To view this discussion on the web visit 
>> https://groups.google.com/a/lbl.gov/d/msgid/singularity/0ef0f32e-0db6-44e2-a98f-cea211da06ba%40lbl.gov 
>> <https://groups.google.com/a/lbl.gov/d/msgid/singularity/0ef0f32e-0db6-44e2-a98f-cea211da06ba%40lbl.gov?utm_medium=email&utm_source=footer>
>> .
>>
>
>
> -- 
> Vanessa Villamia Sochat
> Stanford University '16
> (603) 321-0676
>

------=_Part_1050_1810584487.1559355562348
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Thanks!=C2=A0 We don&#39;t have slurm, but I got it workin=
g with PBS and qsub.=C2=A0 This is very cool.<div>-Dianne<br><br>On Friday,=
 May 31, 2019 at 4:04:15 PM UTC-7, vanessa wrote:<blockquote class=3D"gmail=
_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left: 1px #ccc solid;p=
adding-left: 1ex;"><div dir=3D"ltr">Yep, looks like it was killed because i=
t took too much memory to squash. If you are on HPC, make sure you are on a=
n interactive node (and not the login node). If you need to get an interact=
ive node with more memory, you can usually ask for one with srun:<div><br><=
/div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;bo=
rder-left:1px solid rgb(204,204,204);padding-left:1ex">srun --mem 32000 --t=
ime 8:00:00 --pty bash</blockquote><div><br></div><div>You did everything r=
ight, you just need more memory :)=C2=A0</div></div><br><div class=3D"gmail=
_quote"><div dir=3D"ltr">On Fri, May 31, 2019 at 6:50 PM Dianne Patterson &=
lt;<a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"HZ_IF=
V0sBQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;=
return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">di=
...@gmail.com</a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);paddi=
ng-left:1ex"><div dir=3D"ltr">Thanks, before trying the remote option, I tr=
ied building directly on the HPC, with exactly the command you suggested:<b=
r>







<div><span style=3D"color:rgb(29,29,29);font-family:Menlo;font-size:12px">s=
ingularity build fmriprep.sif </span><a style=3D"font-family:Menlo;font-siz=
e:12px">docker://poldracklab/fmriprep</a></div><div><br></div><div>but got =
this message every time:<div>







<p><span style=3D"background-color:rgb(255,255,0)">FATAL: <span>=C2=A0 </sp=
an>While performing build: While running mksquashfs: signal: killed:<span>=
=C2=A0</span></span></p><p><br></p><span style=3D"color:rgb(29,29,29);font-=
family:arial,sans-serif;font-size:12px">I could build lolcow directly on th=
e HPC, but neither fmriprep not bids/mrtrix3_connectome (both got almost al=
l the way through and then failed with the highlighted message above).=C2=
=A0</span></div><div><span style=3D"color:rgb(29,29,29);font-family:arial,s=
ans-serif;font-size:12px"><br></span></div><div><span style=3D"color:rgb(29=
,29,29);font-family:arial,sans-serif;font-size:12px">I have built successfu=
lly on our linux box at home and transferred the resulting sif files...but =
I&#39;d LOVE to be able to build directly on the HPC.</span></div><div><fon=
t color=3D"#1d1d1d" face=3D"arial, sans-serif"><span style=3D"font-size:12p=
x"><br></span></font></div><div><font color=3D"#1d1d1d" face=3D"arial, sans=
-serif"><span style=3D"font-size:12px">Thanks for your quick reply. I&#39;l=
l try to move the cache and see if it&#39;ll run.</span></font></div><div><=
font color=3D"#1d1d1d" face=3D"arial, sans-serif"><span style=3D"font-size:=
12px"><br></span></font></div><div><font color=3D"#1d1d1d" face=3D"arial, s=
ans-serif"><span style=3D"font-size:12px">-Dianne</span></font></div><div><=
font color=3D"#1d1d1d" face=3D"arial, sans-serif"><span style=3D"font-size:=
12px"><br></span></font>On Friday, May 31, 2019 at 2:08:53 PM UTC-7, Dianne=
 Patterson wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px =
0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=
=3D"ltr"><p style=3D"font-stretch:normal;font-size:14px;line-height:normal;=
font-family:&quot;Avenir Next&quot;;color:rgb(29,29,29)">Dear experts,</p><=
p style=3D"font-stretch:normal;font-size:14px;line-height:normal;font-famil=
y:&quot;Avenir Next&quot;;color:rgb(29,29,29)">From our HPC, I tried this:=
=C2=A0<br></p><p style=3D"font-stretch:normal;font-size:14px;line-height:no=
rmal;font-family:&quot;Avenir Next&quot;;color:rgb(29,29,29)"><span></span>=
</p><pre style=3D"white-space:pre-wrap;color:rgb(0,0,0)">singularity build =
--remote fmriprep.sif docker://poldracklab/fmriprep</pre><pre style=3D"whit=
e-space:pre-wrap;color:rgb(0,0,0)"><font face=3D"arial, sans-serif">However=
, it eventually timed out and thus failed: </font></pre><p style=3D"font-st=
retch:normal;font-size:14px;line-height:normal;font-family:&quot;Avenir Nex=
t&quot;;color:rgb(29,29,29)"><span>5cf17cfd46f1aa23bc9fcd5b build exceeded =
max build time FATAL:=C2=A0<span>=C2=A0=C2=A0</span>While performing build:=
 build has not completed<span>=C2=A0</span></span></p><p style=3D"font-stre=
tch:normal;font-size:14px;line-height:normal;font-family:&quot;Avenir Next&=
quot;;color:rgb(29,29,29)"><span>Can you tell me whether the problem is on =
the HPC side or the remote builder at Sylabs side?</span></p><p style=3D"fo=
nt-stretch:normal;font-size:14px;line-height:normal;font-family:&quot;Aveni=
r Next&quot;;color:rgb(29,29,29)"><span>Thanks so much,</span></p><p style=
=3D"font-stretch:normal;font-size:14px;line-height:normal;font-family:&quot=
;Avenir Next&quot;;color:rgb(29,29,29)"><span><br></span></p><p style=3D"fo=
nt-stretch:normal;font-size:14px;line-height:normal;font-family:&quot;Aveni=
r Next&quot;;color:rgb(29,29,29)"><span>Dianne</span></p></div></blockquote=
></div></div></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
HZ_IFV0sBQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singu...@lbl.<wbr>gov</a>.<br>
To view this discussion on the web visit <a href=3D"https://groups.google.c=
om/a/lbl.gov/d/msgid/singularity/0ef0f32e-0db6-44e2-a98f-cea211da06ba%40lbl=
.gov?utm_medium=3Demail&amp;utm_source=3Dfooter" target=3D"_blank" rel=3D"n=
ofollow" onmousedown=3D"this.href=3D&#39;https://groups.google.com/a/lbl.go=
v/d/msgid/singularity/0ef0f32e-0db6-44e2-a98f-cea211da06ba%40lbl.gov?utm_me=
dium\x3demail\x26utm_source\x3dfooter&#39;;return true;" onclick=3D"this.hr=
ef=3D&#39;https://groups.google.com/a/lbl.gov/d/msgid/singularity/0ef0f32e-=
0db6-44e2-a98f-cea211da06ba%40lbl.gov?utm_medium\x3demail\x26utm_source\x3d=
footer&#39;;return true;">https://groups.google.com/a/<wbr>lbl.gov/d/msgid/=
singularity/<wbr>0ef0f32e-0db6-44e2-a98f-<wbr>cea211da06ba%40lbl.gov</a>.<b=
r>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
>Vanessa Villamia Sochat<br>Stanford University &#39;16<br><div><div><div>(=
603) 321-0676</div></div></div></div>
</blockquote></div></div>
------=_Part_1050_1810584487.1559355562348--

------=_Part_1049_675250097.1559355562347--
