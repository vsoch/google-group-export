X-Received: by 2002:a62:e08d:: with SMTP id d13mr2721491pfm.23.1551467819794;
        Fri, 01 Mar 2019 11:16:59 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:a514:: with SMTP id s20ls4801382plq.0.gmail; Fri, 01
 Mar 2019 11:16:58 -0800 (PST)
X-Received: by 2002:a17:902:4d46:: with SMTP id o6mr6812644plh.302.1551467818645;
        Fri, 01 Mar 2019 11:16:58 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1551467818; cv=none;
        d=google.com; s=arc-20160816;
        b=m7DrFmWRQXIK2Pz/Z2afXp+6QoAklPpkBYyrYZaqFX1EtUP/aRduLJ7H0goZwKe++v
         W7Bc1VZvaIA7tdX6cLSWZr74jFusXLq5DKD22sYBq+fEtNuwOKy69zT3UnYSYBYr40O9
         F+blquxFB2jy/Uo4vK0XcodTOGlJ4L5ERm7acrFvdKgWY5jjYfwvp+gbauZs9pCAjbZR
         wUen4osevEBtyLSrXQOuE+ffC/rCodUlfPPtB5GMSUYSMC0jHrIgYlne0oWChnPAiMZK
         Le7TJf+UfPazNTvldHJSCSqI7o2MA3pxIMgXUNqg8cikhMULHRke+auyuHxmwyrCKTNA
         teWg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature;
        bh=T3F4KapBfrE3JgE0Ggr4y0DT7A0Pfq7jSnQ/SUfH2jQ=;
        b=SKiEOcQTP9v/sliMLXr7wj71Ua4lrbxqWVNeE3xDylwfcayHhXnnauJvN5wysUD7Jd
         cU0OIdohS88OORxIRIVfhI/fje3zzhL2PDkWq16bvcTYK7QHmCmIILP+6wLrK07yyksv
         BMJJOgdKAF0CcpjmtcRXEzsn/kgJmJ+ZN4R3KLvQdaf3tL9c3yt0tatNmy76aGlGF+NM
         PUC1aVZKL2j7iatVk/dzYwFebAD8Y48R/I5lVlQAZ0YHomrwOXDMBuA0+RrM64k4BjHv
         RFzDUz6vghJXhJrqK/4dbcuQyUn3rSrmGJ7fyOKMJXa7oipKkzDh6yZgcAJil3L2MUFd
         4VZg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=njIx1Cse;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.166.169 as permitted sender) smtp.mailfrom=vso...@gmail.com
Return-Path: <vso...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [131.243.228.53])
        by mx.google.com with ESMTP id d14si20769612pgn.536.2019.03.01.11.16.58
        for <singu...@lbl.gov>;
        Fri, 01 Mar 2019 11:16:58 -0800 (PST)
Received-SPF: pass (google.com: domain of vso...@gmail.com designates 209.85.166.169 as permitted sender) client-ip=209.85.166.169;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=njIx1Cse;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.166.169 as permitted sender) smtp.mailfrom=vso...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2F2BADsg3lcf6mmVdFhA4IXgVYFgRJQM?=
 =?us-ascii?q?yeEAQdSS4Jej2mCDZIuhXIUgSs8JQEKC4FJgnUChCIiNAkNAQMBAQMBAwIBAQI?=
 =?us-ascii?q?QAQEJCwsIJyUMgjoFAgMfBwoETTsDAQEBAQEBAQEBJAEBAQEBAQEBAQEBAQEBA?=
 =?us-ascii?q?RoCDSBDAQEBAwEjBBkBDQ4eAwELBgULDSoCAiEBAQ4DAQUBCxEOBwQBGgIEgwI?=
 =?us-ascii?q?mAYE1AQMNCAUKn3M8iV0agSZ8FgUBF4J5BYExAYMLChknDV+BNwIBBRKMOReBf?=
 =?us-ascii?q?4N1LoJXRwICgSwBEgEJAjUMGoJDgjUiAooNMlSGUpFiMwmHQ4N1g3qDPRmTIY8?=
 =?us-ascii?q?rTUKBLosvMIEmbjBxcBVsgjsJgh8XE4M4inEkMAEPjW4OFzCBdwEB?=
X-IronPort-AV: E=Sophos;i="5.58,428,1544515200"; 
   d="scan'208,217";a="53351103"
Received: from mail-it1-f169.google.com ([209.85.166.169])
  by fe4.lbl.gov with ESMTP; 01 Mar 2019 11:16:50 -0800
Received: by mail-it1-f169.google.com with SMTP id d125so11098669ith.1
        for <singu...@lbl.gov>; Fri, 01 Mar 2019 11:16:50 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=T3F4KapBfrE3JgE0Ggr4y0DT7A0Pfq7jSnQ/SUfH2jQ=;
        b=njIx1CseFDZD44FzC25MawbDlL13tn49NPPaWnASpFSTyQ5ZkUCIuXAer4y4HUAWCV
         sg5TQ3jWf8eDzVT6qn17kSVJVQpYfVMclzhwFpvqtwPuV+u1AlfQ+RuVnwIZUINS4Txm
         mEhR0NvYRmSWDslpanEkdOPZEeXEDFmhxFVmNanx5Ip95FZ0iOvVUsaNCe5ZsYvkj7Ad
         awFKUpW9jYFXhG0Mqx2gg2kwsTTUSh3sVTqicf0smebvneip+Y8EsFKsnbi+AzZRNU3x
         QnaT4MKqScpv32vHi5iyyvir7y2gGsywo6vODPQeYTh0ZYXIVGdlsK4wNAxiexGKofTF
         tfEw==
X-Gm-Message-State: APjAAAUUAcUnDSqt7uuxapOne2Gl0mSuKMiCKiwfEVicD4jSP9eihgd1
	uYzPbAA4CpPhknjr3+E0DG4CaAHHKWmdk1tJm6trAZPG
X-Received: by 2002:a02:b4b8:: with SMTP id k53mr3541496jaj.56.1551467809696;
 Fri, 01 Mar 2019 11:16:49 -0800 (PST)
MIME-Version: 1.0
References: <a94c4768-8cd7-43aa-8bb9-b21b3ccf950c@lbl.gov> <CAApQTTh9E=oNwfTLFmjmH6rAXDH2Q+X29D0Ur_Y3VMXvGvsDtg@mail.gmail.com>
 <710b4b5e-f209-41da-bfcb-c19043201fa4@lbl.gov> <CAM=pu++geyfoQ+HWGtWVOStYfuH3+b49VLSk6zwQS0pH7V_WCA@mail.gmail.com>
 <54a4fb06-9c3c-46f0-bcf8-7d788fb7d381@lbl.gov> <CAM=pu+K3LpHL-MbQ==RMumDX7_gADoui=MBmRmTmeGc7JxPn_w@mail.gmail.com>
In-Reply-To: <CAM=pu+K3LpHL-MbQ==RMumDX7_gADoui=MBmRmTmeGc7JxPn_w@mail.gmail.com>
From: v <vso...@gmail.com>
Date: Fri, 1 Mar 2019 14:16:38 -0500
Message-ID: <CAM=pu+LOtSnm-8bTjjowXKxucty50y6o+jBPyFAyF58pfgkWiQ@mail.gmail.com>
Subject: Re: [Singularity] Singularity image download syncronization during
 multiple job startup
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000ad708d05830d3fbc"

--000000000000ad708d05830d3fbc
Content-Type: text/plain; charset="UTF-8"

Kind of random, but this looks pretty cool, if you want some kind of
distributed storage :)

https://github.com/Alluxio/alluxio

On Fri, Mar 1, 2019 at 2:06 PM v <vso...@gmail.com> wrote:

> HPC and cloud are very different use cases, and that seems to be the edge
> we are hitting here!
>
>>
>>   There are several issues with a Job dependencies approach.  The first
>> is that the job to download and cache the image would have to be run on all
>> nodes. We don't explicitly assign jobs to a select group of hosts. It's a
>> batch processing environment.  Every CPU compute node can run every CPU
>> compute job, and the same is true for our GPU nodes. There is no guarantee
>> that the node that runs the download job will take the tasks.
>>
>> This would be rationale for a shared filesystem, which is fairly common
> in HPC
>
>
>>   From the admin side, we are trying to slide in containerization between
>> the existing system image and the applications without requiring a massive
>> change to the user workflows.  A lot of our users are researchers who just
>> run a tool provided to them from the tool builders. They are not cluster
>> aware application developers. We are working with our toolbuilders on this
>> project, but their focus is on new work, not trying to rework old
>> workflows. So we have to make this as seamless as possible to the existing
>> job submission workflows. Introducing job dependencies will likely break
>> these workflows.
>>
>> Singularity was (first) optimized for this use case (HPC), so a shared
> cache would be a good solution. The only other alternative would be through
> educating your users to pull to a file first, and then submit the file to
> the jobs. You don't need to use dependencies exactly, but the simple
> example that @gmk gave pulling first You are saying this isn't an option?
>
>
>>   Our prolog wrapper is how we are going to force the workflows into
>> containerization. We have two "default" container images (one for CPU jobs,
>> one for GPU jobs) that are almost 100% identical to our current images.
>> That 9 GB container image I mentioned in our collection is one of those
>> default. Our toolbuilders have pushed back against the slow download times
>> already. The efforts to enable caching, object storage backends, etc. were
>> attempts to reduce the download times.
>>
>> A shared filesystem cache, so you could download once, would be a typical
> HPC use case (sharing binaries between jobs) and help this, no? I see that
> you opened a PR to do that here:
> https://github.com/sylabs/singularity/pull/2776
>
>
>> I had hoped that by enabling shub caching in Singularity that it would
>> help, and it does so long as we don't run into this corner case. But
>> knowing our workflows, users, and operational procedures, we are going to
>> hit it regularly.  Thats why I am trying to figure out any alternatives
>> short of putting our containers into a shared directory. The shared
>> directory model would work for today.  But we have to move to the cloud and
>> having a running shared directory that is not object based is costly.
>>
>> If you need to run a file, and the file is in some external server, it
> seems logical that you either need to download a gazillion copies of the
> same thing (not ideal) or share a few copies via shared systems (more
> ideal).  The cloud is a different use case because it won't be too costly
> on the instance to download, but it would be a burden on the registry. This
> is good rationale for having a service that can handle that kind of
> concurrency - whether it means deploying multiple front ends to your own
> storage, or using something like Google Storage that can handle it. What
> could be other solutions? You could set up something complicated with
> Globus and then have a shared folder "somewhere else" but that just makes
> it more complicated. You could provide a wrapper for users that enforces a
> pull first, but it sounds like you don't want to do that.
>
>
>> On Thursday, February 28, 2019 at 12:37:43 PM UTC-5, vanessa wrote:
>>>
>>> What if you used job dependencies, with a job do handle a single pull of
>>> all required containers coming before the batch?
>>>
>>> https://slurm.schedmd.com/job_array.html#job-dependencies
>>>
>>> [image: image.png]
>>>
>>> On Thu, Feb 28, 2019 at 8:36 AM Mike Moore <wx...@gmail.com> wrote:
>>>
>>>> Hi Greg,
>>>>
>>>>   So, our current workloads are batch jobs. There is very little MPI at
>>>> all currently. We use Univa Grid Engine for our scheduler. Our users submit
>>>> job arrays with 100->1000s of tasks per job.  Each task is similar, just on
>>>> different data sets. We do have multiple (up to 20) tasks that can start on
>>>> a node almost simultaneously.
>>>>
>>>>   We are deploying singularity images to convert our very large and
>>>> customized bare metal image into a container. This is the first step in
>>>> decoupling our applications from our bare metal installation and start a
>>>> path to computing in the cloud. We went with Singularity because our shared
>>>> environment makes docker a non-starter with our security dept.
>>>>
>>>>   I have been trying to get Sregistry working fast enough to move the
>>>> project out of a POC for functional testing and into production use.
>>>> However, the time to download the image to the system is unacceptably
>>>> slow.  I've patched Singularity 3.1.0 to address the missing Shub local
>>>> caching support, which makes everything better if the image is locally
>>>> cached.  If the image is not cached and begins to download by the first
>>>> task on a node, any additional tasks that are released to the node will
>>>> fail when starting singularity because the image is incomplete. I have test
>>>> cases (up to 500 tasks) with only a handful of tasks successfully
>>>> completing because the first tasks are downloading the image and the
>>>> additional tasks die because the download is in progress.  Our image is on
>>>> the order of 1.5-9.0GB depending on the software included and the function
>>>> of the image. So, time to download the image is significant.
>>>>
>>>>   So, the way we have essentially been running our jobs are:
>>>>
>>>> qsub -q all.q -t1-500 singularity run shub://sregistry/image
>>>> <application>
>>>>
>>>> Once the scheduler releases the job to the compute node, it starts
>>>> singularity which downloads image from sregistry. We have written a wrapper
>>>> that we will be using to force all jobs to run in singularity once our
>>>> users have completed acceptance testing. But the issue of singularity
>>>> crashing because the cached image is incomplete is a blocker.
>>>>
>>>> I was trying to get an idea of what others were doing for storing and
>>>> starting their containers in their HPC environments and if they have a
>>>> similar issue with trying to start multiple instances of a container on the
>>>> same system at the same time if the container is not cached yet.
>>>>
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>> --
>>> Vanessa Villamia Sochat
>>> Stanford University '16
>>> (603) 321-0676
>>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
>>
>
>
> --
> Vanessa Villamia Sochat
> Stanford University '16
> (603) 321-0676
>


-- 
Vanessa Villamia Sochat
Stanford University '16
(603) 321-0676

--000000000000ad708d05830d3fbc
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div dir=3D"ltr">Kind of random, but this looks pretty coo=
l, if you want some kind of distributed storage :)<div><br></div><div><a hr=
ef=3D"https://github.com/Alluxio/alluxio">https://github.com/Alluxio/alluxi=
o</a><br></div></div></div><br><div class=3D"gmail_quote"><div dir=3D"ltr" =
class=3D"gmail_attr">On Fri, Mar 1, 2019 at 2:06 PM v &lt;<a href=3D"mailto=
:vso...@gmail.com">vso...@gmail.com</a>&gt; wrote:<br></div><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid =
rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div dir=3D"ltr"><div d=
ir=3D"ltr">HPC and cloud are very different use cases, and that seems to be=
 the edge we are hitting here!</div></div><div class=3D"gmail_quote"><block=
quote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1=
px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><br>=C2=A0 The=
re are several issues with a Job dependencies approach.=C2=A0 The first is =
that the job to download and cache the image would have to be run on all no=
des. We don&#39;t explicitly assign jobs to a select group of hosts. It&#39=
;s a batch processing environment.=C2=A0 Every CPU compute node can run eve=
ry CPU compute job, and the same is true for our GPU nodes. There is no gua=
rantee that the node that runs the download job will take the tasks. <br><b=
r></div></blockquote><div>This would be rationale for a shared filesystem, =
which is fairly common in HPC</div><div>=C2=A0</div><blockquote class=3D"gm=
ail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,=
204,204);padding-left:1ex"><div dir=3D"ltr">=C2=A0 From the admin side, we =
are trying to slide in containerization between the existing system image a=
nd the applications without requiring a massive change to the user workflow=
s.=C2=A0 A lot of our users are researchers who just run a tool provided to=
 them from the tool builders. They are not cluster aware application develo=
pers. We are working with our toolbuilders on this project, but their focus=
 is on new work, not trying to rework old workflows. So we have to make thi=
s as seamless as possible to the existing job submission workflows. Introdu=
cing job dependencies will likely break these workflows. <br><br></div></bl=
ockquote><div>Singularity was (first) optimized for this use case (HPC), so=
 a shared cache would be a good solution. The only other alternative would =
be through educating your users to pull to a file first, and then submit th=
e file to the jobs. You don&#39;t need to use dependencies exactly, but the=
 simple example that @gmk gave pulling first You are saying this isn&#39;t =
an option?=C2=A0</div><div>=C2=A0<br></div><blockquote class=3D"gmail_quote=
" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);=
padding-left:1ex"><div dir=3D"ltr">=C2=A0 Our prolog wrapper is how we are =
going to force the workflows into containerization. We have two &quot;defau=
lt&quot; container images (one for CPU jobs, one for GPU jobs) that are alm=
ost 100% identical to our current images. That 9 GB container image I menti=
oned in our collection is one of those default. Our toolbuilders have pushe=
d back against the slow download times already. The efforts to enable cachi=
ng, object storage backends, etc. were attempts to reduce the download time=
s. <br><br></div></blockquote><div>A shared filesystem cache, so you could =
download once, would be a typical HPC use case (sharing binaries between jo=
bs) and help this, no? I see that you opened a PR to do that here: <a href=
=3D"https://github.com/sylabs/singularity/pull/2776" target=3D"_blank">http=
s://github.com/sylabs/singularity/pull/2776</a></div><div>=C2=A0</div><bloc=
kquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:=
1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr">I had hoped t=
hat by enabling shub caching in Singularity that it would help, and it does=
 so long as we don&#39;t run into this corner case. But knowing our workflo=
ws, users, and operational procedures, we are going to hit it regularly.=C2=
=A0 Thats why I am trying to figure out any alternatives short of putting o=
ur containers into a shared directory. The shared directory model would wor=
k for today.=C2=A0 But we have to move to the cloud and having a running sh=
ared directory that is not object based is costly. <br><br></div></blockquo=
te><div>If you need to run a file, and the file is in some external server,=
 it seems logical that you either need to download a gazillion copies of th=
e same thing (not ideal) or share a few copies via shared systems (more ide=
al).=C2=A0 The cloud is a different use case because it won&#39;t be too co=
stly on the instance to download, but it would be a burden on the registry.=
 This is good rationale for having a service that can handle that kind of c=
oncurrency - whether it means deploying multiple front ends to your own sto=
rage, or using something like Google Storage that can handle it. What could=
 be other solutions? You could set up something complicated with Globus and=
 then have a shared folder &quot;somewhere else&quot; but that just makes i=
t more complicated. You could provide a wrapper for users that enforces a p=
ull first, but it sounds like you don&#39;t want to do that.=C2=A0</div><di=
v><br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0=
.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"l=
tr"><br>On Thursday, February 28, 2019 at 12:37:43 PM UTC-5, vanessa wrote:=
<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-=
left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div dir=
=3D"ltr">What if you used job dependencies, with a job do handle a single p=
ull of all required containers coming before the batch?<div><br></div><div>=
<a href=3D"https://slurm.schedmd.com/job_array.html#job-dependencies" rel=
=3D"nofollow" target=3D"_blank">https://slurm.schedmd.com/job_array.html#jo=
b-dependencies</a><br><div><br></div><div><div><img alt=3D"image.png" style=
=3D"margin-right: 0px;" width=3D"701" height=3D"495"><br></div></div></div>=
</div></div><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Thu, Feb 28,=
 2019 at 8:36 AM Mike Moore &lt;<a rel=3D"nofollow">wx...@gmail.com</a>&gt;=
 wrote:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px =
0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=
=3D"ltr">Hi Greg,<br><br>=C2=A0 So, our current workloads are batch jobs. T=
here is very little MPI at all currently. We use Univa Grid Engine for our =
scheduler. Our users submit job arrays with 100-&gt;1000s of tasks per job.=
=C2=A0 Each task is similar, just on different data sets. We do have multip=
le (up to 20) tasks that can start on a node almost simultaneously. <br><br=
>=C2=A0 We are deploying singularity images to convert our very large and c=
ustomized bare metal image into a container. This is the first step in deco=
upling our applications from our bare metal installation and start a path t=
o computing in the cloud. We went with Singularity because our shared envir=
onment makes docker a non-starter with our security dept. <br><br>=C2=A0 I =
have been trying to get Sregistry working fast enough to move the project o=
ut of a POC for functional testing and into production use. However, the ti=
me to download the image to the system is unacceptably slow.=C2=A0 I&#39;ve=
 patched Singularity 3.1.0 to address the missing Shub local caching suppor=
t, which makes everything better if the image is locally cached.=C2=A0 If t=
he image is not cached and begins to download by the first task on a node, =
any additional tasks that are released to the node will fail when starting =
singularity because the image is incomplete. I have test cases (up to 500 t=
asks) with only a handful of tasks successfully completing because the firs=
t tasks are downloading the image and the additional tasks die because the =
download is in progress.=C2=A0 Our image is on the order of 1.5-9.0GB depen=
ding on the software included and the function of the image. So, time to do=
wnload the image is significant. <br><br>=C2=A0 So, the way we have essenti=
ally been running our jobs are:<br><br>qsub -q all.q -t1-500 singularity ru=
n shub://sregistry/image &lt;application&gt;<br><br>Once the scheduler rele=
ases the job to the compute node, it starts singularity which downloads ima=
ge from sregistry. We have written a wrapper that we will be using to force=
 all jobs to run in singularity once our users have completed acceptance te=
sting. But the issue of singularity crashing because the cached image is in=
complete is a blocker.<br><br>I was trying to get an idea of what others we=
re doing for storing and starting their containers in their HPC environment=
s and if they have a similar issue with trying to start multiple instances =
of a container on the same system at the same time if the container is not =
cached yet.<br><br><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
>Vanessa Villamia Sochat<br>Stanford University &#39;16<br><div><div><div>(=
603) 321-0676</div></div></div></div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"gmail-m_2799747164697234968gmail_signature">Vanessa Villamia Soch=
at<br>Stanford University &#39;16<br><div><div><div>(603) 321-0676</div></d=
iv></div></div></div>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"gmail_signature">Vanessa Villamia Sochat<br>Stanford University &=
#39;16<br><div><div><div>(603) 321-0676</div></div></div></div>

--000000000000ad708d05830d3fbc--
