X-Received: by 10.129.106.5 with SMTP id f5mr639178ywc.84.1494351804715;
        Tue, 09 May 2017 10:43:24 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.12.200 with SMTP id 69ls349150iom.30.gmail; Tue, 09 May
 2017 10:43:24 -0700 (PDT)
X-Received: by 10.99.112.66 with SMTP id a2mr1442554pgn.7.1494351803803;
        Tue, 09 May 2017 10:43:23 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1494351803; cv=none;
        d=google.com; s=arc-20160816;
        b=frn2oEYC1MLMnZIOz5CPbWY0sg8ahozvjFjaBLpJnrX21uEAobh5l7k1wbEyOuaScm
         q8VVyT2abhgHtiv9YWeaA9AlMnmePRuqtkROUjJTHWqXh3PPsAPAxF2SCwdnRXuqBJs6
         iJOS7nyui+iXy6IaWjD24T0FDTuZpnejNYKeC4JIKg26o0VX7CqnlvHHoS3cOrhuPDoq
         iG6agq0O3Bjjc9oIeVtIKkhpzWUM0nK6J+jBFDl+hBj5UOynrvJFj8fP/WaRjG9hBPPX
         gjN+qZ8gdcZDHqf98b+K8IiVHRspTM4+DP5O01ITq60SW7ygjp9irHuwW31LRgw727ZA
         HB2w==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=4GU/gqKHNSj+F6wpHkzF68lyMela3/xdm/UWzrIZg3s=;
        b=ikUVJOl4apR2bEr4dyD10Y3N6zqEwE/t8BrHwksXiN41pysoKEwMfO0tqkJsTifUwo
         HU46dzUu8QyKwfG0YknGeGucmIuOzv5K2G3OQF9ccCWIzAefcz3IFVb617VCvKc881b0
         A9lLWjsQqlhfxYsCOoTRId5cdAHneLIx+9XH2zi6SmTfILHUY5AjinygquWgLUdVq0z4
         DTgUY7Smyo7EdgE+8uXH+Y2NKdGrBwVRMw/z7iDn7RXe6U109e7OaeeA+bZ6Q2Tvas4V
         0ZhraWJJN3Ztk4vGLvLJ1T2qMgYtfSTN8K19foJBZwQ/FW1ErIc40HYBVKkMdP9/P+mh
         sKGg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of jerrin...@gmail.com designates 209.85.218.42 as permitted sender) smtp.mailfrom=jerrin...@gmail.com
Return-Path: <jerrin...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id w9si553462pls.254.2017.05.09.10.43.23
        for <singu...@lbl.gov>;
        Tue, 09 May 2017 10:43:23 -0700 (PDT)
Received-SPF: pass (google.com: domain of jerrin...@gmail.com designates 209.85.218.42 as permitted sender) client-ip=209.85.218.42;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of jerrin...@gmail.com designates 209.85.218.42 as permitted sender) smtp.mailfrom=jerrin...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EEAQCU/hFZfyraVdFcHAEBBAEBCgEBF?=
 =?us-ascii?q?wEBBAEBCgEBgwGBC4EMB4NaCIoYkVaVcoFMQyEBCoV4AoRmBz8YAQEBAQEBAQE?=
 =?us-ascii?q?BAQECEAEBCQsLCCYxgjMEAgMXBwQERiEILwEBAQEBAQEBAQEBAQEBAQEZAisMG?=
 =?us-ascii?q?QEBGAEBAQECARoJHQEIBQ4eAwELBgULDSAKAgIhAQEOAwEFAQsRDgcEARwEiDS?=
 =?us-ascii?q?BMwEDDQgFCaYqP4wIggQFARyDCgWDUgoZJw1WgmIBAQgBAQEBAQEaAgYShk2Ee?=
 =?us-ascii?q?YJEEIFgEgEGgyCCXwWKMow7hl07ik+Dd4RTkWuLKieHJxQfgRUfeA0zCy8gIXS?=
 =?us-ascii?q?ETIIqJDYHhj9HgWcBAQE?=
X-IronPort-AV: E=Sophos;i="5.38,315,1491289200"; 
   d="scan'208,217";a="73591780"
Received: from mail-oi0-f42.google.com ([209.85.218.42])
  by fe4.lbl.gov with ESMTP; 09 May 2017 10:43:21 -0700
Received: by mail-oi0-f42.google.com with SMTP id b204so8659508oii.1
        for <singu...@lbl.gov>; Tue, 09 May 2017 10:43:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=4GU/gqKHNSj+F6wpHkzF68lyMela3/xdm/UWzrIZg3s=;
        b=ZJiEBn0JFe1HysWa7kgkFSUH3eD8+hxT9ov8LiQ0TpDUO4eJd5PhnjuF23SowOBX82
         TRHVeAaNCZUQcfMG5NI3JcDHNjdMyTafDfCGrtbra+MTsFkW1xkjO1qwaETwq4i2cN1P
         HCOd6cISizY1eK9kYPQ7KlE1Thvqab5OuDIxXxvNPRRi2gM4hMbnR5bWsffgL2RV68n4
         6kimFqYj/gJaRNaozcOnAhemy7/oFb1q3KbphCe384SOVUdpRcPu+ubSy0CpXQgWM1VD
         oBaTOe2+ieeLhhJaOhjLmzOeQGHc941MErIPPtYpGjK833kUrl/DYdl/VoZOSEWYqXiG
         ZfDw==
X-Gm-Message-State: AODbwcBYwvuxMoTsdkYG9+ea858JyoVCQECc0pC7tw+85T3ly5KS9OAu
	skn6Ek6uXXYbXYmFuSTthNhpVmaHJEcg
X-Received: by 10.202.214.6 with SMTP id n6mr550480oig.190.1494351800853; Tue,
 09 May 2017 10:43:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.157.2.40 with HTTP; Tue, 9 May 2017 10:43:00 -0700 (PDT)
In-Reply-To: <CAApQTTiczBgFqvCQCiXbnJj6=N28A6EfujCc=G4HXBfFTf+f4A@mail.gmail.com>
References: <a0e56124-ca94-41e3-b45e-95bb975f3a5b@lbl.gov> <CAApQTTivqe1aPi+WKiojxmU=9KzKRsWc_MHgnxB8_Ld8nkk+4Q@mail.gmail.com>
 <2584fc67-fbbd-444a-9a28-e95be7c10e32@lbl.gov> <CAApQTTj2E4ax8YwJHr78VtPi86Evo0d8T7L_6t2VBMfmN3eegw@mail.gmail.com>
 <3259b1a2-9dc7-4a02-8922-235390a3f907@lbl.gov> <CAApQTTjJfSQmZpJ=H=jNpKV-CJ9dVtuYCf-Pwc9vfcrg9md7CQ@mail.gmail.com>
 <CA+KhMPuEDbs3KhwsnWbcuXOa81fa9ycmRuXVt8iHchFsXULw8g@mail.gmail.com> <CAApQTTiczBgFqvCQCiXbnJj6=N28A6EfujCc=G4HXBfFTf+f4A@mail.gmail.com>
From: Jerrin Suresh <jerrin...@gmail.com>
Date: Tue, 9 May 2017 13:43:00 -0400
Message-ID: <CA+KhMPvv0tqeJHCV4CeSXhoRjs6et+a1t5D5OZ9AAqts2KWiMA@mail.gmail.com>
Subject: Re: [Singularity] Issue with MPI and Singularity
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a113adb12426860054f1ae480

--001a113adb12426860054f1ae480
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Gregory,

I had requested for 2 nodes with 16 processors per node. So, I guess 4 of
them should run on the second node. But thanks for pointing that out, I
just checked it once again and its working fine now.

~~~
:~>qsub -I -l walltime=3D00:30:00 -l nodes=3D2:ppn=3D16 -q debug_gpu
:~> ccmrun mpirun -np 32 --hostfile hosts singularity exec mpi.img
/usr/bin/ring
Hello world!  I am process number: 2 on host nid00170
Hello world!  I am process number: 3 on host nid00170
Hello world!  I am process number: 4 on host nid00170
Hello world!  I am process number: 5 on host nid00170
Hello world!  I am process number: 6 on host nid00170
Hello world!  I am process number: 7 on host nid00170
Hello world!  I am process number: 8 on host nid00170
Hello world!  I am process number: 9 on host nid00170
Hello world!  I am process number: 10 on host nid00170
Hello world!  I am process number: 11 on host nid00170
Hello world!  I am process number: 12 on host nid00170
Hello world!  I am process number: 13 on host nid00170
Hello world!  I am process number: 14 on host nid00170
Hello world!  I am process number: 15 on host nid00170
Hello world!  I am process number: 0 on host nid00170
Hello world!  I am process number: 1 on host nid00170
Hello world!  I am process number: 16 on host nid00171
Hello world!  I am process number: 17 on host nid00171
Hello world!  I am process number: 19 on host nid00171
Hello world!  I am process number: 30 on host nid00171
Hello world!  I am process number: 31 on host nid00171
Hello world!  I am process number: 20 on host nid00171
Hello world!  I am process number: 21 on host nid00171
Hello world!  I am process number: 25 on host nid00171
Hello world!  I am process number: 27 on host nid00171
Hello world!  I am process number: 28 on host nid00171
Hello world!  I am process number: 29 on host nid00171
Hello world!  I am process number: 18 on host nid00171
Hello world!  I am process number: 26 on host nid00171
Hello world!  I am process number: 24 on host nid00171
Hello world!  I am process number: 22 on host nid00171
Hello world!  I am process number: 23 on host nid00171
~~~

Thank you very much for all the help. I am still not sure what went wrong
the first time. And one quick question, if the openmpi version in the
container, as well as the host, are different would this still work? As
many people would have a different version of openmpi in their container
and would assume that it could be moved to any server and still work.

Cheers,
Jerrin

On Tue, May 9, 2017 at 1:22 PM, Gregory M. Kurtzer <gmku...@gmail.com>
wrote:

> Hi Jerrin,
>
> How many processors/cores do you have per node? If it is less then 20 (as
> you showed in your Singularity example), then it would still fit on a
> single node.
>
> Thanks!
>
> On Tue, May 9, 2017 at 10:18 AM, Jerrin Suresh <jerrin...@gmail.com>
> wrote:
>
>> Oops, sorry about that. Nope, we just have the hostname of the compute
>> nodes in the hosts file.
>>
>> ~~~~
>> :~> cat hosts
>> nid00900
>> nid00901
>> ~~~~
>>
>> In addition, the MPI setup in the server is working fine. However, it's
>> when using singularity that we are facing the issue.
>>
>> ~~~~
>> aprun1:~> ccmrun mpirun -np 20 --hostfile hosts singularity exec mpi.img
>> /usr/bin/ring
>> =E2=80=9Copenmpi=E2=80=9D version =E2=80=9C1.8.4=E2=80=9D loaded.
>> Hello world!  I am process number: 8 on host nid00900
>> Hello world!  I am process number: 9 on host nid00900
>> Hello world!  I am process number: 10 on host nid00900
>> Hello world!  I am process number: 11 on host nid00900
>> Hello world!  I am process number: 12 on host nid00900
>> Hello world!  I am process number: 13 on host nid00900
>> Hello world!  I am process number: 14 on host nid00900
>> Hello world!  I am process number: 15 on host nid00900
>> Hello world!  I am process number: 16 on host nid00900
>> Hello world!  I am process number: 17 on host nid00900
>> Hello world!  I am process number: 18 on host nid00900
>> Hello world!  I am process number: 19 on host nid00900
>> Hello world!  I am process number: 0 on host nid00900
>> Hello world!  I am process number: 1 on host nid00900
>> Hello world!  I am process number: 2 on host nid00900
>> Hello world!  I am process number: 3 on host nid00900
>> Hello world!  I am process number: 4 on host nid00900
>> Hello world!  I am process number: 6 on host nid00900
>> Hello world!  I am process number: 7 on host nid00900
>> Hello world!  I am process number: 5 on host nid00900
>> Application 8102212 resources: utime ~2s, stime ~3s, Rss ~5672, inblocks
>> ~62653, outblocks ~1812
>> ~~~~
>>
>> Without singularity,
>>
>> ~~~~
>>
>> :~> ccmrun mpirun -n 64 --hostfile hosts ./a.out
>>
>> Hello world!  I am process number: 0 on host nid00922
>> Hello world!  I am process number: 2 on host nid00922
>> Hello world!  I am process number: 4 on host nid00922
>> Hello world!  I am process number: 6 on host nid00922
>> Hello world!  I am process number: 8 on host nid00922
>> Hello world!  I am process number: 10 on host nid00922
>> Hello world!  I am process number: 14 on host nid00922
>> Hello world!  I am process number: 15 on host nid00922
>> Hello world!  I am process number: 16 on host nid00922
>> Hello world!  I am process number: 17 on host nid00922
>> Hello world!  I am process number: 18 on host nid00922
>> Hello world!  I am process number: 19 on host nid00922
>> Hello world!  I am process number: 20 on host nid00922
>> Hello world!  I am process number: 21 on host nid00922
>> Hello world!  I am process number: 22 on host nid00922
>> Hello world!  I am process number: 23 on host nid00922
>> Hello world!  I am process number: 24 on host nid00922
>> Hello world!  I am process number: 25 on host nid00922
>> Hello world!  I am process number: 26 on host nid00922
>> Hello world!  I am process number: 27 on host nid00922
>> Hello world!  I am process number: 28 on host nid00922
>> Hello world!  I am process number: 29 on host nid00922
>> Hello world!  I am process number: 30 on host nid00922
>> Hello world!  I am process number: 31 on host nid00922
>> Hello world!  I am process number: 1 on host nid00922
>> Hello world!  I am process number: 3 on host nid00922
>> Hello world!  I am process number: 5 on host nid00922
>> Hello world!  I am process number: 7 on host nid00922
>> Hello world!  I am process number: 9 on host nid00922
>> Hello world!  I am process number: 11 on host nid00922
>> Hello world!  I am process number: 12 on host nid00922
>> Hello world!  I am process number: 13 on host nid00922
>> Hello world!  I am process number: 32 on host nid00923
>> Hello world!  I am process number: 34 on host nid00923
>> Hello world!  I am process number: 50 on host nid00923
>> Hello world!  I am process number: 51 on host nid00923
>> Hello world!  I am process number: 52 on host nid00923
>> Hello world!  I am process number: 53 on host nid00923
>> Hello world!  I am process number: 54 on host nid00923
>> Hello world!  I am process number: 55 on host nid00923
>> Hello world!  I am process number: 56 on host nid00923
>> Hello world!  I am process number: 57 on host nid00923
>> Hello world!  I am process number: 58 on host nid00923
>> Hello world!  I am process number: 59 on host nid00923
>> Hello world!  I am process number: 60 on host nid00923
>> Hello world!  I am process number: 61 on host nid00923
>> Hello world!  I am process number: 62 on host nid00923
>> Hello world!  I am process number: 63 on host nid00923
>> Hello world!  I am process number: 33 on host nid00923
>> Hello world!  I am process number: 35 on host nid00923
>> Hello world!  I am process number: 36 on host nid00923
>> Hello world!  I am process number: 37 on host nid00923
>> Hello world!  I am process number: 38 on host nid00923
>> Hello world!  I am process number: 39 on host nid00923
>> Hello world!  I am process number: 40 on host nid00923
>> Hello world!  I am process number: 41 on host nid00923
>> Hello world!  I am process number: 42 on host nid00923
>> Hello world!  I am process number: 43 on host nid00923
>> Hello world!  I am process number: 44 on host nid00923
>> Hello world!  I am process number: 45 on host nid00923
>> Hello world!  I am process number: 46 on host nid00923
>> Hello world!  I am process number: 47 on host nid00923
>> Hello world!  I am process number: 48 on host nid00923
>> Hello world!  I am process number: 49 on host nid00923
>>
>> ~~~~
>> ~jerrin
>>
>> On Tue, May 9, 2017 at 1:01 PM, Gregory M. Kurtzer <gmku...@gmail.com>
>> wrote:
>>
>>> Oh, about the hosts file, I mean what is the order of the hosts, is the
>>> local host also present, do you provide the default and max number of
>>> slots, etc..
>>>
>>> Thanks!
>>>
>>> On Tue, May 9, 2017 at 9:56 AM, jerrin <jerrin...@gmail.com> wrote:
>>>
>>>> Okay, l can try with a higher version of OpenMPI on both the container
>>>> as well as host.
>>>>
>>>> :~> file hosts
>>>>
>>>> hosts: ASCII text
>>>>
>>>> On Tuesday, May 9, 2017 at 12:36:49 PM UTC-4, Gregory Kurtzer wrote:
>>>>>
>>>>> Well, I've had issues running Open MPI < 2.x with Singularity (on bot=
h
>>>>> host and container).
>>>>>
>>>>> BTW, I'm just curious, what is the format of the hosts file?
>>>>>
>>>>> On Tue, May 9, 2017 at 9:29 AM, jerrin <jer...@gmail.com> wrote:
>>>>>
>>>>>> I can try that on a different server. But the highest version of
>>>>>> OpenMPI on the HPC system is 1.8.4. Is this something related to old
>>>>>> version of openmpi?
>>>>>>
>>>>>> On Tuesday, May 9, 2017 at 11:27:41 AM UTC-4, Gregory Kurtzer wrote:
>>>>>>>
>>>>>>> Can you re-test with the Open MPI version inside and outside the
>>>>>>> container being something greater then 2.x?
>>>>>>>
>>>>>>> Thanks!
>>>>>>>
>>>>>>> On Tue, May 9, 2017 at 8:14 AM, jerrin <jer...@gmail.com> wrote:
>>>>>>>
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> I am trying to set up MPI with Singularity. I had set up openmpi
>>>>>>>> version 1.8.4 in the container as the host system has the same ope=
nmpi
>>>>>>>> version. However, the container does not understand that there are=
 2
>>>>>>>> compute nodes even after specifying a hosts file. So each time I s=
pawn the
>>>>>>>> MPI processes it just executes on a single node. The contents of t=
he hosts
>>>>>>>> file is nid00900,nid00901.
>>>>>>>>
>>>>>>>> ~~~~~~
>>>>>>>> aprun1:~> ccmrun mpirun -np 20 --hostfile hosts singularity exec
>>>>>>>> mpi.img /usr/bin/ring
>>>>>>>>
>>>>>>>> =E2=80=9Copenmpi=E2=80=9D version =E2=80=9C1.8.4=E2=80=9D loaded.
>>>>>>>> Hello world! I am process number: 8 on host nid00900
>>>>>>>> Hello world! I am process number: 9 on host nid00900
>>>>>>>> Hello world! I am process number: 10 on host nid00900
>>>>>>>> Hello world! I am process number: 11 on host nid00900
>>>>>>>> Hello world! I am process number: 12 on host nid00900
>>>>>>>> Hello world! I am process number: 13 on host nid00900
>>>>>>>> Hello world! I am process number: 14 on host nid00900
>>>>>>>> Hello world! I am process number: 15 on host nid00900
>>>>>>>> Hello world! I am process number: 16 on host nid00900
>>>>>>>> Hello world! I am process number: 17 on host nid00900
>>>>>>>> Hello world! I am process number: 18 on host nid00900
>>>>>>>> Hello world! I am process number: 19 on host nid00900
>>>>>>>> Hello world! I am process number: 0 on host nid00900
>>>>>>>> Hello world! I am process number: 1 on host nid00900
>>>>>>>> Hello world! I am process number: 2 on host nid00900
>>>>>>>> Hello world! I am process number: 3 on host nid00900
>>>>>>>> Hello world! I am process number: 4 on host nid00900
>>>>>>>> Hello world! I am process number: 6 on host nid00900
>>>>>>>> Hello world! I am process number: 7 on host nid00900
>>>>>>>> Hello world! I am process number: 5 on host nid00900
>>>>>>>>
>>>>>>>> Application 8102212 resources: utime ~2s, stime ~3s, Rss ~5672,
>>>>>>>> inblocks ~62653, outblocks ~1812
>>>>>>>> ~~~~~~
>>>>>>>>
>>>>>>>> In addition, the singularity version in the host is 2.2.1
>>>>>>>>
>>>>>>>>
>>>>>>>> Regards,
>>>>>>>> Jerrin
>>>>>>>>
>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> MS CS Fall-16
>> Indiana University
>> www.linkedin.com/in/jerrinsuresh
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
MS CS Fall-16
Indiana University
www.linkedin.com/in/jerrinsuresh

--001a113adb12426860054f1ae480
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Gregory,<div><br></div><div>I had requested for 2 nodes=
 with 16 processors per node. So, I guess 4 of them should run on the secon=
d node. But thanks for pointing that out, I just checked it once again and =
its working fine now.=C2=A0</div><div><br></div><div>~~~</div><font size=3D=
"1">:~&gt;qsub -I -l walltime=3D00:30:00 -l nodes=3D2:ppn=3D16 -q debug_gpu=
<br>:~&gt; ccmrun mpirun -np 32 --hostfile hosts singularity exec mpi.img /=
usr/bin/ring<br>Hello world!=C2=A0 I am process number: 2 on host nid00170<=
br>Hello world!=C2=A0 I am process number: 3 on host nid00170<br>Hello worl=
d!=C2=A0 I am process number: 4 on host nid00170<br>Hello world!=C2=A0 I am=
 process number: 5 on host nid00170<br>Hello world!=C2=A0 I am process numb=
er: 6 on host nid00170<br>Hello world!=C2=A0 I am process number: 7 on host=
 nid00170<br>Hello world!=C2=A0 I am process number: 8 on host nid00170<br>=
Hello world!=C2=A0 I am process number: 9 on host nid00170<br>Hello world!=
=C2=A0 I am process number: 10 on host nid00170<br>Hello world!=C2=A0 I am =
process number: 11 on host nid00170<br>Hello world!=C2=A0 I am process numb=
er: 12 on host nid00170<br>Hello world!=C2=A0 I am process number: 13 on ho=
st nid00170<br>Hello world!=C2=A0 I am process number: 14 on host nid00170<=
br>Hello world!=C2=A0 I am process number: 15 on host nid00170<br>Hello wor=
ld!=C2=A0 I am process number: 0 on host nid00170<br>Hello world!=C2=A0 I a=
m process number: 1 on host nid00170<br>Hello world!=C2=A0 I am process num=
ber: 16 on host nid00171<br>Hello world!=C2=A0 I am process number: 17 on h=
ost nid00171<br>Hello world!=C2=A0 I am process number: 19 on host nid00171=
<br>Hello world!=C2=A0 I am process number: 30 on host nid00171<br>Hello wo=
rld!=C2=A0 I am process number: 31 on host nid00171<br>Hello world!=C2=A0 I=
 am process number: 20 on host nid00171<br>Hello world!=C2=A0 I am process =
number: 21 on host nid00171<br>Hello world!=C2=A0 I am process number: 25 o=
n host nid00171<br>Hello world!=C2=A0 I am process number: 27 on host nid00=
171<br>Hello world!=C2=A0 I am process number: 28 on host nid00171<br>Hello=
 world!=C2=A0 I am process number: 29 on host nid00171<br>Hello world!=C2=
=A0 I am process number: 18 on host nid00171<br>Hello world!=C2=A0 I am pro=
cess number: 26 on host nid00171<br>Hello world!=C2=A0 I am process number:=
 24 on host nid00171<br>Hello world!=C2=A0 I am process number: 22 on host =
nid00171<br>Hello world!=C2=A0 I am process number: 23 on host nid00171</fo=
nt><div>~~~</div><div><br></div><div>Thank you very much for all the help. =
I am still not sure what went wrong the first time. And one quick question,=
 if the openmpi version in the container, as well as the host, are differen=
t would this still work? As many people would have a different version of o=
penmpi=C2=A0in their container and would assume that it could be moved to a=
ny server and still work.</div><div><br></div><div>Cheers,</div><div>Jerrin=
</div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tu=
e, May 9, 2017 at 1:22 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a href=
=3D"mailto:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a>&gt;</=
span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Jerrin,<=
div><br></div><div>How many processors/cores do you have per node? If it is=
 less then 20 (as you showed in your Singularity example), then it would st=
ill fit on a single node.</div><div><br></div><div>Thanks!</div></div><div =
class=3D"HOEnZb"><div class=3D"h5"><div class=3D"gmail_extra"><br><div clas=
s=3D"gmail_quote">On Tue, May 9, 2017 at 10:18 AM, Jerrin Suresh <span dir=
=3D"ltr">&lt;<a href=3D"mailto:jerrin...@gmail.com" target=3D"_blank">jerri=
n...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div=
 dir=3D"ltr">Oops, sorry about that. Nope, we just have the hostname of the=
 compute nodes in the hosts file.<div><br></div><div>~~~~</div>:~&gt; cat h=
osts <br>nid00900<br>nid00901<div>~~~~</div><div><br></div><div>In addition=
, the MPI setup in the server is working fine. However, it&#39;s when using=
 singularity that we are facing the issue.=C2=A0</div><div><div class=3D"m_=
-4242388468169898395h5"><div><br></div><div>~~~~</div><div><div>a<font size=
=3D"1">prun1:~&gt; ccmrun mpirun -np 20 --hostfile hosts singularity exec m=
pi.img /usr/bin/ring</font></div><div><font size=3D"1">=E2=80=9Copenmpi=E2=
=80=9D version =E2=80=9C1.8.4=E2=80=9D loaded.</font></div><div><font size=
=3D"1">Hello world!=C2=A0 I am process number: 8 on host nid00900</font></d=
iv><div><font size=3D"1">Hello world!=C2=A0 I am process number: 9 on host =
nid00900</font></div><div><font size=3D"1">Hello world!=C2=A0 I am process =
number: 10 on host nid00900</font></div><div><font size=3D"1">Hello world!=
=C2=A0 I am process number: 11 on host nid00900</font></div><div><font size=
=3D"1">Hello world!=C2=A0 I am process number: 12 on host nid00900</font></=
div><div><font size=3D"1">Hello world!=C2=A0 I am process number: 13 on hos=
t nid00900</font></div><div><font size=3D"1">Hello world!=C2=A0 I am proces=
s number: 14 on host nid00900</font></div><div><font size=3D"1">Hello world=
!=C2=A0 I am process number: 15 on host nid00900</font></div><div><font siz=
e=3D"1">Hello world!=C2=A0 I am process number: 16 on host nid00900</font><=
/div><div><font size=3D"1">Hello world!=C2=A0 I am process number: 17 on ho=
st nid00900</font></div><div><font size=3D"1">Hello world!=C2=A0 I am proce=
ss number: 18 on host nid00900</font></div><div><font size=3D"1">Hello worl=
d!=C2=A0 I am process number: 19 on host nid00900</font></div><div><font si=
ze=3D"1">Hello world!=C2=A0 I am process number: 0 on host nid00900</font><=
/div><div><font size=3D"1">Hello world!=C2=A0 I am process number: 1 on hos=
t nid00900</font></div><div><font size=3D"1">Hello world!=C2=A0 I am proces=
s number: 2 on host nid00900</font></div><div><font size=3D"1">Hello world!=
=C2=A0 I am process number: 3 on host nid00900</font></div><div><font size=
=3D"1">Hello world!=C2=A0 I am process number: 4 on host nid00900</font></d=
iv><div><font size=3D"1">Hello world!=C2=A0 I am process number: 6 on host =
nid00900</font></div><div><font size=3D"1">Hello world!=C2=A0 I am process =
number: 7 on host nid00900</font></div><div><font size=3D"1">Hello world!=
=C2=A0 I am process number: 5 on host nid00900</font></div><div><font size=
=3D"1">Application 8102212 resources: utime ~2s, stime ~3s, Rss ~5672, inbl=
ocks ~62653, outblocks ~1812</font></div></div><div>~~~~</div><div><br></di=
v></div></div><div>Without singularity,</div><div><br></div><div>~~~~</div>=
<div>







<p class=3D"m_-4242388468169898395m_-4455386444432382273gmail-p1"><font siz=
e=3D"1"><span class=3D"m_-4242388468169898395m_-4455386444432382273gmail-s1=
">:~&gt; ccmrun mpirun </span><span class=3D"m_-4242388468169898395m_-44553=
86444432382273gmail-s2">-n</span><span class=3D"m_-4242388468169898395m_-44=
55386444432382273gmail-s1"> </span><span class=3D"m_-4242388468169898395m_-=
4455386444432382273gmail-s3">64</span><span class=3D"m_-4242388468169898395=
m_-4455386444432382273gmail-s1"> </span><span class=3D"m_-42423884681698983=
95m_-4455386444432382273gmail-s2">--hostfile</span><span class=3D"m_-424238=
8468169898395m_-4455386444432382273gmail-s1"> hosts ./a.out</span></font></=
p></div><div><p class=3D"m_-4242388468169898395m_-4455386444432382273gmail-=
p1"><font size=3D"1">Hello world!=C2=A0 I am process number: 0 on host nid0=
0922<br>Hello world!=C2=A0 I am process number: 2 on host nid00922<br>Hello=
 world!=C2=A0 I am process number: 4 on host nid00922<br>Hello world!=C2=A0=
 I am process number: 6 on host nid00922<br>Hello world!=C2=A0 I am process=
 number: 8 on host nid00922<br>Hello world!=C2=A0 I am process number: 10 o=
n host nid00922<br>Hello world!=C2=A0 I am process number: 14 on host nid00=
922<br>Hello world!=C2=A0 I am process number: 15 on host nid00922<br>Hello=
 world!=C2=A0 I am process number: 16 on host nid00922<br>Hello world!=C2=
=A0 I am process number: 17 on host nid00922<br>Hello world!=C2=A0 I am pro=
cess number: 18 on host nid00922<br>Hello world!=C2=A0 I am process number:=
 19 on host nid00922<br>Hello world!=C2=A0 I am process number: 20 on host =
nid00922<br>Hello world!=C2=A0 I am process number: 21 on host nid00922<br>=
Hello world!=C2=A0 I am process number: 22 on host nid00922<br>Hello world!=
=C2=A0 I am process number: 23 on host nid00922<br>Hello world!=C2=A0 I am =
process number: 24 on host nid00922<br>Hello world!=C2=A0 I am process numb=
er: 25 on host nid00922<br>Hello world!=C2=A0 I am process number: 26 on ho=
st nid00922<br>Hello world!=C2=A0 I am process number: 27 on host nid00922<=
br>Hello world!=C2=A0 I am process number: 28 on host nid00922<br>Hello wor=
ld!=C2=A0 I am process number: 29 on host nid00922<br>Hello world!=C2=A0 I =
am process number: 30 on host nid00922<br>Hello world!=C2=A0 I am process n=
umber: 31 on host nid00922<br>Hello world!=C2=A0 I am process number: 1 on =
host nid00922<br>Hello world!=C2=A0 I am process number: 3 on host nid00922=
<br>Hello world!=C2=A0 I am process number: 5 on host nid00922<br>Hello wor=
ld!=C2=A0 I am process number: 7 on host nid00922<br>Hello world!=C2=A0 I a=
m process number: 9 on host nid00922<br>Hello world!=C2=A0 I am process num=
ber: 11 on host nid00922<br>Hello world!=C2=A0 I am process number: 12 on h=
ost nid00922<br>Hello world!=C2=A0 I am process number: 13 on host nid00922=
<br>Hello world!=C2=A0 I am process number: 32 on host nid00923<br>Hello wo=
rld!=C2=A0 I am process number: 34 on host nid00923<br>Hello world!=C2=A0 I=
 am process number: 50 on host nid00923<br>Hello world!=C2=A0 I am process =
number: 51 on host nid00923<br>Hello world!=C2=A0 I am process number: 52 o=
n host nid00923<br>Hello world!=C2=A0 I am process number: 53 on host nid00=
923<br>Hello world!=C2=A0 I am process number: 54 on host nid00923<br>Hello=
 world!=C2=A0 I am process number: 55 on host nid00923<br>Hello world!=C2=
=A0 I am process number: 56 on host nid00923<br>Hello world!=C2=A0 I am pro=
cess number: 57 on host nid00923<br>Hello world!=C2=A0 I am process number:=
 58 on host nid00923<br>Hello world!=C2=A0 I am process number: 59 on host =
nid00923<br>Hello world!=C2=A0 I am process number: 60 on host nid00923<br>=
Hello world!=C2=A0 I am process number: 61 on host nid00923<br>Hello world!=
=C2=A0 I am process number: 62 on host nid00923<br>Hello world!=C2=A0 I am =
process number: 63 on host nid00923<br>Hello world!=C2=A0 I am process numb=
er: 33 on host nid00923<br>Hello world!=C2=A0 I am process number: 35 on ho=
st nid00923<br>Hello world!=C2=A0 I am process number: 36 on host nid00923<=
br>Hello world!=C2=A0 I am process number: 37 on host nid00923<br>Hello wor=
ld!=C2=A0 I am process number: 38 on host nid00923<br>Hello world!=C2=A0 I =
am process number: 39 on host nid00923<br>Hello world!=C2=A0 I am process n=
umber: 40 on host nid00923<br>Hello world!=C2=A0 I am process number: 41 on=
 host nid00923<br>Hello world!=C2=A0 I am process number: 42 on host nid009=
23<br>Hello world!=C2=A0 I am process number: 43 on host nid00923<br>Hello =
world!=C2=A0 I am process number: 44 on host nid00923<br>Hello world!=C2=A0=
 I am process number: 45 on host nid00923<br>Hello world!=C2=A0 I am proces=
s number: 46 on host nid00923<br>Hello world!=C2=A0 I am process number: 47=
 on host nid00923<br>Hello world!=C2=A0 I am process number: 48 on host nid=
00923<br>Hello world!=C2=A0 I am process number: 49 on host nid00923</font>=
</p><p class=3D"m_-4242388468169898395m_-4455386444432382273gmail-p1">~~~~<=
/p></div><div>~jerrin</div></div><div class=3D"gmail_extra"><div><div class=
=3D"m_-4242388468169898395h5"><br><div class=3D"gmail_quote">On Tue, May 9,=
 2017 at 1:01 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailt=
o:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a>&gt;</span> wro=
te:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-=
left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Oh, about the hosts =
file, I mean what is the order of the hosts, is the local host also present=
, do you provide the default and max number of slots, etc..<div><br></div><=
div>Thanks!</div></div><div class=3D"m_-4242388468169898395m_-4455386444432=
382273HOEnZb"><div class=3D"m_-4242388468169898395m_-4455386444432382273h5"=
><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, May 9, 2=
017 at 9:56 AM, jerrin <span dir=3D"ltr">&lt;<a href=3D"mailto:jerrin...@gm=
ail.com" target=3D"_blank">jerrin...@gmail.com</a>&gt;</span> wrote:<br><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #=
ccc solid;padding-left:1ex"><div dir=3D"ltr">Okay, l can try with a higher =
version of OpenMPI on both the container as well as host.=C2=A0<div><br></d=
iv><div>







<p class=3D"m_-4242388468169898395m_-4455386444432382273m_-2267512922819520=
31m_8892205703391957631p1"><span class=3D"m_-4242388468169898395m_-44553864=
44432382273m_-226751292281952031m_8892205703391957631s1">:~&gt; file hosts=
=C2=A0</span></p>
<p class=3D"m_-4242388468169898395m_-4455386444432382273m_-2267512922819520=
31m_8892205703391957631p1"><span class=3D"m_-4242388468169898395m_-44553864=
44432382273m_-226751292281952031m_8892205703391957631s1">hosts: ASCII text<=
/span></p><div><span><br>On Tuesday, May 9, 2017 at 12:36:49 PM UTC-4, Greg=
ory Kurtzer wrote:</span><blockquote class=3D"gmail_quote" style=3D"margin:=
0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><span><div=
 dir=3D"ltr">Well, I&#39;ve had issues running Open MPI &lt; 2.x with Singu=
larity (on both host and container).<div><br></div><div>BTW, I&#39;m just c=
urious, what is the format of the hosts file?</div></div></span><div><div c=
lass=3D"m_-4242388468169898395m_-4455386444432382273m_-226751292281952031h5=
"><div><br><div class=3D"gmail_quote">On Tue, May 9, 2017 at 9:29 AM, jerri=
n <span dir=3D"ltr">&lt;<a rel=3D"nofollow">jer...@gmail.com</a>&gt;</span>=
 wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bor=
der-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">I can try that o=
n a different server. But the highest version of OpenMPI on the HPC system =
is 1.8.4. Is this something related to old version of openmpi?<span><br><br=
>On Tuesday, May 9, 2017 at 11:27:41 AM UTC-4, Gregory Kurtzer wrote:</span=
><blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;bord=
er-left:1px #ccc solid;padding-left:1ex"><span><div dir=3D"ltr">Can you re-=
test with the Open MPI version inside and outside the container being somet=
hing greater then 2.x?<div><br></div><div>Thanks!</div></div></span><div><b=
r><div class=3D"gmail_quote"><div><div>On Tue, May 9, 2017 at 8:14 AM, jerr=
in <span dir=3D"ltr">&lt;<a rel=3D"nofollow">jer...@gmail.com</a>&gt;</span=
> wrote:<br></div></div><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div><div><div dir=
=3D"ltr">Hi,<div><br></div><div>I am trying to set up MPI with Singularity.=
 I had set up openmpi version 1.8.4 in the container as the host system has=
 the same openmpi version. However, the container does not understand that =
there are 2 compute nodes even after specifying=C2=A0a hosts file. So each =
time I spawn the MPI processes it just executes on a single node. The conte=
nts of the hosts file is=C2=A0nid00900,nid00901.</div>







<div><br></div><div>~~~~~~</div>aprun1:~&gt; ccmrun mpirun -np 20 --hostfil=
e hosts singularity exec mpi.img /usr/bin/ring <br><br>=E2=80=9Copenmpi=E2=
=80=9D version =E2=80=9C1.8.4=E2=80=9D loaded. <br>Hello world! I am proces=
s number: 8 on host nid00900 <br>Hello world! I am process number: 9 on hos=
t nid00900 <br>Hello world! I am process number: 10 on host nid00900 <br>He=
llo world! I am process number: 11 on host nid00900 <br>Hello world! I am p=
rocess number: 12 on host nid00900 <br>Hello world! I am process number: 13=
 on host nid00900 <br>Hello world! I am process number: 14 on host nid00900=
 <br>Hello world! I am process number: 15 on host nid00900 <br>Hello world!=
 I am process number: 16 on host nid00900 <br>Hello world! I am process num=
ber: 17 on host nid00900 <br>Hello world! I am process number: 18 on host n=
id00900 <br>Hello world! I am process number: 19 on host nid00900<br>Hello =
world! I am process number: 0 on host nid00900 <br>Hello world! I am proces=
s number: 1 on host nid00900 <br>Hello world! I am process number: 2 on hos=
t nid00900 <br>Hello world! I am process number: 3 on host nid00900 <br>Hel=
lo world! I am process number: 4 on host nid00900 <br>Hello world! I am pro=
cess number: 6 on host nid00900 <br>Hello world! I am process number: 7 on =
host nid00900 <br>Hello world! I am process number: 5 on host nid00900 <br>=
<br>Application 8102212 resources: utime ~2s, stime ~3s, Rss ~5672, inblock=
s ~62653, outblocks ~1812<div>~~~~~~</div><div><br></div><div>In addition, =
the singularity version in the host is 2.2.1</div><div><br></div><div><br><=
/div><div>Regards,</div><div>Jerrin=C2=A0</div></div></div></div><span><fon=
t color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br></div>
</div></div></blockquote></div></div></div><div class=3D"m_-424238846816989=
8395m_-4455386444432382273m_-226751292281952031HOEnZb"><div class=3D"m_-424=
2388468169898395m_-4455386444432382273m_-226751292281952031h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div></div><=
/div><span class=3D"m_-4242388468169898395HOEnZb"><font color=3D"#888888">-=
- <br><div class=3D"m_-4242388468169898395m_-4455386444432382273gmail_signa=
ture" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D"=
ltr"><div><div>MS CS Fall-16<br></div><div>Indiana University<br><span><spa=
n><a href=3D"http://www.linkedin.com/in/" target=3D"_blank">www.linkedin.co=
m/in/</a></span><span>jerrinsure<wbr>sh</span></span><br></div><div dir=3D"=
ltr"><div><div><br></div></div></div></div></div></div></div></div>
</font></span></div><div class=3D"m_-4242388468169898395HOEnZb"><div class=
=3D"m_-4242388468169898395h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div>MS CS Fall-16<br></div><div>Indian=
a University<br><span><span><a href=3D"http://www.linkedin.com/in/" target=
=3D"_blank">www.linkedin.com/in/</a></span><span>jerrinsuresh</span></span>=
<br></div><div dir=3D"ltr"><div><div><br></div></div></div></div></div></di=
v></div></div>
</div>

--001a113adb12426860054f1ae480--
