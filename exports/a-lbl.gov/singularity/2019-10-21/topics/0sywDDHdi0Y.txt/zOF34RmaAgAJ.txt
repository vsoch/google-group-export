Date: Wed, 22 Jun 2016 06:56:22 -0700 (PDT)
From: Raimon Bosch <raimo...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov>
In-Reply-To: <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
 <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
 <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
 <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_162_279177459.1466603783072"

------=_Part_162_279177459.1466603783072
Content-Type: multipart/alternative; 
	boundary="----=_Part_163_1757300153.1466603783072"

------=_Part_163_1757300153.1466603783072
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable



Hi Gregory,

I'm not sure if I would achieve the same with your commands. In an=20
environment based on dockers or virtual machines we would do something like=
=20
this [non applicable to Singularity]:

> cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh ./bt.C.4

where hosts.txt* is:

>vm-ip-01-on-*host01* slots=3D2
>vm-ip-01-on-*host02* slots=3D2

* vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX

and trace.sh is:

>#!/bin/bash
>
>export EXTRAE_HOME=3D/opt/extrae/
>export EXTRAE_CONFIG_FILE=3D/extrae.xml
>export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>
>## Run the desired program
>$*

As you see we only perform one execution and OpenMPI transparently manages=
=20
communication between containers or virtual machines. This command would=20
work well rather VMs are on the same host or not.

What I understand from your response is that now we should execute OpenMPI=
=20
on each host and then merge results manually. I don't know yet how to do=20
this merge step or if it is any way to centralize everything like I would=
=20
do with VMs.

Thanks in advance,

El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Kurtze=
r=20
escribi=C3=B3:
>
> Hi Raimon,
>
> The quick answer is you have mpirun handle that as you would normally=20
> where the container file lives on a shared file system:
>
> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>
> Let the MPI outside the container launch the singularity container on eac=
h=20
> host as it would normally launch any MPI program. Then it will call=20
> Singulairty and Singularity will launch the MPI program inside the=20
> container on each of your hosts/servers.=20
>
> Hope that helps!
>
>
>
> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com=20
> <javascript:>> wrote:
>
>>
>> Hi Gregory,
>>
>> Thank you for your answer. One of our experiments needs to run OpenMPI=
=20
>> among several servers. This means that we should put one of our containe=
rs=20
>> in host01, another in host02 and another in host03 and collect the resul=
ts.=20
>>
>> How can I do this execution in parallel if I need to communicate with=20
>> more than one server?
>>
>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer=20
>> escribi=C3=B3:
>>>
>>> Hi Raimon,
>>>
>>> The communication model of a Singularity container is very different=20
>>> from that of a Docker implementation. This is because Docker for all=20
>>> practical purposes emulates a virtual machine as each container has it'=
s=20
>>> own IP address and thus it's own ssh server. It also carries its own se=
t of=20
>>> complexities, for example networks need to be segregated/VLan'ed, DNS/h=
ost=20
>>> resolution needs to be dynamic and passed down to the containers (so th=
ey=20
>>> can reach each other), ssh daemons and other process running inside the=
=20
>>> containers, management via an existing scheduling system, and the list =
goes=20
>>> on and on.
>>>
>>> Think of it this way, Singularity does not do any of that... It runs a=
=20
>>> program within the container as if it were running on the host itself, =
so=20
>>> to communicate between containers is as easy as communicating between=
=20
>>> programs. So for MPI, it would happen with the MPI on the physical host=
=20
>>> (outside the container) invoking the container subsystem which then inv=
okes=20
>>> the MPI programs within the container and the MPI programs within the=
=20
>>> container communicate back to the MPI (orted) outside the container on =
the=20
>>> host to get access to the host resources. In this model all available=
=20
>>> resources and infrastructure can be leveraged at full bandwidth by the=
=20
>>> contained processes and all of the aforementioned complexities akin to=
=20
>>> running on a virtualized mini-cluster are circumvented.
>>>
>>> There is additional information I have written at:
>>>
>>> http://singularity.lbl.gov/#hpc
>>>
>>> That page is still coming along, and needs more information still but i=
f=20
>>> you have any questions, comments or change proposals please let us know=
!
>>>
>>> Thanks and hope that helps!
>>>
>>>
>>>
>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>=20
>>> wrote:
>>>
>>>>
>>>>
>>>> Hi,
>>>>
>>>> We are trying to run experiments using singularity containers. The ide=
a=20
>>>> is to run OpenMPI among several containers and check performance resul=
ts.=20
>>>>
>>>> How can I communicate with another container? In docker this is clear=
=20
>>>> because every container gets an assigned IP and you can ping there, bu=
t=20
>>>> what is the situation in the case of singularity? Is it possible to as=
sign=20
>>>> an IP to each container? Can I connect via ssh to them?
>>>>
>>>> Thanks in advance,
>>>>
>>>> --=20
>>>> You received this message because you are subscribed to the Google=20
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> --=20
>>> Gregory M. Kurtzer
>>> High Performance Computing Services (HPCS)
>>> University of California
>>> Lawrence Berkeley National Laboratory
>>> One Cyclotron Road, Berkeley, CA 94720
>>>
>> --=20
>> You received this message because you are subscribed to the Google Group=
s=20
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n=20
>> email to singu...@lbl.gov.
>>
>
>
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>
>
------=_Part_163_1757300153.1466603783072
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><br>Hi Gregory,<br><br>I&#39;m not sure if I would ach=
ieve the same with your commands. In an environment based on dockers or vir=
tual machines we would do something like this [non applicable to Singularit=
y]:<br><br>&gt; cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile hosts.t=
xt ./trace.sh ./bt.C.4<br><br>where hosts.txt* is:<br><br>&gt;vm-ip-01-on-<=
b>host01</b> slots=3D2<br>&gt;vm-ip-01-on-<b>host02</b> slots=3D2<br><br>* =
vm-ip-XX-on-hostXX<b> </b>are IPs i.e. 172.100.60.XX<br><br>and trace.sh is=
:<br><br>&gt;#!/bin/bash<br>&gt;<br>&gt;export EXTRAE_HOME=3D/opt/extrae/<b=
r>&gt;export EXTRAE_CONFIG_FILE=3D/extrae.xml<br>&gt;export LD_PRELOAD=3D${=
EXTRAE_HOME}/lib/libmpitrace.so<br>&gt;<br>&gt;## Run the desired program<b=
r>&gt;$*<br><br>As you see we only perform one execution and OpenMPI transp=
arently manages communication between containers or virtual machines. This =
command would work well rather VMs are on the same host or not.<br><br>What=
 I understand from your response is that now we should execute OpenMPI on e=
ach host and then merge results manually. I don&#39;t know yet how to do th=
is merge step or if it is any way to centralize everything like I would do =
with VMs.<br><br>Thanks in advance,<br><br>El mi=C3=A9rcoles, 22 de junio d=
e 2016, 14:42:54 (UTC+2), Gregory M. Kurtzer  escribi=C3=B3:<blockquote cla=
ss=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left: 1px #=
ccc solid;padding-left: 1ex;">Hi Raimon,<div><br></div><div>The quick answe=
r is you have mpirun handle that as you would normally where the container =
file lives on a shared file system:</div><div><br></div><div>$ mpirun singu=
larity exec ~/container.img mpi_prog_in_container</div><div><br></div><div>=
Let the MPI outside the container launch the singularity container on each =
host as it would normally launch any MPI program. Then it will call Singula=
irty and Singularity will launch the MPI program inside the container on ea=
ch of your hosts/servers.=C2=A0</div><div><br></div><div>Hope that helps!</=
div><div><br></div><div><span></span><br><br>On Wednesday, June 22, 2016, R=
aimon Bosch &lt;<a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-ma=
ilto=3D"FOCKYxeWAgAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;java=
script:&#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;ret=
urn true;">rai...@gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_qu=
ote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex=
"><div dir=3D"ltr"><br>Hi Gregory,<br><br>Thank you for your answer. One of=
 our experiments needs to run OpenMPI among several servers. This means tha=
t we should put one of our containers in host01, another in host02 and anot=
her in host03 and collect the results. <br><br>How can I do this execution =
in parallel if I need to communicate with more than one server?<br><br>El m=
artes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer  escribi=
=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Raimon,<=
div><br></div><div>The communication model of a Singularity container is ve=
ry different from that of a Docker implementation. This is because Docker f=
or all practical purposes emulates a virtual machine as each container has =
it&#39;s own IP address and thus it&#39;s own ssh server. It also carries i=
ts own set of complexities, for example networks need to be segregated/VLan=
&#39;ed, DNS/host resolution needs to be dynamic and passed down to the con=
tainers (so they can reach each other), ssh daemons and other process runni=
ng inside the containers, management via an existing scheduling system, and=
 the list goes on and on.</div><div><br></div><div>Think of it this way, Si=
ngularity does not do any of that... It runs a program within the container=
 as if it were running on the host itself, so to communicate between contai=
ners is as easy as communicating between programs. So for MPI, it would hap=
pen with the MPI on the physical host (outside the container) invoking the =
container subsystem which then invokes the MPI programs within the containe=
r and the MPI programs within the container communicate back to the MPI (or=
ted) outside the container on the host to get access to the host resources.=
 In this model all available resources and infrastructure can be leveraged =
at full bandwidth by the contained processes and all of the aforementioned =
complexities akin to running on a virtualized mini-cluster are circumvented=
.</div><div><br></div><div>There is additional information I have written a=
t:</div><div><br></div><div><a href=3D"http://singularity.lbl.gov/#hpc" rel=
=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;http://www.=
google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x2=
6sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;" =
onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsin=
gularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN=
-lC0phcop4-SUwsYEjw&#39;;return true;">http://singularity.lbl.gov/#<wbr>hpc=
</a><br></div><div><br></div><div>That page is still coming along, and need=
s more information still but if you have any questions, comments or change =
proposals please let us know!</div><div><br></div><div>Thanks and hope that=
 helps!</div><div><br></div><div><br></div></div><div><br><div class=3D"gma=
il_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <span dir=3D"ltr">&=
lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt;</span> wrote:<br><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi,<br><br>We are trying to r=
un experiments using singularity containers. The idea is to run OpenMPI amo=
ng several containers and check performance results. <br><br>How can I comm=
unicate with another container? In docker this is clear because every conta=
iner gets an assigned IP and you can ping there, but what is the situation =
in the case of singularity? Is it possible to assign an IP to each containe=
r? Can I connect via ssh to them?<br><br>Thanks in advance,<span><font colo=
r=3D"#888888"><br></font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computi=
ng Services (HPCS)<br>University of California<br>Lawrence Berkeley Nationa=
l Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>
</blockquote></div>
------=_Part_163_1757300153.1466603783072--

------=_Part_162_279177459.1466603783072--
