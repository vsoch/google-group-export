X-Received: by 10.36.110.17 with SMTP id w17mr1959823itc.34.1503475409859;
        Wed, 23 Aug 2017 01:03:29 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.55.7 with SMTP id e7ls1006803ioa.35.gmail; Wed, 23 Aug
 2017 01:03:29 -0700 (PDT)
X-Received: by 10.84.132.42 with SMTP id 39mr2093600ple.448.1503475408931;
        Wed, 23 Aug 2017 01:03:28 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1503475408; cv=none;
        d=google.com; s=arc-20160816;
        b=HaQnxlhiknRF64QFlPtFRk7TfNjjc7WGdkPfnyCIv4EL24DxNWSJj2IRU+sH2nC/N9
         H/W5rdjyC36eI+5Lu4R6k8pmFWyJ6HZ963MUsvl9fVFfaDKdo0JCon3WqOJ+CPJHsFw0
         lowrm6uROM9T7Zi4UAugPlwP+sqw3IyO3GsR4AvV4C8iFURRnE5cMimy8ZFZA5ygf/OV
         oANn18ILbY5TIfLByl3ObD4qU1s+3XYEQ3rOxw1zSNCWkUwMGsnW/gtf0QzO1s6IyLXc
         Vsn5QqLuoJGg8d0UvCXG7wydg8m+NPPWqT4zCaTD+Zx1nEru+ZX7K4NMf9ALPIuMlkhP
         7ayQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=055QO9UzHJtSwngC+vY0gu0yN1693/YfT8dbPUsNLzg=;
        b=q2AgAAAQw1yWI93h8L3uK6HWRCW/kzlm6iy48FmGOz1w/HCQv2ua1Vp16uzzcMTIuU
         gu0NF5btDKui/Z0xqz5g8tgyR7V6GJWIrfHdI9cMl1lS8a+Z2YOLmynEBJY0bnQBrztF
         /fBU+SKOEfHkijnWPq84tZIsPntaeziHfbEbd9JMt/MH8iUsueBiOek2rrRygtsOu4lA
         RsS06EsuHVu/pvmw/re7/6B16C1ILlvLHQOZGkon9kKudkNv7ndy9nYwHoG0SHaRpcPG
         V2ievFn6qYiGP87oryFoOpRYH3TNmwORVjb7YXimwgUUYCeJmssCJvghwMuUQ5cpKs0o
         vqJg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=bKuOtHJr;
       spf=pass (google.com: domain of vict...@gmail.com designates 74.125.83.47 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id w189si662460pgb.273.2017.08.23.01.03.28
        for <singu...@lbl.gov>;
        Wed, 23 Aug 2017 01:03:28 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 74.125.83.47 as permitted sender) client-ip=74.125.83.47;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=bKuOtHJr;
       spf=pass (google.com: domain of vict...@gmail.com designates 74.125.83.47 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2FJBgCLNZ1Zhi9TfUpcHQEFAQsBGQYMg?=
 =?us-ascii?q?wRQAT6BFQeDaIE+mQGBcHeBeYpZgx6FPA6BQUMhAQaFHwKELgdBFgEBAQEBAQE?=
 =?us-ascii?q?BAQEBAhABAQEICwsIKC+CMwUCAxoGCEYmAy8BAQEBAQEBAQEBAQEBAQEaAisTE?=
 =?us-ascii?q?gEBGQECAgEjBBkBDQ4eAwELBgULNwICIQEBDgMBBQEcDgcEARwEiD6BOQEDDQg?=
 =?us-ascii?q?FC6FDQIwLgWwYBQEcgwkFR4MfChknDVeDTQEBAQEGAQEBAQEbAgYSgxiCAoFMg?=
 =?us-ascii?q?WOCczSCV4FpARIBCYMpgmEBBIlqDQcHhxaBZ40ZPIdWh3iEdoJrgRSOZEiLd4g?=
 =?us-ascii?q?lFR+BFQ8XA24NMwt3FWOEbh8lgVA+NgiIR0eBawEBAQ?=
X-IronPort-AV: E=Sophos;i="5.41,415,1498546800"; 
   d="scan'208,217";a="85784588"
Received: from mail-pg0-f47.google.com ([74.125.83.47])
  by fe4.lbl.gov with ESMTP; 23 Aug 2017 01:03:24 -0700
Received: by mail-pg0-f47.google.com with SMTP id 83so4580711pgb.3
        for <singu...@lbl.gov>; Wed, 23 Aug 2017 01:03:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=055QO9UzHJtSwngC+vY0gu0yN1693/YfT8dbPUsNLzg=;
        b=bKuOtHJruySqrhRGgSzEVsGjsInGYOGY2gb6JlSqjho+/hELBPmNWdYxbfwSQEA+h2
         7lo2/gLi+3FGfBz6i39a0h57/0DLiPKl3fbFwZhwV/72LbBPkQARP7m7ftYD7NOts/z+
         R0F5mb4bf2adjFG0GN3jJ7qu9syK7PpHlPffKNZRMsfRMfze9kwxLc9E8tUD6RpBIU9p
         L2arqdvkhIZcqLtO75JSxzrQ0p1jrMMn0YuRGYmsRlo5TKc+zKEbUd2BErqAIxXLgX7O
         XUX20eFmLIH52kCfSZ4iODCJynC9cOCfPbUraRYgW1syo3Rq4qg2fmyB5yu5tnelj4Ol
         u3Ug==
X-Gm-Message-State: AHYfb5g+RVCYMaLA9GXHFOESIJsoIo6c7FfkEgG6UJndBmxQuZqy5IA1
	asMSi7hmPstixYNMbTkNBmEZuzY8Sw==
X-Received: by 10.98.205.133 with SMTP id o127mr1813929pfg.241.1503475404217;
 Wed, 23 Aug 2017 01:03:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.100.186.169 with HTTP; Wed, 23 Aug 2017 01:03:23 -0700 (PDT)
In-Reply-To: <1b276d14-5fef-4fa0-9ee2-160aceaa4e3d@lbl.gov>
References: <30715e7a-2b3c-4193-b0b4-501a001b5c3f@lbl.gov> <1b276d14-5fef-4fa0-9ee2-160aceaa4e3d@lbl.gov>
From: victor sv <vict...@gmail.com>
Date: Wed, 23 Aug 2017 10:03:23 +0200
Message-ID: <CA+Wz_Fza1nyigSaORd3VLu84o1VGfakzkEdYf+eKxJtpSvVebA@mail.gmail.com>
Subject: Re: [Singularity] Re: ompi_rte_init returns (-43) error code when
 running from a container
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="94eb2c0441e465a68805576725e2"

--94eb2c0441e465a68805576725e2
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Peleg and Balazs,

which version of openmpi is installed inside the container?

As a starting point, I think you shoul use the same openmpi version both
inside and outside the container.

BR,
V=C3=ADctor.

2017-08-18 16:24 GMT+02:00 Balazs <blaur...@gmail.com>:

> Hi,
>
> I experienced the very same problem described above, with the very same
> tutorial test. We are also running openmpi 2.1.0 on our cluster.
> The same example *without* container works perfectly fine.
>
> Any help would also be appreciated.
>
> Bests,
>
> Balazs
>
>
> Le mercredi 16 ao=C3=BBt 2017 15:44:09 UTC+2, Peleg Bar Sapir a =C3=A9cri=
t :
>>
>> Hello,
>>
>> I'm a student helper in a HPC service, and was asked to check out
>> Singularity and help decide if we provided to our users. We run openmpi
>> 2.1.0 on our cluster.
>>
>> Following this <http://singularity.lbl.gov/docs-hpc> tutorial I was able
>> to form a container and use it on our cluster, but running
>>
>>  mpirun -np 20 singularity exec centos7_mpi_test.img /usr/bin/ring
>> generates an error (I copied it to the end of this message).
>>
>> I tried to look for a clue about what is wrong, but couldn't find any us=
eful help. I would appreciate any help provided.
>>
>> *Error message*:
>>
>> ------------------------------------------------------------------------=
--
>> It looks like MPI_INIT failed for some reason; your parallel process is
>> likely to abort.  There are many reasons that a parallel process can
>> fail during MPI_INIT; some of which are due to configuration or environm=
ent
>> problems.  This failure appears to be an internal failure; here's some
>> additional information (which may only be relevant to an Open MPI
>> developer):
>>
>>   ompi_mpi_init: ompi_rte_init failed
>>   --> Returned "(null)" (-43) instead of "Success" (0)
>> ------------------------------------------------------------------------=
--
>> ------------------------------------------------------------------------=
--
>> It looks like MPI_INIT failed for some reason; your parallel process is
>> likely to abort.  There are many reasons that a parallel process can
>> fail during MPI_INIT; some of which are due to configuration or environm=
ent
>> problems.  This failure appears to be an internal failure; here's some
>> additional information (which may only be relevant to an Open MPI
>> developer):
>>
>>   ompi_mpi_init: ompi_rte_init failed
>>   --> Returned "(null)" (-43) instead of "Success" (0)
>> ------------------------------------------------------------------------=
--
>> *** An error occurred in MPI_Init
>> *** on a NULL communicator
>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
>> ***    and potentially your MPI job)
>> *** An error occurred in MPI_Init
>> *** on a NULL communicator
>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
>> ***    and potentially your MPI job)
>> [(server name):6349] Local abort before MPI_INIT completed completed suc=
cessfully, but am not able to aggregate error messages, and not able to gua=
rantee that all other processes were killed!
>> [(server name):6347] Local abort before MPI_INIT completed completed suc=
cessfully, but am not able to aggregate error messages, and not able to gua=
rantee that all other processes were killed!
>> -------------------------------------------------------
>> Primary job  terminated normally, but 1 process returned
>> a non-zero exit code.. Per user-direction, the job has been aborted.
>> -------------------------------------------------------
>> ------------------------------------------------------------------------=
--
>> mpirun detected that one or more processes exited with non-zero status, =
thus causing
>> the job to be terminated. The first process to do so was:
>>
>>   Process name: [[12088,1],0]
>>   Exit code:    1
>> ------------------------------------------------------------------------=
--
>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--94eb2c0441e465a68805576725e2
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div><div><div>Hi Peleg and Balazs,<br></div><br=
></div>which version of openmpi is installed inside the container?<br><br><=
/div>As a starting point, I think you shoul use the same openmpi version bo=
th inside and outside the container.<br><br></div>BR,<br></div>V=C3=ADctor.=
<br></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">2017-08=
-18 16:24 GMT+02:00 Balazs <span dir=3D"ltr">&lt;<a href=3D"mailto:blaur...=
@gmail.com" target=3D"_blank">blaur...@gmail.com</a>&gt;</span>:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc =
solid;padding-left:1ex"><div dir=3D"ltr">Hi,<div><br></div><div>I experienc=
ed the very same problem described above, with the very same tutorial test.=
 We are also running openmpi 2.1.0 on our cluster.</div><div>The same examp=
le *without* container works perfectly fine.</div><div><br></div><div>Any h=
elp would also be appreciated.</div><div><br></div><div>Bests,</div><div><b=
r></div><div>Balazs<div><div class=3D"h5"><br><br>Le mercredi 16 ao=C3=BBt =
2017 15:44:09 UTC+2, Peleg Bar Sapir a =C3=A9crit=C2=A0:<blockquote class=
=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div dir=3D"ltr">Hello,<br><br>I&#39;m a student hel=
per in a HPC service, and was asked to check out Singularity and help decid=
e if we provided to our users. We run openmpi 2.1.0 on our cluster.<br><br>=
Following <a href=3D"http://singularity.lbl.gov/docs-hpc" rel=3D"nofollow" =
target=3D"_blank">this</a> tutorial I was able to form a container and use =
it on our cluster, but running <br><pre><code><span> </span>mpirun -np 20 s=
ingularity <span>exec</span> centos7_mpi_test.img /usr/bin/ring<br>generate=
s an error (I copied it to the end of this message).<br><br></code><code>I =
tried</code><code> to look for a clue about what is wrong, but couldn&#39;t=
 find any useful help. I would appreciate any help provided.<br><br><u>Erro=
r message</u>:<br>=C2=A0<br></code><code>------------------------------<wbr=
>------------------------------<wbr>--------------</code><br><code>It looks=
 like MPI_INIT failed for some reason; your parallel process is</code><br><=
code>likely to abort.  There are many reasons that a parallel process can</=
code><br><code>fail during MPI_INIT; some of which are due to configuration=
 or environment</code><br><code>problems.  This failure appears to be an in=
ternal failure; here&#39;s some</code><br><code>additional information (whi=
ch may only be relevant to an Open MPI</code><br><code>developer):</code><b=
r><code></code><br><code>  ompi_mpi_init: ompi_rte_init failed</code><br><c=
ode>  --&gt; Returned &quot;(null)&quot; (-43) instead of &quot;Success&quo=
t; (0)</code><br><code>------------------------------<wbr>-----------------=
-------------<wbr>--------------</code><br><code>--------------------------=
----<wbr>------------------------------<wbr>--------------</code><br><code>=
It looks like MPI_INIT failed for some reason; your parallel process is</co=
de><br><code>likely to abort.  There are many reasons that a parallel proce=
ss can</code><br><code>fail during MPI_INIT; some of which are due to confi=
guration or environment</code><br><code>problems.  This failure appears to =
be an internal failure; here&#39;s some</code><br><code>additional informat=
ion (which may only be relevant to an Open MPI</code><br><code>developer):<=
/code><br><code></code><br><code>  ompi_mpi_init: ompi_rte_init failed</cod=
e><br><code>  --&gt; Returned &quot;(null)&quot; (-43) instead of &quot;Suc=
cess&quot; (0)</code><br><code>------------------------------<wbr>---------=
---------------------<wbr>--------------</code><br><code>*** An error occur=
red in MPI_Init</code><br><code>*** on a NULL communicator</code><br><code>=
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,</c=
ode><br><code>***    and potentially your MPI job)</code><br><code>*** An e=
rror occurred in MPI_Init</code><br><code>*** on a NULL communicator</code>=
<br><code>*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now=
 abort,</code><br><code>***    and potentially your MPI job)</code><br><cod=
e>[</code><code><code>(server name)</code>:6349] Local abort before MPI_INI=
T completed completed successfully, but am not able to aggregate error mess=
ages, and not able to guarantee that all other processes were killed!<br>[<=
/code><code><code>(server name)</code>:6347] Local abort before MPI_INIT co=
mpleted completed successfully, but am not able to aggregate error messages=
, and not able to guarantee that all other processes were killed!<br>------=
------------------------<wbr>-------------------------</code><br><code>Prim=
ary job  terminated normally, but 1 process returned</code><br><code>a non-=
zero exit code.. Per user-direction, the job has been aborted.</code><br><c=
ode>------------------------------<wbr>-------------------------</code><br>=
<code>------------------------------<wbr>------------------------------<wbr=
>--------------</code><br><code>mpirun detected that one or more processes =
exited with non-zero status, thus causing</code><br><code>the job to be ter=
minated. The first process to do so was:</code><br><code></code><br><code> =
 Process name: [[12088,1],0]</code><br><code>  Exit code:    1</code><br><c=
ode>------------------------------<wbr>------------------------------<wbr>-=
-------------<br></code><code></code></pre></div></blockquote></div></div><=
/div></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--94eb2c0441e465a68805576725e2--
