X-Received: by 10.36.60.73 with SMTP id m70mr1071349ita.47.1503650782347;
        Fri, 25 Aug 2017 01:46:22 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.43.81 with SMTP id r78ls5109668ior.51.gmail; Fri, 25 Aug
 2017 01:46:21 -0700 (PDT)
X-Received: by 10.99.96.18 with SMTP id u18mr8993671pgb.121.1503650781280;
        Fri, 25 Aug 2017 01:46:21 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1503650781; cv=none;
        d=google.com; s=arc-20160816;
        b=odMyrseCxlawpUGKP+XijdHwnEi8Fyftife2CULwAitl0wcyBo+XJZjcliC2Ir6E5T
         erGjTQOzZEY/HJElHYfV8CVhDX+FtJeoCWLuN9b/OqvpxTpV2z50lczhJbYXq64YPUs1
         M8HDUSqd4DL6hKu/W+ueSGBBvW7460uQgq4PHKFfXJ6GXdUQgWV+YDsdaLfl4sj7xS51
         Yv3U9qO3UeTAWiZU/yYAeiAqtqN2IXO/07ywmsf9zXoOHLAqhMiuPVhxmOuflTLlLiPZ
         c8RZ3TRZXx84uy4PpOfIq2c2e1ms7rcaosq0nfpIV6gXPtR3F27VmRb9joTAFX6yIxb9
         YE/g==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:references:in-reply-to
         :mime-version:dkim-signature:arc-authentication-results;
        bh=xrmLi9lsW+uTprLRjNE4OnzG9iZXrMrQwRvl+2Qo+HI=;
        b=l0K/NVPgCjLOKh6fH0+AMQseOIpILaN73PnTQd7MZSJ3wvfusJObV2Nw8FpYYomZZz
         uO8g69a/HPY9UUOstG9uKkJaO3nLg6EeHVTN6JMLZ7xnBXpQxIsobHA7HGR//ocJVwrG
         wvy79POTUSAIFaJKSWnFjTVlzsK8iqL9NF+UOTLRXQ5u+ZJB34936PoyXoe0Cfn8cRry
         V6Sh9cw6it8fvUM2n2YvuYIZmjHfLQq7h1UsF5JHOuuHXtFaw2ogmhgv7cfcDYmtDsve
         NVAAGiDnIlfvCqQ4N2N2rpG7+xLdxdH33SZH2c/w16Rn03PZc2f0LX8+7E5zHpSF1hZ/
         T2eQ==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=itynTxkM;
       spf=pass (google.com: domain of vict...@gmail.com designates 74.125.83.51 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id a6si4638997pll.33.2017.08.25.01.46.21
        for <singu...@lbl.gov>;
        Fri, 25 Aug 2017 01:46:21 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 74.125.83.51 as permitted sender) client-ip=74.125.83.51;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=itynTxkM;
       spf=pass (google.com: domain of vict...@gmail.com designates 74.125.83.51 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2F/AABr459ZhjNTfUpdGgEBAQECAQEBA?=
 =?us-ascii?q?QgBAQEBFQEBAQECAQEBAQgBAQEBgkQ+AoEPgRUHg2iBPph/gXCCcJNDgSUDGRs?=
 =?us-ascii?q?lAwcBGQEMgV6BXIEQTwKENAdDFAEBAQEBAQEBAQEBAhABAQEICwsIKC+CMwUCA?=
 =?us-ascii?q?xoBBQQERiYBAgMBAQEBAQEBAQEjAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQE?=
 =?us-ascii?q?BAQEBAQEJAggEAR4SARIBARgBAQEBAgEaAQgdAQ0OEgsBAwELBgMCCwoDIAEJA?=
 =?us-ascii?q?gICHwEBDgMBBQEcBwcHBAEaAgICiD5tTAEDDQgFC5BKkRtAjAuCBQUBHIMJBYN?=
 =?us-ascii?q?fChknDVeDRAEBAQEBAQEBAQEBAQEBAQEBAQEBAQ4HAgYSgxiBDXWBTYFjghuBD?=
 =?us-ascii?q?IJXgWYFARECATUVAYJmgmEFhziKNoY5h3s8hDSCIYEBg1aDVU+EdoISWo96iXK?=
 =?us-ascii?q?CU4gpFR+BFQ8ncDQLdxVJGoRFKg8QDIFpPjYBAQEBBIgagWsBAQE?=
X-IronPort-AV: E=Sophos;i="5.41,424,1498546800"; 
   d="sh'?def'?scan'208,217";a="86682349"
Received: from mail-pg0-f51.google.com ([74.125.83.51])
  by fe3.lbl.gov with ESMTP; 25 Aug 2017 01:46:05 -0700
Received: by mail-pg0-f51.google.com with SMTP id r133so11596099pgr.3
        for <singu...@lbl.gov>; Fri, 25 Aug 2017 01:46:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc;
        bh=xrmLi9lsW+uTprLRjNE4OnzG9iZXrMrQwRvl+2Qo+HI=;
        b=itynTxkMa/02oek3BplmH27n8Q8FAnG8vI8Qz+dJSXpBmwX0JD376uVOvPhAjfnUty
         1WZotHeGWDZeVGfTeboWgyGT5sENSlkbXDK8g0ABQRFR0jgLS8EIiPEsAvlGtDdSM259
         olEIOoNUlKM0nJfyjB8lchZcucWumry/DxU5zW3E6GfWFAEZCV/OvRzgW/dUfsv2N9ND
         ev0JfX+zLLb7BrKjJ92PsmaoCgETUyzFF+MfhTwlLJzUy9Y0GYt/5CrgBX1e9wXULup5
         BQbVL+HX9x4e/oqdH8mRFS+5I7qiilFYTpjIfWzocWY1dCAhiKgj5U//JBeN7fFVpQrZ
         HpIg==
X-Gm-Message-State: AHYfb5ijG7QhyLM08hvdQ378YaQg71I7aIvY9VCTCyOKLacBo4XEe/Ii
	+w7QinpqZictUwsHMW4It0r/W+wsFwRl
X-Received: by 10.99.0.207 with SMTP id 198mr9061682pga.452.1503650764724;
 Fri, 25 Aug 2017 01:46:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.100.186.169 with HTTP; Fri, 25 Aug 2017 01:46:03 -0700 (PDT)
In-Reply-To: <CAApQTTjo5VtRcXzpwDeFutOq7O-1igJwsmER2meOz-Y0tU-dvA@mail.gmail.com>
References: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
 <CAApQTTgQwZ3pu7Xa7RsbuHM7ndP05Kv5ciEBk_q5G-8P3xaUGw@mail.gmail.com>
 <CAApQTTg2vQY9wJg0g_Qp5n+558jYqb_ZX8-iezzwJf-6nM63QQ@mail.gmail.com>
 <CA+Wz_Fw=cXLngnXwv=m1DnK6-cYN+496MeHhuHyu9YmsV=CG0g@mail.gmail.com>
 <CAApQTTj4FsbybqNWFxj=JCvNN2i8m0+AZvTNPHd25v3GV3FYSA@mail.gmail.com>
 <CA+Wz_Fwj2KbXCpv4WQqv=SttRJwzXRBZw2KOfOnQs0cwfM3onQ@mail.gmail.com> <CAApQTTjo5VtRcXzpwDeFutOq7O-1igJwsmER2meOz-Y0tU-dvA@mail.gmail.com>
From: victor sv <vict...@gmail.com>
Date: Fri, 25 Aug 2017 10:46:03 +0200
Message-ID: <CA+Wz_FzNLzAj9O=ots9xjz9OmxW9EmhaX8XhZCph1-QGk92QBQ@mail.gmail.com>
Subject: Re: [Singularity] OpenMPI, Slurm & portability
To: singu...@lbl.gov, us...@lists.open-mpi.org
Cc: Ralph Castain <rcas...@gmail.com>
Content-Type: multipart/mixed; boundary="001a1147b03cb2d72c05578ff95d"

--001a1147b03cb2d72c05578ff95d
Content-Type: multipart/alternative; boundary="001a1147b03cb2d72705578ff95b"

--001a1147b03cb2d72705578ff95b
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Dear Singularity & OpenMPI teams, Greg and Ralph,

going back to the Ralph Castain response to this thread:

https://groups.google.com/a/lbl.gov/forum/#!topic/singularity/lQ6sWCWhIWY

In order to get portability of Singularity images containing OpenMPI
distributed applications, he suggested mix some OpenMPI versions with some
external PMIX to check about the interoperability across versions while
using of the Singularity MPI hybrid approach (see his response in the
thread).

I did some experiments and I would like to share with you my results and to
discuss about the conclusions.

First of all, I'm going to describe the environment (some scripts are
attached).

   - I performed this test at CESGA FinisTerrae II cluster (
   https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2
   <https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2>).
   - The compiler used is GCC/6.3.0 and I had to compile some external
   dependencies to be linked from PMIX or OpenMPI:
      - hwloc/1.11.5
      - libevent/2.0.22
   - PMIX versions used in this experiments:
      - 1.2.1
      - 1.2.2
      - 2.0.0
      - I configure PMIX with the following options:
      - ./configure --with-hwloc=3D --with-munge-libdir=3D
      --with-platform=3Doptimized --with-libevent=3D
   - OpenMPI versions used in this experiments:
      - 2.0.X
      - 2.1.1
      - 3.0.0_rcX
      - I configure OpenMPI with the following options:
      - ./configure --with-hwloc=3D --enable-shared --with-slurm
      --enable-mpi-thread-multiple --with-verbs-libdir=3D
      --enable-mpirun-prefix-by-default --disable-dlopen --with-pmix=3D
      --with-libevent=3D --with-knem
      - Version 2.1.1 was also compiled with flag --disable-pmix-dstore
   - I used the well known "Ring" OpenMPI application.
   - I used MPIRUN as process manager

What I expected from previous Ralph response was full cross-version
compatibility using any OpenMPI >=3D 2.0.0  linked against PMIX 1.2.X both,
inside the container and at the host.
In general, my results were not as good as expected, but promising.

   - The worst thing is, my results show that OpenMPI 2.X versions needs
   exactly the same version of OpenMPI inside & outside the container, but =
I
   can mix PMIx 1.2.1 and 1.2.2
   - The better thing, if OpenMPI 3.0.0_rc3 version is present inside or
   outside the container,  seems to work mixing any other OpenMPI >=3D 2.X
   version and also mixing PMIx 1.2.1 and 1.2.2. Some notes* to this result=
:
   - OpenMPI 2.0.0 with PMIx 1.2.2 (In&Out the container) never worked.
      - After getting the expected output from "Ring" app, I randomly get
      SEGFAULT if OpenMPI 3.0.0.rcX is involved.
      - As Ralph said, PMIx 1.2X and 2.0.X are not interoperable.
   - I was not able to compile OpenMPI 2.1.0 with external PMIx

I can conclude that PMIx 1.2.1 and 1.2.2 are interoperable, but only
OpenMPI 3.0.0_rc3 can work*, in general, with other versions of OpenMPI
(>2).

Going back again to Ralph Castain mail to this thread, I would expect full
support for interoperability with different PMIx versions (>1.2) through
PMIx > 2.1 (not yet released)

Some questions about this experiments and conclusions are:

   - What do you think about this results?  Do you have any suggestion? I'm
   missing something?
   - are these results aligned with your expectations?
   - I know that PMIx 2.1 is being developed but, any version is already
   available to check? How can I get it?
   - The SEGFAULT I get with  OpenMPI 3.0.0.rcX is something already
   tracked?

Hope to helpful!

BR,

V=C3=ADctor



2017-07-14 1:34 GMT+02:00 Gregory M. Kurtzer <gmku...@gmail.com>:

> Hi Victor,
>
> The are of ABI compatibility I am referring to is with the container's
> underlying library stack. Meaning that if you link in the libraries
> compiled on the host, and the container you want to run is newer then wha=
t
> is installed on the host, (or potentially vise-versa), you may end up wit=
h
> a conflict between the binary and library.
>
> This is what Nvidia has mitigated by building their library on a very
> recent toolchain, thus the libraries are backwards compatible with older
> binaries.
>
> Does that make sense?
>
> Greg
>
>
>
> On Wed, Jul 12, 2017 at 4:50 AM, victor sv <vict...@gmail.com> wrote:
>
>> Hi Greg and Ralph,
>>
>> yes Greg, I agree with you that the mentioned strategy could be dangerou=
s
>> and goes against the principals of containment.
>>
>> sorry for the basic question ... but what do you mean with ABI compatibl=
e
>> containers? which components of the container environment are involved w=
ith
>> this ABI compatibility?
>>
>> If we talk about libc or the kernel itself, as you say in your web page,
>> "If you require kernel dependent features, a container platform is proba=
bly
>> not the right solution for you."
>>
>> If we focus on OpenMPI ABI compatibility, I figure out that the variable=
s
>> involved in this compatibility could be (1) the compiler (vendor) and (2=
)
>> the OpenMPI library itself.
>>
>> I'm right or I'm missing any other variables?
>>
>> An interesting project called ABI-tracker has performed an OpenMPI ABI
>> compatibility study that you can watch in the following link:
>>
>> https://abi-laboratory.pro/tracker/timeline/openmpi/
>>
>> I think that, at least for OpenMPI 2.X, altough been a dangerous
>> approach, the ABI compatibility seems reasonable.
>>
>> What do you think?
>>
>> BR,
>> V=C3=ADctor.
>>
>> 2017-07-11 16:55 GMT+02:00 Gregory M. Kurtzer <gmku...@gmail.com>:
>>
>>> Hi Victor,
>>>
>>> I will let Ralph comment on the OMPI versions and compatibilities, but
>>> regarding using the MPI host libraries within a container is dangerous =
for
>>> the reason that you are mentioning. If you are running ABI compatible
>>> containers with the host, then things *might* work as expected. But thi=
s
>>> breaks container portability, and goes against the principals of
>>> containment.
>>>
>>> We do however do exactly this for the Nvidia driver libraries, but...
>>> Nvidia builds these libraries with careful attention on ABI compatibili=
ty
>>> such that these binary libraries are indeed reasonably portable across
>>> containers.
>>>
>>> The only way to do this portably is with using a launcher on the host,
>>> outside the container, to spin up the container and launch the MPI with=
in.
>>> PMIx is a fantastic approach to solving this.
>>>
>>> Hope that helps!
>>>
>>> Greg
>>>
>>>
>>>
>>> On Tue, Jul 11, 2017 at 6:03 AM, victor sv <vict...@gmail.com> wrote:
>>>
>>>> Hi Greg and Ralph,
>>>>
>>>> Thank you for your precise and elaborated answers.
>>>>
>>>> Only for confirmation and to sum up some conclussions (if I understood
>>>> correctly):
>>>>
>>>>  - OpenMPI process management compatibility depends on PMIx.
>>>>  - OpenMPI (and also Slurm) complete  backward/forward compatibility
>>>> will come (hopefully) in the future by means of PMIx 2.1.
>>>>  - Nowadays, there exists compatibility with OpenMPI 2.X if we compile
>>>> it with default PMIx (1.X) support.
>>>>  - OpenMPI 2.1 must be compiled with --disable-pmix-dstore due to a
>>>> compatibility break.
>>>>  - OpenMPI 1.X does not suppot PMIx and we can ignore it from this
>>>> thread.
>>>>
>>>> I'm right?
>>>>
>>>> I'm interested in performing the tests you purpose. I will try to buil=
d
>>>> all three OMPI versions (2.0, 2.1 and 3.0) against the same PMIx exter=
nal
>>>> library to check the compatibility. Which PMIx version (1.2.0, 1.2.1 o=
r
>>>> 1.2.2 ) do you recommend as a start point?
>>>>
>>>> I will report this results ASAP to this thread.
>>>>
>>>> On the other hand, although we are planning to add support to PMIx,
>>>> unfortunately, our Slurm version (14.11.10-Bull.1.0) does not support =
it
>>>> yet.
>>>>
>>>> The second strategy we are testing to get compatibility between OpenMP=
I
>>>> inside and outside a Singularity container relies on replacing the Ope=
nMPI
>>>> libraries inside the container by the host libraries hierarchy.
>>>>
>>>> This approach rest upon the assumption that OpenMPI symbols and data
>>>> structures are compatible through several versions of OpenMPI. At leas=
t
>>>> combining several releases that share the same major version.
>>>>
>>>> Although the empirical tests of this approach seem to work properly
>>>> with some tests, benchmarks and real apps, I'm afraid of getting
>>>> unexepected errors/warnings (segfaults, data errors, etc.) in the futu=
re.
>>>>
>>>> What do you think about this approach?
>>>>
>>>> Can you confirm that OpenMPI is compatible in this way?
>>>>
>>>> Finally, I think this thread could be very interesting for other users
>>>> too and I would like to keep it alive with your help.
>>>>
>>>> Thank you again for your support!
>>>>
>>>> BR,
>>>> V=C3=ADctor
>>>>
>>>> 2017-07-09 23:45 GMT+02:00 Gregory M. Kurtzer <gmku...@gmail.com>:
>>>>
>>>>> Hiya Victor, et al.,
>>>>>
>>>>> I didn't realize this but Ralph had to drop off of the Singularity
>>>>> list. Hopefully we will get him back again, as he is a fantastic reso=
urce
>>>>> for all of the OMPI questions and always a great source of informatio=
n and
>>>>> ideas (poke, poke Ralph!). Ralph did send me this in response to the
>>>>> previous email hoping it helps to explain things:
>>>>>
>>>>>
>>>>> On Sun, Jul 9, 2017 at 2:22 PM, r...@open-mpi.org <r...@open-mpi.org>
>>>>>  wrote:
>>>>>
>>>>>> ...
>>>>> You are welcome to forward the following to the list:
>>>>>
>>>>> As Greg said, we have been concerned about this since we started
>>>>> looking at Singularity support. Just for clarity, the version of PMI =
OMPI
>>>>> uses is PMIx (https://pmix.github.io/pmix/). While our plan from the
>>>>> beginning was to support cross-versions specifically to address this
>>>>> problem, we fell behind on its implementation due to priorities. We j=
ust
>>>>> committed the code to the PMIx repo in the last week, and it won=E2=
=80=99t be
>>>>> released into production for a few months while we shake it down.
>>>>>
>>>>> I fear it will be impossible to get the OMPI 1.10 series to work with
>>>>> anything other than itself as it pre-dates PMIx.
>>>>>
>>>>> The OMPI 2.0 and 2.1 series should work across each other as they bot=
h
>>>>> include PMIx 1.x. However, you probably will need to configure the 2.=
1
>>>>> series with --disable-pmix-dstore as there was an unintended compatib=
ility
>>>>> break there (the shared memory store was added during the PMIx 1.x se=
ries
>>>>> and we didn=E2=80=99t catch the compatibility break it introduced).
>>>>>
>>>>> Looking into the future, OMPI 3.0 is about to be released. It include=
s
>>>>> PMIx 2.0, which isn=E2=80=99t backwards compatible at this time, and =
so it won=E2=80=99t
>>>>> cross-version with OMPI 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We have=
n=E2=80=99t tested this, but
>>>>> one thing you could try is to build all three OMPI versions against t=
he
>>>>> same PMIx external library (you would probably have to experiment a b=
it
>>>>> with PMIx versions to see which works across the different OMPI versi=
ons as
>>>>> the glue between the two also changed a bit). This will ensure that t=
he
>>>>> shared memory store in PMIx is compatible across the versions, and th=
ings
>>>>> should work since OMPI doesn=E2=80=99t care how the data is moved acr=
oss the
>>>>> host-container boundary.
>>>>>
>>>>> As I said, we will be adding cross-version support to the PMIx releas=
e
>>>>> series soon, without changing the API, that will ensure support acros=
s all
>>>>> PMIx versions starting with v1.2. Thus, you could (once that happens)=
 build
>>>>> OMPI 2.0, 2.1, and 3.0 against the new PMIx release (probably PMIx v2=
.1.0)
>>>>> and the resulting containers would be future-proof as OMPI moves ahea=
d. The
>>>>> RMs plan to follow that path as well, so you should be in good shape =
once
>>>>> this is done if you prefer to =E2=80=9Cdirect launch=E2=80=9D your co=
ntainers (e.g., =E2=80=9Csrun
>>>>> ./mycontainer=E2=80=9D under SLURM).
>>>>>
>>>>> Sorry if that is all confusing - we sometimes get lost in the
>>>>> numbering schemes between OMPI and PMIx ourselves. Feel free to conta=
ct me
>>>>> directly, or on the OMPI or PMIx mailing lists, if you have more ques=
tions
>>>>> or encounter problems. We definitely want to make this work.
>>>>>
>>>>> Ralph
>>>>>
>>>>> On Sun, Jul 9, 2017 at 12:19 PM, Gregory M. Kurtzer <
>>>>> gmku...@gmail.com> wrote:
>>>>>
>>>>>> Hi Victor,
>>>>>>
>>>>>> Sorry for the latency, I'm on email overload.
>>>>>>
>>>>>> Open MPI uses PMI to communicate both inside and outside of the
>>>>>> container. Ralph Castain (on this list, but possibly not monitoring
>>>>>> actively) is leading the PMI effort and he is an active Open MPI dev=
eloper.
>>>>>> We have had several talks about how to achieve "hetero-versionistic"
>>>>>> compatibility through the PMI handshake. I was under the impression =
that
>>>>>> PMI now supports that, as long as you are running equal or newer ver=
sion on
>>>>>> the host (outside the container). Also, I don't know what version of=
 PMI
>>>>>> this feature was introduced in, nor do I know what version of Open M=
PI
>>>>>> includes that compatibility.
>>>>>>
>>>>>> I have CC'ed Ralph, and hopefully he will be able to offer some
>>>>>> suggestions.
>>>>>>
>>>>>> Regarding your question about supporting the MPI libraries in the
>>>>>> same manner that we are doing the Nvidia libraries, that would be ha=
rd.
>>>>>> Nvidia specifically builds their libraries to be as generally compat=
ible as
>>>>>> possible (e.g. the same libraries/binaries work on a large array of =
Linux
>>>>>> distributions). Most people do not build host libraries in a manner =
that
>>>>>> would be generally compatible as Nvidia does.
>>>>>>
>>>>>> Hope that helps!
>>>>>>
>>>>>> Greg
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Mon, Jul 3, 2017 at 2:07 AM, victor sv <vict...@gmail.com> wrote:
>>>>>>
>>>>>>> Dear Singularity team,
>>>>>>>
>>>>>>> first of all, thanks for the great work with Singularity. It looks
>>>>>>> amazing!
>>>>>>>
>>>>>>> Sorry if this topic is duplicated and for the length of the email,
>>>>>>> but I want to share my experience about Singularity and OpenMPI
>>>>>>> compatibility, and also ask some questions.
>>>>>>>
>>>>>>> I've being reading a lot about OpenMPI and Singularity compatibilit=
y
>>>>>>> because we are trying to find the generic way to run OpenMPI applic=
ations
>>>>>>> within Singularity containers. It was not so clear (for me) in the
>>>>>>> documentation, forums and mailing lists, and this is why we've perf=
ormed an
>>>>>>> OpenMPI empiric compatibility study.
>>>>>>>
>>>>>>> We ran these comparisons in CESGA FinisTerrae II cluster (
>>>>>>> https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2).
>>>>>>>
>>>>>>> We used several versions of OpenMPI. The chosen versions of OpenMPI
>>>>>>> were the versions already installed in the cluster:
>>>>>>>
>>>>>>> - openmpi/1.10.2
>>>>>>> - openmpi/2.0.0
>>>>>>> - openmpi/2.0.1
>>>>>>> - openmpi/2.0.2
>>>>>>> - openmpi/2.1.1
>>>>>>>
>>>>>>> We have created Singularity images containing the same versions of
>>>>>>> OpenMPI and with the basic OpenMPI ring example. I share the bootst=
rap
>>>>>>> definition file template used below:
>>>>>>>
>>>>>>> ```
>>>>>>> BootStrap: docker
>>>>>>> From: ubuntu:16.04
>>>>>>> IncludeCmd: yes
>>>>>>>
>>>>>>> %post
>>>>>>>         sed -i 's/main/main restricted universe/g'
>>>>>>> /etc/apt/sources.list
>>>>>>>         apt-get update
>>>>>>>         apt-get install -y bash git wget build-essential gcc time
>>>>>>> libc6-dev libgcc-5-dev
>>>>>>>         apt-get install -y dapl2-utils libdapl-dev libdapl2
>>>>>>> libibverbs1 librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-=
1
>>>>>>> libmthca1 libnes1 libpmi0 libpmi0-dev libslurm29 libslurm-dev
>>>>>>>
>>>>>>>         ##Install OpenMPI
>>>>>>>         cd /tmp
>>>>>>>         wget 'https://www.open-mpi.org/soft
>>>>>>> ware/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz' -O
>>>>>>> openmpi-X.X.X.tar.gz
>>>>>>>         tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X
>>>>>>>         mkdir -p /tmp/openmpi-X.X.X/build
>>>>>>>         cd /tmp/openmpi-X.X.X/build
>>>>>>>          ../configure --enable-shared --enable-mpi-thread-multiple
>>>>>>> --with-verbs --enable-mpirun-prefix-by-default --with-hwloc
>>>>>>> --disable-dlopen --with-pmi --prefix=3D/usr
>>>>>>>         make all install
>>>>>>>
>>>>>>>         # Install ring
>>>>>>>         cd /tmp
>>>>>>>         wget https://raw.githubusercontent.
>>>>>>> com/open-mpi/ompi/master/examples/ring_c.c
>>>>>>>         mpicc ring_c.c -o /usr/bin/ring
>>>>>>> ```
>>>>>>>
>>>>>>> Once the containers were created, we ran the ring app with mpirun
>>>>>>> using 2 cores of 2 different nodes mixing all possible combinations=
 of
>>>>>>> those OpenMPI versions inside and outside the container.
>>>>>>>
>>>>>>> The obtained results shown that we need the same versions of OpenMP=
I
>>>>>>> inside and outside the container to succesfully run the contained
>>>>>>> application in parallel with mpirun.
>>>>>>>
>>>>>>> Is this the expected behaviour or am I missing something?
>>>>>>>
>>>>>>> Will be this the expected behaviour in the future (with future
>>>>>>> versions of OpenMPI)?
>>>>>>>
>>>>>>> Currently, we have slurm 14.11.10-Bull.1.0 installed as job
>>>>>>> scheduler at FinisTerrae II. We found the following tip/trick to us=
e srun
>>>>>>> as process manager:
>>>>>>>
>>>>>>> http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls
>>>>>>>
>>>>>>> In order to run whatever Singularity image containing OpenMPI
>>>>>>> applications using Slurm, we've adapted it to our infrastructure an=
d
>>>>>>> checked the same test cases running them with srun. It seems that i=
t's
>>>>>>> working properly (no real world applications were tested yet).
>>>>>>>
>>>>>>> What do you think about this strategy?
>>>>>>> Can you confirm that it provides portability of singularity images
>>>>>>> containing OpenMPI applications?
>>>>>>>
>>>>>>> I think this strategy is similar to the one you are following with
>>>>>>> "--nv" option  for NVidia drivers.
>>>>>>>
>>>>>>> Why not to do the same strategy with MPI, PMI, libibverbs, etc.?
>>>>>>>
>>>>>>> Thanks in advance and congrats again for your great work!
>>>>>>>
>>>>>>> V=C3=ADctor.
>>>>>>>
>>>>>>> --
>>>>>>> You received this message because you are subscribed to the Google
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Gregory M. Kurtzer
>>>>>> CEO, SingularityWare, LLC.
>>>>>> Senior Architect, RStor
>>>>>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gregory M. Kurtzer
>>>>> CEO, SingularityWare, LLC.
>>>>> Senior Architect, RStor
>>>>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> CEO, SingularityWare, LLC.
>>> Senior Architect, RStor
>>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> Gregory M. Kurtzer
> CEO, SingularityWare, LLC.
> Senior Architect, RStor
> Computational Science Advisor, Lawrence Berkeley National Laboratory
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a1147b03cb2d72705578ff95b
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div><div><div><div><div><div><div>Dear Singular=
ity &amp; OpenMPI teams, Greg and Ralph,<br><br></div>going back to the Ral=
ph Castain response to this thread:<br><br><a href=3D"https://groups.google=
.com/a/lbl.gov/forum/#!topic/singularity/lQ6sWCWhIWY">https://groups.google=
.com/a/lbl.gov/forum/#!topic/singularity/lQ6sWCWhIWY</a><br><br></div>In or=
der to get portability of Singularity images containing OpenMPI distributed=
 applications, he suggested mix some OpenMPI versions with some external PM=
IX to check about the interoperability across versions while using of the S=
ingularity MPI hybrid approach (see his response in the thread).<br><br></d=
iv>I did some experiments and I would like to share with you my results and=
 to discuss about the conclusions.<br><br></div>First of all, I&#39;m going=
 to describe the environment (some scripts are attached).<br></div><ul><li>=
I performed this test at CESGA FinisTerrae II cluster (<a href=3D"https://w=
ww.cesga.es/en/infraestructuras/computacion/FinisTerrae2" target=3D"_blank"=
>https://www.cesga.es/en/<wbr>infraestructuras/computacion/<wbr>FinisTerrae=
2</a>). </li><li>The compiler used is GCC/6.3.0 and I had to compile some e=
xternal dependencies to be linked from PMIX or OpenMPI:</li><ul><li>hwloc/1=
.11.5</li><li>libevent/2.0.22</li></ul><li>PMIX versions used in this exper=
iments:</li><ul><li>1.2.1</li><li>1.2.2</li><li>2.0.0<br></li></ul><li>I co=
nfigure PMIX with the following options:</li><ul><li>./configure --with-hwl=
oc=3D --with-munge-libdir=3D --with-platform=3Doptimized --with-libevent=3D=
</li></ul><li>OpenMPI versions used in this experiments:</li><ul><li>2.0.X<=
/li><li>2.1.1</li><li>3.0.0_rcX<br></li></ul><li>I configure OpenMPI with t=
he following options:</li><ul><li>./configure --with-hwloc=3D --enable-shar=
ed --with-slurm --enable-mpi-thread-multiple --with-verbs-libdir=3D --enabl=
e-mpirun-prefix-by-default --disable-dlopen --with-pmix=3D --with-libevent=
=3D --with-knem </li><li>Version 2.1.1 was also compiled with flag --disabl=
e-pmix-dstore</li></ul><li>I used the well known &quot;Ring&quot; OpenMPI a=
pplication.<br></li><li>I used MPIRUN as process manager<br></li></ul><p>Wh=
at I expected from previous Ralph response was full cross-version compatibi=
lity using any OpenMPI &gt;=3D 2.0.0=C2=A0 linked against PMIX 1.2.X both, =
inside the container and at the host.</p><div>In general, my results were n=
ot as good as expected, but promising. <br><ul><li>The worst thing is, my r=
esults show that OpenMPI 2.X versions needs exactly the same version of Ope=
nMPI inside &amp; outside the container, but I can mix PMIx 1.2.1 and 1.2.2=
</li><li>The better thing, if OpenMPI 3.0.0_rc3 version is present inside o=
r outside the container,=C2=A0 seems to work mixing any other OpenMPI &gt;=
=3D 2.X version and also mixing PMIx 1.2.1 and 1.2.2. Some notes* to this r=
esult:<br></li><ul><li>OpenMPI 2.0.0 with PMIx 1.2.2 (In&amp;Out the contai=
ner) never worked.</li><li>After getting the expected output from &quot;Rin=
g&quot; app, I randomly get SEGFAULT if OpenMPI 3.0.0.rcX is involved.<br><=
/li></ul><li>As Ralph said, PMIx 1.2X and 2.0.X are not interoperable.</li>=
<li>I was not able to compile OpenMPI 2.1.0 with external PMIx</li></ul><p>=
I can conclude that PMIx 1.2.1 and 1.2.2 are interoperable, but only OpenMP=
I 3.0.0_rc3 can work*, in general, with other versions of OpenMPI (&gt;2).<=
/p><p>Going back again to Ralph Castain mail to this thread, I would expect=
 full support for interoperability with different PMIx versions (&gt;1.2) t=
hrough PMIx &gt; 2.1 (not yet released)<br></p><p>Some questions about this=
 experiments and conclusions are:<br></p><ul><li>What do you think about th=
is results?=C2=A0 Do you have any suggestion? I&#39;m missing something?<br=
></li><li>are these results aligned with your expectations?</li><li>I know =
that PMIx 2.1 is being developed but, any version is already available to c=
heck? How can I get it?<br></li><li>The SEGFAULT I get with=C2=A0 OpenMPI 3=
.0.0.rcX is something already tracked?</li></ul><p>Hope to helpful!</p><p>B=
R,</p><p>V=C3=ADctor<br></p></div></div></div></div></div><div><div><div><d=
iv><div><div><div><br><br></div></div></div></div></div></div></div></div><=
div class=3D"gmail_extra"><br><div class=3D"gmail_quote">2017-07-14 1:34 GM=
T+02:00 Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailto:gmku...@=
gmail.com" target=3D"_blank">gmku...@gmail.com</a>&gt;</span>:<br><blockquo=
te class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc so=
lid;padding-left:1ex"><div dir=3D"ltr">Hi Victor,<div><br></div><div>The ar=
e of ABI compatibility I am referring to is with the container&#39;s underl=
ying library stack. Meaning that if you link in the libraries compiled on t=
he host, and the container you want to run is newer then what is installed =
on the host, (or potentially vise-versa), you may end up with a conflict be=
tween the binary and library.</div><div><br></div><div>This is what Nvidia =
has mitigated by building their library on a very recent toolchain, thus th=
e libraries are backwards compatible with older binaries.</div><div><br></d=
iv><div>Does that make sense?</div><div><br></div><div>Greg</div><div><div =
class=3D"h5"><div><br></div><div><br><div class=3D"gmail_extra"><br><div cl=
ass=3D"gmail_quote">On Wed, Jul 12, 2017 at 4:50 AM, victor sv <span dir=3D=
"ltr">&lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@gm=
ail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D=
"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D=
"ltr">Hi Greg and Ralph,<br><br>yes Greg, I agree with you that the mention=
ed strategy could be dangerous and goes against the principals of containme=
nt.<br><br>sorry
 for the basic question ... but what do you mean with ABI compatible=20
containers? which components of the container environment are involved=20
with this ABI compatibility?<br><br>If we talk about libc or the kernel=20
itself, as you say in your web page, &quot;If you require kernel dependent=
=20
features, a container platform is probably not the right solution for=20
you.&quot;<br><br>If we focus on OpenMPI ABI compatibility, I figure out th=
at
 the variables involved in this compatibility could be (1) the compiler=20
(vendor) and (2) the OpenMPI library itself.<br><br>I&#39;m right or I&#39;=
m missing any other variables?<br><br>An
 interesting project called ABI-tracker has performed an OpenMPI ABI=20
compatibility study that you can watch in the following link:<br><br><span =
class=3D"m_3715304667614162045m_-7566839740247864044gmail-Object" id=3D"m_3=
715304667614162045m_-7566839740247864044gmail-OBJ_PREFIX_DWT1429_com_zimbra=
_url"><a href=3D"https://abi-laboratory.pro/tracker/timeline/openmpi/" targ=
et=3D"_blank">https://abi-laboratory.pro/tra<wbr>cker/timeline/openmpi/</a>=
</span><br><br>I think that, at least for OpenMPI 2.X, altough been a dange=
rous approach, the ABI compatibility seems reasonable.<br><br>What do you t=
hink?<br><br>BR,<br>V=C3=ADctor.</div><div class=3D"m_3715304667614162045HO=
EnZb"><div class=3D"m_3715304667614162045h5"><div class=3D"gmail_extra"><br=
><div class=3D"gmail_quote">2017-07-11 16:55 GMT+02:00 Gregory M. Kurtzer <=
span dir=3D"ltr">&lt;<a href=3D"mailto:gmku...@gmail.com" target=3D"_blank"=
>gmku...@gmail.com</a>&gt;</span>:<br><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div d=
ir=3D"ltr">Hi Victor,<div><br></div><div>I will let Ralph comment on the OM=
PI versions and compatibilities, but regarding using the MPI host libraries=
 within a container is dangerous for the reason that you are mentioning. If=
 you are running ABI compatible containers with the host, then things *migh=
t* work as expected. But this breaks container portability, and goes agains=
t the principals of containment.</div><div><br></div><div>We do however do =
exactly this for the Nvidia driver libraries, but... Nvidia builds these li=
braries with careful attention on ABI compatibility such that these binary =
libraries are indeed reasonably portable across containers.</div><div><br><=
/div><div>The only way to do this portably is with using a launcher on the =
host, outside the container, to spin up the container and launch the MPI wi=
thin. PMIx is a fantastic approach to solving this.</div><div><br></div><di=
v>Hope that helps!</div><div><br></div><div>Greg</div><div><div class=3D"m_=
3715304667614162045m_-7566839740247864044h5"><div><br></div><div><br><div c=
lass=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Jul 11, 2017 at=
 6:03 AM, victor sv <span dir=3D"ltr">&lt;<a href=3D"mailto:vict...@gmail.c=
om" target=3D"_blank">vict...@gmail.com</a>&gt;</span> wrote:<br><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div dir=3D"ltr">Hi Greg and Ralph,<br><br>Thank you f=
or your precise and elaborated answers.<br><br>Only for confirmation and to=
 sum up some conclussions (if I understood correctly):<br><br>=C2=A0- OpenM=
PI process management compatibility depends on PMIx.<br>=C2=A0- OpenMPI (an=
d also Slurm) complete=C2=A0 backward/forward compatibility will come (hope=
fully) in the future by means of PMIx 2.1.<br>=C2=A0- Nowadays, there exist=
s compatibility with OpenMPI 2.X if we compile it with default PMIx (1.X) s=
upport.<br>=C2=A0- OpenMPI 2.1 must be compiled with --disable-pmix-dstore =
due to a compatibility break.<br>=C2=A0- OpenMPI 1.X does not suppot PMIx a=
nd we can ignore it from this thread.<br><br>I&#39;m right?<br><br>I&#39;m =
interested in performing the tests you purpose. I will try to build all thr=
ee OMPI versions (2.0, 2.1 and 3.0) against the same PMIx external library =
to check the compatibility. Which PMIx version (1.2.0, 1.2.1 or 1.2.2 ) do =
you recommend as a start point? <br><br>I will report this results ASAP to =
this thread.<br><br>On the other hand, although we are planning to add supp=
ort to PMIx, unfortunately, our Slurm version (14.11.10-Bull.1.0) does not =
support it yet.<br><br>The second strategy we are testing to get compatibil=
ity between OpenMPI inside and outside a Singularity container relies on re=
placing the OpenMPI libraries inside the container by the host libraries hi=
erarchy.<br><br>This approach rest upon the assumption that OpenMPI symbols=
 and data structures are compatible through several versions of OpenMPI. At=
 least combining several releases that share the same major version.<br><br=
>Although the empirical tests of this approach seem to work properly with s=
ome tests, benchmarks and real apps, I&#39;m afraid of getting unexepected =
errors/warnings (segfaults, data errors, etc.) in the future.<br><br>What d=
o you think about this approach?<br><br>Can you confirm that OpenMPI is com=
patible in this way?<br><br>Finally, I think this thread could be very inte=
resting for other users too and I would like to keep it alive with your hel=
p.<br><br>Thank you again for your support!<br><br>BR,<br>V=C3=ADctor<br></=
div><div class=3D"m_3715304667614162045m_-7566839740247864044m_-45928511472=
06327630HOEnZb"><div class=3D"m_3715304667614162045m_-7566839740247864044m_=
-4592851147206327630h5"><div class=3D"gmail_extra"><br><div class=3D"gmail_=
quote">2017-07-09 23:45 GMT+02:00 Gregory M. Kurtzer <span dir=3D"ltr">&lt;=
<a href=3D"mailto:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a=
>&gt;</span>:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8=
ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hiya Victo=
r, et al.,=C2=A0<div><br></div><div>I didn&#39;t realize this but Ralph had=
 to drop off of the Singularity list. Hopefully we will get him back again,=
 as he is a fantastic resource for all of the OMPI questions and always a g=
reat source of information and ideas (poke, poke Ralph!). Ralph did send me=
 this in response to the previous email hoping it helps to explain things:<=
/div><div><br></div><div><br></div><div>On Sun, Jul 9, 2017 at 2:22 PM, <a =
href=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>=
=C2=A0<span dir=3D"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org" target=3D"=
_blank">rhc@open-mpi<wbr>.org</a>&gt;</span>=C2=A0wrote:<br><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid =
rgb(204,204,204);padding-left:1ex"></blockquote></div><div><div style=3D"fo=
nt-size:12.8px">...</div><div style=3D"font-size:12.8px">You are welcome to=
 forward the following to the list:</div><div style=3D"font-size:12.8px"><b=
r></div><div style=3D"font-size:12.8px">As Greg said, we have been concerne=
d about this since we started looking at Singularity support. Just for clar=
ity, the version of PMI OMPI uses is PMIx (<a href=3D"https://pmix.github.i=
o/pmix/" target=3D"_blank">https://pmix.github.io/pmix/</a>)<wbr>. While ou=
r plan from the beginning was to support cross-versions specifically to add=
ress this problem, we fell behind on its implementation due to priorities. =
We just committed the code to the PMIx repo in the last week, and it won=E2=
=80=99t be released into production for a few months while we shake it down=
.</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-size:12=
.8px">I fear it will be impossible to get the OMPI 1.10 series to work with=
 anything other than itself as it pre-dates PMIx.</div><div style=3D"font-s=
ize:12.8px"><br></div><div style=3D"font-size:12.8px">The OMPI 2.0 and 2.1 =
series should work across each other as they both include PMIx 1.x. However=
, you probably will need to configure the 2.1 series with --disable-pmix-ds=
tore as there was an unintended compatibility break there (the shared memor=
y store was added during the PMIx 1.x series and we didn=E2=80=99t catch th=
e compatibility break it introduced).</div><div style=3D"font-size:12.8px">=
<br></div><div style=3D"font-size:12.8px">Looking into the future, OMPI 3.0=
 is about to be released. It includes PMIx 2.0, which isn=E2=80=99t backwar=
ds compatible at this time, and so it won=E2=80=99t cross-version with OMPI=
 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We haven=E2=80=99t tested this, but =
one thing you could try is to build all three OMPI versions against the sam=
e PMIx external library (you would probably have to experiment a bit with P=
MIx versions to see which works across the different OMPI versions as the g=
lue between the two also changed a bit). This will ensure that the shared m=
emory store in PMIx is compatible across the versions, and things should wo=
rk since OMPI doesn=E2=80=99t care how the data is moved across the host-co=
ntainer boundary.</div><div style=3D"font-size:12.8px"><br></div><div style=
=3D"font-size:12.8px">As I said, we will be adding cross-version support to=
 the PMIx release series soon, without changing the API, that will ensure s=
upport across all PMIx versions starting with v1.2. Thus, you could (once t=
hat happens) build OMPI 2.0, 2.1, and 3.0 against the new PMIx release (pro=
bably PMIx v2.1.0) and the resulting containers would be future-proof as OM=
PI moves ahead. The RMs plan to follow that path as well, so you should be =
in good shape once this is done if you prefer to =E2=80=9Cdirect launch=E2=
=80=9D your containers (e.g., =E2=80=9Csrun ./mycontainer=E2=80=9D under SL=
URM).</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-siz=
e:12.8px">Sorry if that is all confusing - we sometimes get lost in the num=
bering schemes between OMPI and PMIx ourselves. Feel free to contact me dir=
ectly, or on the OMPI or PMIx mailing lists, if you have more questions or =
encounter problems. We definitely want to make this work.</div><div style=
=3D"font-size:12.8px"><br></div><div style=3D"font-size:12.8px">Ralph</div>=
</div></div><div class=3D"m_3715304667614162045m_-7566839740247864044m_-459=
2851147206327630m_-1840755824492384681HOEnZb"><div class=3D"m_3715304667614=
162045m_-7566839740247864044m_-4592851147206327630m_-1840755824492384681h5"=
><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Sun, Jul 9, 2=
017 at 12:19 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailto=
:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a>&gt;</span> wrot=
e:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-l=
eft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Victor,<div><br></=
div><div>Sorry for the latency, I&#39;m on email overload.</div><div><br></=
div><div>Open MPI uses PMI to communicate both inside and outside of the co=
ntainer. Ralph Castain (on this list, but possibly not monitoring actively)=
 is leading the PMI effort and he is an active Open MPI developer. We have =
had several talks about how to achieve &quot;hetero-versionistic&quot; comp=
atibility through the PMI handshake. I was under the impression that PMI no=
w supports that, as long as you are running equal or newer version on the h=
ost (outside the container). Also, I don&#39;t know what version of PMI thi=
s feature was introduced in, nor do I know what version of Open MPI include=
s that compatibility.</div><div><br></div><div>I have CC&#39;ed Ralph, and =
hopefully he will be able to offer some suggestions.</div><div><br></div><d=
iv>Regarding your question about supporting the MPI libraries in the same m=
anner that we are doing the Nvidia libraries, that would be hard. Nvidia sp=
ecifically builds their libraries to be as generally compatible as possible=
 (e.g. the same libraries/binaries work on a large array of Linux distribut=
ions). Most people do not build host libraries in a manner that would be ge=
nerally compatible as Nvidia does.</div><div><br></div><div>Hope that helps=
!</div><div><br></div><div>Greg</div><div><br></div><div><br></div></div><d=
iv class=3D"gmail_extra"><div><div class=3D"m_3715304667614162045m_-7566839=
740247864044m_-4592851147206327630m_-1840755824492384681m_62153731370335845=
22h5"><br><div class=3D"gmail_quote">On Mon, Jul 3, 2017 at 2:07 AM, victor=
 sv <span dir=3D"ltr">&lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_b=
lank">vict...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail=
_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:=
1ex"><div dir=3D"ltr">Dear Singularity team,<br><br>first of all, thanks fo=
r the great work with Singularity. It looks amazing!<br><br>Sorry if this t=
opic is duplicated and for the length of the email, but I want to share my =
experience about Singularity and OpenMPI compatibility, and also ask some q=
uestions.<br><br>I&#39;ve being reading a lot about OpenMPI and Singularity=
 compatibility because we are trying to find the generic way to run OpenMPI=
 applications within Singularity containers. It was not so clear (for me) i=
n the documentation, forums and mailing lists, and this is why we&#39;ve pe=
rformed an OpenMPI empiric compatibility study.<br><br>We ran these compari=
sons in CESGA FinisTerrae II cluster (<a href=3D"https://www.cesga.es/en/in=
fraestructuras/computacion/FinisTerrae2" target=3D"_blank">https://www.cesg=
a.es/en/infra<wbr>estructuras/computacion/FinisT<wbr>errae2</a>).<br><br>We=
 used several versions of OpenMPI. The chosen versions of OpenMPI were the =
versions already installed in the cluster:<br><br>- openmpi/1.10.2<br>- ope=
nmpi/2.0.0<br>- openmpi/2.0.1<br>- openmpi/2.0.2<br>- openmpi/2.1.1<br><br>=
We have created Singularity images containing the same versions of OpenMPI =
and with the basic OpenMPI ring example. I share the bootstrap definition f=
ile template used below:<br><br>```<br>BootStrap: docker<br>From: ubuntu:16=
.04<br>IncludeCmd: yes<br><br>%post<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sed -i &#39;s/main/main restricted universe/g&#39; /etc/apt/sources.=
list<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get update<br>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get install -y bash git wget build=
-essential gcc time libc6-dev libgcc-5-dev<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 apt-get install -y dapl2-utils libdapl-dev libdapl2 libibve=
rbs1 librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 lib=
nes1 libpmi0 libpmi0-dev libslurm29 libslurm-dev<br><br>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 ##Install OpenMPI<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget =
&#39;<a href=3D"https://www.open-mpi.org/software/ompi/vX.X/downloads/openm=
pi-X.X.X.tar.gz" target=3D"_blank">https://www.open-mpi.org/soft<wbr>ware/o=
mpi/vX.X/downloads/openm<wbr>pi-X.X.X.tar.gz</a>&#39; -O openmpi-X.X.X.tar.=
gz<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 tar -xzf openmpi-X.X.X.tar=
.gz -C openmpi-X.X.X<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mkdir -p=
 /tmp/openmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd =
/tmp/openmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 ../configure --enable-shared --enable-mpi-thread-multiple --with-verbs =
--enable-mpirun-prefix-by-defa<wbr>ult --with-hwloc --disable-dlopen --with=
-pmi --prefix=3D/usr<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 make all=
 install<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 # Install ring<b=
r>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 wget <a href=3D"https://raw.githubusercontent.com/=
open-mpi/ompi/master/examples/ring_c.c" target=3D"_blank">https://raw.githu=
busercontent.<wbr>com/open-mpi/ompi/master/examp<wbr>les/ring_c.c</a><br>=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mpicc ring_c.c -o /usr/bin/ring<=
br>```<br><br>Once the containers were created, we ran the ring app with mp=
irun using 2 cores of 2 different nodes mixing all possible combinations of=
 those OpenMPI versions inside and outside the container.<br><br>The obtain=
ed results shown that we need the same versions of OpenMPI inside and outsi=
de the container to succesfully run the contained application in parallel w=
ith mpirun.<br><br>Is this the expected behaviour or am I missing something=
?<br><br>Will be this the expected behaviour in the future (with future ver=
sions of OpenMPI)?<br><br>Currently, we have slurm 14.11.10-Bull.1.0 instal=
led as job scheduler at FinisTerrae II. We found the following tip/trick to=
 use srun as process manager:<br><br><a href=3D"http://singularity.lbl.gov/=
tutorial-gpu-drivers-open-mpi-mtls" target=3D"_blank">http://singularity.lb=
l.gov/tut<wbr>orial-gpu-drivers-open-mpi-mtl<wbr>s</a><br><br>In order to r=
un whatever Singularity image containing OpenMPI applications using Slurm, =
we&#39;ve adapted it to our infrastructure and checked the same test cases =
running them with srun. It seems that it&#39;s working properly (no real wo=
rld applications were tested yet).<br><br>What do you think about this stra=
tegy?<br>Can you confirm that it provides portability of singularity images=
 containing OpenMPI applications?<br><br>I think this strategy is similar t=
o the one you are following with &quot;--nv&quot; option=C2=A0 for NVidia d=
rivers.<br><br>Why not to do the same strategy with MPI, PMI, libibverbs, e=
tc.?<br><br>Thanks in advance and congrats again for your great work!<br><b=
r>V=C3=ADctor.<span class=3D"m_3715304667614162045m_-7566839740247864044m_-=
4592851147206327630m_-1840755824492384681m_6215373137033584522m_83128680028=
30329868HOEnZb"><font color=3D"#888888"><br></font></span></div><span class=
=3D"m_3715304667614162045m_-7566839740247864044m_-4592851147206327630m_-184=
0755824492384681m_6215373137033584522m_8312868002830329868HOEnZb"><font col=
or=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div></div=
></div><span class=3D"m_3715304667614162045m_-7566839740247864044m_-4592851=
147206327630m_-1840755824492384681m_6215373137033584522HOEnZb"><font color=
=3D"#888888">-- <br><div class=3D"m_3715304667614162045m_-75668397402478640=
44m_-4592851147206327630m_-1840755824492384681m_6215373137033584522m_831286=
8002830329868gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtze=
r</div><div>CEO, SingularityWare, LLC.</div><div>Senior Architect, RStor</d=
iv><div><span style=3D"font-size:12.8px">Computational Science Advisor, Law=
rence Berkeley National Laboratory</span><br></div></div></div></div></div>=
</div></div>
</font></span></div>
</blockquote></div><br><br clear=3D"all"><div><br></div>-- <br><div class=
=3D"m_3715304667614162045m_-7566839740247864044m_-4592851147206327630m_-184=
0755824492384681m_6215373137033584522gmail_signature" data-smartmail=3D"gma=
il_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr">=
<div>Gregory M. Kurtzer</div><div>CEO, SingularityWare, LLC.</div><div>Seni=
or Architect, RStor</div><div><span style=3D"font-size:12.8px">Computationa=
l Science Advisor, Lawrence Berkeley National Laboratory</span><br></div></=
div></div></div></div></div></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"m_3715304667614162045m_-7566839740247864044m_-459285114720632=
7630gmail_signature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><d=
iv><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div=
>CEO, SingularityWare, LLC.</div><div>Senior Architect, RStor</div><div><sp=
an style=3D"font-size:12.8px">Computational Science Advisor, Lawrence Berke=
ley National Laboratory</span><br></div></div></div></div></div></div></div=
>
</div></div></div></div></div><div class=3D"m_3715304667614162045m_-7566839=
740247864044HOEnZb"><div class=3D"m_3715304667614162045m_-75668397402478640=
44h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"m_3715304667614162045gmail_signature" data-smartmail=3D"gmail=
_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><d=
iv>Gregory M. Kurtzer</div><div>CEO, SingularityWare, LLC.</div><div>Senior=
 Architect, RStor</div><div><span style=3D"font-size:12.8px">Computational =
Science Advisor, Lawrence Berkeley National Laboratory</span><br></div></di=
v></div></div></div></div></div>
</div></div></div></div></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--001a1147b03cb2d72705578ff95b--

--001a1147b03cb2d72c05578ff95d
Content-Type: application/octet-stream; name="container_bootstrap.def"
Content-Disposition: attachment; filename="container_bootstrap.def"
Content-Transfer-Encoding: base64
X-Attachment-Id: f_j6rlpu2a0
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--001a1147b03cb2d72c05578ff95d
Content-Type: application/x-sh; name="host_install.sh"
Content-Disposition: attachment; filename="host_install.sh"
Content-Transfer-Encoding: base64
X-Attachment-Id: f_j6rlpu3x1
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--001a1147b03cb2d72c05578ff95d--
