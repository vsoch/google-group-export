Date: Wed, 2 Oct 2019 12:28:58 -0700 (PDT)
From: Miguel Angel Salazar de Troya <salazar...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <4a96d449-cca7-4a66-b755-199949e09758@lbl.gov>
Subject: MPI inter node and Singularity
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_4091_1302992047.1570044538932"

------=_Part_4091_1302992047.1570044538932
Content-Type: multipart/alternative; 
	boundary="----=_Part_4092_1231447413.1570044538933"

------=_Part_4092_1231447413.1570044538933
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi, I am trying to run a simulation in my cluster. I have observed that 
simulations that require several nodes are actually slower than intra node 
simulations. From the little that I have heard, it might have to do with 
the MPI implementations at the host and within the container. The image I 
am running is below. It is based on this docker image 
(https://github.com/firedrakeproject/firedrake/blob/master/docker/Dockerfile.env). 
The MPI implementation in my cluster is mvapich2-2.2-intel-18.0.1, whereas 
inside the container seems to be mpich. Could this be an issue? Thanks.

Bootstrap: docker
From: firedrakeproject/firedrake

%post
    echo '. /home/firedrake/firedrake/bin/activate' >> 
$SINGULARITY_ENVIRONMENT
    ldconfig

%files
    WELCOME.Singularity /usr/local/share/WELCOME

%runscript
    cat /usr/local/share/WELCOME 
    exec /bin/bash -i


------=_Part_4092_1231447413.1570044538933
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi, I am trying to run a simulation in my cluster. I have =
observed that simulations that require several nodes are actually slower th=
an intra node simulations. From the little that I have heard, it might have=
 to do with the MPI implementations at the host and within the container. T=
he image I am running is below. It is based on this docker image (https://g=
ithub.com/firedrakeproject/firedrake/blob/master/docker/Dockerfile.env). Th=
e MPI implementation in my cluster is mvapich2-2.2-intel-18.0.1, whereas in=
side the container seems to be mpich. Could this be an issue? Thanks.<br><d=
iv style=3D"margin-left: 40px;"><br></div><div style=3D"margin-left: 40px;"=
>Bootstrap: docker<br>From: firedrakeproject/firedrake<br><br>%post<br>=C2=
=A0=C2=A0=C2=A0 echo &#39;. /home/firedrake/firedrake/bin/activate&#39; &gt=
;&gt; $SINGULARITY_ENVIRONMENT<br>=C2=A0=C2=A0=C2=A0 ldconfig<br><br>%files=
<br>=C2=A0=C2=A0=C2=A0 WELCOME.Singularity /usr/local/share/WELCOME<br><br>=
%runscript<br>=C2=A0=C2=A0=C2=A0 cat /usr/local/share/WELCOME <br>=C2=A0=C2=
=A0=C2=A0 exec /bin/bash -i<br></div><br></div>
------=_Part_4092_1231447413.1570044538933--

------=_Part_4091_1302992047.1570044538932--
