X-Received: by 10.107.20.139 with SMTP id 133mr11469087iou.22.1463155003855;
        Fri, 13 May 2016 08:56:43 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.50.217.131 with SMTP id oy3ls37770igc.18.gmail; Fri, 13 May
 2016 08:56:43 -0700 (PDT)
X-Received: by 10.98.35.212 with SMTP id q81mr24634958pfj.108.1463155003231;
        Fri, 13 May 2016 08:56:43 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id gh4si25413784pac.8.2016.05.13.08.56.43
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 13 May 2016 08:56:43 -0700 (PDT)
Received-SPF: pass (google.com: domain of rhc.o...@gmail.com designates 209.85.192.181 as permitted sender) client-ip=209.85.192.181;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of rhc.o...@gmail.com designates 209.85.192.181 as permitted sender) smtp.mailfrom=rhc.o...@gmail.com
X-Ironport-SBRS: 4.3
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FJAAA1+DVXj7XAVdFehQqDOqF3AQaIOYZuhHcBDYINAYcnOBQBAQEBAQEBAw8BAQEBBwsLCSEvgjc5EFUCK0ABAQEDARIRHQENLAMBCwEFBRgnAwICIRADAQUBCxEOBwQBHAICAYdzAw8IBaJAgTE+MYs7hEIFh2cnDYQvHQIGEIVNBgWCMwiBTIEDgkOBThEBgxwrgi4FjlOJIzGLY0KCA4FfF4Q4gnkOhVqHXIYnMIEOHgEBd4NXTgeHHoE1AQEB
X-IronPort-AV: E=Sophos;i="5.24,614,1455004800"; 
   d="scan'208,217";a="23939757"
Received: from mail-pf0-f181.google.com ([209.85.192.181])
  by fe3.lbl.gov with ESMTP; 13 May 2016 08:56:42 -0700
Received: by mail-pf0-f181.google.com with SMTP id 206so44020177pfu.0
        for <singu...@lbl.gov>; Fri, 13 May 2016 08:56:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=sender:from:message-id:mime-version:subject:date:references:to
         :in-reply-to;
        bh=qvXgR5SkjLq+p9+xbBQIEdT4rNEYEbwGU+ne+QZxRsY=;
        b=nc+pqW7JuD+EbfdT541deXor2Q3h+d99kDsG9iH6DIcjzdsdy3okU0UZe4ClGZKb7p
         JKd/h9/BYsLPZvECFvXVdljaF2D8Quyn/IKYwCtZIPyFoJiSUEJwvKaJESW0s7Vy7f3X
         HL59vS6Nr2jo+cWMOM+dzmk9T0bhuZNqlCoedKmIEMvUf8FUZvCM0mTJacqegVVeBgci
         QoG0d5yCsTW6XFEE2E3hIy2+kAUlodHbXnH/xifN3WQ4M4x6AbMrLzc8v7T3viWz8Bq0
         EVSAAACeur8DyHjRCF91F/DPsOFhRnv4vGcU6w2JOJV78wU4vMvXsDho/GGFDJGLhHwU
         z/FQ==
X-Gm-Message-State: AOPr4FU30IyOCwi2wJnvsNtmz2EagsBMmWbTmFPn33wEPH3nLCdIJkIVDuUU1+nTh+7Yxw==
X-Received: by 10.98.3.135 with SMTP id 129mr24101564pfd.42.1463155001931;
        Fri, 13 May 2016 08:56:41 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from [192.168.0.8] ([208.100.172.177])
        by smtp.gmail.com with ESMTPSA id 1sm28482643pah.7.2016.05.13.08.56.40
        for <singu...@lbl.gov>
        (version=TLSv1/SSLv3 cipher=OTHER);
        Fri, 13 May 2016 08:56:40 -0700 (PDT)
Sender: Ralph Castain <rhc.o...@gmail.com>
From: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary="Apple-Mail=_5A789B92-F45F-422D-BE7D-B1992492E760"
Message-Id: <E98FFA98-7B77-4C7A-8054-A952A23F6CB3@open-mpi.org>
Mime-Version: 1.0 (Mac OS X Mail 9.3 \(3124\))
Subject: Re: [Singularity] SIngularity and MPI implementations
Date: Fri, 13 May 2016 08:56:40 -0700
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov>
To: singularity@lbl.gov
In-Reply-To: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov>
X-Mailer: Apple Mail (2.3124)

--Apple-Mail=_5A789B92-F45F-422D-BE7D-B1992492E760
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

There are some significant differences between Shifter and Singularity, but=
 I=E2=80=99ll let Greg address those.

The OMPI support in 2.0 isn=E2=80=99t complete - we will be updating it in =
2.0.1. What it does is automatically detect that this is a Singularity cont=
ainer, setup the required paths to Singularity on the remote nodes, expand =
the container only once on each node, and then execute the specified number=
 of copies. This makes it a little easier to use (don=E2=80=99t have to wor=
ry about paths), and somewhat faster to launch the job. You won=E2=80=99t n=
otice the launch performance difference at small scale, but you will at lar=
ger scales.

 A container, of course, is just a static envelope surrounding your dynamic=
 application, and so you can avoid the IO node bottleneck that we=E2=80=99v=
e seen on large clusters when apps call dl_open. Back in the RoadRunner day=
s, we quantified and published the results of those studies - bottom line w=
as that you had to avoid dl_open at scale. Without a container, your only c=
hoice was to build your apps static, which meant you took a memory footprin=
t hit.

With a container and the OMPI support, you get the best of both worlds. You=
r container =E2=80=9Cenvelope=E2=80=9D comes down as a single blob (one per=
 node), thus alleviating the bottleneck. We then expand it out, spawn the l=
ocal procs - and they see shared libraries! Keeps the footprint down while =
improving the launch speed.

Singularity also automatically detects that this is an OMPI application you=
 are putting in the container, and ensures that all the required OMPI libra=
ries are included. Thus, the result is a complete =E2=80=9Cpackage=E2=80=9D=
 that can run on a remote node that doesn=E2=80=99t even have OMPI installe=
d on it. So you could, for example, use SLURM or some other resource manage=
r to directly launch the container onto nodes that don=E2=80=99t have OMPI =
installed on it.

In either case, we preserve the ability to use shared memory transports, so=
 the application performance is unaffected.

HTH
Ralph

> On May 13, 2016, at 8:12 AM, Taras Shapovalov <shapov...@gmail.com> wrote=
:
>=20
> Hey guys,
>=20
> I've heard many times that Singularity has a nice support in Open MPI 2.0=
, but could someone describes how exactly such integration affects the exec=
ution of MPI application? Older Open MPI and MPICH work in SAPP as well, so=
 I don't really get what Open MPI 2.0 brings us.
>=20
> Moreover I see MPI support in Singularity is positioned as one of the fea=
tures that is implemented better than in Shifter (correct?). But Shifter al=
so allows to run MPI apps, well, at least I see Cray runs MPICH in Shifter'=
s chroot (not sure about Open MPI though). Could you explain please what is=
 the difference (if any) between running, say, MPICH with Singularity vs ru=
nning it in Shifter (from HPC prospective of course)?
>=20
> Best regards,
>=20
> Taras
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.


--Apple-Mail=_5A789B92-F45F-422D-BE7D-B1992492E760
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html charset=
=3Dutf-8"></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space;" class=3D"">There are some sig=
nificant differences between Shifter and Singularity, but I=E2=80=99ll let =
Greg address those.<div class=3D""><br class=3D""></div><div class=3D"">The=
 OMPI support in 2.0 isn=E2=80=99t complete - we will be updating it in 2.0=
.1. What it does is automatically detect that this is a Singularity contain=
er, setup the required paths to Singularity on the remote nodes, expand the=
 container only once on each node, and then execute the specified number of=
 copies. This makes it a little easier to use (don=E2=80=99t have to worry =
about paths), and somewhat faster to launch the job. You won=E2=80=99t noti=
ce the launch performance difference at small scale, but you will at larger=
 scales.</div><div class=3D""><br class=3D""></div><div class=3D"">&nbsp;A =
container, of course, is just a static envelope surrounding your dynamic ap=
plication, and so you can avoid the IO node bottleneck that we=E2=80=99ve s=
een on large clusters when apps call dl_open. Back in the RoadRunner days, =
we quantified and published the results of those studies - bottom line was =
that you had to avoid dl_open at scale. Without a container, your only choi=
ce was to build your apps static, which meant you took a memory footprint h=
it.</div><div class=3D""><br class=3D""></div><div class=3D"">With a contai=
ner and the OMPI support, you get the best of both worlds. Your container =
=E2=80=9Cenvelope=E2=80=9D comes down as a single blob (one per node), thus=
 alleviating the bottleneck. We then expand it out, spawn the local procs -=
 and they see shared libraries! Keeps the footprint down while improving th=
e launch speed.</div><div class=3D""><br class=3D""></div><div class=3D"">S=
ingularity also automatically detects that this is an OMPI application you =
are putting in the container, and ensures that all the required OMPI librar=
ies are included. Thus, the result is a complete =E2=80=9Cpackage=E2=80=9D =
that can run on a remote node that doesn=E2=80=99t even have OMPI installed=
 on it. So you could, for example, use SLURM or some other resource manager=
 to directly launch the container onto nodes that don=E2=80=99t have OMPI i=
nstalled on it.</div><div class=3D""><br class=3D""></div><div class=3D"">I=
n either case, we preserve the ability to use shared memory transports, so =
the application performance is unaffected.</div><div class=3D""><br class=
=3D""></div><div class=3D"">HTH</div><div class=3D"">Ralph</div><div class=
=3D""><br class=3D""><div><blockquote type=3D"cite" class=3D""><div class=
=3D"">On May 13, 2016, at 8:12 AM, Taras Shapovalov &lt;<a href=3D"mailto:s=
hapov...@gmail.com" class=3D"">shapov...@gmail.com</a>&gt; wrote:</div><br =
class=3D"Apple-interchange-newline"><div class=3D""><div dir=3D"ltr" class=
=3D"">Hey guys,<br class=3D""><br class=3D"">I've heard many times that Sin=
gularity has a nice support in Open MPI 2.0, but could someone describes ho=
w exactly such integration affects the execution of MPI application? Older =
Open MPI and MPICH work in SAPP as well, so I don't really get what Open MP=
I 2.0 brings us.<br class=3D""><br class=3D"">Moreover I see MPI support in=
 Singularity is positioned as one of the features that is implemented bette=
r than in Shifter (correct?). But Shifter also allows to run MPI apps, well=
, at least I see Cray runs MPICH in Shifter's chroot (not sure about Open M=
PI though). Could you explain please what is the difference (if any)  betwe=
en running, say, MPICH with Singularity vs running it in Shifter (from HPC =
prospective of course)?<br class=3D""><br class=3D"">Best regards,<br class=
=3D""><br class=3D"">Taras<br class=3D""></div><div class=3D""><br class=3D=
"webkit-block-placeholder"></div>

-- <br class=3D"">
You received this message because you are subscribed to the Google Groups "=
singularity" group.<br class=3D"">
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" class=3D"">singu...@lbl.gov</a>=
.<br class=3D"">
</div></blockquote></div><br class=3D""></div></body></html>
--Apple-Mail=_5A789B92-F45F-422D-BE7D-B1992492E760--
