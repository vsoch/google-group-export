X-Received: by 10.107.37.1 with SMTP id l1mr12268153iol.2.1465419382607;
        Wed, 08 Jun 2016 13:56:22 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.197.132 with SMTP id v126ls266765iof.21.gmail; Wed, 08 Jun
 2016 13:56:22 -0700 (PDT)
X-Received: by 10.107.191.7 with SMTP id p7mr12727005iof.115.1465419382090;
        Wed, 08 Jun 2016 13:56:22 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id d63si3202881pfd.93.2016.06.08.13.56.21
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 08 Jun 2016 13:56:21 -0700 (PDT)
Received-SPF: pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.53 as permitted sender) client-ip=209.85.220.53;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.53 as permitted sender) smtp.mailfrom=rhc.o...@gmail.com
X-Ironport-SBRS: 2.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2HNAgAFhVhXfTXcVdFehBR9gzyjBQEBAQaDW4Rqi3+BOD4bAQaHPDoSAQEBAQEBAQMPAQEJCwsJIS+COjkKBiwBAQEBAQEBAQEBAQEBAQEaAisTEhsBAQEDARIICR0BDSwDAQsBBQUYIAcDAgIhEAMBBQELEQ4HBAEcBAGHcwMPCAWfE4ExPjGLO4RCBYgXJw2ECgEBARsCBhCFTwYFgjQIgk6CQ4FPEQGDHSuCLgWHfAGGYD6EIIRgNIYDhimEMYZ/DoVbh3WGLTCBDw8WAWOBDUUcgWtOB4hEBAsXgR4BAQE
X-IronPort-AV: E=Sophos;i="5.26,441,1459839600"; 
   d="scan'208,217";a="26500462"
Received: from mail-pa0-f53.google.com ([209.85.220.53])
  by fe3.lbl.gov with ESMTP; 08 Jun 2016 13:56:20 -0700
Received: by mail-pa0-f53.google.com with SMTP id ec8so5755622pac.0
        for <singu...@lbl.gov>; Wed, 08 Jun 2016 13:56:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=sender:from:message-id:mime-version:subject:date:references:to
         :in-reply-to;
        bh=lUFyg5OrF+KB6wzeCT2b8IVN4iwzddlcFKpGOnS36vw=;
        b=ybOfrNCFHAGrr8Hg9ODVOVtnOtNs69R2GpE1MJo/pBDpuJkOabGQl6jdyF4XoJhUEq
         Yg9ajydpQWKR+dKeNIouHxDxFLyxS6DYSTpVNC3jW2p8DuYqwvWnAA0SiHKWn537xjbJ
         ajZvav0jTuENbUYAMY9rNbHt317B1tEIhs4cLT5NmwlvPgTiv8EOV1tAushdRqBq766N
         jvZMbqSdbeuQEIph+uRW+1XnEImeKY5RPXjAy5JGdTEC8xZ0Ms3uvYvfio9gcX2yoca+
         ShuBdLUWcUL6JH9WlJ4dPuzO1yV1+UT2c4m0MpUUvDfwPDDDYrdplyr2aw2X9fe5ggfN
         mlqg==
X-Gm-Message-State: ALyK8tLBod5XEjMD/U3zeU0VJkW6/y7wZGTrH/emW7vM+QS0BU6utD7eAjxLHHfynd8t2A==
X-Received: by 10.66.127.10 with SMTP id nc10mr7698095pab.98.1465419379835;
        Wed, 08 Jun 2016 13:56:19 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from [192.168.0.6] ([208.100.172.177])
        by smtp.gmail.com with ESMTPSA id y2sm4488544pfi.39.2016.06.08.13.56.18
        for <singu...@lbl.gov>
        (version=TLSv1/SSLv3 cipher=OTHER);
        Wed, 08 Jun 2016 13:56:18 -0700 (PDT)
Sender: Ralph Castain <rhc.o...@gmail.com>
From: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary="Apple-Mail=_AE4494AC-3FD8-4EBB-86DA-87688E78A47E"
Message-Id: <83BD3FF3-0027-4C63-B601-FB153EA2091E@open-mpi.org>
Mime-Version: 1.0 (Mac OS X Mail 9.3 \(3124\))
Subject: Re: [Singularity] Singularity with Slurm and PMIx
Date: Wed, 8 Jun 2016 13:56:17 -0700
References: <CA+zw9q3gSaqZgo3tzOXyn5kwfeUS-LdCuyTnmWyPgfVjmxGBqA@mail.gmail.com> <CAN7etTz3V1eSH2r7Yd9y0MFFt24H2vuAB9FcViKbDAsdxtt9Pg@mail.gmail.com> <CA+zw9q0zaF0ajbLuPeT0H-Ag5t-ZfV9ux94hm+9m6XQ_JEQ7LA@mail.gmail.com>
To: singularity@lbl.gov
In-Reply-To: <CA+zw9q0zaF0ajbLuPeT0H-Ag5t-ZfV9ux94hm+9m6XQ_JEQ7LA@mail.gmail.com>
X-Mailer: Apple Mail (2.3124)

--Apple-Mail=_AE4494AC-3FD8-4EBB-86DA-87688E78A47E
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8


> On Jun 8, 2016, at 1:48 PM, yiannis georgiou <goh...@gmail.com> wrote:
>=20
> Hi Greg,
>=20
> bullseye!!! it worked for me like this:
>=20
> [georgioy@rio11 ~]$ salloc -n4 -N2
> [georgioy@rio11 ~]$ export SINGULARITY_NO_NAMESPACE_PID=3D1
> [georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N2 /usr/local/singularity_v2/=
bin/singularity exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openm=
p/mpi_hello
> Hello world from processor rio13, rank 2 out of 4 processors
> Hello world from processor rio13, rank 3 out of 4 processors
> Hello world from processor rio12, rank 0 out of 4 processors
> Hello world from processor rio12, rank 1 out of 4 processors
> ERROR: Could not clear loop device
>=20
> Any idea how I can correct the "ERROR: Could not clear loop device" appea=
ring at the end of the execution?
> And when you have a moment could you explain me how this magic environmen=
t variable make it work! I'm not sure I got it.
>=20
> One more question, I've noticed that there have been some changes in the =
latest OpenMPI to improve performance when using singularity. Are they done=
 for PMIx or for all PMI versions?

Only PMIx is supported, I=E2=80=99m afraid

>=20
> Thanks a lot!
> Yiannis
>=20
>=20
> On Wed, Jun 8, 2016 at 9:16 PM, Gregory M. Kurtzer <gmku...@lbl.gov <mail=
to:gmku...@lbl.gov>> wrote:
> Hi Yiannis,
>=20
> I have a quick thing to test... The address not mapped error seems consis=
tent with something else that I've seen when testing OpenMPI with shared me=
mory and the NEWPID namespace. Try to disable the NEWPID namespace by expor=
ting this environment variable:
>=20
> SINGULARITY_NO_NAMESPACE_PID=3D1
>=20
> Now you will need to export it in a place where all Singularity contexts =
will see it, maybe something like this:
>=20
> $ srun --mpi=3Dpmix -n 4 -N 2 SINGULARITY_NO_NAMESPACE_PID=3D1 singularit=
y exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>=20
> There is an OpenMPI plugin which sets this automatically in the current c=
odebase, but I'm not sure how it will come into play in this usage scenario=
.
>=20
> Let me know how that works for ya!
>=20
> Thanks,
>=20
> Greg
>=20
> On Wed, Jun 8, 2016 at 12:09 PM, yiannis georgiou <goh...@gmail.com <mail=
to:goh...@gmail.com>> wrote:
> Hello,
>=20
> I'm trying to execute an MPI application from within a singularity image =
through Slurm and pmix using the following command=20
>=20
> srun --mpi=3Dpmix -n 4 -N 2 singularity exec /tmp/ubuntu_v4.img /home_nfs=
/georgioy/BENCHS/mpi-openmp/mpi_hello
>=20
> The slurmstepd process is spawned on the compute node and it seems to hav=
e launched correctly the application from what we see on one of the compute=
 nodes:
>=20
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> [root@rio12 ~]# ps -aux
> root     35147  0.1  0.0 562284  4016 ?        Sl   20:06   0:00 slurmste=
pd: [83.0]                 =20
> georgioy 35158  0.2  0.0  10428   896 ?        S    20:06   0:00 Singular=
ity: namespace                              /home_nfs/georgioy/BENCHS/mpi-o=
penmp/mpi_hello
> root     35162  0.0  0.0      0     0 ?        S<   20:06   0:00 [loop0]
> georgioy 35163  0.0  0.0  10428   400 ?        S    20:06   0:00 Singular=
ity: exec                                   /home_nfs/georgioy/BENCHS/mpi-o=
penmp/mpi_hello
> root     35164  0.0  0.0      0     0 ?        S    20:06   0:00 [jbd2/lo=
op0-8]
> root     35165  0.0  0.0      0     0 ?        S    20:06   0:00 [ext4-di=
o-unwrit]
> georgioy 35166  102  0.0 285476 14828 ?        RLl  20:06   0:12 /home_nf=
s/georgioy/BENCHS/mpi-openmp/mpi_hello
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>=20
> and I see the pmix related functions starting correctly in the slurmd log=
 files :
>=20
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>=20
> [2016-06-08T20:12:08.004] [86.0] debug:  (null) [0] mpi_pmix.c:90 [p_mpi_=
hook_slurmstepd_prefork] mpi/pmix: start
> [2016-06-08T20:12:08.004] [86.0] debug:  mpi/pmix: setup sockets
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:78 [err=
handler_reg_callbk] mpi/pmix: Error handler registration callback is called=
 with status=3D0, ref=3D0
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:581 [pm=
ixp_libpmix_job_set] mpi/pmix: task initialization
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:220 [_ag=
ent_thread] mpi/pmix: Start agent thread
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:313 [pmi=
xp_agent_start] mpi/pmix: agent thread started: tid =3D 139672278615808
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84 [_con=
n_readable] mpi/pmix: fd =3D 9
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84 [_con=
n_readable] mpi/pmix: fd =3D 19
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:256 [_pm=
ix_timer_thread] mpi/pmix: Start timer thread
> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:335 [pmi=
xp_agent_start] mpi/pmix: timer thread started: tid =3D 139672277563136
>=20
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>=20
> However the application is never actually started and the srun fails comp=
letely with the following output:
>=20
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> [georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N 2 singularity exec /tmp/ubu=
ntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
> [rio12:00001] *** Process received signal ***
> [rio12:00001] Signal: Segmentation fault (11)
> [rio12:00001] Signal code: Address not mapped (1)
> [rio12:00001] Failing at address: 0x7fbd937e6010
> [rio12:00001] [ 0] [rio12:00001] *** Process received signal ***
> /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f26aeb233d0]
> [rio12:00001] [ 1] /usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f269f=
964035]
> [rio12:00001] [ 2] /usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_c=
reate+0xa3)[0x7f269f964583]
> [rio12:00001] [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_pr=
ocs+0x5f1)[0x7f269fb68771]
> [rio12:00001] [ 4] /usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f26a4=
51fbe7]
> [rio12:00001] [ 5] /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_=
procs+0xd2)[0x7f269ef35662]
> [rio12:00001] [ 6] /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f26=
aed75d31]
> [rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26aed9bd=
b9]
> [rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007e=
c]
> [rio12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0=
)[0x7f26ae769830]
> [rio12:00001] Signal: Segmentation fault (11)
> [rio12:00001] Signal code: Address not mapped (1)
> [rio12:00001] Failing at address: 0x7fbd937e6010
> [rio12:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x40070=
9]
> [rio12:00001] *** End of error message ***
> [rio12:00001] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f3f=
89c063d0]
> [rio12:00001] [ 1] /usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f3f7e=
b1d035]
> [rio12:00001] [ 2] /usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_c=
reate+0xa3)[0x7f3f7eb1d583]
> [rio12:00001] [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_pr=
ocs+0x5f1)[0x7f3f7ed21771]
> [rio12:00001] [ 4] /usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f3f7f=
5f3be7]
> [rio12:00001] [ 5] /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_=
procs+0xd2)[0x7f3f7e0ee662]
> [rio12:00001] [ 6] /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f3f=
89e58d31]
> [rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7ed=
b9]
> [rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007e=
c]
> [rio12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0=
)[0x7f3f8984c830]
> [rio12:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x40070=
9]
> [rio12:00001] *** End of error message ***
> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ER=
ROR: Error handler invoked: status =3D -25, nranges =3D 0: Success (0)
> srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
> slurmstepd: error: *** STEP 86.0 ON rio12 CANCELLED AT 2016-06-08T20:12:0=
8 ***
> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ER=
ROR: Error handler invoked: status =3D -25, nranges =3D 0: Success (0)
> slurmstepd: error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix: ER=
ROR: Error handler invoked: status =3D -25, nranges =3D 0: Success (0)
> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ER=
ROR: Error handler invoked: status =3D -25, nranges =3D 0: Success (0)
> srun: error: rio12: tasks 0-2: Killed
> srun: error: rio13: task 3: Killed
>=20
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>=20
> the results I get when launching with mpirun instead of srun are more or =
less the same.=20
>=20
> From my understanding, the orted process, or in my case slurmstepd proces=
s that launches the singularity container=20
> and the MPI application, enables the communication of MPI libraries and o=
rted (or slurmstepd) through PMI (in my case here PMIx).
> So I suppose the problem I see should be related with the mapping of PMI =
from the container towards the orted or slurmstepd.
>=20
> What do you think?
>=20
> To give some more details, my singularity container has OpenMPI and PMIx =
installed but not Slurm. I don't think that Slurm needs to reside within th=
e container in that context.
> But I wasn't sure if OpenMPI and PMIx are needed both inside and outside =
of the container.
> So, I've tried using the exact same version of PMIx and OpenMPI in and ou=
t of the container and the problem still persists.=20
> The experiments have been done using RedHat hosts with CentOS or Ubuntu s=
ingularity containers and the latest github versions of slurm, OpenMPI, PMI=
x and singularity.
>=20
> By the way, do you have any MPI example for singularity v2?=20
> Because the MPI example that you show here : http://singularity.lbl.gov/#=
hpc <http://singularity.lbl.gov/#hpc>
>=20
> is actually done using singularity v1, no?=20
> In my understanding with singularity v2 we actually build a complete imag=
e with OS not just the application with its libraries. Is this correct?
>=20
> Sorry for the long email and
> thanks a lot for any extra info you can provide regarding singularity v2 =
and MPI.
>=20
> Best Regards,
> Yiannis
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.
>=20
>=20
>=20
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.
>=20
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.


--Apple-Mail=_AE4494AC-3FD8-4EBB-86DA-87688E78A47E
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html charset=
=3Dutf-8"></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space;" class=3D""><br class=3D""><di=
v><blockquote type=3D"cite" class=3D""><div class=3D"">On Jun 8, 2016, at 1=
:48 PM, yiannis georgiou &lt;<a href=3D"mailto:goh...@gmail.com" class=3D""=
>goh...@gmail.com</a>&gt; wrote:</div><br class=3D"Apple-interchange-newlin=
e"><div class=3D""><div dir=3D"ltr" class=3D""><div class=3D""><div class=
=3D""><div class=3D""><div class=3D""><div class=3D""><div class=3D"">Hi Gr=
eg,<br class=3D""><br class=3D""></div>bullseye!!! it worked for me like th=
is:<br class=3D""><br class=3D"">[georgioy@rio11 ~]$ salloc -n4 -N2<br clas=
s=3D"">[georgioy@rio11 ~]$ export SINGULARITY_NO_NAMESPACE_PID=3D1<br class=
=3D"">[georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N2 /usr/local/singularity=
_v2/bin/singularity exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-o=
penmp/mpi_hello<br class=3D"">Hello world from processor rio13, rank 2 out =
of 4 processors<br class=3D"">Hello world from processor rio13, rank 3 out =
of 4 processors<br class=3D"">Hello world from processor rio12, rank 0 out =
of 4 processors<br class=3D"">Hello world from processor rio12, rank 1 out =
of 4 processors<br class=3D"">ERROR: Could not clear loop device<br class=
=3D""><br class=3D""></div>Any idea how I can correct the "ERROR: Could not=
 clear loop device" appearing at the end of the execution?<br class=3D""></=
div>And when you have a moment could you explain me how this magic environm=
ent variable make it work! I'm not sure I got it.<br class=3D""><br class=
=3D""></div>One more question, I've noticed that there have been some chang=
es in the latest OpenMPI to improve performance when using singularity. Are=
 they done for PMIx or for all PMI versions?<br class=3D""></div></div></di=
v></div></blockquote><div><br class=3D""></div>Only PMIx is supported, I=E2=
=80=99m afraid</div><div><br class=3D""><blockquote type=3D"cite" class=3D"=
"><div class=3D""><div dir=3D"ltr" class=3D""><div class=3D""><div class=3D=
""><br class=3D""></div>Thanks a lot!<br class=3D""></div>Yiannis<br class=
=3D""><div class=3D""><div class=3D""><div class=3D""><div class=3D""><div =
class=3D""><div class=3D""><div class=3D""><div class=3D""><br class=3D""><=
/div></div></div></div></div></div></div></div></div><div class=3D"gmail_ex=
tra"><br class=3D""><div class=3D"gmail_quote">On Wed, Jun 8, 2016 at 9:16 =
PM, Gregory M. Kurtzer <span dir=3D"ltr" class=3D"">&lt;<a href=3D"mailto:g=
mku...@lbl.gov" target=3D"_blank" class=3D"">gmku...@lbl.gov</a>&gt;</span>=
 wrote:<br class=3D""><blockquote class=3D"gmail_quote" style=3D"margin:0 0=
 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr" class=
=3D"">Hi Yiannis,<div class=3D""><br class=3D""></div><div class=3D"">I hav=
e a quick thing to test... The address not mapped error seems consistent wi=
th something else that I've seen when testing OpenMPI with shared memory an=
d the NEWPID namespace. Try to disable the NEWPID namespace by exporting th=
is environment variable:</div><div class=3D""><br class=3D""></div><div cla=
ss=3D"">SINGULARITY_NO_NAMESPACE_PID=3D1<br class=3D""></div><div class=3D"=
"><br class=3D""></div><div class=3D"">Now you will need to export it in a =
place where all Singularity contexts will see it, maybe something like this=
:</div><div class=3D""><br class=3D""></div><div class=3D"">$ srun --mpi=3D=
pmix -n 4 -N 2 SINGULARITY_NO_NAMESPACE_PID=3D1 singularity exec /tmp/ubunt=
u_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello</div><div class=3D"=
"><br class=3D""></div><div class=3D"">There is an OpenMPI plugin which set=
s this automatically in the current codebase, but I'm not sure how it will =
come into play in this usage scenario.</div><div class=3D""><br class=3D"">=
</div><div class=3D"">Let me know how that works for ya!</div><div class=3D=
""><br class=3D""></div><div class=3D"">Thanks,</div><div class=3D""><br cl=
ass=3D""></div><div class=3D"">Greg</div></div><div class=3D"gmail_extra"><=
br class=3D""><div class=3D"gmail_quote"><div class=3D""><div class=3D"h5">=
On Wed, Jun 8, 2016 at 12:09 PM, yiannis georgiou <span dir=3D"ltr" class=
=3D"">&lt;<a href=3D"mailto:goh...@gmail.com" target=3D"_blank" class=3D"">=
goh...@gmail.com</a>&gt;</span> wrote:<br class=3D""></div></div><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div class=3D""><div class=3D"h5"><div dir=3D"ltr" cla=
ss=3D""><div class=3D""><div class=3D""><div class=3D"">Hello,<br class=3D"=
"><br class=3D""></div>I'm trying to execute an MPI application from within=
 a singularity image through Slurm and pmix using the following command <br=
 class=3D""><br class=3D""></div>srun --mpi=3Dpmix -n 4 -N 2 singularity ex=
ec
 /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br class=
=3D""><br class=3D""></div>The
 slurmstepd process is spawned on the compute node and it seems to have=20
launched correctly the application from what we see on one of the=20
compute nodes:<br class=3D""><div class=3D""><br class=3D"">=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br class=3D"">[root@rio12 ~]# ps -aux<br cla=
ss=3D"">root&nbsp;&nbsp;&nbsp;&nbsp; 35147&nbsp; 0.1&nbsp; 0.0 562284&nbsp;=
 4016 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sl&nbsp;&nbsp; 20:06&nbsp=
;&nbsp; 0:00 slurmstepd: [83.0]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n=
bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br class=3D"">g=
eorgioy
 35158&nbsp; 0.2&nbsp; 0.0&nbsp; 10428&nbsp;&nbsp; 896 ?&nbsp;&nbsp;&nbsp;&=
nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 20:06&nbsp;&nbsp; 0:00 Singular=
ity:=20
namespace&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nb=
sp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br class=3D"">root&nbsp;&nbs=
p;&nbsp;&nbsp; 35162&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nb=
sp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&lt;&n=
bsp;&nbsp; 20:06&nbsp;&nbsp; 0:00 [loop0]<br class=3D"">georgioy
 35163&nbsp; 0.0&nbsp; 0.0&nbsp; 10428&nbsp;&nbsp; 400 ?&nbsp;&nbsp;&nbsp;&=
nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;&nbsp;&nbsp; 20:06&nbsp;&nbsp; 0:00 Singular=
ity:=20
exec&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=
;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&n=
bsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=20
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br class=3D"">root&nbsp;&nbs=
p;&nbsp;&nbsp; 35164&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0&nb=
sp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&nbsp;=
&nbsp;&nbsp; 20:06&nbsp;&nbsp; 0:00 [jbd2/loop0-8]<br class=3D"">root&nbsp;=
&nbsp;&nbsp;&nbsp; 35165&nbsp; 0.0&nbsp; 0.0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =
0&nbsp;&nbsp;&nbsp;&nbsp; 0 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S&n=
bsp;&nbsp;&nbsp; 20:06&nbsp;&nbsp; 0:00 [ext4-dio-unwrit]<br class=3D"">geo=
rgioy 35166&nbsp; 102&nbsp; 0.0 285476 14828 ?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp=
;&nbsp;&nbsp; RLl&nbsp; 20:06&nbsp;&nbsp; 0:12 /home_nfs/georgioy/BENCHS/mp=
i-openmp/mpi_hello<br class=3D"">=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D<br class=3D""><br class=3D""></div><div class=3D"">and I see the =
pmix related functions starting correctly in the slurmd log files :<br clas=
s=3D""><br class=3D"">=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br cl=
ass=3D""><br class=3D"">[2016-06-08T20:12:08.004] [86.0] debug:&nbsp; (null=
) [0] mpi_pmix.c:90 [p_mpi_hook_slurmstepd_prefork] mpi/pmix: start<br clas=
s=3D"">[2016-06-08T20:12:08.004] [86.0] debug:&nbsp; mpi/pmix: setup socket=
s<br class=3D"">[2016-06-08T20:12:08.005]
 [86.0] debug:&nbsp; rio12 [0] pmixp_client.c:78 [errhandler_reg_callbk]=20
mpi/pmix: Error handler registration callback is called with status=3D0,=20
ref=3D0<br class=3D"">[2016-06-08T20:12:08.005] [86.0] debug:&nbsp; rio12 [=
0] pmixp_client.c:581 [pmixp_libpmix_job_set] mpi/pmix: task initialization=
<br class=3D"">[2016-06-08T20:12:08.005] [86.0] debug:&nbsp; rio12 [0] pmix=
p_agent.c:220 [_agent_thread] mpi/pmix: Start agent thread<br class=3D"">[2=
016-06-08T20:12:08.005]
 [86.0] debug:&nbsp; rio12 [0] pmixp_agent.c:313 [pmixp_agent_start]=20
mpi/pmix: agent thread started: tid =3D 139672278615808<br class=3D"">[2016=
-06-08T20:12:08.005] [86.0] debug:&nbsp; rio12 [0] pmixp_agent.c:84 [_conn_=
readable] mpi/pmix: fd =3D 9<br class=3D"">[2016-06-08T20:12:08.005] [86.0]=
 debug:&nbsp; rio12 [0] pmixp_agent.c:84 [_conn_readable] mpi/pmix: fd =3D =
19<br class=3D"">[2016-06-08T20:12:08.005] [86.0] debug:&nbsp; rio12 [0] pm=
ixp_agent.c:256 [_pmix_timer_thread] mpi/pmix: Start timer thread<br class=
=3D"">[2016-06-08T20:12:08.005]
 [86.0] debug:&nbsp; rio12 [0] pmixp_agent.c:335 [pmixp_agent_start]=20
mpi/pmix: timer thread started: tid =3D 139672277563136<br class=3D""><br c=
lass=3D"">=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br class=3D""></div>=
<div class=3D""><div class=3D""><br class=3D""></div><div class=3D"">Howeve=
r the application is never actually started and the srun fails completely w=
ith the following output:<br class=3D""></div><div class=3D""><br class=3D"=
">=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br class=3D"">[georgioy=
@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N 2 singularity exec /tmp/ubuntu_v4.img =
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br class=3D"">[rio12:00001] =
*** Process received signal ***<br class=3D"">[rio12:00001] Signal: Segment=
ation fault (11)<br class=3D"">[rio12:00001] Signal code: Address not mappe=
d (1)<br class=3D"">[rio12:00001] Failing at address: 0x7fbd937e6010<br cla=
ss=3D"">[rio12:00001] [ 0] [rio12:00001] *** Process received signal ***<br=
 class=3D"">/lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f26aeb233d0]=
<br class=3D"">[rio12:00001] [ 1] /usr/local/lib/libmca_common_sm.so.0(+0x1=
035)[0x7f269f964035]<br class=3D"">[rio12:00001] [ 2] /usr/local/lib/libmca=
_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f269f964583]<br class=3D"">=
[rio12:00001] [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_proc=
s+0x5f1)[0x7f269fb68771]<br class=3D"">[rio12:00001] [ 4] /usr/local/lib/op=
enmpi/mca_bml_r2.so(+0x2be7)[0x7f26a451fbe7]<br class=3D"">[rio12:00001] [ =
5] /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f26=
9ef35662]<br class=3D"">[rio12:00001] [ 6] /usr/local/lib/libmpi.so.0(ompi_=
mpi_init+0xa51)[0x7f26aed75d31]<br class=3D"">[rio12:00001] [ 7] /usr/local=
/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26aed9bdb9]<br class=3D"">[rio12:00001]=
 [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]<br class=3D"=
">[rio12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0=
)[0x7f26ae769830]<br class=3D"">[rio12:00001] Signal: Segmentation fault (1=
1)<br class=3D"">[rio12:00001] Signal code: Address not mapped (1)<br class=
=3D"">[rio12:00001] Failing at address: 0x7fbd937e6010<br class=3D"">[rio12=
:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]<br cl=
ass=3D"">[rio12:00001] *** End of error message ***<br class=3D"">[rio12:00=
001] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f3f89c063d0]<b=
r class=3D"">[rio12:00001] [ 1] /usr/local/lib/libmca_common_sm.so.0(+0x103=
5)[0x7f3f7eb1d035]<br class=3D"">[rio12:00001] [ 2] /usr/local/lib/libmca_c=
ommon_sm.so.0(common_sm_mpool_create+0xa3)[0x7f3f7eb1d583]<br class=3D"">[r=
io12:00001] [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+=
0x5f1)[0x7f3f7ed21771]<br class=3D"">[rio12:00001] [ 4] /usr/local/lib/open=
mpi/mca_bml_r2.so(+0x2be7)[0x7f3f7f5f3be7]<br class=3D"">[rio12:00001] [ 5]=
 /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f3f7e=
0ee662]<br class=3D"">[rio12:00001] [ 6] /usr/local/lib/libmpi.so.0(ompi_mp=
i_init+0xa51)[0x7f3f89e58d31]<br class=3D"">[rio12:00001] [ 7] /usr/local/l=
ib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7edb9]<br class=3D"">[rio12:00001] [=
 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]<br class=3D"">=
[rio12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[=
0x7f3f8984c830]<br class=3D"">[rio12:00001] [10] /home_nfs/georgioy/BENCHS/=
mpi-openmp/mpi_hello[0x400709]<br class=3D"">[rio12:00001] *** End of error=
 message ***<br class=3D"">slurmstepd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br class=3D"">=
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.<br=
 class=3D"">slurmstepd: error: *** STEP 86.0 ON rio12 CANCELLED AT 2016-06-=
08T20:12:08 ***<br class=3D"">slurmstepd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br class=3D"">=
slurmstepd:=20
error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br class=3D"">s=
lurmstepd:=20
error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br class=3D"">s=
run: error: rio12: tasks 0-2: Killed<br class=3D"">srun: error: rio13: task=
 3: Killed<br class=3D""><br class=3D"">=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D<br class=3D""><br class=3D""></div><div class=3D""=
>the results I get when launching with mpirun instead of srun are more or l=
ess the same. <br class=3D""><br class=3D""></div><div class=3D"">From my u=
nderstanding, the orted process, or in my case slurmstepd process that laun=
ches the singularity container <br class=3D""></div><div class=3D"">and
 the MPI application, enables the communication of MPI libraries and=20
orted (or slurmstepd) through PMI (in my case here PMIx).<br class=3D""></d=
iv><div class=3D"">So I suppose the problem I see should be related with th=
e mapping of PMI from the container towards the orted or slurmstepd.<br cla=
ss=3D""><br class=3D""></div><div class=3D"">What do you think?<br class=3D=
""></div><div class=3D""><br class=3D""></div><div class=3D"">To
 give some more details, my singularity container has OpenMPI and PMIx=20
installed but not Slurm. I don't think that Slurm needs to reside within
 the container in that context.<br class=3D""></div><div class=3D"">But I w=
asn't sure if OpenMPI and PMIx are needed both inside and outside of the co=
ntainer.<br class=3D""></div><div class=3D"">So, I've tried using the exact=
 same version of PMIx and OpenMPI in and out of the container and the probl=
em still persists. <br class=3D""></div><div class=3D"">The
 experiments have been done using RedHat hosts with CentOS or Ubuntu=20
singularity containers and the latest github versions of slurm, OpenMPI,
 PMIx and singularity.<br class=3D""></div><div class=3D""><br class=3D""><=
/div><div class=3D"">By the way, do you have any MPI example for singularit=
y v2? <br class=3D""></div><div class=3D"">Because the MPI example that you=
 show here : <a href=3D"http://singularity.lbl.gov/#hpc" target=3D"_blank" =
class=3D"">http://singularity.lbl.gov/#hpc</a><br class=3D""><br class=3D""=
></div><div class=3D"">is actually done using singularity v1, no? <br class=
=3D"">In
 my understanding with singularity v2 we actually build a complete image
 with OS not just the application with its libraries. Is this correct?<br c=
lass=3D""></div><div class=3D""><br class=3D""></div><div class=3D"">Sorry =
for the long email and<br class=3D"">thanks a lot for any extra info you ca=
n provide regarding singularity v2 and MPI.<br class=3D""><br class=3D""></=
div><div class=3D"">Best Regards,<br class=3D""></div><div class=3D"">Yiann=
is</div></div></div></div></div><span class=3D""><font color=3D"#888888" cl=
ass=3D""><div class=3D""><br class=3D"webkit-block-placeholder"></div>

-- <br class=3D"">
You received this message because you are subscribed to the Google Groups "=
singularity" group.<br class=3D"">
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank" class=3D"">si=
ngu...@lbl.gov</a>.<span class=3D"HOEnZb"><font color=3D"#888888" class=3D"=
"><br class=3D"">
</font></span></font></span></blockquote></div><span class=3D"HOEnZb"><font=
 color=3D"#888888" class=3D""><br class=3D""><br clear=3D"all" class=3D""><=
div class=3D""><br class=3D""></div>-- <br class=3D""><div data-smartmail=
=3D"gmail_signature" class=3D""><div dir=3D"ltr" class=3D""><div class=3D""=
>Gregory M. Kurtzer<br class=3D"">High Performance Computing Services (HPCS=
)<br class=3D"">University of California<br class=3D"">Lawrence Berkeley Na=
tional Laboratory<br class=3D"">One Cyclotron Road, Berkeley, CA 94720</div=
></div></div>
</font></span></div><span class=3D"HOEnZb"><font color=3D"#888888" class=3D=
""><div class=3D""><br class=3D"webkit-block-placeholder"></div>

-- <br class=3D"">
You received this message because you are subscribed to the Google Groups "=
singularity" group.<br class=3D"">
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank" class=3D"">si=
ngu...@lbl.gov</a>.<br class=3D"">
</font></span></blockquote></div><br class=3D""></div><div class=3D""><br c=
lass=3D"webkit-block-placeholder"></div>

-- <br class=3D"">
You received this message because you are subscribed to the Google Groups "=
singularity" group.<br class=3D"">
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" class=3D"">singu...@lbl.gov</a>=
.<br class=3D"">
</div></blockquote></div><br class=3D""></body></html>
--Apple-Mail=_AE4494AC-3FD8-4EBB-86DA-87688E78A47E--
