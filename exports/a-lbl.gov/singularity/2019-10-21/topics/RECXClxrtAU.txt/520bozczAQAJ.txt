X-Received: by 2002:a65:624e:: with SMTP id q14-v6mr1766735pgv.85.1530170137166;
        Thu, 28 Jun 2018 00:15:37 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a65:6091:: with SMTP id t17-v6ls1615763pgu.29.gmail; Thu, 28
 Jun 2018 00:15:36 -0700 (PDT)
X-Received: by 2002:a63:7d4c:: with SMTP id m12-v6mr7771675pgn.201.1530170135870;
        Thu, 28 Jun 2018 00:15:35 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1530170135; cv=none;
        d=google.com; s=arc-20160816;
        b=x7yXf4U5obBKpxn6eavVkH0P3lgpSqHrNEzGLaL/CKkg5dKaqWvP0UPyBTapt+W9Vi
         fKDZ5syG9z/InTY+4Nssfqr73JfVb3D+UmlVDZtW5yLYHeXf6O/ZpG7xBVi4rDMHoET4
         7vOrd++HVXfboEIf66DPrC5liu3JKzL6N/a1MvDtN8CoRNhzRGK1P9VTx/6kmUOIk+SA
         tYC0b97DpdHUAVozYU5icIUWUjSZWxvzfA1YbeKxNhuSJlSjWB1CxEtpUFF7ucvLZ+we
         mI6G9Z/bUcGRD7kB4BYu/PjL3zGtfbhkliJKJy2oN7UOLfGyOpCkbSSiK03cKNMpyUgX
         +nBQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=V6EIJZtMbQbDd4hKrTjmvKlZuSkxZtC3fzqceX3Q69g=;
        b=EnytgHg2nBQYaah3MdOdpfvFHwnyRxhsobnEKm1ZBbiCep/YngvUSXvr592xam9pMK
         ACwTC1UHWfd4f8/7hQph9/qIlqAqkZKo6PsVzdgu4zKL/oJ2SR+ZVnw9vzLD2AU7Vpgq
         Pla0e8CLVDktUyamRYUPXXA2Ji4GE9j7gkpAq0/f5qvZIJ1mNtFq275ZTyA91GYOYW57
         IwJh6PH70Cff6hsgfAMR2S7BOKW5OhED6C9yFXYK68aznrKauNhiyLfzqFCB9TUc6jLq
         iEvDSMd4jgdbtVMCQhAmxqGi4RlZgFJRBaGDM6QKrbQe2Kt35062GjicJ/MjdR/Iu3zB
         4OEw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@googlemail.com header.s=20161025 header.b=FJipqkFd;
       spf=pass (google.com: domain of hea...@googlemail.com designates 209.85.192.171 as permitted sender) smtp.mailfrom=hea...@googlemail.com
Return-Path: <hea...@googlemail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id t1-v6si5503856pga.90.2018.06.28.00.15.35
        for <singu...@lbl.gov>;
        Thu, 28 Jun 2018 00:15:35 -0700 (PDT)
Received-SPF: pass (google.com: domain of hea...@googlemail.com designates 209.85.192.171 as permitted sender) client-ip=209.85.192.171;
Authentication-Results: mx.google.com;
       dkim=pass head...@googlemail.com header.s=20161025 header.b=FJipqkFd;
       spf=pass (google.com: domain of hea...@googlemail.com designates 209.85.192.171 as permitted sender) smtp.mailfrom=hea...@googlemail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2B4CAA6ijRbf6vAVdFQBAUDHgEGDIMfg?=
 =?us-ascii?q?Qx/KINzBlJLglCQVIIHAogrjQSBYwMjAQ6BUIJ1AoMZITcVAQIBAQIBAQIBEwE?=
 =?us-ascii?q?BCQsLCCclDII1BQIDAh6CVwEBAQMBGgMGBBkBDSwDAQsGBQsNIAoCAiEBDwMBB?=
 =?us-ascii?q?QEcDgcEARoCBII0TCiBPgEDDQgFCqELPIlXEYEggWkzgnIFaoJtCj8NgS6BFAI?=
 =?us-ascii?q?GEohbghWBD4MPglY3CwEBAQGBHwwBDQUBCQI1DBqCOoJVAodQEQiBRoMZhQ+HN?=
 =?us-ascii?q?SsJhgCGCoMJgUBCg0WCaoUZiidOhnEwgTZVLnEzARkIGxVsgjgJFoFUJAwXegE?=
 =?us-ascii?q?OgjyCZIIwgmaCVwM9MAEPjg4BDhcELIFtBQEB?=
X-IronPort-AV: E=Sophos;i="5.51,282,1526367600"; 
   d="scan'208,217";a="120303290"
Received: from mail-pf0-f171.google.com ([209.85.192.171])
  by fe3.lbl.gov with ESMTP; 28 Jun 2018 00:15:33 -0700
Received: by mail-pf0-f171.google.com with SMTP id c22-v6so2182105pfi.2
        for <singu...@lbl.gov>; Thu, 28 Jun 2018 00:15:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=googlemail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=V6EIJZtMbQbDd4hKrTjmvKlZuSkxZtC3fzqceX3Q69g=;
        b=FJipqkFdD9TeR3W0xXKbhuVTN0r3B+Hq9d3Kg3rHdFsLWNXQfQheXgCADP7x/1w/oe
         6VD63tSoHsOjbk2lh/SFH5ZpBmm+dACG3E5JJLsJRwV6x332qKBnP+soLdzw8FNA9azq
         XO5WXuvy6LQJTEbgb1qCsBnvDJvTo52R5VPodYUtuAqoHnXqqALZctJAaQZusptPWdJT
         oKTtP3YdjuD7v3zy45Y5rYeR35zRnG/yVr+EXCK9TsU2LAa78ZUqG0J3xatS789/9EHC
         HiPRxVxIG+JqLOtjxo+y7ygmWaoXXlmYNrR5aZDL+Wn/x2gALno89oc7RyYyWW6ag1Q7
         9c3Q==
X-Gm-Message-State: APt69E04/UEEzQW15NeO44aOfk+8kxbfX8pM5cHJsPKuBEbptkktGK3t
	zNmji7NO/PzDXEJC9CMU5sLPA3Fnu01DkwEBJnctA0ni
X-Received: by 2002:a63:7f46:: with SMTP id p6-v6mr5389352pgn.303.1530170133301;
 Thu, 28 Jun 2018 00:15:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 2002:a17:90a:2e83:0:0:0:0 with HTTP; Thu, 28 Jun 2018 00:15:02
 -0700 (PDT)
In-Reply-To: <CAM=pu+L-DXgQJ4eSUqSEnriDX9U2+dwOU-e2P2gWFxW3jsPcWQ@mail.gmail.com>
References: <4d550130-4d54-4d4f-bf9f-a46f34132e96@lbl.gov> <CAJZ53CkoPup6sXotzVLO_toCu2c+rwK-A6Y0+TU277Y9km8N9w@mail.gmail.com>
 <CAM=pu+Jq2HcgVEBoYJ-USVQomnT6=pmd_16UdEpz2nniPQewdw@mail.gmail.com>
 <CADf5cTEmD1BEEg=jQHf2rrK1h+cwPYYqSD3Tz0UKS-RpumNNRA@mail.gmail.com>
 <CAMsq4T1c4vA5Yv6_bWJhpNC=UCze1reP2yPQa2YUtutJRPzQbA@mail.gmail.com> <CAM=pu+L-DXgQJ4eSUqSEnriDX9U2+dwOU-e2P2gWFxW3jsPcWQ@mail.gmail.com>
From: John Hearns <hea...@googlemail.com>
Date: Thu, 28 Jun 2018 09:15:02 +0200
Message-ID: <CAPqNE2VEL6oOB2NMP89=BxHqQj5k=Mbg2C=7O=oZuRvN5Q-pnQ@mail.gmail.com>
Subject: Re: [Singularity] Research data and containers
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="0000000000003dcfd9056fae7fff"

--0000000000003dcfd9056fae7fff
Content-Type: text/plain; charset="UTF-8"

I think this a very, very interesting topic.  Please allow me to explore
and hopefully bring out some responses?

Are we talking about large public datasets here - for instance I worked on
a CERN experiment (ahem) many years ago.
One hopes that huge, public datasets do not change. For instance if I went
back to analyze what was tagged at the time with a run number and an event
number I hope I would get the same data.
OK, it is clear that some large institutiosn worldwide will store that
data. And that bitrot or the scarcity of the tape drives to read the data
has not rendered it beyond recovery.

I also worked with the JASMIN project in the UK, where several institutes
have banded together to create a central data store for climate modelling.
Again large public datasets.

I also worked in Formula 1 with CFD engineers, and managed over a petabyte
of data from CFD studies. Those studies were catalogued by year and then
case number.
Again, we assume that the data is held on tape somewhere and if you ind the
correctly named directory you will get the same data back.

I guess here we are talking about reproducibility. So we have a Singularity
container, which is signed, and captures the packages and libraries needed
to run that analysis.
We also have to have the same data available to run on.  Do we need to
physically keep that data 'beside' the container? Maybe, for certain data
and certain values of 'not too large'.
but how about analyses of (say) CERN or JASMIN data? Is it enough to make a
pointer towards those datasets somehow?
I would say URL's or URIs here, but we have seen how the Web is dynamic and
sites disappear and links change.


Its worth saying that in the UK there is a lot of effort in this space. The
Research Councils there mandated that if they fund research the data must
be retained for N years (I forget N)
So many Universities now have specific services for research data, for
instance:
http://www.ucl.ac.uk/research-it-services/research-data-service
https://www.ukdataservice.ac.uk/

Arkivum https://arkivum.com/ works closely with the UK academic network to
provide large data storage. I just found this blog on their website - well
worth reading.
https://arkivum.com/articles/blog-series-part-1-four-economic-factors-make-cost-nothing-expensive-something/
https://arkivum.com/pharma/blog-series-part-4-final-economic-fallacy-long-term-data-temporarily-dynamic-path-dependent/


To end up, I guess what we are aiming towards is an S3 compatible storage
bucket, whether this is hosted in-house, by Amazon or by a compatible
provider.
Such a bucket would have to be locked, and signed.  Ideas??
















































On 28 June 2018 at 03:44, v <vso...@gmail.com> wrote:

> Your description of the different components is on point.
>
> Finding hosting for large data is, and will continue to be, always one of
> the biggest problems. It's expensive. Someone has to pay for it. Nobody
> will do it for you for free. We are lucky to have things like Github repos,
> Google Drive, Dropbox, that gives some (mediocre) access to small data. For
> large data, everyone is pretty much on their own. Most academics have a
> university that provides some sort of web space / library data archive, and
> if you are publishing, some journals will support it too -->
> https://www.nature.com/sdata/policies/repositories. I try to figure out
> hacks to get everything hosted, for free, in version control, but that
> breaks when anything gets large.
>
> The entity that manages a data respository, if this is a thing, must have
> incentive to do it. The datasets have to bring in money in some way, or
> someone has to value it enough to pay for it. That's sort of what most
> things come down to, unfortunately.
>
>
> On Wed, Jun 27, 2018 at 6:38 PM Maxime Hebrard <maxime...@gmail.com>
> wrote:
>
>> I start to use singularity / containers and obviously I ran into the
>> same question ...
>>
>> a half solution that I found by exploring existing solution (thanks to
>> nf-core and SciLifeLab to share their workflow / containers) is ( as
>> far I understand ):
>>
>> the container encapsulate only the sorftware.
>> the data are store in external repositories.
>>
>> the user receive the container.
>>
>> case 1:
>> the user download the data by himself and point his files to the container
>> the container use local files as resources
>> pros:
>>  * the container is slim
>>  * user can store the data on machine A (data server) and run the
>> software on machine B (analyse server)
>> cons:
>> * user is involved in downloading the right data
>>
>> case 2:
>> the user do not have the data
>> the container download the data from the external repository
>> the container use the data it just download
>> pro:
>> * the container manage everything
>> cons:
>> * the step of downloading the data require bandwith + disk space =
>> time and resources
>>
>> a general worry for both scenarii is:
>> who manage the data repository ? can we ensure the version of the data
>> package will be maintain ?
>>
>> Waiting for your feedback ;)
>> Maxime
>>
>> On Thu, Jun 28, 2018 at 8:15 AM, 'Chris Hines' via singularity
>> <singu...@lbl.gov> wrote:
>> > To quote 90's band "The Offspring" IMHO "You gotta keep 'em separated"
>> >
>> > Personally I think this is a larger question: How do we (as research
>> > software engineers, or whatever you would like to call us today)
>> catalog and
>> > attach references (DOI's I guess) to larger data sets. Does it make
>> sense to
>> > mksquashfs or tar.bz2? What if they are already in a single hdf5 file?
>> And
>> > what about the output data set? we obviously want that to land outside
>> the
>> > computational container, but how should we package the output so it can
>> be
>> > referenced directly? How do we make that whole thing discoverable and
>> > encourage researchers to attach sufficient metadata to make it
>> searchable?
>> >
>> > I'm wondering if part of the solution is some sort of "meta container"
>> that
>> > downloads a set of versioned data with specific checksums, and a
>> versioned
>> > tool container with a specific checksum, and attaches them in a
>> reproducible
>> > way.
>> >
>> > To address Dominque's question directly: I'd keep the "semi-separated".
>> > Whatever your researchers use for archiving/cataloging/managing their
>> data
>> > sets, keep that in place and add the singularity container to the data
>> set
>> > (rather than adding the data set to the singularity container).
>> >
>> > Regards,
>> > --
>> > Chris.
>> > Monash eResearch Centre, Monash University, Australia
>> >
>> > On Thu, 28 Jun 2018 at 01:19, v <vso...@gmail.com> wrote:
>> >>
>> >> If you are making containers with singularity (and using Squashfs
>> anyway)
>> >> it wouldn't be so nutty to just compress with mksquashfs and then mount
>> >> where needed - I did this with a huge dataset and it worked pretty
>> nicely
>> >> :_) It relies on FUSE then and all the issues around that, but it's an
>> >> option!
>> >>
>> >> This is good showing of how use use mksquashfs (it's really
>> >> simple,actually!) -->
>> >> http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.html
>> >> And then mount -->
>> >> https://vsoch.github.io/datasets/2018/zenodo/#mount-with-sudo
>> >>
>> >> On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker
>> >> <brando...@cornell.edu> wrote:
>> >>>
>> >>> I seem to recall code ocean may have explored this idea. Sorry I can't
>> >>> say more, at the moment.
>> >>>
>> >>> On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen
>> >>> <dominiqu...@gmail.com> wrote:
>> >>>>
>> >>>> Hello everyone,
>> >>>>
>> >>>> I am seeking for tips and experience on how to handle research data,
>> >>>> especially bigger sets of data (inside the GB range), in combination
>> with
>> >>>> containers.
>> >>>>
>> >>>> I am a student assistant working in the IT department of a research
>> >>>> institute. And I am involved in building the infrastructure for
>> singularity
>> >>>> containerization and introducing researchers to the technology. We
>> have
>> >>>> already build a few base images containing frequently used tools.
>> Recently a
>> >>>> research group approached us, wishing to integrate data, they
>> created for
>> >>>> one tool, into a container with the tool. The data takes up several
>> GB of
>> >>>> disk space and we are unsure how to handle this and similar future
>> use
>> >>>> cases.
>> >>>>
>> >>>> Does anyone have a set of best practices or is there an intended way
>> to
>> >>>> use singularity with big research data?
>> >>>>
>> >>>> The options we considered are:
>> >>>>
>> >>>> Moving the data into the container at build time.
>> >>>>
>> >>>> Pro:
>> >>>>
>> >>>> Keeps the whole thing mobile
>> >>>> Keeps work away from the researchers
>> >>>> Can be referenced as a single entity in a publication.
>> >>>>
>> >>>> Con:
>> >>>>
>> >>>> Where would you store such big containers, especially several of
>> them?
>> >>>> What would happen, if separate versions of the tool are needed? Keep
>> the
>> >>>> old and accumulate redundant data? Delete the old and risk
>> reproducibility
>> >>>> problems? (Maybe container Apps can be used to minimize this.)
>> >>>>
>> >>>> Let the researchers mount the data into the container at startup.
>> >>>>
>> >>>> Pro:
>> >>>>
>> >>>> Keeps the containers slimmer, the tools more modular.
>> >>>>
>> >>>> Con:
>> >>>>
>> >>>> Adds to the workload and the things the researchers have to keep
>> track
>> >>>> of.
>> >>>> Spreads the parts needed for reproduction over at least two points.
>> >>>> Hampers mobility.
>> >>>>
>> >>>> Mounting during early phases and publish with a container, containing
>> >>>> the data.
>> >>>>
>> >>>> Pro:
>> >>>>
>> >>>> Reduces amount of containers with redundant and deprecated  data.
>> >>>> Makes reproduction of results easier after publication.
>> >>>>
>> >>>> Con:
>> >>>>
>> >>>> Ads to the workload.
>> >>>> When is the point when its time to "freeze" the data inside the
>> >>>> container?
>> >>>> Storage of the container is still problematic.
>> >>>> Might introduce reproducibility problems, since you change the
>> original
>> >>>> container to put the data into it.
>> >>>>
>> >>>> Are there any recommendations from experience?
>> >>>>
>> >>>> Thank you and best regards,
>> >>>> Dominique
>> >>>>
>> >>>> --
>> >>>> You received this message because you are subscribed to the Google
>> >>>> Groups "singularity" group.
>> >>>> To unsubscribe from this group and stop receiving emails from it,
>> send
>> >>>> an email to singu...@lbl.gov.
>> >>>
>> >>> --
>> >>> You received this message because you are subscribed to the Google
>> Groups
>> >>> "singularity" group.
>> >>> To unsubscribe from this group and stop receiving emails from it,
>> send an
>> >>> email to singu...@lbl.gov.
>> >>>
>> >>> --
>> >>> Vanessa Villamia Sochat
>> >>> Stanford University '16
>> >>> (603) 321-0676
>> >>
>> >> --
>> >> You received this message because you are subscribed to the Google
>> Groups
>> >> "singularity" group.
>> >> To unsubscribe from this group and stop receiving emails from it, send
>> an
>> >> email to singu...@lbl.gov.
>> >
>> > --
>> > You received this message because you are subscribed to the Google
>> Groups
>> > "singularity" group.
>> > To unsubscribe from this group and stop receiving emails from it, send
>> an
>> > email to singu...@lbl.gov.
>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
>>
>
>
> --
> Vanessa Villamia Sochat
> Stanford University '16
> (603) 321-0676
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--0000000000003dcfd9056fae7fff
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>I think this a very, very interesting topic.=C2=A0 Pl=
ease allow me to explore and hopefully bring out some responses?</div><div>=
<br></div><div>Are we talking about large public datasets here - for instan=
ce I worked on a CERN experiment (ahem) many years ago.</div><div>One hopes=
 that huge, public datasets do not change. For instance if I went back to a=
nalyze what was tagged at the time with a run number and an event number I =
hope I would get the same data.</div><div>OK, it is clear that some large i=
nstitutiosn worldwide will store that data. And that bitrot or the scarcity=
 of the tape drives to read the data has not rendered it beyond recovery.</=
div><div><br></div><div>I also worked with the JASMIN project in the UK, wh=
ere several institutes have banded together to create a central data store =
for climate modelling. Again large public datasets.</div><div><br></div><di=
v>I also worked in Formula 1 with CFD engineers, and managed over a petabyt=
e of data from CFD studies. Those studies were catalogued by year and then =
case number.</div><div>Again, we assume that the data is held on tape somew=
here and if you ind the correctly named directory you will get the same dat=
a back.</div><div><br></div><div>I guess here we are talking about reproduc=
ibility. So we have a Singularity container, which is signed, and captures =
the packages and libraries needed to run that analysis.</div><div>We also h=
ave to have the same data available to run on.=C2=A0 Do we need to physical=
ly keep that data &#39;beside&#39; the container? Maybe, for certain data a=
nd certain values of &#39;not too large&#39;.</div><div>but how about analy=
ses of (say) CERN or JASMIN data? Is it enough to make a pointer towards th=
ose datasets somehow?</div><div>I would say URL&#39;s or URIs here, but we =
have seen how the Web is dynamic and sites disappear and links change. <br>=
</div><div><br></div><div><br></div><div>Its worth saying that in the UK th=
ere is a lot of effort in this space. The Research Councils there mandated =
that if they fund research the data must be retained for N years (I forget =
N)</div><div>So many Universities now have specific services for research d=
ata, for instance:</div><div><a href=3D"http://www.ucl.ac.uk/research-it-se=
rvices/research-data-service">http://www.ucl.ac.uk/research-it-services/res=
earch-data-service</a></div><div><a href=3D"https://www.ukdataservice.ac.uk=
/">https://www.ukdataservice.ac.uk/</a></div><div><br></div><div>Arkivum <a=
 href=3D"https://arkivum.com/">https://arkivum.com/</a> works closely with =
the UK academic network to provide large data storage. I just found this bl=
og on their website - well worth reading. <br></div><div><a href=3D"https:/=
/arkivum.com/articles/blog-series-part-1-four-economic-factors-make-cost-no=
thing-expensive-something/">https://arkivum.com/articles/blog-series-part-1=
-four-economic-factors-make-cost-nothing-expensive-something/</a></div><div=
><a href=3D"https://arkivum.com/pharma/blog-series-part-4-final-economic-fa=
llacy-long-term-data-temporarily-dynamic-path-dependent/">https://arkivum.c=
om/pharma/blog-series-part-4-final-economic-fallacy-long-term-data-temporar=
ily-dynamic-path-dependent/</a></div><div><br></div><div><br></div><div>To =
end up, I guess what we are aiming towards is an S3 compatible storage buck=
et, whether this is hosted in-house, by Amazon or by a compatible provider.=
</div><div>Such a bucket would have to be locked, and signed.=C2=A0 Ideas??=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div><div><br></div><div><br></div><div>=
<br></div><div><br></div><div><br></div></div><div class=3D"gmail_extra"><b=
r><div class=3D"gmail_quote">On 28 June 2018 at 03:44, v <span dir=3D"ltr">=
&lt;<a href=3D"mailto:vso...@gmail.com" target=3D"_blank">vso...@gmail.com<=
/a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:=
0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Yo=
ur description of the different components is on point.=C2=A0<div><br></div=
><div>Finding hosting for large data is, and will continue to be, always on=
e of the biggest problems. It&#39;s expensive. Someone has to pay for it. N=
obody will do it for you for free. We are lucky to have things like Github =
repos, Google Drive, Dropbox, that gives some (mediocre) access to small da=
ta. For large data, everyone is pretty much on their own. Most academics ha=
ve a university that provides some sort of web space / library data archive=
, and if you are publishing, some journals will support it too --&gt;=C2=A0=
<a href=3D"https://www.nature.com/sdata/policies/repositories" target=3D"_b=
lank">https://www.nature.com/<wbr>sdata/policies/repositories</a>. I try to=
 figure out hacks to get everything hosted, for free, in version control, b=
ut that breaks when anything gets large.</div><div><br></div><div>The entit=
y that manages a data respository, if this is a thing, must have incentive =
to do it. The datasets have to bring in money in some way, or someone has t=
o value it enough to pay for it. That&#39;s sort of what most things come d=
own to, unfortunately.</div><div><br></div></div><div class=3D"HOEnZb"><div=
 class=3D"h5"><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Wed, Jun 2=
7, 2018 at 6:38 PM Maxime Hebrard &lt;<a href=3D"mailto:maxime...@gmail.com=
" target=3D"_blank">maxime...@gmail.com</a>&gt; wrote:<br></div><blockquote=
 class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px so=
lid rgb(204,204,204);padding-left:1ex">I start to use singularity / contain=
ers and obviously I ran into the<br>
same question ...<br>
<br>
a half solution that I found by exploring existing solution (thanks to<br>
nf-core and SciLifeLab to share their workflow / containers) is ( as<br>
far I understand ):<br>
<br>
the container encapsulate only the sorftware.<br>
the data are store in external repositories.<br>
<br>
the user receive the container.<br>
<br>
case 1:<br>
the user download the data by himself and point his files to the container<=
br>
the container use local files as resources<br>
pros:<br>
=C2=A0* the container is slim<br>
=C2=A0* user can store the data on machine A (data server) and run the<br>
software on machine B (analyse server)<br>
cons:<br>
* user is involved in downloading the right data<br>
<br>
case 2:<br>
the user do not have the data<br>
the container download the data from the external repository<br>
the container use the data it just download<br>
pro:<br>
* the container manage everything<br>
cons:<br>
* the step of downloading the data require bandwith + disk space =3D<br>
time and resources<br>
<br>
a general worry for both scenarii is:<br>
who manage the data repository ? can we ensure the version of the data<br>
package will be maintain ?<br>
<br>
Waiting for your feedback ;)<br>
Maxime<br>
<br>
On Thu, Jun 28, 2018 at 8:15 AM, &#39;Chris Hines&#39; via singularity<br>
&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov<=
/a>&gt; wrote:<br>
&gt; To quote 90&#39;s band &quot;The Offspring&quot; IMHO &quot;You gotta =
keep &#39;em separated&quot;<br>
&gt;<br>
&gt; Personally I think this is a larger question: How do we (as research<b=
r>
&gt; software engineers, or whatever you would like to call us today) catal=
og and<br>
&gt; attach references (DOI&#39;s I guess) to larger data sets. Does it mak=
e sense to<br>
&gt; mksquashfs or tar.bz2? What if they are already in a single hdf5 file?=
 And<br>
&gt; what about the output data set? we obviously want that to land outside=
 the<br>
&gt; computational container, but how should we package the output so it ca=
n be<br>
&gt; referenced directly? How do we make that whole thing discoverable and<=
br>
&gt; encourage researchers to attach sufficient metadata to make it searcha=
ble?<br>
&gt;<br>
&gt; I&#39;m wondering if part of the solution is some sort of &quot;meta c=
ontainer&quot; that<br>
&gt; downloads a set of versioned data with specific checksums, and a versi=
oned<br>
&gt; tool container with a specific checksum, and attaches them in a reprod=
ucible<br>
&gt; way.<br>
&gt;<br>
&gt; To address Dominque&#39;s question directly: I&#39;d keep the &quot;se=
mi-separated&quot;.<br>
&gt; Whatever your researchers use for archiving/cataloging/managing their =
data<br>
&gt; sets, keep that in place and add the singularity container to the data=
 set<br>
&gt; (rather than adding the data set to the singularity container).<br>
&gt;<br>
&gt; Regards,<br>
&gt; --<br>
&gt; Chris.<br>
&gt; Monash eResearch Centre, Monash University, Australia<br>
&gt;<br>
&gt; On Thu, 28 Jun 2018 at 01:19, v &lt;<a href=3D"mailto:vso...@gmail.com=
" target=3D"_blank">vso...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; If you are making containers with singularity (and using Squashfs =
anyway)<br>
&gt;&gt; it wouldn&#39;t be so nutty to just compress with mksquashfs and t=
hen mount<br>
&gt;&gt; where needed - I did this with a huge dataset and it worked pretty=
 nicely<br>
&gt;&gt; :_) It relies on FUSE then and all the issues around that, but it&=
#39;s an<br>
&gt;&gt; option!<br>
&gt;&gt;<br>
&gt;&gt; This is good showing of how use use mksquashfs (it&#39;s really<br=
>
&gt;&gt; simple,actually!) --&gt;<br>
&gt;&gt; <a href=3D"http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.h=
tml" rel=3D"noreferrer" target=3D"_blank">http://tldp.org/HOWTO/<wbr>Squash=
FS-HOWTO/<wbr>creatingandusing.html</a><br>
&gt;&gt; And then mount --&gt;<br>
&gt;&gt; <a href=3D"https://vsoch.github.io/datasets/2018/zenodo/#mount-wit=
h-sudo" rel=3D"noreferrer" target=3D"_blank">https://vsoch.github.io/<wbr>d=
atasets/2018/zenodo/#mount-<wbr>with-sudo</a><br>
&gt;&gt;<br>
&gt;&gt; On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker<br>
&gt;&gt; &lt;<a href=3D"mailto:brando...@cornell.edu" target=3D"_blank">bra=
ndo...@cornell.edu</a>&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I seem to recall code ocean may have explored this idea. Sorry=
 I can&#39;t<br>
&gt;&gt;&gt; say more, at the moment.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen<br>
&gt;&gt;&gt; &lt;<a href=3D"mailto:dominiqu...@gmail.com" target=3D"_blank"=
>dominiqu...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Hello everyone,<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I am seeking for tips and experience on how to handle rese=
arch data,<br>
&gt;&gt;&gt;&gt; especially bigger sets of data (inside the GB range), in c=
ombination with<br>
&gt;&gt;&gt;&gt; containers.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I am a student assistant working in the IT department of a=
 research<br>
&gt;&gt;&gt;&gt; institute. And I am involved in building the infrastructur=
e for singularity<br>
&gt;&gt;&gt;&gt; containerization and introducing researchers to the techno=
logy. We have<br>
&gt;&gt;&gt;&gt; already build a few base images containing frequently used=
 tools. Recently a<br>
&gt;&gt;&gt;&gt; research group approached us, wishing to integrate data, t=
hey created for<br>
&gt;&gt;&gt;&gt; one tool, into a container with the tool. The data takes u=
p several GB of<br>
&gt;&gt;&gt;&gt; disk space and we are unsure how to handle this and simila=
r future use<br>
&gt;&gt;&gt;&gt; cases.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Does anyone have a set of best practices or is there an in=
tended way to<br>
&gt;&gt;&gt;&gt; use singularity with big research data?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; The options we considered are:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Moving the data into the container at build time.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Keeps the whole thing mobile<br>
&gt;&gt;&gt;&gt; Keeps work away from the researchers<br>
&gt;&gt;&gt;&gt; Can be referenced as a single entity in a publication.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Where would you store such big containers, especially seve=
ral of them?<br>
&gt;&gt;&gt;&gt; What would happen, if separate versions of the tool are ne=
eded? Keep the<br>
&gt;&gt;&gt;&gt; old and accumulate redundant data? Delete the old and risk=
 reproducibility<br>
&gt;&gt;&gt;&gt; problems? (Maybe container Apps can be used to minimize th=
is.)<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Let the researchers mount the data into the container at s=
tartup.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Keeps the containers slimmer, the tools more modular.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Adds to the workload and the things the researchers have t=
o keep track<br>
&gt;&gt;&gt;&gt; of.<br>
&gt;&gt;&gt;&gt; Spreads the parts needed for reproduction over at least tw=
o points.<br>
&gt;&gt;&gt;&gt; Hampers mobility.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Mounting during early phases and publish with a container,=
 containing<br>
&gt;&gt;&gt;&gt; the data.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Reduces amount of containers with redundant and deprecated=
=C2=A0 data.<br>
&gt;&gt;&gt;&gt; Makes reproduction of results easier after publication.<br=
>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Ads to the workload.<br>
&gt;&gt;&gt;&gt; When is the point when its time to &quot;freeze&quot; the =
data inside the<br>
&gt;&gt;&gt;&gt; container?<br>
&gt;&gt;&gt;&gt; Storage of the container is still problematic.<br>
&gt;&gt;&gt;&gt; Might introduce reproducibility problems, since you change=
 the original<br>
&gt;&gt;&gt;&gt; container to put the data into it.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Are there any recommendations from experience?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Thank you and best regards,<br>
&gt;&gt;&gt;&gt; Dominique<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; --<br>
&gt;&gt;&gt;&gt; You received this message because you are subscribed to th=
e Google<br>
&gt;&gt;&gt;&gt; Groups &quot;singularity&quot; group.<br>
&gt;&gt;&gt;&gt; To unsubscribe from this group and stop receiving emails f=
rom it, send<br>
&gt;&gt;&gt;&gt; an email to <a href=3D"mailto:singularity%...@lbl.gov" tar=
get=3D"_blank">singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; You received this message because you are subscribed to the Go=
ogle Groups<br>
&gt;&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt;&gt; To unsubscribe from this group and stop receiving emails from =
it, send an<br>
&gt;&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"=
_blank">singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; Vanessa Villamia Sochat<br>
&gt;&gt;&gt; Stanford University &#39;16<br>
&gt;&gt;&gt; (603) 321-0676<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singul=
arity+unsubscribe@lbl.<wbr>gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"m_-8585967889488559609gmail_signature"><div class=3D"m_-858596788=
9488559609gmail_signature">Vanessa Villamia Sochat<br>Stanford University &=
#39;16<br><div><div><div>(603) 321-0676</div></div></div></div></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--0000000000003dcfd9056fae7fff--
