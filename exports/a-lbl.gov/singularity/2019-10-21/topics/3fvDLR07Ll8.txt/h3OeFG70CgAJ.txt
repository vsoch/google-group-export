Date: Sat, 10 Jun 2017 02:07:41 -0700 (PDT)
From: Stefan Kombrink <stefan....@googlemail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <6117a8ba-063e-4799-aeb3-38a2558a0458@lbl.gov>
In-Reply-To: <cc84e6d1-49ad-416d-8480-78863790f5fd@lbl.gov>
References: <cc84e6d1-49ad-416d-8480-78863790f5fd@lbl.gov>
Subject: Re: Is there any way to run mpirun command inside container for
 multi-host system?
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_1772_1402217056.1497085661108"

------=_Part_1772_1402217056.1497085661108
Content-Type: multipart/alternative; 
	boundary="----=_Part_1773_405131855.1497085661108"

------=_Part_1773_405131855.1497085661108
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

I've got some experience on running multi node jobs with mpirun inside the 
containers.
Greg is right that it will be difficult to find an general purpose solution 
for arbitary HPC systems and software.
I made it work for two heavily used applications on our cluster one of 
which used IntelMPI and the other OpenMPI 1.6
The main thing was to replace /bin/ssh in order to tweak mpirun to wrap 
"ssh hostname cmd" to "ssh 'singularity ContainerName exec cmd'
I had to apply some more workarounds to make IB work properly (mainly 
envvars to configure MPI properly) but essentially the efford was 
justifiable.

Sorry cant find the link to the ssh wrapper script right now but I can post 
it later when you are interested in it. It is really just a few lines of 
bash code.

Stefan

Am Donnerstag, 8. Juni 2017 15:17:30 UTC+2 schrieb Guohua Li:
>
>
> For our HPC system, really need run mpirun command inside the container on 
> multi-host. 
>
> How can we fix the issues? 
>
> In my experience, when I build mpi job on multihost, everytime I change 
> the version of MPI inside the container, I have to change the version of 
> MPI on the host. 
>
> Is there any solution for run mpi command inside the container on 
> multi-host system? 
>
>
>
>
>
------=_Part_1773_405131855.1497085661108
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">I&#39;ve got some experience on running multi node jobs wi=
th mpirun inside the containers.<br>Greg is right that it will be difficult=
 to find an general purpose solution for arbitary HPC systems and software.=
<br>I made it work for two heavily used applications on our cluster one of =
which used IntelMPI and the other OpenMPI 1.6<br>The main thing was to repl=
ace /bin/ssh in order to tweak mpirun to wrap &quot;ssh hostname cmd&quot; =
to &quot;ssh &#39;singularity ContainerName exec cmd&#39;<br>I had to apply=
 some more workarounds to make IB work properly (mainly envvars to configur=
e MPI properly) but essentially the efford was justifiable.<br><br>Sorry ca=
nt find the link to the ssh wrapper script right now but I can post it late=
r when you are interested in it. It is really just a few lines of bash code=
.<br><br>Stefan<br><br>Am Donnerstag, 8. Juni 2017 15:17:30 UTC+2 schrieb G=
uohua Li:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left: =
0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr"><br>=
<div><font size=3D"4">For our HPC system, really need run mpirun command in=
side the container on multi-host.=C2=A0</font></div><div><font size=3D"4"><=
br></font></div><div><font size=3D"4">How can we fix the issues?=C2=A0</fon=
t></div><div><font size=3D"4"><br></font></div><div><font size=3D"4">In my =
experience, when I build mpi job on multihost, everytime I change the versi=
on of MPI inside the container, I have to change the version of MPI on the =
host.=C2=A0</font></div><div><font size=3D"4"><br></font></div><div><font s=
ize=3D"4">Is there any solution for run mpi=C2=A0command inside the contain=
er on multi-host system?=C2=A0</font></div><div><font size=3D"4"><br></font=
></div><div><font size=3D"4"><br></font></div><div><font size=3D"4"><br></f=
ont></div><div><font size=3D"4"><br></font></div></div></blockquote></div>
------=_Part_1773_405131855.1497085661108--

------=_Part_1772_1402217056.1497085661108--
