X-Received: by 10.66.77.137 with SMTP id s9mr12004231paw.33.1468523030716;
        Thu, 14 Jul 2016 12:03:50 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.142.77 with SMTP id q74ls832339iod.23.gmail; Thu, 14 Jul
 2016 12:03:50 -0700 (PDT)
X-Received: by 10.66.76.132 with SMTP id k4mr25073609paw.22.1468523030164;
        Thu, 14 Jul 2016 12:03:50 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id jy14si4352399pad.41.2016.07.14.12.03.50
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 14 Jul 2016 12:03:50 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.69 as permitted sender) client-ip=209.85.215.69;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.69 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.2
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2HvAACx4YdXekXXVdFUCIQUfAaDNqkXhxWFBIF7hhkCgSwHOBQBAQEBAQEBAw8BAQkLDAghL4RcAQEEARIRKzALCQILDSoCAiEBDwMBBQELEQYIBwQBBxUEAYd0Aw8IBZVWj0KBMT4xizuKGw2EDwEBCAEBAQEBIhCKZ4JDgU8MBQGDHYI9HQWIFQdfhQx1P4QkhQw0AYt/Q4IWgWsXh3CFP4ZdgUOGOhIegQ8egj8cgWwcMgeGMoE1AQEB
X-IronPort-AV: E=Sophos;i="5.28,364,1464678000"; 
   d="scan'208,217";a="29673277"
Received: from mail-lf0-f69.google.com ([209.85.215.69])
  by fe4.lbl.gov with ESMTP; 14 Jul 2016 12:03:47 -0700
Received: by mail-lf0-f69.google.com with SMTP id l89so58812035lfi.3
        for <singu...@lbl.gov>; Thu, 14 Jul 2016 12:03:47 -0700 (PDT)
X-Gm-Message-State: ALyK8tI0MC2V5b4UIgvSsnUtqFaTWCjmXrpNBolHUsqgKCih07n5vMeDbWKuMEJfHLgHJq6+Ey7D+zw5J+DNcDC1pkDmPeuUQS8kZlXEVYZFE4ozIThbhaBssY5N3nX8Df70qGDiMWKN1OwjC7bxvVSmOB8=
X-Received: by 10.25.24.85 with SMTP id o82mr7204951lfi.23.1468523026600;
        Thu, 14 Jul 2016 12:03:46 -0700 (PDT)
X-Received: by 10.25.24.85 with SMTP id o82mr7204946lfi.23.1468523026382; Thu,
 14 Jul 2016 12:03:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.158 with HTTP; Thu, 14 Jul 2016 12:03:45 -0700 (PDT)
In-Reply-To: <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov>
References: <03a19fb0-27ce-43c4-9400-8e58cf726500@lbl.gov> <CAN7etTwRbSe1MMh9wdQAMYoKVJTb_SGJeHto+WrZ=aU7NoBmhQ@mail.gmail.com>
 <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 14 Jul 2016 12:03:45 -0700
Message-ID: <CAN7etTyqGkWy1P57-cVJgyru5BT_DvnhwDzLe1p38BV8z_PPww@mail.gmail.com>
Subject: Re: [Singularity] Image is locked by another process
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a11405526552d8e05379d2937

--001a11405526552d8e05379d2937
Content-Type: text/plain; charset=UTF-8

On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <sgmeh...@gmail.com>
wrote:

> Gregory,
>
> Thanks for the quick suggestions.  There don't seem to be any processes
> attached to the container and I can't seem to run other commands (in read
> mode). I'm not sure what's going on.
>
> Just lazy with root right now, I need to create a normal uid.
>

Ahh, ok. Been there, done that! lol


>
> Steve
>
> [root@mach0 ~]# ls -la c7
> -rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7
> [root@mach0 ~]# singularity shell -w c7
> ERROR  : Image is locked by another process
>

Can you run this command again in --debug mode (singularity --debug ....)


> [root@mach0 ~]# lsof c7
>

What about the command "lslocks"?


> [root@mach0 ~]# singularity shell c7 whoami
> /usr/bin/whoami: /usr/bin/whoami: cannot execute binary file
>

Ahh, this is normal. You are asking the shell script to read in
/usr/bin/whoami. If you want to use shell to run whoami, you must prefix it
with the -c (e.g. -c "whoami [args]"), or use the 'exec' Singularity
subcommand: "singularity exec c7 whoami"


> [root@mach0 ~]#
> [root@mach0 ~]# ps -ef |grep c7
> root      7479  2002  0 11:49 pts/0    00:00:00 grep --color=auto c7
> [root@mach0 ~]#
>

What kind of file system does your image exist on? Is it NFS by chance? I
wonder if there is a host issue with a locking daemon or something else
weird going on where it is not giving the exclusive lock properly. If this
is NFS or other non local file system, can you copy the image to /tmp,
rerun the MPI command to get it to fail again, and try again?

Thanks!




>
>
>
>
> On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M. Kurtzer wrote:
>>
>> Hi Steve,
>>
>> That means there is an active file descriptor/process still running and
>> attached to the container maintaining a shared lock. You can run other
>> commands against the container as long as long as the container is not
>> being requested as --writable(-w), because that will try and obtain an
>> exclusive lock and it will fail if there are any active shared locks. Try
>> an "lsof /path/to/c7" to see what processes are attached to it. You may see
>> a list like:
>>
>> # lsof /tmp/Demo-2.img
>> COMMAND    PID USER   FD   TYPE DEVICE   SIZE/OFF      NODE NAME
>> sexec   107975 root    6rR  REG  253,0 1073741856 202112247
>> /tmp/Demo-2.img
>> sexec   107977 root    6r   REG  253,0 1073741856 202112247
>> /tmp/Demo-2.img
>> bash    107982 root    6r   REG  253,0 1073741856 202112247
>> /tmp/Demo-2.img
>>
>> Notice the two top ones are 'sexec' which are part of the Singularity
>> process stack. Kill the bottom one, and those should go away naturally.
>>
>> BTW, as long as you have installed Singularity as root, there is no need
>> to run Singularity commands as root (unless you want to make system changes
>> within the container).
>>
>> Hope that helps!
>>
>>
>> On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <sg...@gmail.com>
>> wrote:
>>
>>> Running mpirun tests, when an abort occurs, my image ends up locked.  Is
>>> there a way to clear the lock without rebooting?  I looked for processes
>>> that I could kill, but didn't see anything worthy.
>>>
>>> I'm using singularity v2.1 on Centos 7.2 (both host and container).
>>>
>>> Regards,
>>>
>>> Steve
>>>
>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H mach1,mach2
>>> singularity exec c7 /usr/bin/ring
>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>> Process 0 sent to 1
>>> Process 0 decremented value: 9
>>> Process 0 decremented value: 8
>>> Process 0 decremented value: 7
>>> Process 0 decremented value: 6
>>> Process 0 decremented value: 5
>>> Process 0 decremented value: 4
>>> Process 0 decremented value: 3
>>> Process 0 decremented value: 2
>>> Process 0 decremented value: 1
>>> Process 0 decremented value: 0
>>> Process 0 exiting
>>> Process 1 exiting
>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H mach0,mach1,mach2
>>> singularity exec c7 /usr/bin/ring
>>>
>>> --------------------------------------------------------------------------
>>> It appears as if there is not enough space for
>>> /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory
>>> backing
>>> file). It is likely that your MPI job will now either abort or experience
>>> performance degradation.
>>>
>>>   Local host:  mach0
>>>   Space Requested: 4194312 B
>>>   Space Available: 0 B
>>>
>>> --------------------------------------------------------------------------
>>> [mach0:02308] create_and_attach: unable to create shared memory BTL
>>> coordinating structure :: size 134217728
>>> [mach0:02291] 2 more processes have sent help message
>>> help-opal-shmem-mmap.txt / target full
>>> [mach0:02291] Set MCA parameter "orte_base_help_aggregate" to 0 to see
>>> all help / error messages
>>> ^CKilled by signal 2.
>>> Killed by signal 2.
>>> Singularity is sending SIGKILL to child pid: 2308
>>> Singularity is sending SIGKILL to child pid: 2309
>>> [warn] Epoll ADD(4) on fd 31 failed.  Old events were 0; read change was
>>> 0 (none); write change was 1 (add): Bad file descriptor
>>> ^C[root@mach0 ~]singularity shell -w c7
>>> ERROR  : Image is locked by another process
>>> [root@mach0 ~]# tail -30 /var/log/messages
>>> Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon management.
>>> Jul 14 10:42:17 mach0 systemd: Reached target Multi-User System.
>>> Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.
>>> Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about System
>>> Runlevel Changes...
>>> Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data Collection
>>> 10s After Completed Startup.
>>> Jul 14 10:42:17 mach0 systemd: Started Update UTMP about System Runlevel
>>> Changes.
>>> Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel
>>> Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]
>>> Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel arming.
>>> Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms (kernel) +
>>> 1.100s (initrd) + 4.931s (userspace) = 6.446s.
>>> Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.
>>> Jul 14 10:42:34 mach0 systemd: Starting user-0.slice.
>>> Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user root.
>>> Jul 14 10:42:34 mach0 systemd: Started Session 1 of user root.
>>> Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user root.
>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2023)> Command=exec,
>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Command=exec,
>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Image is locked
>>> by another process
>>> Jul 14 10:42:36 mach0 kernel: loop: module loaded
>>> Jul 14 10:43:38 mach0 Singularity: sexec (U=0,P=2050)> Command=shell,
>>> Container=c7, CWD=/root, Arg1=(null)
>>> Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted filesystem with
>>> ordered data mode. Opts: discard
>>> Jul 14 10:49:17 mach0 Singularity: sexec (U=0,P=2203)> Command=shell,
>>> Container=c7, CWD=/root, Arg1=(null)
>>> Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted filesystem with
>>> ordered data mode. Opts: discard
>>> Jul 14 10:50:39 mach0 Singularity: sexec (U=0,P=2244)> Command=exec,
>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>> Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted filesystem with
>>> ordered data mode. Opts: discard
>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> Command=exec,
>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2300)> Command=exec,
>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>> Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted filesystem with
>>> ordered data mode. Opts: discard
>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Command=shell,
>>> Container=c7, CWD=/root, Arg1=(null)
>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Image is locked
>>> by another process
>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a11405526552d8e05379d2937
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <span dir=3D"ltr">&lt;=
<a href=3D"mailto:sgmeh...@gmail.com" target=3D"_blank">sgmeh...@gmail.com<=
/a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:=
0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Gr=
egory,<br><br>Thanks for the quick suggestions.=C2=A0 There don&#39;t seem =
to be any processes attached to the container and I can&#39;t seem to run o=
ther commands (in read mode). I&#39;m not sure what&#39;s going on.<br><br>=
Just lazy with root right now, I need to create a normal uid.<br></div></bl=
ockquote><div><br></div><div>Ahh, ok. Been there, done that! lol</div><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>Steve<br><b=
r><span style=3D"font-family:courier new,monospace">[root@mach0 ~]# ls -la =
c7<br>-rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7<br>[root@mach0 ~]# =
singularity shell -w c7<span class=3D""><br>ERROR=C2=A0 : Image is locked b=
y another process<br></span></span></div></blockquote><div><br></div><div>C=
an you run this command again in --debug mode (singularity --debug ....)</d=
iv><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0=
 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><span s=
tyle=3D"font-family:courier new,monospace"><span class=3D""></span>[root@ma=
ch0 ~]# lsof c7<br></span></div></blockquote><div><br></div><div>What about=
 the command &quot;lslocks&quot;?</div><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div dir=3D"ltr"><span style=3D"font-family:courier new,monos=
pace">[root@mach0 ~]# singularity shell c7 whoami<br>/usr/bin/whoami: /usr/=
bin/whoami: cannot execute binary file<br></span></div></blockquote><div><b=
r></div><div>Ahh, this is normal. You are asking the shell script to read i=
n /usr/bin/whoami. If you want to use shell to run whoami, you must prefix =
it with the -c (e.g. -c &quot;whoami [args]&quot;), or use the &#39;exec&#3=
9; Singularity subcommand: &quot;singularity exec c7 whoami&quot;</div><div=
>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><span style=3D=
"font-family:courier new,monospace">[root@mach0 ~]#<br>[root@mach0 ~]# ps -=
ef |grep c7<br>root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 7479=C2=A0 2002=C2=A0 0 1=
1:49 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --color=3Dauto c7<br>[root@mach0=
 ~]#</span></div></blockquote><div><br></div><div>What kind of file system =
does your image exist on? Is it NFS by chance? I wonder if there is a host =
issue with a locking daemon or something else weird going on where it is no=
t giving the exclusive lock properly. If this is NFS or other non local fil=
e system, can you copy the image to /tmp, rerun the MPI command to get it t=
o fail again, and try again?=C2=A0</div><div><br></div><div>Thanks!</div><d=
iv><br></div><div><br></div><div>=C2=A0</div><blockquote class=3D"gmail_quo=
te" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"=
><div dir=3D"ltr"><span class=3D""><br><br><br><br>On Thursday, July 14, 20=
16 at 12:30:01 PM UTC-6, Gregory M. Kurtzer wrote:</span><blockquote class=
=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><span class=3D""><div dir=3D"ltr">Hi Steve,<div><br>=
</div><div>That means there is an active file descriptor/process still runn=
ing and attached to the container maintaining a shared lock. You can run ot=
her commands against the container as long as long as the container is not =
being requested as --writable(-w), because that will try and obtain an excl=
usive lock and it will fail if there are any active shared locks. Try an &q=
uot;lsof /path/to/c7&quot; to see what processes are attached to it. You ma=
y see a list like:</div><div><br></div><div><div># lsof /tmp/Demo-2.img=C2=
=A0</div><div>COMMAND =C2=A0 =C2=A0PID USER =C2=A0 FD =C2=A0 TYPE DEVICE =
=C2=A0 SIZE/OFF =C2=A0 =C2=A0 =C2=A0NODE NAME</div><div>sexec =C2=A0 107975=
 root =C2=A0 =C2=A06rR =C2=A0REG =C2=A0253,0 1073741856 202112247 /tmp/Demo=
-2.img</div><div>sexec =C2=A0 107977 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A0=
253,0 1073741856 202112247 /tmp/Demo-2.img</div><div>bash =C2=A0 =C2=A01079=
82 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 1073741856 202112247 /tmp/De=
mo-2.img</div></div><div><br></div><div>Notice the two top ones are &#39;se=
xec&#39; which are part of the Singularity process stack. Kill the bottom o=
ne, and those should go away naturally.</div><div><br></div><div>BTW, as lo=
ng as you have installed Singularity as root, there is no need to run Singu=
larity commands as root (unless you want to make system changes within the =
container).</div><div><br></div><div>Hope that helps!</div><div><br></div><=
/div></span><div><br><div class=3D"gmail_quote"><div><div class=3D"h5">On T=
hu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=
=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br></div></div><blockqu=
ote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div><div class=3D"h5"><div dir=3D"ltr">Running mpir=
un tests, when an abort occurs, my image ends up locked.=C2=A0 Is there a w=
ay to clear the lock without rebooting?=C2=A0 I looked for processes that I=
 could kill, but didn&#39;t see anything worthy.<br><br>I&#39;m using singu=
larity v2.1 on Centos 7.2 (both host and container).<br><br>Regards,<br><br=
>Steve<br><br><span style=3D"font-family:courier new,monospace">[root@mach0=
 ~]# mpirun --allow-run-as-root -n 2 -H mach1,mach2 singularity exec c7 /us=
r/bin/ring<br>Process 0 sending 10 to 1, tag 201 (2 processes in ring)<br>P=
rocess 0 sent to 1<br>Process 0 decremented value: 9<br>Process 0 decrement=
ed value: 8<br>Process 0 decremented value: 7<br>Process 0 decremented valu=
e: 6<br>Process 0 decremented value: 5<br>Process 0 decremented value: 4<br=
>Process 0 decremented value: 3<br>Process 0 decremented value: 2<br>Proces=
s 0 decremented value: 1<br>Process 0 decremented value: 0<br>Process 0 exi=
ting<br>Process 1 exiting<br>[root@mach0 ~]# mpirun --allow-run-as-root -n =
3 -H mach0,mach1,mach2 singularity exec c7 /usr/bin/ring<br>---------------=
-----------------------------------------------------------<br>It appears a=
s if there is not enough space for /tmp/ompi.mach0.2291/54935/1/0/vader_seg=
ment.mach0.0 (the shared-memory backing<br>file). It is likely that your MP=
I job will now either abort or experience<br>performance degradation.<br><b=
r>=C2=A0 Local host:=C2=A0 mach0<br>=C2=A0 Space Requested: 4194312 B<br>=
=C2=A0 Space Available: 0 B<br>--------------------------------------------=
------------------------------<br>[mach0:02308] create_and_attach: unable t=
o create shared memory BTL coordinating structure :: size 134217728<br>[mac=
h0:02291] 2 more processes have sent help message help-opal-shmem-mmap.txt =
/ target full<br>[mach0:02291] Set MCA parameter &quot;orte_base_help_aggre=
gate&quot; to 0 to see all help / error messages<br>^CKilled by signal 2.<b=
r>Killed by signal 2.<br>Singularity is sending SIGKILL to child pid: 2308<=
br>Singularity is sending SIGKILL to child pid: 2309<br>[warn] Epoll ADD(4)=
 on fd 31 failed.=C2=A0 Old events were 0; read change was 0 (none); write =
change was 1 (add): Bad file descriptor<br>^C[root@mach0 ~]singularity shel=
l -w c7<br>ERROR=C2=A0 : Image is locked by another process<br>[root@mach0 =
~]# tail -30 /var/log/messages<br>Jul 14 10:42:17 mach0 systemd: Started LS=
B: slurm daemon management.<br>Jul 14 10:42:17 mach0 systemd: Reached targe=
t Multi-User System.<br>Jul 14 10:42:17 mach0 systemd: Starting Multi-User =
System.<br>Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about System=
 Runlevel Changes...<br>Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ah=
ead Data Collection 10s After Completed Startup.<br>Jul 14 10:42:17 mach0 s=
ystemd: Started Update UTMP about System Runlevel Changes.<br>Jul 14 10:42:=
18 mach0 kdumpctl: kexec: loaded kdump kernel<br>Jul 14 10:42:18 mach0 kdum=
pctl: Starting kdump: [OK]<br>Jul 14 10:42:18 mach0 systemd: Started Crash =
recovery kernel arming.<br>Jul 14 10:42:18 mach0 systemd: Startup finished =
in 415ms (kernel) + 1.100s (initrd) + 4.931s (userspace) =3D 6.446s.<br>Jul=
 14 10:42:34 mach0 systemd: Created slice user-0.slice.<br>Jul 14 10:42:34 =
mach0 systemd: Starting user-0.slice.<br>Jul 14 10:42:34 mach0 systemd-logi=
nd: New session 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: Started S=
ession 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: Starting Session 1=
 of user root.<br>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D2023)=
&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>J=
ul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D2024)&gt; Command=3Dexec=
, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:36 mach=
0 Singularity: sexec (U=3D0,P=3D2024)&gt; Image is locked by another proces=
s<br>Jul 14 10:42:36 mach0 kernel: loop: module loaded<br>Jul 14 10:43:38 m=
ach0 Singularity: sexec (U=3D0,P=3D2050)&gt; Command=3Dshell, Container=3Dc=
7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:43:38 mach0 kernel: EXT4-fs (loo=
p0): mounted filesystem with ordered data mode. Opts: discard<br>Jul 14 10:=
49:17 mach0 Singularity: sexec (U=3D0,P=3D2203)&gt; Command=3Dshell, Contai=
ner=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:49:17 mach0 kernel: EXT4-=
fs (loop0): mounted filesystem with ordered data mode. Opts: discard<br>Jul=
 14 10:50:39 mach0 Singularity: sexec (U=3D0,P=3D2244)&gt; Command=3Dexec, =
Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:50:39 mach0 =
kernel: EXT4-fs (loop0): mounted filesystem with ordered data mode. Opts: d=
iscard<br>Jul 14 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2299)&gt; Com=
mand=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10=
:51:34 mach0 Singularity: sexec (U=3D0,P=3D2300)&gt; Command=3Dexec, Contai=
ner=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mach0 kernel=
: EXT4-fs (loop0): mounted filesystem with ordered data mode. Opts: discard=
<br>Jul 14 10:51:57 mach0 Singularity: sexec (U=3D0,P=3D2322)&gt; Command=
=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:51:57 mac=
h0 Singularity: sexec (U=3D0,P=3D2322)&gt; Image is locked by another proce=
ss</span><span><font color=3D"#888888"><br><br><br></font></span></div></di=
v></div><span><font color=3D"#888888"><div><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span class=3D""><br><br clear=3D"all"><di=
v><br></div>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Pe=
rformance Computing Services (HPCS)<br>University of California<br>Lawrence=
 Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</di=
v></div></div>
</span></div>
</blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>

--001a11405526552d8e05379d2937--
