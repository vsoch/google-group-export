X-Received: by 10.200.45.107 with SMTP id o40mr13108174qta.31.1496935489930;
        Thu, 08 Jun 2017 08:24:49 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.34.201 with SMTP id o192ls2175979ito.17.gmail; Thu, 08 Jun
 2017 08:24:49 -0700 (PDT)
X-Received: by 10.84.174.3 with SMTP id q3mr34703250plb.52.1496935488922;
        Thu, 08 Jun 2017 08:24:48 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1496935488; cv=none;
        d=google.com; s=arc-20160816;
        b=nZ4dRiBbip+SGY5vsW0oB6NAR85RqXrmvl0PILV4Z7Dvsiv9Eq2f+YXdDh1OwYfhTu
         lqALwBcPQihnpKwQAJBdodi+X9D3wy1PIzyaKO1c9fhA/au38u6DXOuM4Nug4nN1vnMB
         1ovdF+u1AkLi73ADKgxk17Hae735UyK/+wlqS1Pxq7LHBDfEZVep+h+Ucr67UVAaf5QT
         fOSlKJqBOZkCznG49eW3wMvYne9ER8ujRU9T1A7z387r4uDWpvVSVTaRAi36z4B7up3m
         jS+yEih6G/zGnerFLmey1ValPDD7CR99cUDT+JWRDKFQts/UyGIq6dvQNIhrNwaRK1+k
         ZNvQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=4O2N98N8Fvf6DWitHx31pR4Ql3nC1D1qFTQ8vCYUPsM=;
        b=HL3leOMx35vc8ZZS4cr5uypiidAI/5Qs1nsPp4V6q2FG0JOhWjK6X2AO/6MsWj+VVz
         VJGxBKonJpCvvy33308dlXSgkyU28c95ULI7lH+l31GEl7OwF+zh6pB5nYkDYfPrkbaI
         dIW1asajH9zxnhoXvz4RhPGW568cnWOj2dxvtoEvsKwlIlH0rgcYGVTMJUJF9KqivkfJ
         xTXeslEQOZNMqylf7O0lJIHAj86bti1RToFFRQGeiandAATZg0yihfRdV9rw4pI/hrF1
         uSTUTeiNMUfaPyali2KAFs0pzMug8ViBS+GEdXaXWp6Ym5Wg5Rs4AYF6VhgAJW111E+y
         eYng==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.223.176 as permitted sender) smtp.mailfrom=gmku...@gmail.com
Return-Path: <gmku...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id v73si4657990pgb.89.2017.06.08.08.24.48
        for <singu...@lbl.gov>;
        Thu, 08 Jun 2017 08:24:48 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@gmail.com designates 209.85.223.176 as permitted sender) client-ip=209.85.223.176;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.223.176 as permitted sender) smtp.mailfrom=gmku...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2GoCACTazlZf7DfVdFeHgYMGQYMgkQ+T?=
 =?us-ascii?q?gGBSweDZAiBNppMkEmFOYFOQyEBhHKBEAKCcwdAFwEBAQEBAQEBAQEBAhABAQk?=
 =?us-ascii?q?LCwgmMYIzBQIDGgYIRikvAQEBAQEBAQEBAR8CKyUBGgECAgEjHQENDgsTAwwGB?=
 =?us-ascii?q?QQHNwICIQEBDgMBBQEcDgcEARoCBIg4gTkBAw0IBaQ5P4wHggQFARyDCgWDaAo?=
 =?us-ascii?q?ZJw1Wg1wBAQEBBgEBAQEBARoCBhKFXIJTgyCCWIFjEgFogkaCYQWRPIVMhnc7A?=
 =?us-ascii?q?o5bhF+SAYtDh10UH4EVIAGBAzMLdHWEYg8cGYFsIDYIhxJHgWkBAQE?=
X-IronPort-AV: E=Sophos;i="5.39,315,1493708400"; 
   d="scan'208,217";a="77392578"
Received: from mail-io0-f176.google.com ([209.85.223.176])
  by fe4.lbl.gov with ESMTP; 08 Jun 2017 08:24:48 -0700
Received: by mail-io0-f176.google.com with SMTP id k93so21644816ioi.2
        for <singu...@lbl.gov>; Thu, 08 Jun 2017 08:24:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=4O2N98N8Fvf6DWitHx31pR4Ql3nC1D1qFTQ8vCYUPsM=;
        b=GQW2i1WmlyQ4s3q877HcIDmkee+jEsYlDWq57AMDISismdnTeos6mwYk4KerJVq4Lv
         tcRBAex9kAbree1pTriE6ei7OG09Ko3alPqyoBkJC3clP7+YU72/xJbD3fPWJeQa9eui
         odXI4Tq2Lm9SGKHlcqz56t7qTfTWoJWMuAcMteQjEwg1CiZ/y/+SaWjskBaw0t/Kkusn
         w/Z38Ew30B4ykzGvY4Yc4hEaV8YxdZqJh4WnKnPSGukdqLdEBp9wthctRjijkQ8DU+lL
         xOcLKuZt/T96F4H8JoQQ7b+cEkx+MqIZzxiJCw0SiPwcxnMIzo9z+oy8OMrlA55/ACX0
         i78Q==
X-Gm-Message-State: AKS2vOzXbr1usA84zBvj2Mx9gx7+lgJ5s2NeDQx8AcSlJFyhFrV/fsOJ
	HUj15SIv4zDJfsgjx8O49Do2jLHJnw==
X-Received: by 10.107.37.19 with SMTP id l19mr2535253iol.216.1496935487584;
 Thu, 08 Jun 2017 08:24:47 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.153.81 with HTTP; Thu, 8 Jun 2017 08:24:47 -0700 (PDT)
In-Reply-To: <cc84e6d1-49ad-416d-8480-78863790f5fd@lbl.gov>
References: <cc84e6d1-49ad-416d-8480-78863790f5fd@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@gmail.com>
Date: Thu, 8 Jun 2017 08:24:47 -0700
Message-ID: <CAApQTThW=GhZz941_OvOVRcmgQ8TWSVA+rirsUbBOL69c2qnKg@mail.gmail.com>
Subject: Re: [Singularity] Is there any way to run mpirun command inside
 container for multi-host system?
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="001a11409d8efd66ea05517473b5"

--001a11409d8efd66ea05517473b5
Content-Type: text/plain; charset="UTF-8"

Running a multi-node job from within the container carries many caveats
that are difficult to resolve easily. It maybe possible using your resource
manager to spin up the contained remote node MPI process daemons (for
example ORTE in the case of OMPI), but that would require direct
integration with the resource manager.

But with that said, the hybrid mode should work even with different
versions. What MPI and versions are you trying to run?

Thanks!

On Thu, Jun 8, 2017 at 6:17 AM, Guohua Li <heave...@gmail.com> wrote:

>
> For our HPC system, really need run mpirun command inside the container on
> multi-host.
>
> How can we fix the issues?
>
> In my experience, when I build mpi job on multihost, everytime I change
> the version of MPI inside the container, I have to change the version of
> MPI on the host.
>
> Is there any solution for run mpi command inside the container on
> multi-host system?
>
>
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
--
Gregory M. Kurtzer
CEO, SingularityWare, LLC.
Senior Architect, RStor
Computational Science Advisor, Lawrence Berkeley National Laboratory

--001a11409d8efd66ea05517473b5
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Running a multi-node job from within the container carries=
 many caveats that are difficult to resolve easily. It maybe possible using=
 your resource manager to spin up the contained remote node MPI process dae=
mons (for example ORTE in the case of OMPI), but that would require direct =
integration with the resource manager.<div><br></div><div>But with that sai=
d, the hybrid mode should work even with different versions. What MPI and v=
ersions are you trying to run?</div><div><br></div><div>Thanks!</div></div>=
<div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Thu, Jun 8, 20=
17 at 6:17 AM, Guohua Li <span dir=3D"ltr">&lt;<a href=3D"mailto:heave...@g=
mail.com" target=3D"_blank">heave...@gmail.com</a>&gt;</span> wrote:<br><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #=
ccc solid;padding-left:1ex"><div dir=3D"ltr"><br><div><font size=3D"4">For =
our HPC system, really need run mpirun command inside the container on mult=
i-host.=C2=A0</font></div><div><font size=3D"4"><br></font></div><div><font=
 size=3D"4">How can we fix the issues?=C2=A0</font></div><div><font size=3D=
"4"><br></font></div><div><font size=3D"4">In my experience, when I build m=
pi job on multihost, everytime I change the version of MPI inside the conta=
iner, I have to change the version of MPI on the host.=C2=A0</font></div><d=
iv><font size=3D"4"><br></font></div><div><font size=3D"4">Is there any sol=
ution for run mpi=C2=A0command inside the container on multi-host system?=
=C2=A0</font></div><span class=3D"HOEnZb"><font color=3D"#888888"><div><fon=
t size=3D"4"><br></font></div><div><font size=3D"4"><br></font></div><div><=
font size=3D"4"><br></font></div><div><font size=3D"4"><br></font></div></f=
ont></span></div><span class=3D"HOEnZb"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div di=
r=3D"ltr"><div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"=
><div dir=3D"ltr">--<div>Gregory M. Kurtzer</div><div>CEO, SingularityWare,=
 LLC.</div><div>Senior Architect, RStor</div><div><span style=3D"font-size:=
12.8px">Computational Science Advisor, Lawrence Berkeley National Laborator=
y</span><br></div></div></div></div></div>
</div>

--001a11409d8efd66ea05517473b5--
