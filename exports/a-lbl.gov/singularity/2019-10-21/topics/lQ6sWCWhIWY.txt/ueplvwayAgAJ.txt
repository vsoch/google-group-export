X-Received: by 10.129.102.193 with SMTP id a184mr23309232ywc.111.1499072861846;
        Mon, 03 Jul 2017 02:07:41 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.57.4 with SMTP id g4ls6044944ioa.14.gmail; Mon, 03 Jul
 2017 02:07:41 -0700 (PDT)
X-Received: by 10.99.95.216 with SMTP id t207mr9393092pgb.19.1499072860879;
        Mon, 03 Jul 2017 02:07:40 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1499072860; cv=none;
        d=google.com; s=arc-20160816;
        b=b5FSbxXgfBTXsYCIXIH6heH4MlWfhdjSmtjaazX7u3OL2zO/xwv1ECna8P66+/ChTz
         Zc/P9nNRYMQ/XRgUcT6fIQy11NnxMNQQrTldmy90fa4bVZmG1H214caIt0pJ2Bn/rq06
         pPqN+xrVNEnGy6J7luBcSSTsLBKY322j7rhoNjSmoBpy/z+noNNSmdqTuVAyHdmFOMBP
         2o8+b6a4zYmlg24OMNolqlMLbREUU00jMpkgATi7vRNj8mgQmXYEAx5re3j5LvMRzQ2e
         +BCyDSCod+m08kbP4qNdw0Us/hcTZT08QixD2937QIo8lk4y7thHJLF4GpxtFFDuy2fu
         cOIw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:mime-version:dkim-signature
         :arc-authentication-results;
        bh=6/aoUfW7ckQIZdRy2TuqsZE9n28Ty60CCCCbEwdMw+g=;
        b=Hug6WyN6iEoI4skVKv2PFGpXhNPxJl8TfU7nkeIGxiMfGbQNGDqEU17bC4e0cLMxSn
         ua2uE+y3c2CFOL2TuLQl2WAdlCxPMB3WUzGVKpRLNCcUgmkkZQpnK2ZwdCoqTBc73/4s
         a2sreooYwZs8HGWxrxZHixhbnObv4LmLxpX588a9kl9wOta7qqBYRbyHmGfZLOhX2LON
         JRzHGi+s4xW1SUsmQMhvJ7JAnTD5jvO52VX60cfWqV/cjXrBL6gLeHX7lCB0t3V9v6Pz
         Z4vAFw8z3COqNnXbla3yTwgHS28P1jSV4LZ7KgqWAWcvFIN24kumOyHOIOs/xcs0KIc5
         Zs1w==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.b=o4rAY2YI;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.192.181 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id g21si11766647pfg.344.2017.07.03.02.07.40
        for <singu...@lbl.gov>;
        Mon, 03 Jul 2017 02:07:40 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 209.85.192.181 as permitted sender) client-ip=209.85.192.181;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.b=o4rAY2YI;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.192.181 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EgAgAaCVpZhrXAVdFcHhgHDBgHgnZPA?=
 =?us-ascii?q?T6BDgeFG505hjGHNoUrgU4bKA4ggjOCbE+Cbgc/GAEBAQEBAQEBAQEBAhABAQE?=
 =?us-ascii?q?ICwsIKC+CMyQORiYDAwEBAQEBAQEBASMBAQEBAQEBAQEBAQEBAQEaAggFHhIBE?=
 =?us-ascii?q?kQdARsSDAMSAw0KLQIkAREBBQGJEoE5AQMVBQuTRo1Vg0U/jAqCBAUBHIMIBYN?=
 =?us-ascii?q?TChknDVaDSR4CBhKDFYNMiUASAUmCZoJhBZ5/h0eDQ4h3ggxWjzuJMoo2FB+BF?=
 =?us-ascii?q?Q8QgTgLdRVJGoUJgXQ+NgEBAQGGTIIwAQEB?=
X-IronPort-AV: E=Sophos;i="5.40,302,1496127600"; 
   d="scan'208,217";a="81008640"
Received: from mail-pf0-f181.google.com ([209.85.192.181])
  by fe3.lbl.gov with ESMTP; 03 Jul 2017 02:07:16 -0700
Received: by mail-pf0-f181.google.com with SMTP id q86so97211255pfl.3
        for <singu...@lbl.gov>; Mon, 03 Jul 2017 02:07:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:from:date:message-id:subject:to;
        bh=6/aoUfW7ckQIZdRy2TuqsZE9n28Ty60CCCCbEwdMw+g=;
        b=o4rAY2YIg0pYt72IvV0skcat4r4vR5stteAEdPyirBbZDT371kOUmOWMsKxaW9cL96
         nkafL0HFzHZ45WRjaILjY3V7RgK2q0Ko7VvncOTjsD8jnlWxllLqPJ75B8BckBhfnD+e
         B6Z2aB2W3SC7fvPGyk6NRvkDLcI94SSOhJmTkllr609WgYzivVTRRvP3DM1eBhjuJkpl
         LTapZJu20m38juhB4n+VvIOHgScihYa2np7y7dz9OYcaN976MAFuPxvJMo0x330wG8WW
         xK/OGo48k9Y/nTpQTe+RSMOY1O5C8v6MOYEn5XwPggNU265UoRCCUyH5anuDLThSamFp
         sobw==
X-Gm-Message-State: AIVw113eoqDG/sRlLElbDSz0zemGLv0BF9LOKaMy4lQmdEk+pjwSdEgW
	5lUvFtV6OYH+ocPibq6aJpfpZxncCZMO
X-Received: by 10.84.131.101 with SMTP id 92mr9724763pld.23.1499072835551;
 Mon, 03 Jul 2017 02:07:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.100.185.14 with HTTP; Mon, 3 Jul 2017 02:07:15 -0700 (PDT)
From: victor sv <vict...@gmail.com>
Date: Mon, 3 Jul 2017 11:07:15 +0200
Message-ID: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
Subject: OpenMPI, Slurm & portability
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="94eb2c12e96edb073c0553661740"

--94eb2c12e96edb073c0553661740
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Dear Singularity team,

first of all, thanks for the great work with Singularity. It looks amazing!

Sorry if this topic is duplicated and for the length of the email, but I
want to share my experience about Singularity and OpenMPI compatibility,
and also ask some questions.

I've being reading a lot about OpenMPI and Singularity compatibility
because we are trying to find the generic way to run OpenMPI applications
within Singularity containers. It was not so clear (for me) in the
documentation, forums and mailing lists, and this is why we've performed an
OpenMPI empiric compatibility study.

We ran these comparisons in CESGA FinisTerrae II cluster (
https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2).

We used several versions of OpenMPI. The chosen versions of OpenMPI were
the versions already installed in the cluster:

- openmpi/1.10.2
- openmpi/2.0.0
- openmpi/2.0.1
- openmpi/2.0.2
- openmpi/2.1.1

We have created Singularity images containing the same versions of OpenMPI
and with the basic OpenMPI ring example. I share the bootstrap definition
file template used below:

```
BootStrap: docker
From: ubuntu:16.04
IncludeCmd: yes

%post
        sed -i 's/main/main restricted universe/g' /etc/apt/sources.list
        apt-get update
        apt-get install -y bash git wget build-essential gcc time libc6-dev
libgcc-5-dev
        apt-get install -y dapl2-utils libdapl-dev libdapl2 libibverbs1
librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 libnes1
libpmi0 libpmi0-dev libslurm29 libslurm-dev

        ##Install OpenMPI
        cd /tmp
        wget '
https://www.open-mpi.org/software/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz'
-O openmpi-X.X.X.tar.gz
        tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X
        mkdir -p /tmp/openmpi-X.X.X/build
        cd /tmp/openmpi-X.X.X/build
         ../configure --enable-shared --enable-mpi-thread-multiple
--with-verbs --enable-mpirun-prefix-by-default --with-hwloc
--disable-dlopen --with-pmi --prefix=3D/usr
        make all install

        # Install ring
        cd /tmp
        wget
https://raw.githubusercontent.com/open-mpi/ompi/master/examples/ring_c.c
        mpicc ring_c.c -o /usr/bin/ring
```

Once the containers were created, we ran the ring app with mpirun using 2
cores of 2 different nodes mixing all possible combinations of those
OpenMPI versions inside and outside the container.

The obtained results shown that we need the same versions of OpenMPI inside
and outside the container to succesfully run the contained application in
parallel with mpirun.

Is this the expected behaviour or am I missing something?

Will be this the expected behaviour in the future (with future versions of
OpenMPI)?

Currently, we have slurm 14.11.10-Bull.1.0 installed as job scheduler at
FinisTerrae II. We found the following tip/trick to use srun as process
manager:

http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls

In order to run whatever Singularity image containing OpenMPI applications
using Slurm, we've adapted it to our infrastructure and checked the same
test cases running them with srun. It seems that it's working properly (no
real world applications were tested yet).

What do you think about this strategy?
Can you confirm that it provides portability of singularity images
containing OpenMPI applications?

I think this strategy is similar to the one you are following with "--nv"
option  for NVidia drivers.

Why not to do the same strategy with MPI, PMI, libibverbs, etc.?

Thanks in advance and congrats again for your great work!

V=C3=ADctor.

--94eb2c12e96edb073c0553661740
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Dear Singularity team,<br><br>first of all, thanks for the=
 great work with Singularity. It looks amazing!<br><br>Sorry if this topic =
is duplicated and for the length of the email, but I want to share my exper=
ience about Singularity and OpenMPI compatibility, and also ask some questi=
ons.<br><br>I&#39;ve being reading a lot about OpenMPI and Singularity comp=
atibility because we are trying to find the generic way to run OpenMPI appl=
ications within Singularity containers. It was not so clear (for me) in the=
 documentation, forums and mailing lists, and this is why we&#39;ve perform=
ed an OpenMPI empiric compatibility study.<br><br>We ran these comparisons =
in CESGA FinisTerrae II cluster (<a href=3D"https://www.cesga.es/en/infraes=
tructuras/computacion/FinisTerrae2">https://www.cesga.es/en/infraestructura=
s/computacion/FinisTerrae2</a>).<br><br>We used several versions of OpenMPI=
. The chosen versions of OpenMPI were the versions already installed in the=
 cluster:<br><br>- openmpi/1.10.2<br>- openmpi/2.0.0<br>- openmpi/2.0.1<br>=
- openmpi/2.0.2<br>- openmpi/2.1.1<br><br>We have created Singularity image=
s containing the same versions of OpenMPI and with the basic OpenMPI ring e=
xample. I share the bootstrap definition file template used below:<br><br>`=
``<br>BootStrap: docker<br>From: ubuntu:16.04<br>IncludeCmd: yes<br><br>%po=
st<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sed -i &#39;s/main/main re=
stricted universe/g&#39; /etc/apt/sources.list<br>=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 apt-get update<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 apt-get install -y bash git wget build-essential gcc time libc6-dev =
libgcc-5-dev<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get install =
-y dapl2-utils libdapl-dev libdapl2 libibverbs1 librdmacm1 libcxgb3-1 libip=
athverbs1 libmlx4-1 libmlx5-1 libmthca1 libnes1 libpmi0 libpmi0-dev libslur=
m29 libslurm-dev<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 ##Instal=
l OpenMPI<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget &#39;<a href=3D"https://www.open-=
mpi.org/software/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz">https://www.open=
-mpi.org/software/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz</a>&#39; -O open=
mpi-X.X.X.tar.gz<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 tar -xzf ope=
nmpi-X.X.X.tar.gz -C openmpi-X.X.X<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 mkdir -p /tmp/openmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 cd /tmp/openmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 ../configure --enable-shared --enable-mpi-thread-multipl=
e --with-verbs --enable-mpirun-prefix-by-default --with-hwloc --disable-dlo=
pen --with-pmi --prefix=3D/usr<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 make all install<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 # In=
stall ring<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget <a href=3D"https://raw.githubuser=
content.com/open-mpi/ompi/master/examples/ring_c.c">https://raw.githubuserc=
ontent.com/open-mpi/ompi/master/examples/ring_c.c</a><br>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 mpicc ring_c.c -o /usr/bin/ring<br>```<br><br>Once=
 the containers were created, we ran the ring app with mpirun using 2 cores=
 of 2 different nodes mixing all possible combinations of those OpenMPI ver=
sions inside and outside the container.<br><br>The obtained results shown t=
hat we need the same versions of OpenMPI inside and outside the container t=
o succesfully run the contained application in parallel with mpirun.<br><br=
>Is this the expected behaviour or am I missing something?<br><br>Will be t=
his the expected behaviour in the future (with future versions of OpenMPI)?=
<br><br>Currently, we have slurm 14.11.10-Bull.1.0 installed as job schedul=
er at FinisTerrae II. We found the following tip/trick to use srun as proce=
ss manager:<br><br><a href=3D"http://singularity.lbl.gov/tutorial-gpu-drive=
rs-open-mpi-mtls">http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-=
mtls</a><br><br>In order to run whatever Singularity image containing OpenM=
PI applications using Slurm, we&#39;ve adapted it to our infrastructure and=
 checked the same test cases running them with srun. It seems that it&#39;s=
 working properly (no real world applications were tested yet).<br><br>What=
 do you think about this strategy?<br>Can you confirm that it provides port=
ability of singularity images containing OpenMPI applications?<br><br>I thi=
nk this strategy is similar to the one you are following with &quot;--nv&qu=
ot; option=C2=A0 for NVidia drivers.<br><br>Why not to do the same strategy=
 with MPI, PMI, libibverbs, etc.?<br><br>Thanks in advance and congrats aga=
in for your great work!<br><br>V=C3=ADctor.<br></div>

--94eb2c12e96edb073c0553661740--
