X-Received: by 10.129.157.76 with SMTP id u73mr12776475ywg.16.1468624047625;
        Fri, 15 Jul 2016 16:07:27 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.142.77 with SMTP id q74ls1798669iod.23.gmail; Fri, 15 Jul
 2016 16:07:27 -0700 (PDT)
X-Received: by 10.98.29.201 with SMTP id d192mr19591173pfd.142.1468624047060;
        Fri, 15 Jul 2016 16:07:27 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id 1si11463485pau.29.2016.07.15.16.07.26
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 15 Jul 2016 16:07:27 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) client-ip=209.85.215.70;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2GFAQD3a4lXf0bXVdEaAzgIgnCBJHwGgzaBDKgTjBqBe4JTg0cCgSgHOBQBAQEBAQEBAw8BAQkLCwkfMYRcAQEEARIIAQgrGhYLCQILDRYBCQEJAgIhAQ8DAQUBCxEGCAcEARwEAYd0Aw8IBZJPFI9CgTE+MYs7iWgNhB4BCgEBAQEiEIpngkOBTwwFAWQBgjiCPR0FhgmCBQgHX4UMdT+EJIUNNAGLcQ9DgheBaxeHcYVChl2BRYY6Eh6BDx6CQRyBbBwyB4YvDReBHgEBAQ
X-IronPort-AV: E=Sophos;i="5.28,370,1464678000"; 
   d="scan'208,217";a="30494305"
Received: from mail-lf0-f70.google.com ([209.85.215.70])
  by fe3.lbl.gov with ESMTP; 15 Jul 2016 16:07:22 -0700
Received: by mail-lf0-f70.google.com with SMTP id f199so16163048lfg.2
        for <singu...@lbl.gov>; Fri, 15 Jul 2016 16:07:22 -0700 (PDT)
X-Gm-Message-State: ALyK8tJRKetLiJtd1PgfwBA9ZafI4eQEk7aviFqcqp/HY+jvqWOpY2qBNy+UQEnBUSqIuU90KS35D+aYHXDBrT99VqY4C6729HfyoXeenLse8+41kcRcqc+55SeTD1CwdheOhx/dyGIh3l2HIcRp6duQzJM=
X-Received: by 10.25.24.85 with SMTP id o82mr10070612lfi.23.1468624041773;
        Fri, 15 Jul 2016 16:07:21 -0700 (PDT)
X-Received: by 10.25.24.85 with SMTP id o82mr10070604lfi.23.1468624041315;
 Fri, 15 Jul 2016 16:07:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.158 with HTTP; Fri, 15 Jul 2016 16:07:19 -0700 (PDT)
In-Reply-To: <CAN7etTxyzBC_VSa8Zv6RVEe1C5gETyaMJM=Dkzej94jWaMnbwA@mail.gmail.com>
References: <03a19fb0-27ce-43c4-9400-8e58cf726500@lbl.gov> <CAN7etTwRbSe1MMh9wdQAMYoKVJTb_SGJeHto+WrZ=aU7NoBmhQ@mail.gmail.com>
 <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov> <CAN7etTyqGkWy1P57-cVJgyru5BT_DvnhwDzLe1p38BV8z_PPww@mail.gmail.com>
 <66b82c74-2778-44f2-ae5c-87e01ec8885d@lbl.gov> <CAN7etTz09uzfP-NpZ3-+HnijfrC+u+=pOuBQDTShXG+unxgOVg@mail.gmail.com>
 <c4e5864c-cbaa-4269-9522-db61d63d7cff@lbl.gov> <CAN7etTztBSE1YXY3etq9ipMNnPSKh2Eatz5i-QQOH=ecdNDVCg@mail.gmail.com>
 <0686e644-e7d6-45d7-a371-bf17bead57a4@lbl.gov> <CAN7etTy7cLFaG_NK8-izwc4V-WiU6tsGqNwyVY79TPND9eZnrw@mail.gmail.com>
 <f942d15c-1e7f-401a-8f6b-01ecfec1c7e9@lbl.gov> <CAN7etTz8Sj3CvHsn6ywyLVxfk3g5+kPy021sBbvXCZTtjsyi4w@mail.gmail.com>
 <8d23e9ce-1ce5-4ddb-ab81-f3aafdec4577@lbl.gov> <CAN7etTxyzBC_VSa8Zv6RVEe1C5gETyaMJM=Dkzej94jWaMnbwA@mail.gmail.com>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Fri, 15 Jul 2016 16:07:19 -0700
Message-ID: <CAN7etTx3rzFp_SZ9w_XrgX=04JufSBhHm2G9vqMd9WNU9aGV9Q@mail.gmail.com>
Subject: Re: [Singularity] Image is locked by another process
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a114055264aa8340537b4ae20

--001a114055264aa8340537b4ae20
Content-Type: text/plain; charset=UTF-8

Hi Steve,

Sorry for the delay. The cause of the first error was an easy fix, but
debugging the second had me really stumped for a while! It just so happens
that I was on a conference call with several other developers a bit ago,
and when we were going over this Ben Allen (LANL) pointed me in a new
direction to look, and sure enough it was the loop driver in the kernel
holding the file descriptor that contained the lock open! So to fix that
and ensure that it won't happen again, I decided to use the
LO_FLAGS_AUTOCLEAR flag option, but that brings up a potential race
condition. Which luckily, after much stumpedness (and reviewing the loop
kernel source) I think that I came up with a very simple workaround of
simply opening the loop device itself so it doesn't auto clear itself until
each process has completed! Anyway, long story short...

Can you test the latest master and let me know if it is indeed fixed!

Thanks!



On Fri, Jul 15, 2016 at 11:28 AM, Gregory M. Kurtzer <gmku...@lbl.gov>
wrote:

> Hi Steve,
>
> I think I have fixed the initial error you got regarding being unable to
> create the sessiondir. The second issue regarding file locks, I am
> debugging now. I'll let you know shortly when to retest.
>
> Thanks!
>
>
>
> On Fri, Jul 15, 2016 at 11:26 AM, Steve Mehlberg <sgmeh...@gmail.com>
> wrote:
>
>> The problem only arises when there is an error.
>> The tmp directory is different on each machine, it is not shared or an
>> NFS mount.  Is that a problem?
>>
>> Steve
>>
>> On Friday, July 15, 2016 at 9:51:55 AM UTC-6, Gregory M. Kurtzer wrote:
>>
>>>
>>> On Fri, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <sg...@gmail.com>
>>> wrote:
>>>
>>>> Hello,
>>>>
>>>> I went back and recreated everything from scratch using v2.1.  When I
>>>> moved the image file to the mach1 and mach2 nodes I used tar with the -S
>>>> option.  I used /tmp for the image files.  After everything was set I ran
>>>> mpirun on each individual system (only starting jobs on that system) with
>>>> no errors.  For some reason when I ran mpirun from mach0 and asked for jobs
>>>> to be run on mach0 and another system I get the error.  The mpirun
>>>> complains about not being able to create a file (already exists) and
>>>> hangs.  The singularity image file on mach0 and the other system selected
>>>> are locked with pids that are no longer running.
>>>>
>>>> Saying all that, I decided to go back and put --debug on the
>>>> singularity command in the mpirun to get you better diagnostics.  I
>>>> rebooted mach0 and mach2, then reran the command and it worked correctly.
>>>> I then removed the --debug command and it worked.  I added mach1 to the mix
>>>> and everything worked fine.  What? I guess the bottom line is that if the
>>>> mpirun aborts (and it seems to hang) then you can get this condition.
>>>>
>>>
>>> So does the problem seem to arise only when there is a failure on one of
>>> the processes, and MPI kills the remaining processes?
>>>
>>> Ralph, do you know how OMPI kills the leftover processes in a job abort
>>> and what signal it uses?
>>>
>>> (additional comments inline)
>>>
>>>
>>>> Steve
>>>>
>>>>
>>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 2 -H mach2
>>>> singularity exec /tmp/c7new.img /usr/bin/ring
>>>> Process 1 exiting
>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>> Process 0 sent to 1
>>>> Process 0 decremented value: 9
>>>> Process 0 decremented value: 8
>>>> Process 0 decremented value: 7
>>>> Process 0 decremented value: 6
>>>> Process 0 decremented value: 5
>>>> Process 0 decremented value: 4
>>>> Process 0 decremented value: 3
>>>> Process 0 decremented value: 2
>>>> Process 0 decremented value: 1
>>>> Process 0 decremented value: 0
>>>> Process 0 exiting
>>>>
>>>
>>> Is /tmp NFS mounted or shared?
>>>
>>>
>>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 4 -H mach0,mach2
>>>> singularity exec /tmp/c7new.img /usr/bin/ring
>>>> ERROR  : Could not create directory
>>>> /tmp/.singularity-session-0.64768.158221: File exists
>>>> ERROR  : Failed creating session directory:
>>>> /tmp/.singularity-session-0.64768.158221
>>>> -------------------------------------------------------
>>>> Primary job  terminated normally, but 1 process returned
>>>> a non-zero exit code. Per user-direction, the job has been aborted.
>>>> -------------------------------------------------------
>>>> ^CKilled by signal 2.
>>>> mpirun: abort is already in progress...hit ctrl-c again to forcibly
>>>> terminate
>>>>
>>>
>>> Ahhhhh, I think you may have found a race condition in Singularity which
>>> killed one of the processes. Then the MPI killed the rest.
>>>
>>>
>>>> [root@mach0 ompi]# singularity shell -w /tmp/c7new.img
>>>> ERROR  : Image is locked by another process
>>>> [root@mach0 ompi]# lslocks
>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>>> crond             606 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>> master            820 FLOCK  33B WRITE 0     0   0
>>>> /var/spool/postfix/pid/master.pid
>>>> master            820 FLOCK  33B WRITE 0     0   0
>>>> /var/lib/postfix/master.lock
>>>> slurmctld         924 POSIX   4B WRITE 0     0   0 /run/slurmctld.pid
>>>> slurmd           1103 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
>>>> (unknown)        2477 FLOCK   0B READ  0     0   0 /
>>>> [root@mach0 ompi]# ls /proc/2477
>>>> ls: cannot access /proc/2477: No such file or directory
>>>>
>>>
>>> OK, that is weird. If process 2477 is dead, there should be no active
>>> lock. I would guess this might be a kernel bug? What distribution and
>>> kernel are you running?
>>>
>>>
>>>> [root@mach0 ompi]# ssh mach2
>>>> Last login: Fri Jul 15 07:52:56 2016 from mach0
>>>> [root@mach2 ~]# singularity shell -w /tmp/c7new.img
>>>> ERROR  : Image is locked by another process
>>>> [root@mach2 ~]# lslocks
>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>>> master            851 FLOCK  33B WRITE 0     0   0
>>>> /var/spool/postfix/pid/master.pid
>>>> master            851 FLOCK  33B WRITE 0     0   0
>>>> /var/lib/postfix/master.lock
>>>> (unknown)       10110 FLOCK   0B READ  0     0   0 /
>>>> crond             668 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>> slurmd           1610 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
>>>> [root@mach2 ~]# ls /proc/10110
>>>> ls: cannot access /proc/10110: No such file or directory
>>>>
>>>
>>> Very weird. Seems both nodes have the same issue..?
>>>
>>> I wonder if I can replicate this by kill -9 the outer parent of a
>>> Singularity run. I will test when I get to my development box.
>>>
>>>
>>>>
>>>> Good Run:
>>>>
>>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 6 -H
>>>> mach0,mach1,mach2 singularity exec /tmp/c7new.img /usr/bin/ring
>>>> Process 0 sending 10 to 1, tag 201 (6 processes in ring)
>>>> Process 0 sent to 1
>>>> Process 1 exiting
>>>> Process 0 decremented value: 9
>>>> Process 0 decremented value: 8
>>>> Process 0 decremented value: 7
>>>> Process 0 decremented value: 6
>>>> Process 0 decremented value: 5
>>>> Process 0 decremented value: 4
>>>> Process 0 decremented value: 3
>>>> Process 0 decremented value: 2
>>>> Process 0 decremented value: 1
>>>> Process 0 decremented value: 0
>>>> Process 0 exiting
>>>> Process 2 exiting
>>>> Process 5 exiting
>>>> Process 3 exiting
>>>> Process 4 exiting
>>>>
>>>
>>> So as long as we don't crash (or get killed), all seems fine...
>>> Interesting.
>>>
>>> I will try and replicate the error condition.
>>>
>>> Greg
>>>
>>>
>>>>
>>>>
>>>>
>>>> On Thursday, July 14, 2016 at 3:56:48 PM UTC-6, Gregory M. Kurtzer
>>>> wrote:
>>>>
>>>>> I am wondering if some aspect of the sparse file has caused this.
>>>>> Sparse files are weird, and do require some attention when copying around.
>>>>> With that said, I do not think it should ever corrupt an image, but it
>>>>> might take along garbage data with it. One way to retest this is to go back
>>>>> to the original image, and tar it before sending using the -S/--sparse tar
>>>>> option when packaging it up. Then bring it to the other test system and see
>>>>> if it still does that.
>>>>>
>>>>> Aside from that, I'm a bit confused as to what is happening. There
>>>>> should be no issue with 2.0 vs. 2.1, but it might be worth checking with
>>>>> some images also created with 2.1 on the same target system where you are
>>>>> seeing the problem.
>>>>>
>>>>> On Thu, Jul 14, 2016 at 2:46 PM, Steve Mehlberg <sg...@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> It is very possible that I did copy it from another system - maybe
>>>>>> created it with singularity 2.0 and now running it with 2.1??  I will start
>>>>>> over and recreate from scratch and see if I still have the problem.
>>>>>>
>>>>>> Sorry, I checked there is no /proc/2299
>>>>>>
>>>>>> The only way I've found to get the file unlocked is to reboot.  Not
>>>>>> an issue with the little VM.
>>>>>>
>>>>>> Steve
>>>>>>
>>>>>> On Thursday, July 14, 2016 at 3:30:12 PM UTC-6, Gregory M. Kurtzer
>>>>>> wrote:
>>>>>>
>>>>>>> Very odd...
>>>>>>>
>>>>>>> I'm concerned about the Buffer I/O errors. It is almost like the
>>>>>>> image itself has a problem with it. Did you copy this image from another
>>>>>>> system? I wonder if it is the sparseness...
>>>>>>>
>>>>>>> Does the directory exist for /proc/2299/ ?
>>>>>>>
>>>>>>> Can you kill it with a -9?
>>>>>>>
>>>>>>> On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlberg <sg...@gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Seems that process isn't running any more.  I did find this in the
>>>>>>>> /var/log/messages file:
>>>>>>>>
>>>>>>>> [root@mach0 ~]# cat /var/log/messages | grep 2299
>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)>
>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1,
>>>>>>>> logical block 22299
>>>>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1,
>>>>>>>> logical block 22990
>>>>>>>> ...
>>>>>>>>
>>>>>>>> [root@mach0 ~]# ps -ef |grep 2299
>>>>>>>> root     10545  2002  0 14:15 pts/0    00:00:00 grep --color=auto
>>>>>>>> 2299
>>>>>>>> [root@mach0 ~]# ps |grep 2299
>>>>>>>>
>>>>>>>> On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> What is running at PID 2299?
>>>>>>>>>
>>>>>>>>> On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <
>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>
>>>>>>>>>> Ok, I see my error with shell vs exec.  I'm running on a VM with
>>>>>>>>>> my own space, no NFS involved.  Here is the debug command run and lsofl
>>>>>>>>>>
>>>>>>>>>> [root@mach0 ~]# df -h
>>>>>>>>>> Filesystem             Size  Used Avail Use% Mounted on
>>>>>>>>>> /dev/mapper/vg1-lv001  5.7G  4.3G  1.1G  80% /
>>>>>>>>>> devtmpfs               911M     0  911M   0% /dev
>>>>>>>>>> tmpfs                  920M     0  920M   0% /dev/shm
>>>>>>>>>> tmpfs                  920M   41M  880M   5% /run
>>>>>>>>>> tmpfs                  920M     0  920M   0% /sys/fs/cgroup
>>>>>>>>>> /dev/vda1              190M  110M   67M  63% /boot
>>>>>>>>>> /dev/mapper/vg1-lv002   12G  203M   11G   2% /var
>>>>>>>>>> tmpfs                  184M     0  184M   0% /run/user/0
>>>>>>>>>> [root@mach0 ~]# singularity --debug shell -w c7
>>>>>>>>>> enabling debugging
>>>>>>>>>> ending argument loop
>>>>>>>>>> Exec'ing: /usr/local/libexec/singularity/cli/shell.exec -w+ '['
>>>>>>>>>> -f /usr/local/etc/singularity/init ']'
>>>>>>>>>> + . /usr/local/etc/singularity/init
>>>>>>>>>> ++ unset module
>>>>>>>>>> ++
>>>>>>>>>> PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin
>>>>>>>>>> ++ HISTFILE=/dev/null
>>>>>>>>>> ++ export PATH HISTFILE
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>> + true
>>>>>>>>>> + case ${1:-} in
>>>>>>>>>> + shift
>>>>>>>>>> + SINGULARITY_WRITABLE=1
>>>>>>>>>> + export SINGULARITY_WRITABLE
>>>>>>>>>> + true
>>>>>>>>>> + case ${1:-} in
>>>>>>>>>> + break
>>>>>>>>>> + '[' -z c7 ']'
>>>>>>>>>> + SINGULARITY_IMAGE=c7
>>>>>>>>>> + export SINGULARITY_IMAGE
>>>>>>>>>> + shift
>>>>>>>>>> + exec /usr/local/libexec/singularity/sexec
>>>>>>>>>> VERBOSE [U=0,P=8045]
>>>>>>>>>> message.c:52:init()                        : Set messagelevel to: 5
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:127:main()                         : Gathering and caching user
>>>>>>>>>> info.
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:43:get_user_privs()            : Called get_user_privs(struct
>>>>>>>>>> s_privinfo *uinfo)
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:54:get_user_privs()            : Returning
>>>>>>>>>> get_user_privs(struct s_privinfo *uinfo) = 0
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:134:main()                         : Checking if we can escalate
>>>>>>>>>> privs properly.
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:61:escalate_privs()            : Called escalate_privs(void)
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:73:escalate_privs()            : Returning escalate_privs(void)
>>>>>>>>>> = 0
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:141:main()                         : Setting privs to calling user
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:79:drop_privs()                : Called drop_privs(struct
>>>>>>>>>> s_privinfo *uinfo)
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:87:drop_privs()                : Dropping privileges to GID =
>>>>>>>>>> '0'
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:93:drop_privs()                : Dropping privileges to UID =
>>>>>>>>>> '0'
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:103:drop_privs()               : Confirming we have correct GID
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:109:drop_privs()               : Confirming we have correct UID
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> privilege.c:115:drop_privs()               : Returning drop_privs(struct
>>>>>>>>>> s_privinfo *uinfo) = 0
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:146:main()                         : Obtaining user's homedir
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:150:main()                         : Obtaining file descriptor to
>>>>>>>>>> current directory
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:155:main()                         : Getting current working
>>>>>>>>>> directory path string
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:161:main()                         : Obtaining SINGULARITY_COMMAND
>>>>>>>>>> from environment
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:168:main()                         : Obtaining SINGULARITY_IMAGE
>>>>>>>>>> from environment
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:174:main()                         : Checking container image is a
>>>>>>>>>> file: c7
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:180:main()                         : Building configuration file
>>>>>>>>>> location
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:183:main()                         : Config location:
>>>>>>>>>> /usr/local/etc/singularity/singularity.conf
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:185:main()                         : Checking Singularity
>>>>>>>>>> configuration is a file: /usr/local/etc/singularity/singularity.conf
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:191:main()                         : Checking Singularity
>>>>>>>>>> configuration file is owned by root
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:197:main()                         : Opening Singularity
>>>>>>>>>> configuration file
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:210:main()                         : Checking Singularity
>>>>>>>>>> configuration for 'sessiondir prefix'
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> config_parser.c:47:config_get_key_value()  : Called
>>>>>>>>>> config_get_key_value(fp, sessiondir prefix)
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> config_parser.c:66:config_get_key_value()  : Return
>>>>>>>>>> config_get_key_value(fp, sessiondir prefix) = NULL
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> file.c:48:file_id()                        : Called file_id(c7)
>>>>>>>>>> VERBOSE [U=0,P=8045]
>>>>>>>>>> file.c:58:file_id()                        : Generated file_id:
>>>>>>>>>> 0.64768.26052
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> file.c:60:file_id()                        : Returning file_id(c7) =
>>>>>>>>>> 0.64768.26052
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:217:main()                         : Set sessiondir to:
>>>>>>>>>> /tmp/.singularity-session-0.64768.26052
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:221:main()                         : Set containername to: c7
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:223:main()                         : Setting loop_dev_* paths
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> config_parser.c:47:config_get_key_value()  : Called
>>>>>>>>>> config_get_key_value(fp, container dir)
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> config_parser.c:58:config_get_key_value()  : Return
>>>>>>>>>> config_get_key_value(fp, container dir) = /var/singularity/mnt
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:232:main()                         : Set image mount path to:
>>>>>>>>>> /var/singularity/mnt
>>>>>>>>>> LOG     [U=0,P=8045]
>>>>>>>>>> sexec.c:234:main()                         : Command=shell, Container=c7,
>>>>>>>>>> CWD=/root, Arg1=(null)
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:247:main()                         : Set prompt to: Singularity.c7>
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:249:main()                         : Checking if we are opening
>>>>>>>>>> image as read/write
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:264:main()                         : Opening image as read/write
>>>>>>>>>> only: c7
>>>>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>>>>> sexec.c:271:main()                         : Setting exclusive lock on file
>>>>>>>>>> descriptor: 6
>>>>>>>>>> ERROR   [U=0,P=8045]
>>>>>>>>>> sexec.c:273:main()                         : Image is locked by another
>>>>>>>>>> process
>>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>>> [root@mach0 ~]# lslocks
>>>>>>>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>>>>>>>> crond             601 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>>>>>>>> master            826 FLOCK  33B WRITE 0     0   0
>>>>>>>>>> /var/spool/postfix/pid/master.pid
>>>>>>>>>> master            826 FLOCK  33B WRITE 0     0   0
>>>>>>>>>> /var/lib/postfix/master.lock
>>>>>>>>>> lvmetad           478 POSIX   4B WRITE 0     0   0
>>>>>>>>>> /run/lvmetad.pid
>>>>>>>>>> slurmctld         940 POSIX   4B WRITE 0     0   0
>>>>>>>>>> /run/slurmctld.pid
>>>>>>>>>> slurmd            966 POSIX   4B WRITE 0     0   0 /run/slurmd.pid
>>>>>>>>>> (unknown)        2299 FLOCK   0B READ  0     0   0 /
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gregory M.
>>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <
>>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>>
>>>>>>>>>>>> Gregory,
>>>>>>>>>>>>
>>>>>>>>>>>> Thanks for the quick suggestions.  There don't seem to be any
>>>>>>>>>>>> processes attached to the container and I can't seem to run other commands
>>>>>>>>>>>> (in read mode). I'm not sure what's going on.
>>>>>>>>>>>>
>>>>>>>>>>>> Just lazy with root right now, I need to create a normal uid.
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> Ahh, ok. Been there, done that! lol
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> Steve
>>>>>>>>>>>>
>>>>>>>>>>>> [root@mach0 ~]# ls -la c7
>>>>>>>>>>>> -rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7
>>>>>>>>>>>> [root@mach0 ~]# singularity shell -w c7
>>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> Can you run this command again in --debug mode (singularity
>>>>>>>>>>> --debug ....)
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> What about the command "lslocks"?
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>> [root@mach0 ~]# singularity shell c7 whoami
>>>>>>>>>>>> /usr/bin/whoami: /usr/bin/whoami: cannot execute binary file
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> Ahh, this is normal. You are asking the shell script to read in
>>>>>>>>>>> /usr/bin/whoami. If you want to use shell to run whoami, you must prefix it
>>>>>>>>>>> with the -c (e.g. -c "whoami [args]"), or use the 'exec' Singularity
>>>>>>>>>>> subcommand: "singularity exec c7 whoami"
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>>>> [root@mach0 ~]# ps -ef |grep c7
>>>>>>>>>>>> root      7479  2002  0 11:49 pts/0    00:00:00 grep
>>>>>>>>>>>> --color=auto c7
>>>>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> What kind of file system does your image exist on? Is it NFS by
>>>>>>>>>>> chance? I wonder if there is a host issue with a locking daemon or
>>>>>>>>>>> something else weird going on where it is not giving the exclusive lock
>>>>>>>>>>> properly. If this is NFS or other non local file system, can you copy the
>>>>>>>>>>> image to /tmp, rerun the MPI command to get it to fail again, and try
>>>>>>>>>>> again?
>>>>>>>>>>>
>>>>>>>>>>> Thanks!
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M.
>>>>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>>>>
>>>>>>>>>>>>> Hi Steve,
>>>>>>>>>>>>>
>>>>>>>>>>>>> That means there is an active file descriptor/process still
>>>>>>>>>>>>> running and attached to the container maintaining a shared lock. You can
>>>>>>>>>>>>> run other commands against the container as long as long as the container
>>>>>>>>>>>>> is not being requested as --writable(-w), because that will try and obtain
>>>>>>>>>>>>> an exclusive lock and it will fail if there are any active shared locks.
>>>>>>>>>>>>> Try an "lsof /path/to/c7" to see what processes are attached to it. You may
>>>>>>>>>>>>> see a list like:
>>>>>>>>>>>>>
>>>>>>>>>>>>> # lsof /tmp/Demo-2.img
>>>>>>>>>>>>> COMMAND    PID USER   FD   TYPE DEVICE   SIZE/OFF      NODE
>>>>>>>>>>>>> NAME
>>>>>>>>>>>>> sexec   107975 root    6rR  REG  253,0 1073741856 202112247
>>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>> sexec   107977 root    6r   REG  253,0 1073741856 202112247
>>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>> bash    107982 root    6r   REG  253,0 1073741856 202112247
>>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>>
>>>>>>>>>>>>> Notice the two top ones are 'sexec' which are part of the
>>>>>>>>>>>>> Singularity process stack. Kill the bottom one, and those should go away
>>>>>>>>>>>>> naturally.
>>>>>>>>>>>>>
>>>>>>>>>>>>> BTW, as long as you have installed Singularity as root, there
>>>>>>>>>>>>> is no need to run Singularity commands as root (unless you want to make
>>>>>>>>>>>>> system changes within the container).
>>>>>>>>>>>>>
>>>>>>>>>>>>> Hope that helps!
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <
>>>>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>>>>
>>>>>>>>>>>>>> Running mpirun tests, when an abort occurs, my image ends up
>>>>>>>>>>>>>> locked.  Is there a way to clear the lock without rebooting?  I looked for
>>>>>>>>>>>>>> processes that I could kill, but didn't see anything worthy.
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> I'm using singularity v2.1 on Centos 7.2 (both host and
>>>>>>>>>>>>>> container).
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Regards,
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Steve
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H
>>>>>>>>>>>>>> mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>>>>>>>>>>>> Process 0 sent to 1
>>>>>>>>>>>>>> Process 0 decremented value: 9
>>>>>>>>>>>>>> Process 0 decremented value: 8
>>>>>>>>>>>>>> Process 0 decremented value: 7
>>>>>>>>>>>>>> Process 0 decremented value: 6
>>>>>>>>>>>>>> Process 0 decremented value: 5
>>>>>>>>>>>>>> Process 0 decremented value: 4
>>>>>>>>>>>>>> Process 0 decremented value: 3
>>>>>>>>>>>>>> Process 0 decremented value: 2
>>>>>>>>>>>>>> Process 0 decremented value: 1
>>>>>>>>>>>>>> Process 0 decremented value: 0
>>>>>>>>>>>>>> Process 0 exiting
>>>>>>>>>>>>>> Process 1 exiting
>>>>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H
>>>>>>>>>>>>>> mach0,mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>>>>> It appears as if there is not enough space for
>>>>>>>>>>>>>> /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory
>>>>>>>>>>>>>> backing
>>>>>>>>>>>>>> file). It is likely that your MPI job will now either abort
>>>>>>>>>>>>>> or experience
>>>>>>>>>>>>>> performance degradation.
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>   Local host:  mach0
>>>>>>>>>>>>>>   Space Requested: 4194312 B
>>>>>>>>>>>>>>   Space Available: 0 B
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>>>>> [mach0:02308] create_and_attach: unable to create shared
>>>>>>>>>>>>>> memory BTL coordinating structure :: size 134217728
>>>>>>>>>>>>>> [mach0:02291] 2 more processes have sent help message
>>>>>>>>>>>>>> help-opal-shmem-mmap.txt / target full
>>>>>>>>>>>>>> [mach0:02291] Set MCA parameter "orte_base_help_aggregate" to
>>>>>>>>>>>>>> 0 to see all help / error messages
>>>>>>>>>>>>>> ^CKilled by signal 2.
>>>>>>>>>>>>>> Killed by signal 2.
>>>>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2308
>>>>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2309
>>>>>>>>>>>>>> [warn] Epoll ADD(4) on fd 31 failed.  Old events were 0; read
>>>>>>>>>>>>>> change was 0 (none); write change was 1 (add): Bad file descriptor
>>>>>>>>>>>>>> ^C[root@mach0 ~]singularity shell -w c7
>>>>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>>>>> [root@mach0 ~]# tail -30 /var/log/messages
>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon
>>>>>>>>>>>>>> management.
>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Reached target Multi-User
>>>>>>>>>>>>>> System.
>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.
>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about
>>>>>>>>>>>>>> System Runlevel Changes...
>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data
>>>>>>>>>>>>>> Collection 10s After Completed Startup.
>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Update UTMP about
>>>>>>>>>>>>>> System Runlevel Changes.
>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel
>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]
>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel
>>>>>>>>>>>>>> arming.
>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms
>>>>>>>>>>>>>> (kernel) + 1.100s (initrd) + 4.931s (userspace) = 6.446s.
>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.
>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting user-0.slice.
>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user
>>>>>>>>>>>>>> root.
>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Started Session 1 of user root.
>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user
>>>>>>>>>>>>>> root.
>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2023)>
>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)>
>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Image
>>>>>>>>>>>>>> is locked by another process
>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 kernel: loop: module loaded
>>>>>>>>>>>>>> Jul 14 10:43:38 mach0 Singularity: sexec (U=0,P=2050)>
>>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>>> Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>> Jul 14 10:49:17 mach0 Singularity: sexec (U=0,P=2203)>
>>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>>> Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>> Jul 14 10:50:39 mach0 Singularity: sexec (U=0,P=2244)>
>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>> Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)>
>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2300)>
>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>> Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)>
>>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Image
>>>>>>>>>>>>>> is locked by another process
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> --
>>>>>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from
>>>>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> --
>>>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>>>>> University of California
>>>>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>>>>
>>>>>>>>>>>> --
>>>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from
>>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> --
>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>>> University of California
>>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>>
>>>>>>>>>> --
>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>> University of California
>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>
>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Gregory M. Kurtzer
>>>>>>> High Performance Computing Services (HPCS)
>>>>>>> University of California
>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>
>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gregory M. Kurtzer
>>>>> High Performance Computing Services (HPCS)
>>>>> University of California
>>>>> Lawrence Berkeley National Laboratory
>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> High Performance Computing Services (HPCS)
>>> University of California
>>> Lawrence Berkeley National Laboratory
>>> One Cyclotron Road, Berkeley, CA 94720
>>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a114055264aa8340537b4ae20
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Steve,<div><br></div><div>Sorry for the delay. The caus=
e of the first error was an easy fix, but debugging the second had me reall=
y stumped for a while! It just so happens that I was on a conference call w=
ith several other developers a bit ago, and when we were going over this Be=
n Allen (LANL) pointed me in a new direction to look, and sure enough it wa=
s the loop driver in the kernel holding the file descriptor that contained =
the lock open! So to fix that and ensure that it won&#39;t happen again, I =
decided to use the LO_FLAGS_AUTOCLEAR flag option, but that brings up a pot=
ential race condition. Which luckily, after much stumpedness (and reviewing=
 the loop kernel source) I think that I came up with a very simple workarou=
nd of simply opening the loop device itself so it doesn&#39;t auto clear it=
self until each process has completed! Anyway, long story short...=C2=A0</d=
iv><div><br></div><div>Can you test the latest master and let me know if it=
 is indeed fixed!</div><div><br></div><div>Thanks!</div><div><br></div><div=
><br></div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">=
On Fri, Jul 15, 2016 at 11:28 AM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;=
<a href=3D"mailto:gmku...@lbl.gov" target=3D"_blank">gmku...@lbl.gov</a>&gt=
;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 =
.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Steve=
,<div><br></div><div>I think I have fixed the initial error you got regardi=
ng being unable to create the sessiondir. The second issue regarding file l=
ocks, I am debugging now. I&#39;ll let you know shortly when to retest.</di=
v><div><br></div><div>Thanks!</div><div><div class=3D"h5"><div><br></div><d=
iv><br><div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Fr=
i, Jul 15, 2016 at 11:26 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a href=
=3D"mailto:sgmeh...@gmail.com" target=3D"_blank">sgmeh...@gmail.com</a>&gt;=
</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>The =
problem only arises when there is an error.</div><div>The tmp directory is =
different on each machine, it is not shared or an NFS mount.=C2=A0 Is that =
a problem?</div><div><br></div><div>Steve<br><br>On Friday, July 15, 2016 a=
t 9:51:55 AM UTC-6, Gregory M. Kurtzer wrote:</div><div><div><blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;bord=
er-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:soli=
d"><div dir=3D"ltr"><div><br><div class=3D"gmail_quote">On Fri, Jul 15, 201=
6 at 8:26 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg..=
.@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" styl=
e=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,20=
4,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div=
>Hello,</div><div><br></div><div>I went back and recreated everything from =
scratch using v2.1.=C2=A0 When I moved the image file to the mach1 and mach=
2 nodes I used tar with the -S option.=C2=A0 I used /tmp for the image file=
s.=C2=A0 After everything was set I ran mpirun on each individual system (o=
nly starting jobs on that system)=C2=A0with no errors.=C2=A0 For some reaso=
n when I ran mpirun from mach0 and asked for jobs to be run on mach0 and an=
other system I get the error.=C2=A0 The mpirun complains about not being ab=
le to create a file (already exists) and hangs.=C2=A0 The singularity image=
 file on mach0 and the other system selected are locked with pids that are =
no longer running.</div><div><br></div><div>Saying all that, I decided to g=
o back and put --debug on the singularity command in the mpirun to get you =
better diagnostics.=C2=A0 I rebooted mach0 and mach2, then reran the comman=
d and it worked correctly.=C2=A0 I then removed the --debug command and it =
worked.=C2=A0 I added mach1 to the mix and everything worked fine.=C2=A0 Wh=
at? I guess the bottom line is that if the mpirun aborts (and it seems to h=
ang) then you can get this condition.</div></div></blockquote><div><br></di=
v><div>So does the problem seem to arise only when there is a failure on on=
e of the processes, and MPI kills the remaining processes?<br></div><div><b=
r></div><div>Ralph, do you know how OMPI kills the leftover processes in a =
job abort and what signal it uses?</div><div><br></div><div>(additional com=
ments inline)</div><div><br></div><blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204=
,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div>=
<br>Steve</div><div><br></div><div><br></div><div><font face=3D"courier new=
,monospace">[root@mach0 ompi]# mpirun --allow-run-as-root -np 2 -H mach2 si=
ngularity exec /tmp/c7new.img /usr/bin/ring<br>Process 1 exiting<span><br>P=
rocess 0 sending 10 to 1, tag 201 (2 processes in ring)<br>Process 0 sent t=
o 1<br>Process 0 decremented value: 9<br>Process 0 decremented value: 8<br>=
Process 0 decremented value: 7<br>Process 0 decremented value: 6<br>Process=
 0 decremented value: 5<br>Process 0 decremented value: 4<br>Process 0 decr=
emented value: 3<br>Process 0 decremented value: 2<br>Process 0 decremented=
 value: 1<br>Process 0 decremented value: 0<br>Process 0 exiting<br></span>=
</font></div></div></blockquote><div><br></div><div>Is /tmp NFS mounted or =
shared?</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);=
border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div><font =
face=3D"courier new,monospace"><span></span>[root@mach0 ompi]# mpirun --all=
ow-run-as-root -np 4 -H mach0,mach2 singularity exec /tmp/c7new.img /usr/bi=
n/ring<br><font color=3D"#ff0000">ERROR=C2=A0 : Could not create directory =
/tmp/.singularity-session-0.64768.158221: File exists<br>ERROR=C2=A0 : Fail=
ed creating session directory: /tmp/.singularity-session-0.64768.158221</fo=
nt><br>-------------------------------------------------------<br>Primary j=
ob=C2=A0 terminated normally, but 1 process returned<br>a non-zero exit cod=
e. Per user-direction, the job has been aborted.<br>-----------------------=
--------------------------------<br><font color=3D"#ff0000">^C</font>Killed=
 by signal 2.<br>mpirun: abort is already in progress...hit ctrl-c again to=
 forcibly terminate</font></div></div></blockquote><div><br></div><div>Ahhh=
hh, I think you may have found a race condition in Singularity which killed=
 one of the processes. Then the MPI killed the rest.</div><div>=C2=A0</div>=
<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding=
-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-l=
eft-style:solid"><div dir=3D"ltr"><div><font face=3D"courier new,monospace"=
>[root@mach0 ompi]# singularity shell -w /tmp/c7new.img<span><br>ERROR=C2=
=A0 : Image is locked by another process<br></span>[root@mach0 ompi]# lsloc=
ks<span><br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH<br></span>lvmetad=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 473 POSIX=C2=A0=
=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/lvmetad.pid<=
br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 606 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=
=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 820 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=
=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.pid<br>master=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 820 FLOCK=C2=A0 33B WRI=
TE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix/master.lock<b=
r>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 924 POSIX=C2=A0=
=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmctld.pi=
d<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 110=
3 POSIX=C2=A0=C2=A0 5B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run=
/slurmd.pid<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <font co=
lor=3D"#ff0000">2477</font> FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>[root@mach0 ompi]# ls /proc/2477<br>ls: c=
annot access /proc/2477: No such file or directory<br></font></div></div></=
blockquote><div><br></div><div>OK, that is weird. If process 2477 is dead, =
there should be no active lock. I would guess this might be a kernel bug? W=
hat distribution and kernel are you running?</div><div>=C2=A0</div><blockqu=
ote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1e=
x;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-styl=
e:solid"><div dir=3D"ltr"><div><font face=3D"courier new,monospace">[root@m=
ach0 ompi]# ssh mach2<br>Last login: Fri Jul 15 07:52:56 2016 from mach0<br=
>[root@mach2 ~]# singularity shell -w /tmp/c7new.img<span><br>ERROR=C2=A0 :=
 Image is locked by another process<br></span>[root@mach2 ~]# lslocks<span>=
<br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=
=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH<br></span>lvmetad=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 473 POSIX=C2=A0=C2=A0 4B W=
RITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/lvmetad.pid<br>master=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 851 FLOC=
K=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spool/post=
fix/pid/master.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 851 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=
=C2=A0=C2=A0 0 /var/lib/postfix/master.lock<br>(unknown)=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 <font color=3D"#ff0000">10110</font> FLOCK=C2=A0=C2=A0 0=
B READ=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>crond=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 668 FLOCK=
=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.=
pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1=
610 POSIX=C2=A0=C2=A0 5B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /r=
un/slurmd.pid<br>[root@mach2 ~]# ls /proc/10110<br>ls: cannot access /proc/=
10110: No such file or directory</font></div></div></blockquote><div><br></=
div><div>Very weird. Seems both nodes have the same issue..?</div><div><br>=
</div><div>I wonder if I can replicate this by kill -9 the outer parent of =
a Singularity run. I will test when I get to my development box.</div><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0=
.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:=
1px;border-left-style:solid"><div dir=3D"ltr"><div><br></div><div>Good Run:=
</div><div><br></div><div>[<font face=3D"courier new,monospace">root@mach0 =
ompi]# mpirun --allow-run-as-root -np 6 -H mach0,mach1,mach2 singularity ex=
ec /tmp/c7new.img /usr/bin/ring<br>Process 0 sending 10 to 1, tag 201 (6 pr=
ocesses in ring)<span><br>Process 0 sent to 1<br></span>Process 1 exiting<s=
pan><br>Process 0 decremented value: 9<br>Process 0 decremented value: 8<br=
>Process 0 decremented value: 7<br>Process 0 decremented value: 6<br>Proces=
s 0 decremented value: 5<br>Process 0 decremented value: 4<br>Process 0 dec=
remented value: 3<br>Process 0 decremented value: 2<br>Process 0 decremente=
d value: 1<br>Process 0 decremented value: 0<br>Process 0 exiting<br></span=
>Process 2 exiting<br>Process 5 exiting<br>Process 3 exiting<br>Process 4 e=
xiting</font></div></div></blockquote><div><br></div><div>So as long as we =
don&#39;t crash (or get killed), all seems fine... Interesting.</div><div><=
br></div><div>I will try and replicate the error condition.</div><div><br><=
/div><div>Greg</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" styl=
e=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,20=
4,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div=
><br></div><span><div><br></div><div><br>On Thursday, July 14, 2016 at 3:56=
:48 PM UTC-6, Gregory M. Kurtzer wrote:</div></span><blockquote class=3D"gm=
ail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-c=
olor:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><span>=
<div dir=3D"ltr">I am wondering if some aspect of the sparse file has cause=
d this. Sparse files are weird, and do require some attention when copying =
around. With that said, I do not think it should ever corrupt an image, but=
 it might take along garbage data with it. One way to retest this is to go =
back to the original image, and tar it before sending using the -S/--sparse=
 tar option when packaging it up. Then bring it to the other test system an=
d see if it still does that.<div><br></div><div>Aside from that, I&#39;m a =
bit confused as to what is happening. There should be no issue with 2.0 vs.=
 2.1, but it might be worth checking with some images also created with 2.1=
 on the same target system where you are seeing the problem.</div></div></s=
pan><div><div><div><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 2=
:46 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmai=
l.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"m=
argin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204)=
;border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div>It is=
 very possible that I did copy it from another system - maybe created it wi=
th singularity 2.0 and now running it with 2.1??=C2=A0 I will start over an=
d recreate from scratch and see if I still have the problem.</div><div><br>=
</div><div>Sorry, I checked there is no /proc/2299</div><div><br></div><div=
>The only way I&#39;ve found to get the file unlocked is to reboot.=C2=A0 N=
ot an issue with the little VM.</div><span><font color=3D"#888888"><div><br=
></div></font></span><div><span><font color=3D"#888888">Steve</font></span>=
<span><br><br>On Thursday, July 14, 2016 at 3:30:12 PM UTC-6, Gregory M. Ku=
rtzer wrote:</span></div><blockquote class=3D"gmail_quote" style=3D"margin:=
0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);borde=
r-left-width:1px;border-left-style:solid"><span><div dir=3D"ltr">Very odd..=
.<div><br></div><div>I&#39;m concerned about the Buffer I/O errors. It is a=
lmost like the image itself has a problem with it. Did you copy this image =
from another system? I wonder if it is the sparseness...</div><div><br></di=
v><div>Does the directory exist for /proc/2299/ ?</div><div><br></div><div>=
Can you kill it with a -9?</div></div></span><div><div><div><br><div class=
=3D"gmail_quote">On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlberg <span dir=
=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><=
blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-=
left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-le=
ft-style:solid"><div dir=3D"ltr"><div>Seems that process isn&#39;t running =
any more.=C2=A0 I did find this in the /var/log/messages file:</div><div><b=
r></div><div><font face=3D"courier new,monospace">[root@mach0 ~]# cat /var/=
log/messages | grep 2299<span><br>Jul 14 10:51:34 mach0 Singularity: sexec =
(U=3D0,P=3D2299)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/u=
sr/bin/ring<br></span>Jul 14 11:37:17 mach0 kernel: Buffer I/O error on dev=
ice loop1, logical block 22299<br>Jul 14 11:37:17 mach0 kernel: Buffer I/O =
error on device loop1, logical block 22990<br>...</font></div><div><font fa=
ce=3D"courier new,monospace"><br></font></div><div><font face=3D"courier ne=
w,monospace">[root@mach0 ~]# ps -ef |grep 2299<br>root=C2=A0=C2=A0=C2=A0=C2=
=A0 10545=C2=A0 2002=C2=A0 0 14:15 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --=
color=3Dauto 2299<br>[root@mach0 ~]# ps |grep 2299</font><br></div><span><d=
iv><br>On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer w=
rote:</div></span><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px=
 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-=
width:1px;border-left-style:solid"><span><div dir=3D"ltr">What is running a=
t PID=C2=A0<span style=3D"font-size:12.8px">2299?</span></div></span><div><=
div><div><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 1:35 PM, St=
eve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&=
gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px =
0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-le=
ft-width:1px;border-left-style:solid"><div dir=3D"ltr">Ok, I see my error w=
ith shell vs exec.=C2=A0 I&#39;m running on a VM with my own space, no NFS =
involved.=C2=A0 Here is the debug command run and lsofl<br><br>[root@mach0 =
~]# df -h<br>Filesystem=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 Size=C2=A0 Used Avail Use% Mounted on<br>/dev/mapper/=
vg1-lv001=C2=A0 5.7G=C2=A0 4.3G=C2=A0 1.1G=C2=A0 80% /<br>devtmpfs=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 911M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 911M=C2=A0=C2=A0 0% /dev<br>tmpfs=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=C2=
=A0 0% /dev/shm<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0 41M=C2=
=A0 880M=C2=A0=C2=A0 5% /run<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=
=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=C2=A0 0% /sys/fs/cgroup<br>/dev/vd=
a1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 190M=C2=A0 110M=C2=A0=C2=A0 67M=C2=A0 63% /boot<br>/dev/mapper/vg1-l=
v002=C2=A0=C2=A0 12G=C2=A0 203M=C2=A0=C2=A0 11G=C2=A0=C2=A0 2% /var<br>tmpf=
s=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 184M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 184M=C2=
=A0=C2=A0 0% /run/user/0<br>[root@mach0 ~]# singularity --debug shell -w c7=
<br>enabling debugging<br>ending argument loop<br>Exec&#39;ing: /usr/local/=
libexec/singularity/cli/shell.exec -w+ &#39;[&#39; -f /usr/local/etc/singul=
arity/init &#39;]&#39;<br>+ . /usr/local/etc/singularity/init<br>++ unset m=
odule<br>++ PATH=3D/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/=
bin:/bin:/sbin:/usr/bin:/usr/sbin<br>++ HISTFILE=3D/dev/null<br>++ export P=
ATH HISTFILE<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; =
-n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++=
 &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39=
;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#=
39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>+ true<=
br>+ case ${1:-} in<br>+ shift<br>+ SINGULARITY_WRITABLE=3D1<br>+ export SI=
NGULARITY_WRITABLE<br>+ true<br>+ case ${1:-} in<br>+ break<br>+ &#39;[&#39=
; -z c7 &#39;]&#39;<br>+ SINGULARITY_IMAGE=3Dc7<br>+ export SINGULARITY_IMA=
GE<br>+ shift<br>+ exec /usr/local/libexec/singularity/sexec<br>VERBOSE [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 message.c:52:init()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set messagel=
evel to: 5<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 sexec.c:127:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Gathering and caching user info.<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:43:=
get_user_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Called get_user_privs(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:54:=
get_user_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Returning get_user_privs(struct s_privinfo *uinfo) =3D 0<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c=
:134:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Checking if we can escalate privs properly.<br>DEBUG=C2=A0=C2=A0 [=
U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:61:escalate=
_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
: Called escalate_privs(void)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:73:escalate_privs()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning escalate=
_privs(void) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:141:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting privs to calling user<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege=
.c:79:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called drop_privs(struct s_privinfo *ui=
nfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 privilege.c:87:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping privileges to G=
ID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 privilege.c:93:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping =
privileges to UID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:103:drop_privs()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Confirming we have correct GID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:109:drop_privs()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Con=
firming we have correct UID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:115:drop_privs()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Return=
ing drop_privs(struct s_privinfo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:146:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining us=
er&#39;s homedir<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:150:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining file descriptor to current direc=
tory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:155:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Getting current working directory path string<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:1=
61:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Obtaining SINGULARITY_COMMAND from environment<br>DEBUG=C2=A0=C2=A0 [=
U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:168:main()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtain=
ing SINGULARITY_IMAGE from environment<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:174:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking container im=
age is a file: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:180:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Building configuration file location<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sex=
ec.c:183:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Config location: /usr/local/etc/singularity/singularity.conf<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sex=
ec.c:185:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Checking Singularity configuration is a file: /usr/local/etc/si=
ngularity/singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:191:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity configura=
tion file is owned by root<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:197:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening Singularity configurat=
ion file<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 sexec.c:210:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity configuration for &#39;sess=
iondir prefix&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 config_parser.c:47:config_get_key_value()=C2=A0 : Called=
 config_get_key_value(fp, sessiondir prefix)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:66:config_get_=
key_value()=C2=A0 : Return config_get_key_value(fp, sessiondir prefix) =3D =
NULL<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 file.c:48:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Called file_id(c7)<br>VERBOSE [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 file.c:58:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Generated file_id: 0.64768.26052<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:=
60:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Returning file_id(c7) =3D 0.64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:217:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set sessiondir to:=
 /tmp/.singularity-session-0.64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D80=
45]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:221:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set containername =
to: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:223:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Setting loop_dev_* paths<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:47:config_g=
et_key_value()=C2=A0 : Called config_get_key_value(fp, container dir)<br>DE=
BUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config=
_parser.c:58:config_get_key_value()=C2=A0 : Return config_get_key_value(fp,=
 container dir) =3D /var/singularity/mnt<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D80=
45]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:232:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set image mount pa=
th to: /var/singularity/mnt<br>LOG=C2=A0=C2=A0=C2=A0=C2=A0 [U=3D0,P=3D8045]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:234:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Command=3Dshell, Cont=
ainer=3Dc7, CWD=3D/root, Arg1=3D(null)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:247:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set prompt to: Singul=
arity.c7&gt;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:249:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we are opening image as read/writ=
e<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 sexec.c:264:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Opening image as read/write only: c7<br>DEBUG=C2=A0=C2=A0=
 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:271:main()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Settin=
g exclusive lock on file descriptor: 6<br>ERROR=C2=A0=C2=A0 [U=3D0,P=3D8045=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:273:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Image is locked by an=
other process<span><br>[root@mach0 ~]# lsof c7<br></span>[root@mach0 ~]# ls=
locks<br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH<br>crond=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 601 FLOCK=C2=A0=
=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br=
>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 8=
26 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spo=
ol/postfix/pid/master.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix/master.lock<br>lvmetad=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 478 POSIX=C2=A0=C2=A0 4B W=
RITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/lvmetad.pid<br>slurmctl=
d=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 940 POSIX=C2=A0=C2=A0 4B =
WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmctld.pid<br>slurm=
d=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 966 POS=
IX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slur=
md.pid<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2299 FLOCK=C2=
=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br><br>=
<br>On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gregory M. Kurtzer wrot=
e:<div><div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0=
.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:=
1px;border-left-style:solid"><div dir=3D"ltr"><br><div><br><div class=3D"gm=
ail_quote">On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <span dir=3D"lt=
r">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1=
ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-sty=
le:solid"><div dir=3D"ltr">Gregory,<br><br>Thanks for the quick suggestions=
.=C2=A0 There don&#39;t seem to be any processes attached to the container =
and I can&#39;t seem to run other commands (in read mode). I&#39;m not sure=
 what&#39;s going on.<br><br>Just lazy with root right now, I need to creat=
e a normal uid.<br></div></blockquote><div><br></div><div>Ahh, ok. Been the=
re, done that! lol</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(20=
4,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">=
<br>Steve<br><br><span style=3D"font-family:&quot;courier new&quot;,monospa=
ce">[root@mach0 ~]# ls -la c7<br>-rwxr-xr-x 1 root root 1610612769 Jul 14 1=
0:49 c7<br>[root@mach0 ~]# singularity shell -w c7<span><br>ERROR=C2=A0 : I=
mage is locked by another process<br></span></span></div></blockquote><div>=
<br></div><div>Can you run this command again in --debug mode (singularity =
--debug ....)</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204=
,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><span=
 style=3D"font-family:&quot;courier new&quot;,monospace"><span></span>[root=
@mach0 ~]# lsof c7<br></span></div></blockquote><div><br></div><div>What ab=
out the command &quot;lslocks&quot;?</div><div>=C2=A0</div><blockquote clas=
s=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border=
-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid"=
><div dir=3D"ltr"><span style=3D"font-family:&quot;courier new&quot;,monosp=
ace">[root@mach0 ~]# singularity shell c7 whoami<br>/usr/bin/whoami: /usr/b=
in/whoami: cannot execute binary file<br></span></div></blockquote><div><br=
></div><div>Ahh, this is normal. You are asking the shell script to read in=
 /usr/bin/whoami. If you want to use shell to run whoami, you must prefix i=
t with the -c (e.g. -c &quot;whoami [args]&quot;), or use the &#39;exec&#39=
; Singularity subcommand: &quot;singularity exec c7 whoami&quot;</div><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0=
.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:=
1px;border-left-style:solid"><div dir=3D"ltr"><span style=3D"font-family:&q=
uot;courier new&quot;,monospace">[root@mach0 ~]#<br>[root@mach0 ~]# ps -ef =
|grep c7<br>root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 7479=C2=A0 2002=C2=A0 0 11:4=
9 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --color=3Dauto c7<br>[root@mach0 ~]=
#</span></div></blockquote><div><br></div><div>What kind of file system doe=
s your image exist on? Is it NFS by chance? I wonder if there is a host iss=
ue with a locking daemon or something else weird going on where it is not g=
iving the exclusive lock properly. If this is NFS or other non local file s=
ystem, can you copy the image to /tmp, rerun the MPI command to get it to f=
ail again, and try again?=C2=A0</div><div><br></div><div>Thanks!</div><div>=
<br></div><div><br></div><div>=C2=A0</div><blockquote class=3D"gmail_quote"=
 style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(2=
04,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"=
><span><br><br><br><br>On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gre=
gory M. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" style=3D"mar=
gin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);b=
order-left-width:1px;border-left-style:solid"><span><div dir=3D"ltr">Hi Ste=
ve,<div><br></div><div>That means there is an active file descriptor/proces=
s still running and attached to the container maintaining a shared lock. Yo=
u can run other commands against the container as long as long as the conta=
iner is not being requested as --writable(-w), because that will try and ob=
tain an exclusive lock and it will fail if there are any active shared lock=
s. Try an &quot;lsof /path/to/c7&quot; to see what processes are attached t=
o it. You may see a list like:</div><div><br></div><div><div># lsof /tmp/De=
mo-2.img=C2=A0</div><div>COMMAND =C2=A0 =C2=A0PID USER =C2=A0 FD =C2=A0 TYP=
E DEVICE =C2=A0 SIZE/OFF =C2=A0 =C2=A0 =C2=A0NODE NAME</div><div>sexec =C2=
=A0 107975 root =C2=A0 =C2=A06rR =C2=A0REG =C2=A0253,0 1073741856 202112247=
 /tmp/Demo-2.img</div><div>sexec =C2=A0 107977 root =C2=A0 =C2=A06r =C2=A0 =
REG =C2=A0253,0 1073741856 202112247 /tmp/Demo-2.img</div><div>bash =C2=A0 =
=C2=A0107982 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 1073741856 2021122=
47 /tmp/Demo-2.img</div></div><div><br></div><div>Notice the two top ones a=
re &#39;sexec&#39; which are part of the Singularity process stack. Kill th=
e bottom one, and those should go away naturally.</div><div><br></div><div>=
BTW, as long as you have installed Singularity as root, there is no need to=
 run Singularity commands as root (unless you want to make system changes w=
ithin the container).</div><div><br></div><div>Hope that helps!</div><div><=
br></div></div></span><div><br><div class=3D"gmail_quote"><div><div>On Thu,=
 Jul 14, 2016 at 10:56 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"n=
ofollow">sg...@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;bor=
der-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:sol=
id"><div><div><div dir=3D"ltr">Running mpirun tests, when an abort occurs, =
my image ends up locked.=C2=A0 Is there a way to clear the lock without reb=
ooting?=C2=A0 I looked for processes that I could kill, but didn&#39;t see =
anything worthy.<br><br>I&#39;m using singularity v2.1 on Centos 7.2 (both =
host and container).<br><br>Regards,<br><br>Steve<br><br><span style=3D"fon=
t-family:&quot;courier new&quot;,monospace">[root@mach0 ~]# mpirun --allow-=
run-as-root -n 2 -H mach1,mach2 singularity exec c7 /usr/bin/ring<br>Proces=
s 0 sending 10 to 1, tag 201 (2 processes in ring)<br>Process 0 sent to 1<b=
r>Process 0 decremented value: 9<br>Process 0 decremented value: 8<br>Proce=
ss 0 decremented value: 7<br>Process 0 decremented value: 6<br>Process 0 de=
cremented value: 5<br>Process 0 decremented value: 4<br>Process 0 decrement=
ed value: 3<br>Process 0 decremented value: 2<br>Process 0 decremented valu=
e: 1<br>Process 0 decremented value: 0<br>Process 0 exiting<br>Process 1 ex=
iting<br>[root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H mach0,mach1,mac=
h2 singularity exec c7 /usr/bin/ring<br>-----------------------------------=
---------------------------------------<br>It appears as if there is not en=
ough space for /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the sh=
ared-memory backing<br>file). It is likely that your MPI job will now eithe=
r abort or experience<br>performance degradation.<br><br>=C2=A0 Local host:=
=C2=A0 mach0<br>=C2=A0 Space Requested: 4194312 B<br>=C2=A0 Space Available=
: 0 B<br>------------------------------------------------------------------=
--------<br>[mach0:02308] create_and_attach: unable to create shared memory=
 BTL coordinating structure :: size 134217728<br>[mach0:02291] 2 more proce=
sses have sent help message help-opal-shmem-mmap.txt / target full<br>[mach=
0:02291] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see=
 all help / error messages<br>^CKilled by signal 2.<br>Killed by signal 2.<=
br>Singularity is sending SIGKILL to child pid: 2308<br>Singularity is send=
ing SIGKILL to child pid: 2309<br>[warn] Epoll ADD(4) on fd 31 failed.=C2=
=A0 Old events were 0; read change was 0 (none); write change was 1 (add): =
Bad file descriptor<br>^C[root@mach0 ~]singularity shell -w c7<br>ERROR=C2=
=A0 : Image is locked by another process<br>[root@mach0 ~]# tail -30 /var/l=
og/messages<br>Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon man=
agement.<br>Jul 14 10:42:17 mach0 systemd: Reached target Multi-User System=
.<br>Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.<br>Jul 14 1=
0:42:17 mach0 systemd: Starting Update UTMP about System Runlevel Changes..=
.<br>Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data Collection=
 10s After Completed Startup.<br>Jul 14 10:42:17 mach0 systemd: Started Upd=
ate UTMP about System Runlevel Changes.<br>Jul 14 10:42:18 mach0 kdumpctl: =
kexec: loaded kdump kernel<br>Jul 14 10:42:18 mach0 kdumpctl: Starting kdum=
p: [OK]<br>Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel arm=
ing.<br>Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms (kernel) +=
 1.100s (initrd) + 4.931s (userspace) =3D 6.446s.<br>Jul 14 10:42:34 mach0 =
systemd: Created slice user-0.slice.<br>Jul 14 10:42:34 mach0 systemd: Star=
ting user-0.slice.<br>Jul 14 10:42:34 mach0 systemd-logind: New session 1 o=
f user root.<br>Jul 14 10:42:34 mach0 systemd: Started Session 1 of user ro=
ot.<br>Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user root.<br>J=
ul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D2023)&gt; Command=3Dexec=
, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:36 mach=
0 Singularity: sexec (U=3D0,P=3D2024)&gt; Command=3Dexec, Container=3Dc7, C=
WD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:36 mach0 Singularity: sexe=
c (U=3D0,P=3D2024)&gt; Image is locked by another process<br>Jul 14 10:42:3=
6 mach0 kernel: loop: module loaded<br>Jul 14 10:43:38 mach0 Singularity: s=
exec (U=3D0,P=3D2050)&gt; Command=3Dshell, Container=3Dc7, CWD=3D/root, Arg=
1=3D(null)<br>Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted filesy=
stem with ordered data mode. Opts: discard<br>Jul 14 10:49:17 mach0 Singula=
rity: sexec (U=3D0,P=3D2203)&gt; Command=3Dshell, Container=3Dc7, CWD=3D/ro=
ot, Arg1=3D(null)<br>Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted=
 filesystem with ordered data mode. Opts: discard<br>Jul 14 10:50:39 mach0 =
Singularity: sexec (U=3D0,P=3D2244)&gt; Command=3Dexec, Container=3Dc7, CWD=
=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:50:39 mach0 kernel: EXT4-fs (lo=
op0): mounted filesystem with ordered data mode. Opts: discard<br>Jul 14 10=
:51:34 mach0 Singularity: sexec (U=3D0,P=3D2299)&gt; Command=3Dexec, Contai=
ner=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mach0 Singul=
arity: sexec (U=3D0,P=3D2300)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/ro=
ot, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): =
mounted filesystem with ordered data mode. Opts: discard<br>Jul 14 10:51:57=
 mach0 Singularity: sexec (U=3D0,P=3D2322)&gt; Command=3Dshell, Container=
=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:51:57 mach0 Singularity: sex=
ec (U=3D0,P=3D2322)&gt; Image is locked by another process</span><span><fon=
t color=3D"#888888"><br><br><br></font></span></div></div></div><span><font=
 color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance C=
omputing Services (HPCS)<br>University of California<br>Lawrence Berkeley N=
ational Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></d=
iv>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div>Gregory M. Ku=
rtzer<br>High Performance Computing Services (HPCS)<br>University of Califo=
rnia<br>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkel=
ey, CA 94720</div></div></div>
</div></div></div></div></div></div>
</blockquote></div><br><br clear=3D"all"><div><br></div>-- <br><div class=
=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><d=
iv>Gregory M. Kurtzer<br>High Performance Computing Services (HPCS)<br>Univ=
ersity of California<br>Lawrence Berkeley National Laboratory<br>One Cyclot=
ron Road, Berkeley, CA 94720</div></div></div>
</div>

--001a114055264aa8340537b4ae20--
