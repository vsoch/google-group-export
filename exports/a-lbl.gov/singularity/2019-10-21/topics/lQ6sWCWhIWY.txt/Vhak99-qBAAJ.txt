X-Received: by 10.13.192.195 with SMTP id b186mr11013176ywd.140.1499627948653;
        Sun, 09 Jul 2017 12:19:08 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.91.20 with SMTP id g20ls4058276itb.6.gmail; Sun, 09 Jul
 2017 12:19:07 -0700 (PDT)
X-Received: by 10.99.18.65 with SMTP id 1mr11123090pgs.132.1499627947666;
        Sun, 09 Jul 2017 12:19:07 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1499627947; cv=none;
        d=google.com; s=arc-20160816;
        b=diSsZ7APb7fB1E3YUjDxyNCkZGuAg6T76YY8LiFBtdV5aHhG+PTxz9fopk3c4HjUV2
         jJLvz8Em/mH0pO293Yr4G7Fflb9k5ZcDlp9RQSZeZ/tsgcTyI38RzOKto2Lmhzy47mIV
         RGNtzEJ3WGRRNFQuAXpfJR7hZNvH3tqbTaddsYZb5eox3TcT51U+TosvK+DinUFS3CMr
         hcCeiPuT2GLcLoRlOnRl6lJ9Ckm+YUtgczbetlyaaGl986vIKKHw8j8u+Ky589mm0b+g
         KzObgF6ALmCkxsQxKW+//zOFciRQTdDPKtKe7KOMh4yvmLo1NmGbM4V5Vq6V9YP08OVg
         XAAg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:references:in-reply-to
         :mime-version:dkim-signature:arc-authentication-results;
        bh=aFmiKIR0D5ipgwMCqu6wJ4/PQaC/2DFlOg92JmY6woc=;
        b=PKmTvmi7u98uDZOxGq5bcdFltEUq+wSQwxyAW+Db9fLBpjiN3CCp0UzgZF4d++DTOr
         S+O9W5fFRPzWqKREuQEVdNWzaXpzDiV9kes3ynf7jNT2T9CdbZofupI8aCvZGUoVqxAH
         hzoYHudwTNUrZORKf3gpEOld3nNZnPHZlkmOMXcMQRCzhYg2bddnCydPerDCpC0nh91r
         qTmih5Xb6jY4d7jBAFo2fojNNNtdhWSyRmd7CJKsBfY0iZxIeljCF21OyvhRwrg/+erE
         2SL1xkgAVS7NQWtN4aEb95Sw9h//YtBk4yRx6E0ZfNK8ghi0Kp7m3gMMbZY9SUzi8B9T
         DlcQ==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.b=U4O3EXN+;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.214.43 as permitted sender) smtp.mailfrom=gmku...@gmail.com
Return-Path: <gmku...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id c16si7147603plk.615.2017.07.09.12.19.07
        for <singu...@lbl.gov>;
        Sun, 09 Jul 2017 12:19:07 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@gmail.com designates 209.85.214.43 as permitted sender) client-ip=209.85.214.43;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.b=U4O3EXN+;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.214.43 as permitted sender) smtp.mailfrom=gmku...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EhAQDhgGJZhivWVdFcHQEFAQsBGAEFA?=
 =?us-ascii?q?QsBgkRAgQ+BFAeDYYE+iGORaYJsjWyFLIFOGyghAQyCM4FcgRBPAoNCBz8YAQE?=
 =?us-ascii?q?BAQEBAQEBAQECEAEBAQgLCwgoL4IzBQIDGgEFCEYmAwMBAQEBAQEBAQEjAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEBGgIIBR4SARIBARgBAQEBAgEjHQENDhILAQMBCwYDAgsKLQI?=
 =?us-ascii?q?CIQEBDgMBBQEcDgcEARwEiDyBOQEDDQgFC4xmkRo/jAqCBAUBHIMIBYNUChknD?=
 =?us-ascii?q?VaDKgEBAQEBAQQBAQEBAQEBAQEBARUCBhKDFoJShX+CV4FkEgFJgmaCYQWRWIV?=
 =?us-ascii?q?dhy47AodGg0WED4RuggxXjz+JOIJBh38UH4EVDxCBBTMLdV4ahEAqH4ITIDYBA?=
 =?us-ascii?q?QEBBIZDR4FpAQEB?=
X-IronPort-AV: E=Sophos;i="5.40,335,1496127600"; 
   d="scan'208,217";a="80981163"
Received: from mail-it0-f43.google.com ([209.85.214.43])
  by fe4.lbl.gov with ESMTP; 09 Jul 2017 12:19:04 -0700
Received: by mail-it0-f43.google.com with SMTP id v202so16833065itb.0
        for <singu...@lbl.gov>; Sun, 09 Jul 2017 12:19:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc;
        bh=aFmiKIR0D5ipgwMCqu6wJ4/PQaC/2DFlOg92JmY6woc=;
        b=U4O3EXN+rgBFA2N/TsjtVEN9/nPGMNfWcQK9W8LDVfukXbzqZoszru8wEOTnnS5WOr
         kSGr8SkaEEXNrbyecJy/7JtA131iLCuQ6GwR9eAdF+jWBPh1G0JlyJCvSDo9WY0uOsNO
         TKIstsNIeoKI4XjTl0OqmZapL4pFBp98/A80EqBqbn2Ek0Hnatph9Gs+r4MBBbrUjIKS
         76IshBPKIdZkfMSLIzrSM9eWmJN2FdFYRwkAwGJ+fA5W+eUf9Dj0ZmHcDGyR2LAJLdHZ
         5lBFM2RysN+3zf6t8CUsmvPPXHuCsWRJosGTcNOMgsXyTPTOFhf7JVAGv1CX0IA5TmW2
         vINw==
X-Gm-Message-State: AIVw110D/w8fHnsHWTnYlK2YUJFcOqdpJl798j6PgRR1FeIQUVYgoIo3
	Y1j0B5ggMRf6zAo4faQmAuZedcbVkKsa
X-Received: by 10.107.6.203 with SMTP id f72mr279519ioi.144.1499627944308;
 Sun, 09 Jul 2017 12:19:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.23.132 with HTTP; Sun, 9 Jul 2017 12:19:03 -0700 (PDT)
In-Reply-To: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
References: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
From: "Gregory M. Kurtzer" <gmku...@gmail.com>
Date: Sun, 9 Jul 2017 12:19:03 -0700
Message-ID: <CAApQTTgQwZ3pu7Xa7RsbuHM7ndP05Kv5ciEBk_q5G-8P3xaUGw@mail.gmail.com>
Subject: Re: [Singularity] OpenMPI, Slurm & portability
To: singularity@lbl.gov
Cc: Ralph Castain <rcas...@gmail.com>
Content-Type: multipart/alternative; boundary="001a113f9044ea91fa0553e756b6"

--001a113f9044ea91fa0553e756b6
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Victor,

Sorry for the latency, I'm on email overload.

Open MPI uses PMI to communicate both inside and outside of the container.
Ralph Castain (on this list, but possibly not monitoring actively) is
leading the PMI effort and he is an active Open MPI developer. We have had
several talks about how to achieve "hetero-versionistic" compatibility
through the PMI handshake. I was under the impression that PMI now supports
that, as long as you are running equal or newer version on the host
(outside the container). Also, I don't know what version of PMI this
feature was introduced in, nor do I know what version of Open MPI includes
that compatibility.

I have CC'ed Ralph, and hopefully he will be able to offer some suggestions=
.

Regarding your question about supporting the MPI libraries in the same
manner that we are doing the Nvidia libraries, that would be hard. Nvidia
specifically builds their libraries to be as generally compatible as
possible (e.g. the same libraries/binaries work on a large array of Linux
distributions). Most people do not build host libraries in a manner that
would be generally compatible as Nvidia does.

Hope that helps!

Greg



On Mon, Jul 3, 2017 at 2:07 AM, victor sv <vict...@gmail.com> wrote:

> Dear Singularity team,
>
> first of all, thanks for the great work with Singularity. It looks amazin=
g!
>
> Sorry if this topic is duplicated and for the length of the email, but I
> want to share my experience about Singularity and OpenMPI compatibility,
> and also ask some questions.
>
> I've being reading a lot about OpenMPI and Singularity compatibility
> because we are trying to find the generic way to run OpenMPI applications
> within Singularity containers. It was not so clear (for me) in the
> documentation, forums and mailing lists, and this is why we've performed =
an
> OpenMPI empiric compatibility study.
>
> We ran these comparisons in CESGA FinisTerrae II cluster (
> https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2).
>
> We used several versions of OpenMPI. The chosen versions of OpenMPI were
> the versions already installed in the cluster:
>
> - openmpi/1.10.2
> - openmpi/2.0.0
> - openmpi/2.0.1
> - openmpi/2.0.2
> - openmpi/2.1.1
>
> We have created Singularity images containing the same versions of OpenMP=
I
> and with the basic OpenMPI ring example. I share the bootstrap definition
> file template used below:
>
> ```
> BootStrap: docker
> From: ubuntu:16.04
> IncludeCmd: yes
>
> %post
>         sed -i 's/main/main restricted universe/g' /etc/apt/sources.list
>         apt-get update
>         apt-get install -y bash git wget build-essential gcc time
> libc6-dev libgcc-5-dev
>         apt-get install -y dapl2-utils libdapl-dev libdapl2 libibverbs1
> librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 libnes=
1
> libpmi0 libpmi0-dev libslurm29 libslurm-dev
>
>         ##Install OpenMPI
>         cd /tmp
>         wget 'https://www.open-mpi.org/software/ompi/vX.X/downloads/
> openmpi-X.X.X.tar.gz' -O openmpi-X.X.X.tar.gz
>         tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X
>         mkdir -p /tmp/openmpi-X.X.X/build
>         cd /tmp/openmpi-X.X.X/build
>          ../configure --enable-shared --enable-mpi-thread-multiple
> --with-verbs --enable-mpirun-prefix-by-default --with-hwloc
> --disable-dlopen --with-pmi --prefix=3D/usr
>         make all install
>
>         # Install ring
>         cd /tmp
>         wget https://raw.githubusercontent.com/open-mpi/ompi/master/
> examples/ring_c.c
>         mpicc ring_c.c -o /usr/bin/ring
> ```
>
> Once the containers were created, we ran the ring app with mpirun using 2
> cores of 2 different nodes mixing all possible combinations of those
> OpenMPI versions inside and outside the container.
>
> The obtained results shown that we need the same versions of OpenMPI
> inside and outside the container to succesfully run the contained
> application in parallel with mpirun.
>
> Is this the expected behaviour or am I missing something?
>
> Will be this the expected behaviour in the future (with future versions o=
f
> OpenMPI)?
>
> Currently, we have slurm 14.11.10-Bull.1.0 installed as job scheduler at
> FinisTerrae II. We found the following tip/trick to use srun as process
> manager:
>
> http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls
>
> In order to run whatever Singularity image containing OpenMPI application=
s
> using Slurm, we've adapted it to our infrastructure and checked the same
> test cases running them with srun. It seems that it's working properly (n=
o
> real world applications were tested yet).
>
> What do you think about this strategy?
> Can you confirm that it provides portability of singularity images
> containing OpenMPI applications?
>
> I think this strategy is similar to the one you are following with "--nv"
> option  for NVidia drivers.
>
> Why not to do the same strategy with MPI, PMI, libibverbs, etc.?
>
> Thanks in advance and congrats again for your great work!
>
> V=C3=ADctor.
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
Gregory M. Kurtzer
CEO, SingularityWare, LLC.
Senior Architect, RStor
Computational Science Advisor, Lawrence Berkeley National Laboratory

--001a113f9044ea91fa0553e756b6
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Victor,<div><br></div><div>Sorry for the latency, I&#39=
;m on email overload.</div><div><br></div><div>Open MPI uses PMI to communi=
cate both inside and outside of the container. Ralph Castain (on this list,=
 but possibly not monitoring actively) is leading the PMI effort and he is =
an active Open MPI developer. We have had several talks about how to achiev=
e &quot;hetero-versionistic&quot; compatibility through the PMI handshake. =
I was under the impression that PMI now supports that, as long as you are r=
unning equal or newer version on the host (outside the container). Also, I =
don&#39;t know what version of PMI this feature was introduced in, nor do I=
 know what version of Open MPI includes that compatibility.</div><div><br><=
/div><div>I have CC&#39;ed Ralph, and hopefully he will be able to offer so=
me suggestions.</div><div><br></div><div>Regarding your question about supp=
orting the MPI libraries in the same manner that we are doing the Nvidia li=
braries, that would be hard. Nvidia specifically builds their libraries to =
be as generally compatible as possible (e.g. the same libraries/binaries wo=
rk on a large array of Linux distributions). Most people do not build host =
libraries in a manner that would be generally compatible as Nvidia does.</d=
iv><div><br></div><div>Hope that helps!</div><div><br></div><div>Greg</div>=
<div><br></div><div><br></div></div><div class=3D"gmail_extra"><br><div cla=
ss=3D"gmail_quote">On Mon, Jul 3, 2017 at 2:07 AM, victor sv <span dir=3D"l=
tr">&lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@gmai=
l.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"m=
argin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"l=
tr">Dear Singularity team,<br><br>first of all, thanks for the great work w=
ith Singularity. It looks amazing!<br><br>Sorry if this topic is duplicated=
 and for the length of the email, but I want to share my experience about S=
ingularity and OpenMPI compatibility, and also ask some questions.<br><br>I=
&#39;ve being reading a lot about OpenMPI and Singularity compatibility bec=
ause we are trying to find the generic way to run OpenMPI applications with=
in Singularity containers. It was not so clear (for me) in the documentatio=
n, forums and mailing lists, and this is why we&#39;ve performed an OpenMPI=
 empiric compatibility study.<br><br>We ran these comparisons in CESGA Fini=
sTerrae II cluster (<a href=3D"https://www.cesga.es/en/infraestructuras/com=
putacion/FinisTerrae2" target=3D"_blank">https://www.cesga.es/en/<wbr>infra=
estructuras/computacion/<wbr>FinisTerrae2</a>).<br><br>We used several vers=
ions of OpenMPI. The chosen versions of OpenMPI were the versions already i=
nstalled in the cluster:<br><br>- openmpi/1.10.2<br>- openmpi/2.0.0<br>- op=
enmpi/2.0.1<br>- openmpi/2.0.2<br>- openmpi/2.1.1<br><br>We have created Si=
ngularity images containing the same versions of OpenMPI and with the basic=
 OpenMPI ring example. I share the bootstrap definition file template used =
below:<br><br>```<br>BootStrap: docker<br>From: ubuntu:16.04<br>IncludeCmd:=
 yes<br><br>%post<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sed -i &#39=
;s/main/main restricted universe/g&#39; /etc/apt/sources.list<br>=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get update<br>=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 apt-get install -y bash git wget build-essential gcc =
time libc6-dev libgcc-5-dev<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 a=
pt-get install -y dapl2-utils libdapl-dev libdapl2 libibverbs1 librdmacm1 l=
ibcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 libnes1 libpmi0 libp=
mi0-dev libslurm29 libslurm-dev<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 ##Install OpenMPI<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /=
tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget &#39;<a href=3D"http=
s://www.open-mpi.org/software/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz" tar=
get=3D"_blank">https://www.open-mpi.org/<wbr>software/ompi/vX.X/downloads/<=
wbr>openmpi-X.X.X.tar.gz</a>&#39; -O openmpi-X.X.X.tar.gz<br>=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X=
.X<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mkdir -p /tmp/openmpi-X.X.=
X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp/openmpi-X.X.X=
/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 ../configure --e=
nable-shared --enable-mpi-thread-multiple --with-verbs --enable-mpirun-pref=
ix-by-<wbr>default --with-hwloc --disable-dlopen --with-pmi --prefix=3D/usr=
<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 make all install<br><br>=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 # Install ring<br>=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 wget <a href=3D"https://raw.githubusercontent.com/open-mpi/ompi/mast=
er/examples/ring_c.c" target=3D"_blank">https://raw.githubusercontent.<wbr>=
com/open-mpi/ompi/master/<wbr>examples/ring_c.c</a><br>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 mpicc ring_c.c -o /usr/bin/ring<br>```<br><br>Once=
 the containers were created, we ran the ring app with mpirun using 2 cores=
 of 2 different nodes mixing all possible combinations of those OpenMPI ver=
sions inside and outside the container.<br><br>The obtained results shown t=
hat we need the same versions of OpenMPI inside and outside the container t=
o succesfully run the contained application in parallel with mpirun.<br><br=
>Is this the expected behaviour or am I missing something?<br><br>Will be t=
his the expected behaviour in the future (with future versions of OpenMPI)?=
<br><br>Currently, we have slurm 14.11.10-Bull.1.0 installed as job schedul=
er at FinisTerrae II. We found the following tip/trick to use srun as proce=
ss manager:<br><br><a href=3D"http://singularity.lbl.gov/tutorial-gpu-drive=
rs-open-mpi-mtls" target=3D"_blank">http://singularity.lbl.gov/<wbr>tutoria=
l-gpu-drivers-open-mpi-<wbr>mtls</a><br><br>In order to run whatever Singul=
arity image containing OpenMPI applications using Slurm, we&#39;ve adapted =
it to our infrastructure and checked the same test cases running them with =
srun. It seems that it&#39;s working properly (no real world applications w=
ere tested yet).<br><br>What do you think about this strategy?<br>Can you c=
onfirm that it provides portability of singularity images containing OpenMP=
I applications?<br><br>I think this strategy is similar to the one you are =
following with &quot;--nv&quot; option=C2=A0 for NVidia drivers.<br><br>Why=
 not to do the same strategy with MPI, PMI, libibverbs, etc.?<br><br>Thanks=
 in advance and congrats again for your great work!<br><br>V=C3=ADctor.<spa=
n class=3D"HOEnZb"><font color=3D"#888888"><br></font></span></div><span cl=
ass=3D"HOEnZb"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div di=
r=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtz=
er</div><div>CEO, SingularityWare, LLC.</div><div>Senior Architect, RStor</=
div><div><span style=3D"font-size:12.8px">Computational Science Advisor, La=
wrence Berkeley National Laboratory</span><br></div></div></div></div></div=
></div></div>
</div>

--001a113f9044ea91fa0553e756b6--
