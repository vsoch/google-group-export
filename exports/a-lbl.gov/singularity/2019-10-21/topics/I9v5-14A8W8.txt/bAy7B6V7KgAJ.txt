X-Received: by 10.129.85.76 with SMTP id j73mr11487210ywb.55.1463163534980;
        Fri, 13 May 2016 11:18:54 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.50.47.166 with SMTP id e6ls220375ign.29.canary; Fri, 13 May
 2016 11:18:54 -0700 (PDT)
X-Received: by 10.66.164.133 with SMTP id yq5mr25311888pab.107.1463163534455;
        Fri, 13 May 2016 11:18:54 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id n10si25936393pay.80.2016.05.13.11.18.54
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 13 May 2016 11:18:54 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.217.197 as permitted sender) client-ip=209.85.217.197;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.217.197 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 4.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FGAAB9GTZXkMXZVdFehHsPBoM0qjmGboR3AQ2BdhcBhXkCgSUHOBQBAQEBAQEBAw8BAQEBBw0JCSEvhEIBAQEDARIRKzsLCw0qAgIhAQ8DAQUBHAYIBwQBGQMEAYdzAw8IBaM+gTE+MYs7jGANhCMBCgEBASMQiV+BA4JDgU4RAQaDFoJZBY5ThF2ERjEBiHWDL4F5jxmHXIYnEh6BDh4BAYJIEQqBaxwyB4cegTUBAQE
X-IronPort-AV: E=Sophos;i="5.24,614,1455004800"; 
   d="scan'208,217";a="23307430"
Received: from mail-lb0-f197.google.com ([209.85.217.197])
  by fe4.lbl.gov with ESMTP; 13 May 2016 11:18:52 -0700
Received: by mail-lb0-f197.google.com with SMTP id ne4so32546734lbc.1
        for <singu...@lbl.gov>; Fri, 13 May 2016 11:18:52 -0700 (PDT)
X-Gm-Message-State: AOPr4FVhATcFGR/+6ep1FZbr1kPHbbCE1cXvq3FeePqUhGeo7Z3c850aRQhRkQYmS7pWPP5Xj+WQpGAb7ffdu18KQ4d8vb7NhTd/z3qmjQs8W0HwLMTe8kgrFzu2r4xPfz60L2HqKFaUk3XZi1a8rFTStTI=
X-Received: by 10.28.105.200 with SMTP id z69mr5060030wmh.78.1463163531911;
        Fri, 13 May 2016 11:18:51 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.28.105.200 with SMTP id z69mr5060020wmh.78.1463163531771;
 Fri, 13 May 2016 11:18:51 -0700 (PDT)
Received: by 10.28.144.200 with HTTP; Fri, 13 May 2016 11:18:51 -0700 (PDT)
In-Reply-To: <414C0039-42A3-4C2E-89F6-3D97D082C742@open-mpi.org>
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov>
	<CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com>
	<CAAS-_CBb3sy393W1Bga5Wnr9-EvSHC6NPNaOx58Mpdw-LiFq8g@mail.gmail.com>
	<414C0039-42A3-4C2E-89F6-3D97D082C742@open-mpi.org>
Date: Fri, 13 May 2016 11:18:51 -0700
Message-ID: <CAN7etTxQkXe6uEDZgt+kkAex0Hzt9UWYE6+V4qS4vnKJAMgFzQ@mail.gmail.com>
Subject: Re: [Singularity] SIngularity and MPI implementations
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a11467c0e8f5c650532bd4ebc

--001a11467c0e8f5c650532bd4ebc
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On Fri, May 13, 2016 at 10:10 AM, Ralph Castain <r...@open-mpi.org> wrote:

>
> On May 13, 2016, at 9:52 AM, Taras Shapovalov <shapov...@gmail.com>
> wrote:
>
> Hi Ralph and Gregory,
>
> Thank you the both for the so detailed answers! I see your replies
> complement each other. Although I am a bit confused now with the whole
> picture, so could you confirm that I get the ideas correctly:
>
> 1. All implementations of MPI by default should work with Singularity
> containers (maybe not as optimal as could be, but should start and finish
> correctly always). Actually I've tested recently MPICH+Singularity with
> several workload managers, worked fine (did not benchmark it comparing wi=
th
> Open MPI). I did not manage to make Singularity+MPI work in LSF, but this
> is a different story that deserves a separate thread.
>
>
I wouldn't necessarily say they would all work by default. For example,
some namespaces may necessitate being disabled in order to get proper
shared memory IO performance. But ... If you have tested this, that is
great news and I'd love to hear more about your findings!


>
> Correct - the LSF issue is likely a problem of getting the required setup
> info passed by LSF
>
>
> 2. MPI process calls dl_open, thus the more MPI processes starts on a
> node, the more times dl_open will be called. Open MPI 2.0.1 somehow solve=
s
> this magically (I don't get how) and dl_open is called only once per node=
.
> Other implementations of MPI and older Open MPI are not Singularity aware=
,
> thus they still will call dl_open each time when MPI process spawns.
>
>
> Not exactly. Singularity will solve the dl_open problem by itself. What
> the container does is wrap all the dl_open libraries into the container,
> and so all dl_open calls by the app are locally resolved. Thus, you
> automatically resolve the IO node bottleneck scaling issue.
>
> What OMPI adds is that it pulls the container only once/node. Other
> mpiexec implementations will pull the container again for every local
> process. So if you have 100 procs/node, OMPI will result in 100x fewer
> =E2=80=9Cpulls=E2=80=9D thru that IO node.
>

Yep, and additionally I want to make sure we keep Singularity v1 and v2
features separate. Version 2 has several huge benefits (including this)
over v1, but it is a departure from using SAPPs (and now uses images).


>
>
> 3. dl_open issue affects only process start time and does not effect the
> process execution, so on small scale with long running processes there is
> no difference between Open MPI 2.0.1 and older Open MPI versions (as well
> as other MPI implementations).
>
>
> Correct
>

Correct, just keep in mind start times at massive scale have been stated by
several large centers to approach 30 minutes. During that 30 minutes, it
basically looks like a distributed denial of service attack to the file
system metadata server killing file system performance to the rest of the
system.


>
>
> 4. When sapp is built then Singularity detects Open MPI (even older then
> 2.0.1, right?) and resolves all dependencies automatically adding all fil=
es
> to the sapp. But with, say, MVAPICH2 the dependencies are not resolved
> automatically, so user should add some stuff manually.
>
>
> Correct
>

And in v2, this will get handled either by an RPM installation of Open MPI,
or the 'singularity exec --writable /path/to/Container.img make install'.

>
>
> 5. Apart of solving dl_open issue Open MPI 2.0.1 does some splitting
> between the host and the container, which allows user/admin to not optimi=
ze
> Open MPI for a target platform. I really don't get how Singularity does
> this, but I get the problem. Could you explain what Singularity or Open M=
PI
> 2.0.1 does for that specificaly?
>
>
> When running under mpiexec with Singularity, OMPI=E2=80=99s local daemon =
on each
> node actually runs outside of the containers. We then fork/exec the
> container itself, and the container is defined so it auto-executes the
> application process. This allows us to minimize the services overhead,
> keeping all services outside of your container (and thus shared across al=
l
> containers.
>
> Other approaches have the daemon -inside- the container, and you get one
> daemon for each container - and thus, one daemon for each local
> application. So you get a higher overhead and therefore lower performance=
.
>

Maybe this will help to articulate it... I have described this via the
MPI/Singularity invocation pathway as follows (hopefully it is reasonably
correct and doesn't cause Ralph to kick me). Considering the command:

$ mpirun -np X singularity exec ~/Centos-7.img mpi_program

1. From shell (or resource manager) mpirun gets called
2. mpirun forks and exec orte daemon
3. Orted process creates PMI
4. Orted forks =3D=3D to the number of process per node requested
5. Orted children exec to original command passed to mpirun (Singularity)
6. Each Singularity execs the command passed inside the given container
7. Each MPI program links in the dynamic Open MPI libraries (ldd)
8. Open MPI libraries continue to open the non-ldd shared libraries (dlopen=
)
9. Open MPI libraries connect back to original orted via PMI
10. All non-shared memory communication occurs through the PMI and then to
local interfaces (e.g. InfiniBand)

While the workflow from the MPI perspective seems simpler with the daemon
inside the container, it is much more complicated from the system
perspective because each orted process must also know about the other
hosts, be able to communicate with them and mitigate performance factors of
host/resource/interconnect tuning.

Hope that helps!




>
> HTH
> Ralph
>
>
>
> Best regards,
>
> Taras
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a11467c0e8f5c650532bd4ebc
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Fri, May 13, 2016 at 10:10 AM, Ralph Castain <span dir=3D"ltr">&lt;<=
a href=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>=
&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0=
 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wra=
p:break-word"><br><div><span class=3D""><blockquote type=3D"cite"><div>On M=
ay 13, 2016, at 9:52 AM, Taras Shapovalov &lt;<a href=3D"mailto:shapov...@g=
mail.com" target=3D"_blank">shapov...@gmail.com</a>&gt; wrote:</div><br><di=
v><div dir=3D"ltr"><div><div><div><div><div><div>Hi Ralph and Gregory,<br><=
br></div>Thank you the both for the so detailed answers! I see your replies=
 complement each other. Although I am a bit confused now with the whole pic=
ture, so could you confirm that I get the ideas correctly:<br><br></div>1. =
All implementations of MPI by default should work with Singularity containe=
rs (maybe not as optimal as could be, but should start and finish correctly=
 always). Actually I&#39;ve tested recently MPICH+Singularity with several =
workload managers, worked fine (did not benchmark it comparing with Open MP=
I). I did not manage to make Singularity+MPI work in LSF, but this is a dif=
ferent story that deserves a separate thread.<br></div></div></div></div></=
div></div></blockquote></span></div></div></blockquote><div><br></div><div>=
I wouldn&#39;t necessarily say they would all work by default. For example,=
 some namespaces may necessitate being disabled in order to get proper shar=
ed memory IO performance. But ... If you have tested this, that is great ne=
ws and I&#39;d love to hear more about your findings!</div><div>=C2=A0</div=
><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1=
px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><div><s=
pan class=3D""><blockquote type=3D"cite"><div><div dir=3D"ltr"><div><div><d=
iv><div></div></div></div></div></div></div></blockquote><div><br></div></s=
pan>Correct - the LSF issue is likely a problem of getting the required set=
up info passed by LSF</div><div><span class=3D""><br><blockquote type=3D"ci=
te"><div><div dir=3D"ltr"><div><div><div><div><br></div>2. MPI process call=
s dl_open, thus the more MPI processes starts on a node, the more times dl_=
open will be called. Open MPI 2.0.1 somehow solves this magically (I don&#3=
9;t get how) and dl_open is called only once per node. Other implementation=
s of MPI and older Open MPI are not Singularity aware, thus they still will=
 call dl_open each time when MPI process spawns. <br></div></div></div></di=
v></div></blockquote><div><br></div></span>Not exactly. Singularity will so=
lve the dl_open problem by itself. What the container does is wrap all the =
dl_open libraries into the container, and so all dl_open calls by the app a=
re locally resolved. Thus, you automatically resolve the IO node bottleneck=
 scaling issue.</div><div><br></div><div>What OMPI adds is that it pulls th=
e container only once/node. Other mpiexec implementations will pull the con=
tainer again for every local process. So if you have 100 procs/node, OMPI w=
ill result in 100x fewer =E2=80=9Cpulls=E2=80=9D thru that IO node.</div></=
div></blockquote><div><br></div><div>Yep, and additionally I want to make s=
ure we keep Singularity v1 and v2 features separate. Version 2 has several =
huge benefits (including this) over v1, but it is a departure from using SA=
PPs (and now uses images).</div><div>=C2=A0</div><blockquote class=3D"gmail=
_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:=
1ex"><div style=3D"word-wrap:break-word"><div><span class=3D""><br><blockqu=
ote type=3D"cite"><div><div dir=3D"ltr"><div><div><div><br>3. dl_open issue=
 affects only process start time and does not effect the process execution,=
 so on small scale with long running processes there is no difference betwe=
en Open MPI 2.0.1 and older Open MPI versions (as well as other MPI impleme=
ntations).<br></div></div></div></div></div></blockquote><div><br></div></s=
pan>Correct</div></div></blockquote><div><br></div><div>Correct, just keep =
in mind start times at massive scale have been stated by several large cent=
ers to approach 30 minutes. During that 30 minutes, it basically looks like=
 a distributed denial of service attack to the file system metadata server =
killing file system performance to the rest of the system.</div><div>=C2=A0=
</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-l=
eft:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><d=
iv><span class=3D""><br><blockquote type=3D"cite"><div><div dir=3D"ltr"><di=
v><div><div><br></div><div>4. When sapp is built then Singularity detects O=
pen MPI (even older then 2.0.1, right?) and resolves all dependencies autom=
atically adding all files to the sapp. But with, say, MVAPICH2 the dependen=
cies are not resolved automatically, so user should add some stuff manually=
.<br></div></div></div></div></div></blockquote><div><br></div></span>Corre=
ct</div></div></blockquote><div><br></div><div>And in v2, this will get han=
dled either by an RPM installation of Open MPI, or the &#39;singularity exe=
c --writable /path/to/Container.img make install&#39;.=C2=A0</div><blockquo=
te class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc so=
lid;padding-left:1ex"><div style=3D"word-wrap:break-word"><div><span class=
=3D""><br><blockquote type=3D"cite"><div><div dir=3D"ltr"><div><div><div><b=
r></div>5. Apart of solving dl_open issue Open MPI 2.0.1 does some splittin=
g between the host and the container, which allows user/admin to not optimi=
ze Open MPI for a target platform. I really don&#39;t get how Singularity d=
oes this, but I get the problem. Could you explain what Singularity or Open=
 MPI 2.0.1 does for that specificaly?<br></div></div></div></div></blockquo=
te><div><br></div></span><div>When running under mpiexec with Singularity, =
OMPI=E2=80=99s local daemon on each node actually runs outside of the conta=
iners. We then fork/exec the container itself, and the container is defined=
 so it auto-executes the application process. This allows us to minimize th=
e services overhead, keeping all services outside of your container (and th=
us shared across all containers.</div><div><br></div><div>Other approaches =
have the daemon -inside- the container, and you get one daemon for each con=
tainer - and thus, one daemon for each local application. So you get a high=
er overhead and therefore lower performance.</div></div></div></blockquote>=
<div><br></div><div>Maybe this will help to articulate it... I have describ=
ed this via the MPI/Singularity invocation pathway as follows (hopefully it=
 is reasonably correct and doesn&#39;t cause Ralph to kick me). Considering=
 the command:</div><div><br></div><div>$ mpirun -np X singularity exec ~/Ce=
ntos-7.img mpi_program</div><div><br></div><div>1. From shell (or resource =
manager) mpirun gets called</div><div>2. mpirun forks and exec orte daemon<=
/div><div>3. Orted process creates PMI</div><div>4. Orted forks =3D=3D to t=
he number of process per node requested</div><div>5. Orted children exec to=
 original command passed to mpirun (Singularity)</div><div>6. Each Singular=
ity execs the command passed inside the given container</div><div>7. Each M=
PI program links in the dynamic Open MPI libraries (ldd)</div><div>8. Open =
MPI libraries continue to open the non-ldd shared libraries (dlopen)</div><=
div>9. Open MPI libraries connect back to original orted via PMI</div><div>=
10. All non-shared memory communication occurs through the PMI and then to =
local interfaces (e.g. InfiniBand)</div><div><br></div><div>While the workf=
low from the MPI perspective seems simpler with the daemon inside the conta=
iner, it is much more complicated from the system perspective because each =
orted process must also know about the other hosts, be able to communicate =
with them and mitigate performance factors of host/resource/interconnect tu=
ning.</div><div><br></div><div>Hope that helps!</div><div><br></div><div><b=
r></div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:=
0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-=
wrap:break-word"><div><div><br></div><div>HTH</div><span class=3D"HOEnZb"><=
font color=3D"#888888"><div>Ralph</div></font></span><span class=3D""><div>=
<br></div><br><blockquote type=3D"cite"><div><div dir=3D"ltr"><div><div><br=
></div>Best regards,<br><br></div>Taras<br><br></div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></blockquote></span></div><br></div><div class=3D"HOEnZb"><div class=
=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature"><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>=
High Performance Computing Services (HPCS)<br>University of California<br>L=
awrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94=
720</div></div></div>
</div></div>

--001a11467c0e8f5c650532bd4ebc--
