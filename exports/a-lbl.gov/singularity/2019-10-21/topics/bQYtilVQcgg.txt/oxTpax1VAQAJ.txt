X-Received: by 10.99.38.199 with SMTP id m190mr231103pgm.42.1488510622302;
        Thu, 02 Mar 2017 19:10:22 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.12.146 with SMTP id 18ls2520732iom.11.gmail; Thu, 02 Mar
 2017 19:10:21 -0800 (PST)
X-Received: by 10.98.223.76 with SMTP id u73mr687983pfg.147.1488510621477;
        Thu, 02 Mar 2017 19:10:21 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id i85si7780110pfi.35.2017.03.02.19.10.21
        for <singu...@lbl.gov>;
        Thu, 02 Mar 2017 19:10:21 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.161.199 as permitted sender) client-ip=209.85.161.199;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.161.199 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FIAAAj3rhYgMehVdFeHAEBBAEBCgEBFwEBBAEBCgEBgkOBQ3gRB4NOCIoKkWiCZIpfEII2gkOCaYFKGygfAQyBbYIvgVoCgkkHPxgBAQEBAQEBAQEBAQIQAQEJDQkKGzGCMwQCAwEdBAQ9CgECLgEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBARICDSICDQMPAhgBAQEDASMrMAsLCw0qAgIiDwMBBQELEQYIBwQBBxUEiHdaCAUJpVk/jAOCJosIAQEIAQEBAQEjEooggQmDF4EPEQGDIoJfBYkDCQQHaIZVEDuEVIY2AYZ0hWSFWYF7U4EJjUiIQYkvFB6BFQ8QgQIwCBkKN2QFgXICggY5HYICHzUHhyhHgWcBAQE
X-IronPort-AV: E=Sophos;i="5.35,235,1484035200"; 
   d="scan'208,217";a="66428853"
Received: from mail-yw0-f199.google.com ([209.85.161.199])
  by fe3.lbl.gov with ESMTP; 02 Mar 2017 19:10:19 -0800
Received: by mail-yw0-f199.google.com with SMTP id 204so158732033ywo.6
        for <singu...@lbl.gov>; Thu, 02 Mar 2017 19:10:19 -0800 (PST)
X-Gm-Message-State: AMke39lSAo8zEDs9gVvBehvihY3j4WEODS6Qt5OZ1DuhzlrY3ZT/9zA5qjfORJFTqQisSErE+M3/cwl4rQ0s9WlH+QgM/Q1aOSKdTWNIZ7GrOn5hoxSQNXwFJbBldbLlh7xJyShKIln89iCz9UOdrxImc1U=
X-Received: by 10.37.123.69 with SMTP id w66mr324792ybc.141.1488510619010;
        Thu, 02 Mar 2017 19:10:19 -0800 (PST)
X-Received: by 10.37.123.69 with SMTP id w66mr324782ybc.141.1488510618696;
 Thu, 02 Mar 2017 19:10:18 -0800 (PST)
MIME-Version: 1.0
Received: by 10.129.110.69 with HTTP; Thu, 2 Mar 2017 19:10:18 -0800 (PST)
In-Reply-To: <104158ea-3718-400a-a536-b0c64ccad1fa@lbl.gov>
References: <4a675b1d-5116-4702-a9e4-cc0024dcb436@lbl.gov> <CAN7etTzUMksoSB_H1ZgA3+-fFvM2pO+XzeD3E-pLqLiiL69c4w@mail.gmail.com>
 <5c39ff15-22d6-4b4f-8566-a670d2910e6a@lbl.gov> <2e6d299f-12d6-42ea-aacb-55f0d8924e36@lbl.gov>
 <CAN7etTyX+_RYBb8m4eNFORisfa848Bksm6s5vWZZjMgsdD1WHg@mail.gmail.com> <104158ea-3718-400a-a536-b0c64ccad1fa@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 2 Mar 2017 19:10:18 -0800
Message-ID: <CAN7etTx65za_GXuaYMvY44iYQc3erzYQWgYqSNAR_ZDiaCRqfA@mail.gmail.com>
Subject: Re: [Singularity] job with multiple singularity calls (mixed
 MPI/serial) fails
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a114e5756ac45ee0549cae2d6

--001a114e5756ac45ee0549cae2d6
Content-Type: text/plain; charset=UTF-8

Heya, I don't know off the top of my head. Can you provide some debugging
output (both from the MPI program and Singularity on the errors you are
getting)?

Thanks!

On Fri, Feb 24, 2017 at 5:52 AM, Geert Jan BEX <geert...@uhasselt.be>
wrote:

> Dear Greg,
>
> Thanks for the suggestion.
>
> Some update: my initial impression that this problem had to do with
> multiple singularity calls was wrong.  I've reproduced this in isolation as
> well.
>
> When I try your suggestion with Singularity 2.2, I get the following error:
> Primary job  terminated normally, but 1 process returned
> a non-zero exit code.. Per user-direction, the job has been aborted.
> -------------------------------------------------------
> --------------------------------------------------------------------------
> A system call failed during shared memory initialization that should
> not have.  It is likely that your MPI job will now either abort or
> experience performance degradation.
>
>   Local host:  r04i15n1
>   System call: open(2)
>   Error:       No such file or directory (errno 2)
>
> [r04i15n1:29725] 9 more processes have sent help message
> help-opal-shmem-mmap.txt / sys call fail
>
> This is the same type of error I get with Singularity 2.2.1, without
> setting SINGULARITY_NOSESSIONCLEANUP..  That would indeed imply that the
> session directory cleanup is fixed in 2.2.1, but now there seems to be an
> issue with the shared memory file of OpenMPI.
>
> Any ideas?
>
> Thanks in advance, best regards, Geert Jan Bex
>
>
>
> On Thursday, February 23, 2017 at 4:23:08 PM UTC+1, Gregory M. Kurtzer
> wrote:
>>
>> While I think you are hitting some locking bug in Singularity, that I
>> think/hope is already fixed in one of our development branches, I do think
>> there is a workaround you can use. Modify your MPI line to include:
>>
>> SINGULARITY_NOSESSIONCLEANUP=1
>>
>> I think you can do it like:
>>
>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" SINGULARITY_NOSE
>> SSIONCLEANUP=1 singularity --debug exec -B ${SIMDIR}:/wrf/run
>> $VSC_SCRATCH/containers/wrfrun-latest.img bash -c 'cd /wrf/run/wrfwkdir;
>> real.exe'
>>
>> Or you can make sure that the variable is set on every node running the
>> process. A better fix is included (or rather will be included) in the
>> latest version of Open MPI that will automatically set that variable, and
>> allow Open MPI to clean up the Singularity sessionfiles directly after the
>> job runs. Presently those optimizations exist in Open MPI's master branch.
>> So if you can build that yourself and test, that is also an option.
>>
>> Sorry for the delay, and hope that helps!
>>
>> Greg
>>
>> On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BEX <gee...@uhasselt.be>
>> wrote:
>>
>>> Dear Gregory,
>>>
>>> Any feedback you can give me?
>>>
>>> Thanks in advance, best regards, -gjb-
>>>
>>>
>>> On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:
>>>>
>>>> Dear Gregory,
>>>>
>>>> Thanks for looking into this.
>>>>
>>>> The /tmp is on the local disks of the compute nodes, so no NFS mounts
>>>> involved.
>>>>
>>>> The /tmp is also used for the MCA orte_tmpdir_base for mpirun.
>>>>
>>>> I've put the output of a run with --debug on Google Drive, you can
>>>> access it via:
>>>> https://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8
>>>> /view?usp=sharing
>>>>
>>>> The "offending" statement in the job script is:
>>>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" singularity --debug exec -B
>>>> ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img bash -c
>>>> 'cd /wrf/run/wrfwkdir; real.exe'
>>>>
>>>> So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH and
>>>> SIMDIR refer to a GPFS file system, and that is also where the working
>>>> directory for the job is on.
>>>>
>>>> Singularity's config file is unaltered from default.
>>>>
>>>> Thanks in advance, -gjb-
>>>>
>>>> On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. Kurtzer
>>>> wrote:
>>>>>
>>>>> Hi Geert,
>>>>>
>>>>> The directory is only supposed to be removed when the flock()s have
>>>>> all been released. I just did some fixups for this in one of the
>>>>> development branches, but 2.2* should be working in the same way. Is `/tmp`
>>>>> a local directory, or is it on some form of shared NFS? Does it have any
>>>>> mount options to disable locking?
>>>>>
>>>>> Can you also try with --debug enabled and send all of the output?
>>>>>
>>>>> Thanks!
>>>>>
>>>>> On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <gee...@uhasselt.be>
>>>>> wrote:
>>>>>
>>>>>> Dear,
>>>>>>
>>>>>> I've run into a problem with singularity, and out of ideas by now.
>>>>>>
>>>>>> When running a job that involves multiple calls to singularity (same
>>>>>> image each time), some of them serial, some MPI, the MPI ones
>>>>>> intermittently fail with:
>>>>>> ERROR  : Could not remove session directory
>>>>>> /tmp/.singularity-session-2530140.42.465829344: Device or resource
>>>>>> busy
>>>>>>
>>>>>> This is for OpenMPI 1.10.2 on host and in image.  The host OS is
>>>>>> CentOS 7, the image OS Ubuntu 16.04,  Singularity 2.2.  The job runs on 4
>>>>>> nodes, 27 processes each.
>>>>>>
>>>>>> I've tried several approaches (sleep between calls, setting a session
>>>>>> directory per singularity call, ...).
>>>>>>
>>>>>> Any ideas?
>>>>>>
>>>>>> Thanks in advance, Geert Jan Bex
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gregory M. Kurtzer
>>>>> HPC Systems Architect and Technology Developer
>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>> University of California Berkeley Research IT
>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>> er.com/gmkurtzer
>>>>>
>>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--001a114e5756ac45ee0549cae2d6
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Heya, I don&#39;t know off the top of my head. Can you pro=
vide some debugging output (both from the MPI program and Singularity on th=
e errors you are getting)?<div><br></div><div>Thanks!</div></div><div class=
=3D"gmail_extra"><br><div class=3D"gmail_quote">On Fri, Feb 24, 2017 at 5:5=
2 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a href=3D"mailto:geert...@uhasse=
lt.be" target=3D"_blank">geert...@uhasselt.be</a>&gt;</span> wrote:<br><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #c=
cc solid;padding-left:1ex"><div dir=3D"ltr">Dear Greg,<div><br></div><div>T=
hanks for the suggestion.</div><div><br></div><div>Some update: my initial =
impression that this problem had to do with multiple singularity calls was =
wrong.=C2=A0 I&#39;ve reproduced this in isolation as well.</div><div><br><=
/div><div>When I try your suggestion with Singularity 2.2, I get the follow=
ing error:</div><div><div>Primary job =C2=A0terminated normally, but 1 proc=
ess returned</div><div>a non-zero exit code.. Per user-direction, the job h=
as been aborted.</div><div>------------------------------<wbr>-------------=
------------</div><div>------------------------------<wbr>-----------------=
-------------<wbr>--------------</div><div>A system call failed during shar=
ed memory initialization that should</div><div>not have.=C2=A0 It is likely=
 that your MPI job will now either abort or</div><div>experience performanc=
e degradation.</div><div><br></div><div>=C2=A0 Local host: =C2=A0r04i15n1</=
div><div>=C2=A0 System call: open(2)=C2=A0</div><div>=C2=A0 Error: =C2=A0 =
=C2=A0 =C2=A0 No such file or directory (errno 2)</div><div><div><br></div>=
<div>[r04i15n1:29725] 9 more processes have sent help message help-opal-shm=
em-mmap.txt / sys call fail</div></div><div><br></div><div>This is the same=
 type of error I get with Singularity 2.2.1, without setting=C2=A0<span sty=
le=3D"font-size:small">SINGULARITY_</span><span style=3D"font-size:small">N=
OSESSIONC<wbr>LEANUP..=C2=A0 That would indeed imply that the session direc=
tory cleanup is fixed in 2.2.1, but now there seems to be an issue with the=
 shared memory file of OpenMPI.</span></div><div><span style=3D"font-size:s=
mall"><br></span></div><div><span style=3D"font-size:small">Any ideas?</spa=
n></div><div><span style=3D"font-size:small"><br></span></div><div><span st=
yle=3D"font-size:small">Thanks in advance, best regards, Geert Jan Bex</spa=
n></div><span class=3D""><div><span style=3D"font-size:small"><br></span></=
div><div><span style=3D"font-size:small"><br></span></div><br>On Thursday, =
February 23, 2017 at 4:23:08 PM UTC+1, Gregory M. Kurtzer wrote:</span><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><span class=3D""><div dir=3D"ltr"><div>=
While I think you are hitting some locking bug in Singularity, that I think=
/hope is already fixed in one of our development branches, I do think there=
 is a workaround you can use. Modify your MPI line to include:</div><div><b=
r></div><div>SINGULARITY_NOSESSIONCLEANUP=3D1</div><div><br></div><div>I th=
ink you can do it like:</div><div><br></div><div><div style=3D"font-size:12=
.8px">mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP}&quot;=C2=A0<span style=
=3D"font-size:small">SINGULARITY_NOSE<wbr>SSIONCLEANUP=3D1=C2=A0</span><spa=
n style=3D"font-size:12.8px">singularity --debug exec -B ${SIMDIR}:/wrf/run=
 $VSC_SCRATCH/containers/</span><span style=3D"font-size:12.8px">wrfrun<wbr=
>-latest.img bash -c &#39;cd /wrf/run/wrfwkdir; real.exe&#39;</span></div><=
/div><div><br></div><div>Or you can make sure that the variable is set on e=
very node running the process. A better fix is included (or rather will be =
included) in the latest version of Open MPI that will automatically set tha=
t variable, and allow Open MPI to clean up the Singularity sessionfiles dir=
ectly after the job runs. Presently those optimizations exist in Open MPI&#=
39;s master branch. So if you can build that yourself and test, that is als=
o an option.</div><div><br></div>Sorry for the delay, and hope that helps!<=
div><br></div><div>Greg</div></div></span><div><div class=3D"h5"><div><br><=
div class=3D"gmail_quote">On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BEX <s=
pan dir=3D"ltr">&lt;<a rel=3D"nofollow">gee...@uhasselt.be</a>&gt;</span> w=
rote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div>=
<br></div><div>Any feedback you can give me?</div><div><br></div><div>Thank=
s in advance, best regards, -gjb-</div><div><div><div><br><br>On Tuesday, F=
ebruary 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:<blockquote class=
=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div><br></div><div>Th=
anks for looking into this.</div><div><br></div><div>The /tmp is on the loc=
al disks of the compute nodes, so no NFS mounts involved.</div><div><br></d=
iv><div>The /tmp is also used for the MCA orte_tmpdir_base for mpirun.</div=
><div><br></div><div>I&#39;ve put the output of a run with --debug on Googl=
e Drive, you can access it via:</div><div><a href=3D"https://drive.google.c=
om/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=3Dsharing" rel=3D"nofollow"=
 target=3D"_blank">https://drive.google.com/file/<wbr>d/0BxlSeep1Z7B-MzRsd2=
pwV2dUTG8<wbr>/view?usp=3Dsharing</a></div><div><br></div><div>The &quot;of=
fending&quot; statement in the job script is:</div><div><div>mpirun -mca or=
te_tmpdir_base &quot;${ORTE_TMP}&quot; singularity --debug exec -B ${SIMDIR=
}:/wrf/run $VSC_SCRATCH/containers/wrfrun<wbr>-latest.img bash -c &#39;cd /=
wrf/run/wrfwkdir; real.exe&#39;</div></div><div><br></div><div>So ORTE_TMP =
points to a directory in /tmp (local disk), VSC_SCRATCH and SIMDIR refer to=
 a GPFS file system, and that is also where the working directory for the j=
ob is on.</div><div><br></div><div>Singularity&#39;s config file is unalter=
ed from default.</div><div><br></div><div>Thanks in advance, -gjb-<br><br>O=
n Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. Kurtzer wrote:<=
blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border=
-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Geert,<div><br><=
/div><div>The directory is only supposed to be removed when the flock()s ha=
ve all been released. I just did some fixups for this in one of the develop=
ment branches, but 2.2* should be working in the same way. Is `/tmp` a loca=
l directory, or is it on some form of shared NFS? Does it have any mount op=
tions to disable locking?</div><div><br></div><div>Can you also try with --=
debug enabled and send all of the output?=C2=A0</div><div><br></div><div>Th=
anks!</div></div><div><br><div class=3D"gmail_quote">On Mon, Feb 20, 2017 a=
t 6:25 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a rel=3D"nofollow">gee...@u=
hasselt.be</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=
=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr">Dear,<div><br></div><div>I&#39;ve run into a problem with singular=
ity, and out of ideas by now.</div><div><br></div><div>When running a job t=
hat involves multiple calls to singularity (same image each time), some of =
them serial, some MPI, the MPI ones intermittently fail with:</div><div><di=
v>ERROR =C2=A0: Could not remove session directory /tmp/.singularity-sessio=
n-2530<wbr>140.42.465829344: Device or resource busy</div></div><div><br></=
div><div>This is for OpenMPI 1.10.2 on host and in image.=C2=A0 The host OS=
 is CentOS 7, the image OS Ubuntu 16.04, =C2=A0Singularity 2.2.=C2=A0 The j=
ob runs on 4 nodes, 27 processes each.</div><div><br></div><div>I&#39;ve tr=
ied several approaches (sleep between calls, setting a session directory pe=
r singularity call, ...).</div><div><br></div><div>Any ideas?</div><div><br=
></div><div>Thanks in advance, Geert Jan Bex</div><span><font color=3D"#888=
888"><div><br></div><div><br></div><div><br></div></font></span></div><span=
><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.go=
v/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warew=
ulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.g=
ov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" re=
l=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</blockquote></div></div></blockquote></div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.gov/=
</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewul=
f.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.gov=
/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=
=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</div></div></blockquote></div></div><div class=3D"HOEnZb"><div class=3D"h5=
">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr">=
<div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sys=
tems Architect and Technology Developer</div><div>Lawrence Berkeley Nationa=
l Laboratory HPCS<br>University of California Berkeley Research IT<br>Singu=
larity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targe=
t=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster Ma=
nagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http:=
//warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.c=
om/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<spa=
n style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitte=
r.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twitt=
er.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div><=
/div></div>
</div>

--001a114e5756ac45ee0549cae2d6--
