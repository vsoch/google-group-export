X-Received: by 10.13.192.134 with SMTP id b128mr37230939ywd.43.1466688106389;
        Thu, 23 Jun 2016 06:21:46 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.18.214 with SMTP id 83ls3019287ios.1.gmail; Thu, 23 Jun
 2016 06:21:46 -0700 (PDT)
X-Received: by 10.98.155.218 with SMTP id e87mr42960575pfk.13.1466688105869;
        Thu, 23 Jun 2016 06:21:45 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id to3si163017pac.1.2016.06.23.06.21.45
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 23 Jun 2016 06:21:45 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.217.199 as permitted sender) client-ip=209.85.217.199;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.217.199 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.2
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2BQAgCX4WtXesfZVdFdgnCBJG4PBoM2gQyke4RxjAaBehcBBoV5AoEgBzoSAQEBAQEBARIBAQkLDAghJAuETAEBAQMBEhErGRcLCwsNIAoCAiEBDwMBBQELEQYIBwQBBxUEAYd0Aw8IBagsgTE+MYs7jEANg3UBAQgBAQEBAQEdBAsFimSCQ4FPEQEGgxeCWgWIc4V6hF6FADQBhgeGK4F+gWlOhAWIaIgPhjASHoEPDxUBhD8cMgeIaIE1AQEB
X-IronPort-AV: E=Sophos;i="5.26,516,1459839600"; 
   d="scan'208,217";a="27873480"
Received: from mail-lb0-f199.google.com ([209.85.217.199])
  by fe3.lbl.gov with ESMTP; 23 Jun 2016 06:21:43 -0700
Received: by mail-lb0-f199.google.com with SMTP id na2so58013540lbb.1
        for <singu...@lbl.gov>; Thu, 23 Jun 2016 06:21:43 -0700 (PDT)
X-Gm-Message-State: ALyK8tKBbASbR1BwEsqlfzTSWX71lEJomhi2tBYms7rMAXttQ5wP6UUoHv5wg+MGJUJv6911kFiidhxQgdqMpTq5xSn6b33Y4HjRLe56Cwzl5RAtm2smWMWzzDFbLDNL7c9neNh4bglfTEoZJEAeeZET9D0=
X-Received: by 10.25.156.11 with SMTP id f11mr9409057lfe.177.1466688102117;
        Thu, 23 Jun 2016 06:21:42 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.25.156.11 with SMTP id f11mr9409048lfe.177.1466688101887;
 Thu, 23 Jun 2016 06:21:41 -0700 (PDT)
Received: by 10.25.214.158 with HTTP; Thu, 23 Jun 2016 06:21:41 -0700 (PDT)
In-Reply-To: <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
	<CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
	<3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
	<CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
	<1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov>
	<CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
	<054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov>
	<613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
Date: Thu, 23 Jun 2016 06:21:41 -0700
Message-ID: <CAN7etTxs8tGjL269oecbL4JUyiHAUX23sr4FXwJw43KeQm8ZZw@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: "singu...@lbl.gov" <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a11410fc64f4b2f0535f1ef39

--001a11410fc64f4b2f0535f1ef39
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

In addition to what Ralph mentioned if you are just referring to
benchmarking different containers on the same host, that is just a matter
of launching a different container image. There are some limitations indeed
due to consumable resource allocations (eg. loop interfaces) but nothing
you should worry about for normal usage.


On Thursday, June 23, 2016, Ralph Castain <r...@open-mpi.org> wrote:

> I think you are misunderstanding the basic nature of the Singularity
> =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overlay. So =
=E2=80=9Csharing=E2=80=9D a container is
> no different than running on a node where the procs all see the same file
> system. Thus, having multiple containers that are identical makes no sens=
e
> - it=E2=80=99s all the same file system.
>
> Now if you want to run different containers (e.g., with different
> libraries or OS in them), then you would use mpirun=E2=80=99s MPMD syntax=
 - for
> example:
>
> mpirun -n 1 <container1> : -n 1 <container2>
>
> HTH
> Ralph
>
> On Jun 23, 2016, at 1:53 AM, Raimon Bosch <raimo...@gmail.com
> <javascript:_e(%7B%7D,'cvml','raimo...@gmail.com');>> wrote:
>
>
> One last question: What if I want to execute more than one container in
> the same host? With this technique I am bounded always to the same
> container. One of our experiments was based in measuring performance of
> several containers working in parallel in the same node. Also we had
> experiments with N containers per host in a multihost environment.
>
> El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Kurt=
zer
> escribi=C3=B3:
>>
>> Hi Raimon,
>>
>> Sorry I wasn't clear. I am not yet at my computer and thinking while
>> typing on an iPhone hinders my mental processes. Lol
>>
>> If I understand your example properly, you have a docker or VM
>> infrastructure already set up and you are invoking the mpirun commands f=
rom
>> within the virtual environment. Singularity works on a very different
>> premis because integrating a virtual cluster into an existing cluster an=
d
>> scheduling system is a mess.
>>
>> So starting from the physical nodes, which already have access to all
>> other nodes in the cluster, and already scheduled properly, and in direc=
t
>> access to optimized hardware and file systems.... You call mpirun.
>>
>> The mpirun command will take the standard format as you illustrated with
>> the following change to call Singularity inline:
>>
>> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img
>> trace.sh bt.C.4
>>
>> This assumes the following:
>>
>> 1. The container image which contains the program's you want to run is a=
t
>> ~/container.img and accessible at this path on all nodes referenced in
>> hosts.txt
>> 2. The hosts.txt references other physical nodes you want to run on
>> 3. The executables trace.sh and bt.C.4 are both inside the container and
>> in a standard path
>>
>> In this case we are performing one execution and MPI + singularity is
>> managing all of the communication between processes, nodes and container=
s.
>> Also it is now using any optimized hardware (eg. Infiniband) and existin=
g
>> high performance file systems (which should not be accessible via a
>> virtualized or Docker'ized cluster for security reasons).
>>
>> This way is actually MUCH simpler then what you are proposing because
>> there is no need to manage any virtual nodes, virtual networks, or resou=
rce
>> manager hacks. It really is as easy as just running any other MPI proces=
s
>> on an existing cluster.
>>
>> Hope that helps better!
>>
>>
>>
>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>
>>>
>>>
>>> Hi Gregory,
>>>
>>> I'm not sure if I would achieve the same with your commands. In an
>>> environment based on dockers or virtual machines we would do something =
like
>>> this [non applicable to Singularity]:
>>>
>>> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh
>>> ./bt.C.4
>>>
>>> where hosts.txt* is:
>>>
>>> >vm-ip-01-on-*host01* slots=3D2
>>> >vm-ip-01-on-*host02* slots=3D2
>>>
>>> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>>>
>>> and trace.sh is:
>>>
>>> >#!/bin/bash
>>> >
>>> >export EXTRAE_HOME=3D/opt/extrae/
>>> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
>>> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>>> >
>>> >## Run the desired program
>>> >$*
>>>
>>> As you see we only perform one execution and OpenMPI transparently
>>> manages communication between containers or virtual machines. This comm=
and
>>> would work well rather VMs are on the same host or not.
>>>
>>> What I understand from your response is that now we should execute
>>> OpenMPI on each host and then merge results manually. I don't know yet =
how
>>> to do this merge step or if it is any way to centralize everything like=
 I
>>> would do with VMs.
>>>
>>> Thanks in advance,
>>>
>>> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Ku=
rtzer
>>> escribi=C3=B3:
>>>>
>>>> Hi Raimon,
>>>>
>>>> The quick answer is you have mpirun handle that as you would normally
>>>> where the container file lives on a shared file system:
>>>>
>>>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>>>
>>>> Let the MPI outside the container launch the singularity container on
>>>> each host as it would normally launch any MPI program. Then it will ca=
ll
>>>> Singulairty and Singularity will launch the MPI program inside the
>>>> container on each of your hosts/servers.
>>>>
>>>> Hope that helps!
>>>>
>>>>
>>>>
>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>>>
>>>>>
>>>>> Hi Gregory,
>>>>>
>>>>> Thank you for your answer. One of our experiments needs to run OpenMP=
I
>>>>> among several servers. This means that we should put one of our conta=
iners
>>>>> in host01, another in host02 and another in host03 and collect the re=
sults.
>>>>>
>>>>>
>>>>> How can I do this execution in parallel if I need to communicate with
>>>>> more than one server?
>>>>>
>>>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer
>>>>> escribi=C3=B3:
>>>>>>
>>>>>> Hi Raimon,
>>>>>>
>>>>>> The communication model of a Singularity container is very different
>>>>>> from that of a Docker implementation. This is because Docker for all
>>>>>> practical purposes emulates a virtual machine as each container has =
it's
>>>>>> own IP address and thus it's own ssh server. It also carries its own=
 set of
>>>>>> complexities, for example networks need to be segregated/VLan'ed, DN=
S/host
>>>>>> resolution needs to be dynamic and passed down to the containers (so=
 they
>>>>>> can reach each other), ssh daemons and other process running inside =
the
>>>>>> containers, management via an existing scheduling system, and the li=
st goes
>>>>>> on and on.
>>>>>>
>>>>>> Think of it this way, Singularity does not do any of that... It runs
>>>>>> a program within the container as if it were running on the host its=
elf, so
>>>>>> to communicate between containers is as easy as communicating betwee=
n
>>>>>> programs. So for MPI, it would happen with the MPI on the physical h=
ost
>>>>>> (outside the container) invoking the container subsystem which then =
invokes
>>>>>> the MPI programs within the container and the MPI programs within th=
e
>>>>>> container communicate back to the MPI (orted) outside the container =
on the
>>>>>> host to get access to the host resources. In this model all availabl=
e
>>>>>> resources and infrastructure can be leveraged at full bandwidth by t=
he
>>>>>> contained processes and all of the aforementioned complexities akin =
to
>>>>>> running on a virtualized mini-cluster are circumvented.
>>>>>>
>>>>>> There is additional information I have written at:
>>>>>>
>>>>>> http://singularity.lbl.gov/#hpc
>>>>>>
>>>>>> That page is still coming along, and needs more information still bu=
t
>>>>>> if you have any questions, comments or change proposals please let u=
s know!
>>>>>>
>>>>>> Thanks and hope that helps!
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> We are trying to run experiments using singularity containers. The
>>>>>>> idea is to run OpenMPI among several containers and check performan=
ce
>>>>>>> results.
>>>>>>>
>>>>>>> How can I communicate with another container? In docker this is
>>>>>>> clear because every container gets an assigned IP and you can ping =
there,
>>>>>>> but what is the situation in the case of singularity? Is it possibl=
e to
>>>>>>> assign an IP to each container? Can I connect via ssh to them?
>>>>>>>
>>>>>>> Thanks in advance,
>>>>>>>
>>>>>>> --
>>>>>>> You received this message because you are subscribed to the Google
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>


--=20
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a11410fc64f4b2f0535f1ef39
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

In addition to what Ralph mentioned if you are just referring to benchmarki=
ng different containers on the same host, that is just a matter of launchin=
g a different container image. There are some limitations indeed due to con=
sumable resource allocations (eg. loop interfaces) but nothing you should <=
span></span>worry about for normal usage.=C2=A0<div><br></div><div><br>On T=
hursday, June 23, 2016, Ralph Castain &lt;<a href=3D"mailto:r...@open-mpi.o=
rg">r...@open-mpi.org</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div=
 style=3D"word-wrap:break-word"><div>I think you are misunderstanding the b=
asic nature of the Singularity =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s ju=
st a file system overlay. So =E2=80=9Csharing=E2=80=9D a container is no di=
fferent than running on a node where the procs all see the same file system=
. Thus, having multiple containers that are identical makes no sense - it=
=E2=80=99s all the same file system.</div><div><br></div><div>Now if you wa=
nt to run different containers (e.g., with different libraries or OS in the=
m), then you would use mpirun=E2=80=99s MPMD syntax - for example:</div><di=
v><br></div><div>mpirun -n 1 &lt;container1&gt; : -n 1 &lt;container2&gt;</=
div><div><br></div><div>HTH</div><div>Ralph</div><div><br><blockquote type=
=3D"cite"><div>On Jun 23, 2016, at 1:53 AM, Raimon Bosch &lt;<a href=3D"jav=
ascript:_e(%7B%7D,&#39;cvml&#39;,&#39;raim...@gmail.com&#39;);" target=3D"_=
blank">raimo...@gmail.com</a>&gt; wrote:</div><br><div><div dir=3D"ltr" sty=
le=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight:no=
rmal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:=
none;white-space:normal;word-spacing:0px"><br>One last question: What if I =
want to execute more than one container in the same host? With this techniq=
ue I am bounded always to the same container. One of our experiments was ba=
sed in measuring performance of several containers working in parallel in t=
he same node. Also we had experiments with N containers per host in a multi=
host environment.<br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (=
UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:r=
gb(204,204,204);border-left-style:solid;padding-left:1ex">Hi Raimon,<div><b=
r></div><div>Sorry I wasn&#39;t clear. I am not yet at my computer and thin=
king while typing on an iPhone hinders my mental processes. Lol</div><div><=
br></div><div>If I understand your example properly, you have a docker or V=
M infrastructure already set up and you are invoking the mpirun commands fr=
om within the virtual environment. Singularity works on a very different pr=
emis because integrating a virtual cluster into an existing cluster and sch=
eduling system is a mess.</div><div><br></div><div>So starting from the phy=
sical nodes, which already have access to all other nodes in the cluster, a=
nd already scheduled properly, and in direct access to optimized hardware a=
nd file systems.... You call mpirun.=C2=A0</div><div><br></div><div>The mpi=
run command will take the standard format as you illustrated with the follo=
wing change to call Singularity inline:</div><div><br></div><div>$ mpirun -=
np 4 --hostfile hosts.txt singularity exec ~/container.img trace.sh bt.C.4<=
/div><div><br></div><div>This assumes the following:</div><div><br></div><d=
iv>1. The container image which contains the program&#39;s you want to run =
is at ~/container.img and accessible at this path=C2=A0on all nodes referen=
ced in hosts.txt</div><div>2. The hosts.txt references other physical nodes=
 you want to run on</div><div>3. The executables trace.sh and bt.C.4 are bo=
th inside the container and in a standard path</div><br><div>In this case w=
e are performing one execution and MPI + singularity is managing all of the=
 communication between processes, nodes and containers. Also it is now usin=
g any optimized hardware (eg. Infiniband)=C2=A0and existing high performanc=
e=C2=A0file systems (which should not be accessible via a virtualized or Do=
cker&#39;ized=C2=A0cluster for security reasons).</div><div><br></div><div>=
This way is actually MUCH simpler then what you are proposing because there=
 is no need to manage any virtual nodes, virtual networks, or resource mana=
ger hacks. It really is as easy as just running any other MPI=C2=A0process =
on an existing cluster.=C2=A0</div><div><br></div><div>Hope that helps bett=
er!</div><div><br></div><div><span></span><br><br>On Wednesday, June 22, 20=
16, Raimon Bosch &lt;<a rel=3D"nofollow">raimon...@</a><a href=3D"http://gm=
ail.com/" target=3D"_blank">gmail.com</a>&gt; wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">=
<div dir=3D"ltr"><br><br>Hi Gregory,<br><br>I&#39;m not sure if I would ach=
ieve the same with your commands. In an environment based on dockers or vir=
tual machines we would do something like this [non applicable to Singularit=
y]:<br><br>&gt; cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile hosts.t=
xt ./trace.sh ./bt.C.4<br><br>where hosts.txt* is:<br><br>&gt;vm-ip-01-on-<=
b>host01</b><span>=C2=A0</span>slots=3D2<br>&gt;vm-ip-01-on-<b>host02</b><s=
pan>=C2=A0</span>slots=3D2<br><br>* vm-ip-XX-on-hostXX<b><span>=C2=A0</span=
></b>are IPs i.e. 172.100.60.XX<br><br>and trace.sh is:<br><br>&gt;#!/bin/b=
ash<br>&gt;<br>&gt;export EXTRAE_HOME=3D/opt/extrae/<br>&gt;export EXTRAE_C=
ONFIG_FILE=3D/extrae.xml<br>&gt;export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libm=
pitrace.so<br>&gt;<br>&gt;## Run the desired program<br>&gt;$*<br><br>As yo=
u see we only perform one execution and OpenMPI transparently manages commu=
nication between containers or virtual machines. This command would work we=
ll rather VMs are on the same host or not.<br><br>What I understand from yo=
ur response is that now we should execute OpenMPI on each host and then mer=
ge results manually. I don&#39;t know yet how to do this merge step or if i=
t is any way to centralize everything like I would do with VMs.<br><br>Than=
ks in advance,<br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC=
+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" sty=
le=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(=
204,204,204);border-left-style:solid;padding-left:1ex">Hi Raimon,<div><br><=
/div><div>The quick answer is you have mpirun handle that as you would norm=
ally where the container file lives on a shared file system:</div><div><br>=
</div><div>$ mpirun singularity exec ~/container.img mpi_prog_in_container<=
/div><div><br></div><div>Let the MPI outside the container launch the singu=
larity container on each host as it would normally launch any MPI program. =
Then it will call Singulairty and Singularity will launch the MPI program i=
nside the container on each of your hosts/servers.=C2=A0</div><div><br></di=
v><div>Hope that helps!</div><div><br></div><div><span></span><br><br>On We=
dnesday, June 22, 2016, Raimon Bosch &lt;<a rel=3D"nofollow">rai...@gmail.c=
om</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px =
0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);bord=
er-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<br><=
br>Thank you for your answer. One of our experiments needs to run OpenMPI a=
mong several servers. This means that we should put one of our containers i=
n host01, another in host02 and another in host03 and collect the results.<=
span>=C2=A0</span><br><br>How can I do this execution in parallel if I need=
 to communicate with more than one server?<br><br>El martes, 21 de junio de=
 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">=
<div dir=3D"ltr">Hi Raimon,<div><br></div><div>The communication model of a=
 Singularity container is very different from that of a Docker implementati=
on. This is because Docker for all practical purposes emulates a virtual ma=
chine as each container has it&#39;s own IP address and thus it&#39;s own s=
sh server. It also carries its own set of complexities, for example network=
s need to be segregated/VLan&#39;ed, DNS/host resolution needs to be dynami=
c and passed down to the containers (so they can reach each other), ssh dae=
mons and other process running inside the containers, management via an exi=
sting scheduling system, and the list goes on and on.</div><div><br></div><=
div>Think of it this way, Singularity does not do any of that... It runs a =
program within the container as if it were running on the host itself, so t=
o communicate between containers is as easy as communicating between progra=
ms. So for MPI, it would happen with the MPI on the physical host (outside =
the container) invoking the container subsystem which then invokes the MPI =
programs within the container and the MPI programs within the container com=
municate back to the MPI (orted) outside the container on the host to get a=
ccess to the host resources. In this model all available resources and infr=
astructure can be leveraged at full bandwidth by the contained processes an=
d all of the aforementioned complexities akin to running on a virtualized m=
ini-cluster are circumvented.</div><div><br></div><div>There is additional =
information I have written at:</div><div><br></div><div><a href=3D"http://s=
ingularity.lbl.gov/#hpc" rel=3D"nofollow" target=3D"_blank">http://singular=
ity.lbl.gov/#hpc</a><br></div><div><br></div><div>That page is still coming=
 along, and needs more information still but if you have any questions, com=
ments or change proposals please let us know!</div><div><br></div><div>Than=
ks and hope that helps!</div><div><br></div><div><br></div></div><div><br><=
div class=3D"gmail_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch<spa=
n>=C2=A0</span><span dir=3D"ltr">&lt;<a rel=3D"nofollow">rai...@gmail.com</=
a>&gt;</span><span>=C2=A0</span>wrote:<br><blockquote class=3D"gmail_quote"=
 style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:=
rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir=3D"ltr"=
><br><br>Hi,<br><br>We are trying to run experiments using singularity cont=
ainers. The idea is to run OpenMPI among several containers and check perfo=
rmance results.<span>=C2=A0</span><br><br>How can I communicate with anothe=
r container? In docker this is clear because every container gets an assign=
ed IP and you can ping there, but what is the situation in the case of sing=
ularity? Is it possible to assign an IP to each container? Can I connect vi=
a ssh to them?<br><br>Thanks in advance,<span><font color=3D"#888888"><br><=
/font></span></div><span><font color=3D"#888888"><div><br></div>--<span>=C2=
=A0</span><br>You received this message because you are subscribed to the G=
oogle Groups &quot;singularity&quot; group.<br>To unsubscribe from this gro=
up and stop receiving emails from it, send an email to<span>=C2=A0</span><a=
 rel=3D"nofollow">singu...@lbl.gov</a>.<br></font></span></blockquote></div=
><br><br clear=3D"all"><div><br></div>--<span>=C2=A0</span><br><div><div di=
r=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (H=
PCS)<br>University of California<br>Lawrence Berkeley National Laboratory<b=
r>One Cyclotron Road, Berkeley, CA 94720</div></div></div></div></blockquot=
e></div><div><br></div>--<span>=C2=A0</span><br>You received this message b=
ecause you are subscribed to the Google Groups &quot;singularity&quot; grou=
p.<br>To unsubscribe from this group and stop receiving emails from it, sen=
d an email to<span>=C2=A0</span><a>singu...@lbl.gov</a>.<br></blockquote></=
div><br><br>--<span>=C2=A0</span><br><div dir=3D"ltr"><div>Gregory M. Kurtz=
er<br>High Performance Computing Services (HPCS)<br>University of Californi=
a<br>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley,=
 CA 94720</div></div><br></blockquote></div><div><br></div>--<span>=C2=A0</=
span><br>You received this message because you are subscribed to the Google=
 Groups &quot;singularity&quot; group.<br>To unsubscribe from this group an=
d stop receiving emails from it, send an email to<span>=C2=A0</span><a>sing=
u...@lbl.gov</a>.<br></blockquote></div><br><br>--<span>=C2=A0</span><br><d=
iv dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Servic=
es (HPCS)<br>University of California<br>Lawrence Berkeley National Laborat=
ory<br>One Cyclotron Road, Berkeley, CA 94720</div></div><br></blockquote><=
/div><div style=3D"font-family:Helvetica;font-size:12px;font-style:normal;f=
ont-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;te=
xt-transform:none;white-space:normal;word-spacing:0px"><br></div><span styl=
e=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight:nor=
mal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:n=
one;white-space:normal;word-spacing:0px;float:none;display:inline!important=
">--<span>=C2=A0</span></span><br style=3D"font-family:Helvetica;font-size:=
12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align:=
start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0=
px"><span style=3D"font-family:Helvetica;font-size:12px;font-style:normal;f=
ont-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;te=
xt-transform:none;white-space:normal;word-spacing:0px;float:none;display:in=
line!important">You received this message because you are subscribed to the=
 Google Groups &quot;singularity&quot; group.</span><br style=3D"font-famil=
y:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-spac=
ing:normal;text-align:start;text-indent:0px;text-transform:none;white-space=
:normal;word-spacing:0px"><span style=3D"font-family:Helvetica;font-size:12=
px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align:st=
art;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px=
;float:none;display:inline!important">To unsubscribe from this group and st=
op receiving emails from it, send an email to<span>=C2=A0</span></span><a h=
ref=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@lbl.gov&#39;=
);" style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-we=
ight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-tra=
nsform:none;white-space:normal;word-spacing:0px" target=3D"_blank">singu...=
@lbl.gov</a><span style=3D"font-family:Helvetica;font-size:12px;font-style:=
normal;font-weight:normal;letter-spacing:normal;text-align:start;text-inden=
t:0px;text-transform:none;white-space:normal;word-spacing:0px;float:none;di=
splay:inline!important">.</span></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>

--001a11410fc64f4b2f0535f1ef39--
