Date: Sat, 30 Sep 2017 08:04:22 -0700 (PDT)
From: Martin Cuma <mart...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <398a962a-7a1f-4ba1-8cce-87f5ca13fe69@lbl.gov>
In-Reply-To: <47b9a516-6aa6-4134-ba3d-e0cbfbf066a5@lbl.gov>
References: <47b9a516-6aa6-4134-ba3d-e0cbfbf066a5@lbl.gov>
Subject: Re: MPI and compiled applications
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_5226_2079759592.1506783862816"

------=_Part_5226_2079759592.1506783862816
Content-Type: multipart/alternative; 
	boundary="----=_Part_5227_1416802256.1506783862816"

------=_Part_5227_1416802256.1506783862816
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Aaron,

as you elude, you either need to bring the libgpfs.so into the container or 
build host OpenMPI without it. 

I would test if you can run using the host OpenMPI on a non-gpfs hardware 
(probably testing some MPI-IO aware code to touch a file system), and if 
you can, then it should be OK to bring the libgpfs.so in without losing the 
portability. I haven't tested this lately but we have built MPIs (the ROMIO 
MPI-IO part to be exact) with multiple file systems support in the pastso, 
I think that bringing the libgpfs.so in the container should not compromise 
its portability.

Now, how to bring it in - in the %setup section I'd just copy the files 
from the host to the container. We've done something similar in the past 
with the GPU stack before the --nv option was implemented (which 
essentially does the same thing for you, but at runtime). Some rough 
example is 
here: https://github.com/CHPC-UofU/Singularity-tensorflow/blob/master/Singularity.2.2

HTH,
MC

------=_Part_5227_1416802256.1506783862816
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Aaron,<div><br></div><div>as you elude, you either need to=
 bring the libgpfs.so into the container or build host OpenMPI without it.=
=C2=A0<br></div><div><br></div><div>I would test if you can run using the h=
ost OpenMPI on a non-gpfs hardware (probably testing some MPI-IO aware code=
 to touch a file system), and if you can, then it should be OK to bring the=
 libgpfs.so in without losing the portability. I haven&#39;t tested this la=
tely but we have built MPIs (the ROMIO MPI-IO part to be exact) with multip=
le file systems support in the pastso, I think that bringing the libgpfs.so=
 in the container should not compromise its portability.</div><div><br></di=
v><div>Now, how to bring it in - in the %setup section I&#39;d just copy th=
e files from the host to the container. We&#39;ve done something similar in=
 the past with the GPU stack before the --nv option was implemented (which =
essentially does the same thing for you, but at runtime). Some rough exampl=
e is here:=C2=A0https://github.com/CHPC-UofU/Singularity-tensorflow/blob/ma=
ster/Singularity.2.2</div><div><br></div><div>HTH,</div><div>MC</div></div>
------=_Part_5227_1416802256.1506783862816--

------=_Part_5226_2079759592.1506783862816--
