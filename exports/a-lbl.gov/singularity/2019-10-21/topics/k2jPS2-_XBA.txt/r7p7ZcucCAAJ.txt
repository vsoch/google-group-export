Date: Wed, 16 Aug 2017 06:44:09 -0700 (PDT)
From: Peleg Bar Sapir <pel...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <30715e7a-2b3c-4193-b0b4-501a001b5c3f@lbl.gov>
Subject: ompi_rte_init returns (-43) error code when running from a
 container
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_6869_1772502722.1502891049481"

------=_Part_6869_1772502722.1502891049481
Content-Type: multipart/alternative; 
	boundary="----=_Part_6870_1098939842.1502891049481"

------=_Part_6870_1098939842.1502891049481
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hello,

I'm a student helper in a HPC service, and was asked to check out 
Singularity and help decide if we provided to our users. We run openmpi 
2.1.0 on our cluster.

Following this <http://singularity.lbl.gov/docs-hpc> tutorial I was able to 
form a container and use it on our cluster, but running 

 mpirun -np 20 singularity exec centos7_mpi_test.img /usr/bin/ring
generates an error (I copied it to the end of this message).

I tried to look for a clue about what is wrong, but couldn't find any useful help. I would appreciate any help provided.

*Error message*:
 
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "(null)" (-43) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  ompi_mpi_init: ompi_rte_init failed
  --> Returned "(null)" (-43) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[(server name):6349] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[(server name):6347] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[12088,1],0]
  Exit code:    1
--------------------------------------------------------------------------


------=_Part_6870_1098939842.1502891049481
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello,<br><br>I&#39;m a student helper in a HPC service, a=
nd was asked to check out Singularity and help decide if we provided to our=
 users. We run openmpi 2.1.0 on our cluster.<br><br>Following <a href=3D"ht=
tp://singularity.lbl.gov/docs-hpc">this</a> tutorial I was able to form a c=
ontainer and use it on our cluster, but running <br><pre class=3D"highlight=
"><code><span class=3D"gp"> </span>mpirun -np 20 singularity <span class=3D=
"nb">exec</span> centos7_mpi_test.img /usr/bin/ring<br>generates an error (=
I copied it to the end of this message).<br><br></code><code>I tried</code>=
<code> to look for a clue about what is wrong, but couldn&#39;t find any us=
eful help. I would appreciate any help provided.<br><br><u>Error message</u=
>:<br>=C2=A0<br></code><code>----------------------------------------------=
----------------------------</code><br><code>It looks like MPI_INIT failed =
for some reason; your parallel process is</code><br><code>likely to abort. =
 There are many reasons that a parallel process can</code><br><code>fail du=
ring MPI_INIT; some of which are due to configuration or environment</code>=
<br><code>problems.  This failure appears to be an internal failure; here&#=
39;s some</code><br><code>additional information (which may only be relevan=
t to an Open MPI</code><br><code>developer):</code><br><code></code><br><co=
de>  ompi_mpi_init: ompi_rte_init failed</code><br><code>  --&gt; Returned =
&quot;(null)&quot; (-43) instead of &quot;Success&quot; (0)</code><br><code=
>--------------------------------------------------------------------------=
</code><br><code>----------------------------------------------------------=
----------------</code><br><code>It looks like MPI_INIT failed for some rea=
son; your parallel process is</code><br><code>likely to abort.  There are m=
any reasons that a parallel process can</code><br><code>fail during MPI_INI=
T; some of which are due to configuration or environment</code><br><code>pr=
oblems.  This failure appears to be an internal failure; here&#39;s some</c=
ode><br><code>additional information (which may only be relevant to an Open=
 MPI</code><br><code>developer):</code><br><code></code><br><code>  ompi_mp=
i_init: ompi_rte_init failed</code><br><code>  --&gt; Returned &quot;(null)=
&quot; (-43) instead of &quot;Success&quot; (0)</code><br><code>-----------=
---------------------------------------------------------------</code><br><=
code>*** An error occurred in MPI_Init</code><br><code>*** on a NULL commun=
icator</code><br><code>*** MPI_ERRORS_ARE_FATAL (processes in this communic=
ator will now abort,</code><br><code>***    and potentially your MPI job)</=
code><br><code>*** An error occurred in MPI_Init</code><br><code>*** on a N=
ULL communicator</code><br><code>*** MPI_ERRORS_ARE_FATAL (processes in thi=
s communicator will now abort,</code><br><code>***    and potentially your =
MPI job)</code><br><code>[</code><code><code>(server name)</code>:6349] Loc=
al abort before MPI_INIT completed completed successfully, but am not able =
to aggregate error messages, and not able to guarantee that all other proce=
sses were killed!<br>[</code><code><code>(server name)</code>:6347] Local a=
bort before MPI_INIT completed completed successfully, but am not able to a=
ggregate error messages, and not able to guarantee that all other processes=
 were killed!<br>-------------------------------------------------------</c=
ode><br><code>Primary job  terminated normally, but 1 process returned</cod=
e><br><code>a non-zero exit code.. Per user-direction, the job has been abo=
rted.</code><br><code>-----------------------------------------------------=
--</code><br><code>--------------------------------------------------------=
------------------</code><br><code>mpirun detected that one or more process=
es exited with non-zero status, thus causing</code><br><code>the job to be =
terminated. The first process to do so was:</code><br><code></code><br><cod=
e>  Process name: [[12088,1],0]</code><br><code>  Exit code:    1</code><br=
><code>--------------------------------------------------------------------=
------<br></code><code></code></pre></div>
------=_Part_6870_1098939842.1502891049481--

------=_Part_6869_1772502722.1502891049481--
