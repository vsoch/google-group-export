Date: Wed, 18 Jan 2017 03:00:47 -0800 (PST)
From: Stefan Kombrink <stefan....@googlemail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <3589df38-4328-41e6-9c55-ad51fbc8271d@lbl.gov>
In-Reply-To: <CAN7etTz5bYr-TTXJibNH+5qii9z_7pKpU_HXyR-SyNLiLRQk-A@mail.gmail.com>
References: <8a569f24-bd38-4bf0-b025-5843a1525141@lbl.gov>
 <CAN7etTz5bYr-TTXJibNH+5qii9z_7pKpU_HXyR-SyNLiLRQk-A@mail.gmail.com>
Subject: Re: [Singularity] Survey: MPI Hybrid mode vs MPI native?
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_2060_432123028.1484737247828"

------=_Part_2060_432123028.1484737247828
Content-Type: multipart/alternative; 
	boundary="----=_Part_2061_504995003.1484737247828"

------=_Part_2061_504995003.1484737247828
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi Greg,

 thanks for your reply.

Am Mittwoch, 18. Januar 2017 04:51:46 UTC+1 schrieb Gregory M. Kurtzer:
>
>
>
> On Tue, Jan 10, 2017 at 12:25 AM, 'Stefan Kombrink' via singularity <
> si...@lbl.gov <javascript:>> wrote:
>
>> Happy new year, dear Singularity-community,
>>
>>  I am asking those of you using the MPI Hybrid mode with OpenMPI 2 for 
>> sharing your experience.
>> What does the hybrid approach do for you that cannot be done by having 
>> MPI within the container?
>>
>
> A few that come up quick for me are:
>
> * if MPI is inside container jobs are typically limited to single node runs
> * resource managers can not communicate reasonably to MPI inside container 
> (e.g. no coupling of the MPI and RM)
> * In this model, MPI would try to interface with ssh within the container, 
> and thus expect sshd running within the other containers (which is opening 
> pandora's box on traditional HPC)
>

That is how I currently run my experimental container jobs. I contain the 
originally installed MPI and bind mount /var/lib/torque.
The most tricky part was to replace ssh inside the container with a wrapper 
which launches the passed command inside another instance of the origin 
container.
It works now with my IntelMPI and OpenMPI examples and I did not see 
significant changes in runtime.
We use a single-user policy for our nodes i.e. only one user may submit 
jobs on a compute node at a time. 
I didn't see problems of wrong accounting of the scheduler / RM but then 
again this is just a very small testbed.

Greets
Stefan

------=_Part_2061_504995003.1484737247828
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Greg,<br><br>=C2=A0thanks for your reply.<br><br>Am Mit=
twoch, 18. Januar 2017 04:51:46 UTC+1 schrieb Gregory M. Kurtzer:<blockquot=
e class=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left: =
1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr"><br><div><br><div class=
=3D"gmail_quote">On Tue, Jan 10, 2017 at 12:25 AM, &#39;Stefan Kombrink&#39=
; via singularity <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_=
blank" gdf-obfuscated-mailto=3D"zt2jFmqZCAAJ" rel=3D"nofollow" onmousedown=
=3D"this.href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D=
&#39;javascript:&#39;;return true;">si...@lbl.gov</a>&gt;</span> wrote:<br>=
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Happy new year, dear Singul=
arity-community,<br><br>=C2=A0I am asking those of you using the MPI Hybrid=
 mode with OpenMPI 2 for sharing your experience.<br>What does the hybrid a=
pproach do for you that cannot be done by having MPI within the container?<=
br></div></blockquote><div><br></div><div>A few that come up quick for me a=
re:</div><div><br></div><div>* if MPI is inside container jobs are typicall=
y limited to single node runs</div><div>* resource managers can not communi=
cate reasonably to MPI inside container (e.g. no coupling of the MPI and RM=
)</div><div>* In this model, MPI would try to interface with ssh within the=
 container, and thus expect sshd running within the other containers (which=
 is opening pandora&#39;s box on traditional HPC)</div></div></div></div></=
blockquote><div><br>That is how I currently run my experimental container j=
obs. I contain the originally installed MPI and bind mount /var/lib/torque.=
<br>The most tricky part was to replace ssh inside the container with a wra=
pper which launches the passed command inside another instance of the origi=
n container.<br>It works now with my IntelMPI and OpenMPI examples and I di=
d not see significant changes in runtime.<br>We use a single-user policy fo=
r our nodes i.e. only one user may submit jobs on a compute node at a time.=
 <br>I didn&#39;t see problems of wrong accounting of the scheduler / RM bu=
t then again this is just a very small testbed.</div><br>Greets<br>Stefan<b=
r></div>
------=_Part_2061_504995003.1484737247828--

------=_Part_2060_432123028.1484737247828--
