X-Received: by 10.157.37.35 with SMTP id k32mr13340205otb.138.1487863388306;
        Thu, 23 Feb 2017 07:23:08 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.46.193 with SMTP id i184ls3383703ita.10.canary-gmail; Thu,
 23 Feb 2017 07:23:07 -0800 (PST)
X-Received: by 10.99.36.3 with SMTP id k3mr49381400pgk.136.1487863387355;
        Thu, 23 Feb 2017 07:23:07 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id 18si4597380pfa.133.2017.02.23.07.23.07
        for <singu...@lbl.gov>;
        Thu, 23 Feb 2017 07:23:07 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.161.197 as permitted sender) client-ip=209.85.161.197;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.161.197 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 3.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EBAgDz/a5Yh8WhVdFdHgYMGQYMgkOBQ3gRB4NMCJtigmSKXhWCMYIQM4JpgUobKB8BDIFtgi+BWgKDGgdBFgEBAQEBAQEBAQEBAhABAQEIDQkKHS+CMwQCAwEdBAQ9CgECLgEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBARICDSICDQMPGgEBAQMBIyswCwsLDSoCAiIPAwEFAQsRBggHBAEHFQSIcloIBQmhNz+MA4Imi0IBAQgBAQEBASMSiiCBCYMXgQ8RAYMigl8FiQdzhlIQhQ2GKwGGc4VghVCCToEHjTyRYRQegRUPFwR3LwgZCjdkBYFuAoIGOREMggIfNQeJEUeBZwEBAQ
X-IronPort-AV: E=Sophos;i="5.35,198,1484035200"; 
   d="scan'208,217";a="65633621"
Received: from mail-yw0-f197.google.com ([209.85.161.197])
  by fe3.lbl.gov with ESMTP; 23 Feb 2017 07:23:05 -0800
Received: by mail-yw0-f197.google.com with SMTP id c204so16748519ywc.7
        for <singu...@lbl.gov>; Thu, 23 Feb 2017 07:23:05 -0800 (PST)
X-Gm-Message-State: AMke39m7vNKbvqXLdGszoUyixZJrZXsOsjVNEuCIDhGKcjzmr7AkHGXaLTn5tTdnVNoZUrNEmXIuouR73oreNG9kugUg3PkVJmSTagBFZWOrjpNm7EOmfgo5nejoc030BA3BzvQuQr2KDkWddiCsim3LS7k=
X-Received: by 10.13.238.1 with SMTP id x1mr27000354ywe.342.1487863385313;
        Thu, 23 Feb 2017 07:23:05 -0800 (PST)
X-Received: by 10.13.238.1 with SMTP id x1mr27000334ywe.342.1487863384924;
 Thu, 23 Feb 2017 07:23:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.129.110.69 with HTTP; Thu, 23 Feb 2017 07:23:04 -0800 (PST)
In-Reply-To: <2e6d299f-12d6-42ea-aacb-55f0d8924e36@lbl.gov>
References: <4a675b1d-5116-4702-a9e4-cc0024dcb436@lbl.gov> <CAN7etTzUMksoSB_H1ZgA3+-fFvM2pO+XzeD3E-pLqLiiL69c4w@mail.gmail.com>
 <5c39ff15-22d6-4b4f-8566-a670d2910e6a@lbl.gov> <2e6d299f-12d6-42ea-aacb-55f0d8924e36@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 23 Feb 2017 07:23:04 -0800
Message-ID: <CAN7etTyX+_RYBb8m4eNFORisfa848Bksm6s5vWZZjMgsdD1WHg@mail.gmail.com>
Subject: Re: [Singularity] job with multiple singularity calls (mixed
 MPI/serial) fails
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=94eb2c188a4a8874620549343047

--94eb2c188a4a8874620549343047
Content-Type: text/plain; charset=UTF-8

While I think you are hitting some locking bug in Singularity, that I
think/hope is already fixed in one of our development branches, I do think
there is a workaround you can use. Modify your MPI line to include:

SINGULARITY_NOSESSIONCLEANUP=1

I think you can do it like:

mpirun -mca orte_tmpdir_base "${ORTE_TMP}"
SINGULARITY_NOSESSIONCLEANUP=1 singularity
--debug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img
bash -c 'cd /wrf/run/wrfwkdir; real.exe'

Or you can make sure that the variable is set on every node running the
process. A better fix is included (or rather will be included) in the
latest version of Open MPI that will automatically set that variable, and
allow Open MPI to clean up the Singularity sessionfiles directly after the
job runs. Presently those optimizations exist in Open MPI's master branch.
So if you can build that yourself and test, that is also an option.

Sorry for the delay, and hope that helps!

Greg

On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BEX <geert...@uhasselt.be>
wrote:

> Dear Gregory,
>
> Any feedback you can give me?
>
> Thanks in advance, best regards, -gjb-
>
>
> On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:
>>
>> Dear Gregory,
>>
>> Thanks for looking into this.
>>
>> The /tmp is on the local disks of the compute nodes, so no NFS mounts
>> involved.
>>
>> The /tmp is also used for the MCA orte_tmpdir_base for mpirun.
>>
>> I've put the output of a run with --debug on Google Drive, you can access
>> it via:
>> https://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8
>> /view?usp=sharing
>>
>> The "offending" statement in the job script is:
>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" singularity --debug exec -B
>> ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img bash -c 'cd
>> /wrf/run/wrfwkdir; real.exe'
>>
>> So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH and
>> SIMDIR refer to a GPFS file system, and that is also where the working
>> directory for the job is on.
>>
>> Singularity's config file is unaltered from default.
>>
>> Thanks in advance, -gjb-
>>
>> On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. Kurtzer
>> wrote:
>>>
>>> Hi Geert,
>>>
>>> The directory is only supposed to be removed when the flock()s have all
>>> been released. I just did some fixups for this in one of the development
>>> branches, but 2.2* should be working in the same way. Is `/tmp` a local
>>> directory, or is it on some form of shared NFS? Does it have any mount
>>> options to disable locking?
>>>
>>> Can you also try with --debug enabled and send all of the output?
>>>
>>> Thanks!
>>>
>>> On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <gee...@uhasselt.be>
>>> wrote:
>>>
>>>> Dear,
>>>>
>>>> I've run into a problem with singularity, and out of ideas by now.
>>>>
>>>> When running a job that involves multiple calls to singularity (same
>>>> image each time), some of them serial, some MPI, the MPI ones
>>>> intermittently fail with:
>>>> ERROR  : Could not remove session directory
>>>> /tmp/.singularity-session-2530140.42.465829344: Device or resource busy
>>>>
>>>> This is for OpenMPI 1.10.2 on host and in image.  The host OS is CentOS
>>>> 7, the image OS Ubuntu 16.04,  Singularity 2.2.  The job runs on 4 nodes,
>>>> 27 processes each.
>>>>
>>>> I've tried several approaches (sleep between calls, setting a session
>>>> directory per singularity call, ...).
>>>>
>>>> Any ideas?
>>>>
>>>> Thanks in advance, Geert Jan Bex
>>>>
>>>>
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> HPC Systems Architect and Technology Developer
>>> Lawrence Berkeley National Laboratory HPCS
>>> University of California Berkeley Research IT
>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>> er.com/gmkurtzer
>>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--94eb2c188a4a8874620549343047
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>While I think you are hitting some locking bug in Sin=
gularity, that I think/hope is already fixed in one of our development bran=
ches, I do think there is a workaround you can use. Modify your MPI line to=
 include:</div><div><br></div><div>SINGULARITY_NOSESSIONCLEANUP=3D1</div><d=
iv><br></div><div>I think you can do it like:</div><div><br></div><div><div=
 style=3D"font-size:12.8px">mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP}&=
quot;=C2=A0<span style=3D"font-size:small">SINGULARITY_NOSESSIONCLEANUP=3D1=
=C2=A0</span><span style=3D"font-size:12.8px">singularity --debug exec -B $=
{SIMDIR}:/wrf/run $VSC_SCRATCH/containers/</span><wbr style=3D"font-size:12=
.8px"><span style=3D"font-size:12.8px">wrfrun-latest.img bash -c &#39;cd /w=
rf/run/wrfwkdir; real.exe&#39;</span></div></div><div><br></div><div>Or you=
 can make sure that the variable is set on every node running the process. =
A better fix is included (or rather will be included) in the latest version=
 of Open MPI that will automatically set that variable, and allow Open MPI =
to clean up the Singularity sessionfiles directly after the job runs. Prese=
ntly those optimizations exist in Open MPI&#39;s master branch. So if you c=
an build that yourself and test, that is also an option.</div><div><br></di=
v>Sorry for the delay, and hope that helps!<div><br></div><div>Greg</div></=
div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Thu, Feb 2=
3, 2017 at 5:11 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a href=3D"mailto:g=
eert...@uhasselt.be" target=3D"_blank">geert...@uhasselt.be</a>&gt;</span> =
wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bord=
er-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div=
><br></div><div>Any feedback you can give me?</div><div><br></div><div>Than=
ks in advance, best regards, -gjb-</div><div><div class=3D"h5"><div><br><br=
>On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:<bl=
ockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-l=
eft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div><br=
></div><div>Thanks for looking into this.</div><div><br></div><div>The /tmp=
 is on the local disks of the compute nodes, so no NFS mounts involved.</di=
v><div><br></div><div>The /tmp is also used for the MCA orte_tmpdir_base fo=
r mpirun.</div><div><br></div><div>I&#39;ve put the output of a run with --=
debug on Google Drive, you can access it via:</div><div><a href=3D"https://=
drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=3Dsharing" re=
l=3D"nofollow" target=3D"_blank">https://drive.google.com/file/<wbr>d/0BxlS=
eep1Z7B-MzRsd2pwV2dUTG8<wbr>/view?usp=3Dsharing</a></div><div><br></div><di=
v>The &quot;offending&quot; statement in the job script is:</div><div><div>=
mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP}&quot; singularity --debug ex=
ec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun<wbr>-latest.img bas=
h -c &#39;cd /wrf/run/wrfwkdir; real.exe&#39;</div></div><div><br></div><di=
v>So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH and S=
IMDIR refer to a GPFS file system, and that is also where the working direc=
tory for the job is on.</div><div><br></div><div>Singularity&#39;s config f=
ile is unaltered from default.</div><div><br></div><div>Thanks in advance, =
-gjb-<br><br>On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. K=
urtzer wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-lef=
t:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Ge=
ert,<div><br></div><div>The directory is only supposed to be removed when t=
he flock()s have all been released. I just did some fixups for this in one =
of the development branches, but 2.2* should be working in the same way. Is=
 `/tmp` a local directory, or is it on some form of shared NFS? Does it hav=
e any mount options to disable locking?</div><div><br></div><div>Can you al=
so try with --debug enabled and send all of the output?=C2=A0</div><div><br=
></div><div>Thanks!</div></div><div><br><div class=3D"gmail_quote">On Mon, =
Feb 20, 2017 at 6:25 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a rel=3D"nofo=
llow">gee...@uhasselt.be</a>&gt;</span> wrote:<br><blockquote class=3D"gmai=
l_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left=
:1ex"><div dir=3D"ltr">Dear,<div><br></div><div>I&#39;ve run into a problem=
 with singularity, and out of ideas by now.</div><div><br></div><div>When r=
unning a job that involves multiple calls to singularity (same image each t=
ime), some of them serial, some MPI, the MPI ones intermittently fail with:=
</div><div><div>ERROR =C2=A0: Could not remove session directory /tmp/.sing=
ularity-session-2530<wbr>140.42.465829344: Device or resource busy</div></d=
iv><div><br></div><div>This is for OpenMPI 1.10.2 on host and in image.=C2=
=A0 The host OS is CentOS 7, the image OS Ubuntu 16.04, =C2=A0Singularity 2=
.2.=C2=A0 The job runs on 4 nodes, 27 processes each.</div><div><br></div><=
div>I&#39;ve tried several approaches (sleep between calls, setting a sessi=
on directory per singularity call, ...).</div><div><br></div><div>Any ideas=
?</div><div><br></div><div>Thanks in advance, Geert Jan Bex</div><span><fon=
t color=3D"#888888"><div><br></div><div><br></div><div><br></div></font></s=
pan></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.go=
v/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warew=
ulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.g=
ov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" re=
l=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</blockquote></div></div></blockquote></div></div></div></div><div class=3D=
"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr">=
<div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sys=
tems Architect and Technology Developer</div><div>Lawrence Berkeley Nationa=
l Laboratory HPCS<br>University of California Berkeley Research IT<br>Singu=
larity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targe=
t=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster Ma=
nagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http:=
//warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.c=
om/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<spa=
n style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitte=
r.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twitt=
er.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div><=
/div></div>
</div>

--94eb2c188a4a8874620549343047--
