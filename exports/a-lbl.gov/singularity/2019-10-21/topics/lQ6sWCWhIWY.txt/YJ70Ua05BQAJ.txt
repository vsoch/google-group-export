X-Received: by 10.13.210.130 with SMTP id u124mr64296ywd.112.1499784961194;
        Tue, 11 Jul 2017 07:56:01 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.35.76 with SMTP id j73ls1612740ioj.2.gmail; Tue, 11 Jul
 2017 07:56:00 -0700 (PDT)
X-Received: by 10.99.147.19 with SMTP id b19mr327734pge.154.1499784960000;
        Tue, 11 Jul 2017 07:56:00 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1499784959; cv=none;
        d=google.com; s=arc-20160816;
        b=lH9B0LDKbu+dMxl76eLh5v8SeWgO6+QaDIltIkAFS8angBbOu9z97lP6Ux/UFAvVmw
         QgLwA6xP/y76B/JTRcNVoK1h8s3MGUpr6zyhljVYg7JHLWsF1TYbgZshnAeigHLubnOd
         KmZO4NzfKf6tMsir4reXkW3BZNp5Bhkh+1TVgeDRKiOvzVeQQuSWknItuVUe2YV5DniY
         K3V2GHpdTNBhElEJoHqGfnVTJ2ExIKDDTWRqZ4fPMmi71lJUCLoCDHLmkDYpwGJV6jaK
         uPreBfmY0shuBvNbJBBD6J182uNLDogsVgG8cIdh8n2sM5YfK6H+D+sfbRZaaWhF0IUC
         edeg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:references:in-reply-to
         :mime-version:dkim-signature:arc-authentication-results;
        bh=nI318jK98pKDNx+Rl+Lh+cGYhC32jdFG9KsXpLZRYYg=;
        b=ziRjH50n64SyZd1pJZk6AwgYOyJk4xUmVMgNrzBl6wiTSAyOdHXsHVzB0zI//vmhwo
         MObZquJGML5chx4BKaPpjnEzGRxs70SjYPJ9T5mVkTZ+0VE2+iG3wg8gZ5SaCht6WpoD
         13jWLAUgDi2JQX3sfwceY/CrElpKRhkBEq92yCtf8wEa4aGfxIvHXGZmxqwqcsHzFNrj
         GESueV21kJtH+hzrQitk777p8s1PF2y7zUCB2NkfM98Ly+IUBgmCNt85OFIH5omWF/MK
         0s5UDNs9aNtAXKQzCVN9RtAO+RGdZ5Hj5D3yZ9TH7xCVO4YD7TYHplyqxKMYKJvfCyeh
         NmRg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.b=T4LF7t+s;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.214.52 as permitted sender) smtp.mailfrom=gmku...@gmail.com
Return-Path: <gmku...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id o32si119603pld.154.2017.07.11.07.55.59
        for <singu...@lbl.gov>;
        Tue, 11 Jul 2017 07:55:59 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@gmail.com designates 209.85.214.52 as permitted sender) client-ip=209.85.214.52;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.b=T4LF7t+s;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.214.52 as permitted sender) smtp.mailfrom=gmku...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2FgAABa5mRZfzTWVdFdHAEBBAEBCgEBF?=
 =?us-ascii?q?wEBBAEBCgEBgkQ+AoEPgRQHg2GBPohjkW6CbJMXgU4bJQMhAQyCDYFcgRBPAoM?=
 =?us-ascii?q?2Bz8YAQEBAQEBAQEBAQECEAEBCQsLCCYxgjMFAgMaAQUEBEYmAwMBAQEBAQEBA?=
 =?us-ascii?q?QEjAQEBAQEBAQEBAQEBAQEBGgIIBR4SARIBARgBAQEBAgEaCR0BDQ4SCwEDAQs?=
 =?us-ascii?q?GAwILCgMgCgICIQEBDgMBBQEcDgcEARwCAog8gTkBAw0IBQuOEJEaP4wKggQFA?=
 =?us-ascii?q?RyDBgWDWgoZJw1WgyIBAQEBAQEBAQEBAQEBAQEBAQEBAQEVAgYSgxaCU4JbgyS?=
 =?us-ascii?q?CV4FkEgFJgmaCYQWHIoo7hV6HLjsCh0aDRYNGS4RuggxXj0KJO4JCiAIUH4EVD?=
 =?us-ascii?q?xCBBTMLdV4ahEAqDxyCByA2AQEBAQSFdkeBaQEBAQ?=
X-IronPort-AV: E=Sophos;i="5.40,347,1496127600"; 
   d="scan'208,217";a="81751223"
Received: from mail-it0-f52.google.com ([209.85.214.52])
  by fe3.lbl.gov with ESMTP; 11 Jul 2017 07:55:57 -0700
Received: by mail-it0-f52.google.com with SMTP id m84so23317900ita.0
        for <singu...@lbl.gov>; Tue, 11 Jul 2017 07:55:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc;
        bh=nI318jK98pKDNx+Rl+Lh+cGYhC32jdFG9KsXpLZRYYg=;
        b=T4LF7t+sln2vzYpLiCgTe19uRFZ3UgtjCd+Z/HJIrT4+WsW7QWOFv08qICCnOYdrUS
         bUPxnzsdSNSPn/UQRztEgjWTQaWqgoMyz3L8R8bpBtoZuWMV30RIdDdO5bYXCaq71C8K
         hfc1LjHcF7TdZQfYKGlmaq3wiilT2EywvPHi7lr2jeuRb4ZNMpUkIPE4sdMMtf6ug7df
         2jMZ+ZOJK8OT0i4vcvXtaVpbYcTBY1Y9p/Tq/rqPnnnVK1Cqc7Axd6mByoqUMtuvSEhW
         EDDDrQcs8WTZ07zXrI8s6FTct7Uyy/KRLuhWEQh0dO1GeebwOee4kHiXaMroG+RHg6u4
         Lbow==
X-Gm-Message-State: AIVw112zL3jRR69yPtkEFLvdUvZh0ADbjuCitxhs2U0iMInz1ERjBhvo
	J15hiAD0efzbkHWqYof5fWc1Ym3lS0wn
X-Received: by 10.107.36.136 with SMTP id k130mr336831iok.7.1499784956329;
 Tue, 11 Jul 2017 07:55:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.23.132 with HTTP; Tue, 11 Jul 2017 07:55:55 -0700 (PDT)
In-Reply-To: <CA+Wz_Fw=cXLngnXwv=m1DnK6-cYN+496MeHhuHyu9YmsV=CG0g@mail.gmail.com>
References: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
 <CAApQTTgQwZ3pu7Xa7RsbuHM7ndP05Kv5ciEBk_q5G-8P3xaUGw@mail.gmail.com>
 <CAApQTTg2vQY9wJg0g_Qp5n+558jYqb_ZX8-iezzwJf-6nM63QQ@mail.gmail.com> <CA+Wz_Fw=cXLngnXwv=m1DnK6-cYN+496MeHhuHyu9YmsV=CG0g@mail.gmail.com>
From: "Gregory M. Kurtzer" <gmku...@gmail.com>
Date: Tue, 11 Jul 2017 07:55:55 -0700
Message-ID: <CAApQTTj4FsbybqNWFxj=JCvNN2i8m0+AZvTNPHd25v3GV3FYSA@mail.gmail.com>
Subject: Re: [Singularity] OpenMPI, Slurm & portability
To: singularity@lbl.gov
Cc: Ralph Castain <rcas...@gmail.com>
Content-Type: multipart/alternative; boundary="001a1140e3308ffbb705540be599"

--001a1140e3308ffbb705540be599
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Victor,

I will let Ralph comment on the OMPI versions and compatibilities, but
regarding using the MPI host libraries within a container is dangerous for
the reason that you are mentioning. If you are running ABI compatible
containers with the host, then things *might* work as expected. But this
breaks container portability, and goes against the principals of
containment.

We do however do exactly this for the Nvidia driver libraries, but...
Nvidia builds these libraries with careful attention on ABI compatibility
such that these binary libraries are indeed reasonably portable across
containers.

The only way to do this portably is with using a launcher on the host,
outside the container, to spin up the container and launch the MPI within.
PMIx is a fantastic approach to solving this.

Hope that helps!

Greg



On Tue, Jul 11, 2017 at 6:03 AM, victor sv <vict...@gmail.com> wrote:

> Hi Greg and Ralph,
>
> Thank you for your precise and elaborated answers.
>
> Only for confirmation and to sum up some conclussions (if I understood
> correctly):
>
>  - OpenMPI process management compatibility depends on PMIx.
>  - OpenMPI (and also Slurm) complete  backward/forward compatibility will
> come (hopefully) in the future by means of PMIx 2.1.
>  - Nowadays, there exists compatibility with OpenMPI 2.X if we compile it
> with default PMIx (1.X) support.
>  - OpenMPI 2.1 must be compiled with --disable-pmix-dstore due to a
> compatibility break.
>  - OpenMPI 1.X does not suppot PMIx and we can ignore it from this thread=
.
>
> I'm right?
>
> I'm interested in performing the tests you purpose. I will try to build
> all three OMPI versions (2.0, 2.1 and 3.0) against the same PMIx external
> library to check the compatibility. Which PMIx version (1.2.0, 1.2.1 or
> 1.2.2 ) do you recommend as a start point?
>
> I will report this results ASAP to this thread.
>
> On the other hand, although we are planning to add support to PMIx,
> unfortunately, our Slurm version (14.11.10-Bull.1.0) does not support it
> yet.
>
> The second strategy we are testing to get compatibility between OpenMPI
> inside and outside a Singularity container relies on replacing the OpenMP=
I
> libraries inside the container by the host libraries hierarchy.
>
> This approach rest upon the assumption that OpenMPI symbols and data
> structures are compatible through several versions of OpenMPI. At least
> combining several releases that share the same major version.
>
> Although the empirical tests of this approach seem to work properly with
> some tests, benchmarks and real apps, I'm afraid of getting unexepected
> errors/warnings (segfaults, data errors, etc.) in the future.
>
> What do you think about this approach?
>
> Can you confirm that OpenMPI is compatible in this way?
>
> Finally, I think this thread could be very interesting for other users to=
o
> and I would like to keep it alive with your help.
>
> Thank you again for your support!
>
> BR,
> V=C3=ADctor
>
> 2017-07-09 23:45 GMT+02:00 Gregory M. Kurtzer <gmku...@gmail.com>:
>
>> Hiya Victor, et al.,
>>
>> I didn't realize this but Ralph had to drop off of the Singularity list.
>> Hopefully we will get him back again, as he is a fantastic resource for =
all
>> of the OMPI questions and always a great source of information and ideas
>> (poke, poke Ralph!). Ralph did send me this in response to the previous
>> email hoping it helps to explain things:
>>
>>
>> On Sun, Jul 9, 2017 at 2:22 PM, r...@open-mpi.org <r...@open-mpi.org>
>>  wrote:
>>
>>> ...
>> You are welcome to forward the following to the list:
>>
>> As Greg said, we have been concerned about this since we started looking
>> at Singularity support. Just for clarity, the version of PMI OMPI uses i=
s
>> PMIx (https://pmix.github.io/pmix/). While our plan from the beginning
>> was to support cross-versions specifically to address this problem, we f=
ell
>> behind on its implementation due to priorities. We just committed the co=
de
>> to the PMIx repo in the last week, and it won=E2=80=99t be released into=
 production
>> for a few months while we shake it down.
>>
>> I fear it will be impossible to get the OMPI 1.10 series to work with
>> anything other than itself as it pre-dates PMIx.
>>
>> The OMPI 2.0 and 2.1 series should work across each other as they both
>> include PMIx 1.x. However, you probably will need to configure the 2.1
>> series with --disable-pmix-dstore as there was an unintended compatibili=
ty
>> break there (the shared memory store was added during the PMIx 1.x serie=
s
>> and we didn=E2=80=99t catch the compatibility break it introduced).
>>
>> Looking into the future, OMPI 3.0 is about to be released. It includes
>> PMIx 2.0, which isn=E2=80=99t backwards compatible at this time, and so =
it won=E2=80=99t
>> cross-version with OMPI 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We haven=
=E2=80=99t tested this, but
>> one thing you could try is to build all three OMPI versions against the
>> same PMIx external library (you would probably have to experiment a bit
>> with PMIx versions to see which works across the different OMPI versions=
 as
>> the glue between the two also changed a bit). This will ensure that the
>> shared memory store in PMIx is compatible across the versions, and thing=
s
>> should work since OMPI doesn=E2=80=99t care how the data is moved across=
 the
>> host-container boundary.
>>
>> As I said, we will be adding cross-version support to the PMIx release
>> series soon, without changing the API, that will ensure support across a=
ll
>> PMIx versions starting with v1.2. Thus, you could (once that happens) bu=
ild
>> OMPI 2.0, 2.1, and 3.0 against the new PMIx release (probably PMIx v2.1.=
0)
>> and the resulting containers would be future-proof as OMPI moves ahead. =
The
>> RMs plan to follow that path as well, so you should be in good shape onc=
e
>> this is done if you prefer to =E2=80=9Cdirect launch=E2=80=9D your conta=
iners (e.g., =E2=80=9Csrun
>> ./mycontainer=E2=80=9D under SLURM).
>>
>> Sorry if that is all confusing - we sometimes get lost in the numbering
>> schemes between OMPI and PMIx ourselves. Feel free to contact me directl=
y,
>> or on the OMPI or PMIx mailing lists, if you have more questions or
>> encounter problems. We definitely want to make this work.
>>
>> Ralph
>>
>> On Sun, Jul 9, 2017 at 12:19 PM, Gregory M. Kurtzer <gmku...@gmail.com>
>> wrote:
>>
>>> Hi Victor,
>>>
>>> Sorry for the latency, I'm on email overload.
>>>
>>> Open MPI uses PMI to communicate both inside and outside of the
>>> container. Ralph Castain (on this list, but possibly not monitoring
>>> actively) is leading the PMI effort and he is an active Open MPI develo=
per.
>>> We have had several talks about how to achieve "hetero-versionistic"
>>> compatibility through the PMI handshake. I was under the impression tha=
t
>>> PMI now supports that, as long as you are running equal or newer versio=
n on
>>> the host (outside the container). Also, I don't know what version of PM=
I
>>> this feature was introduced in, nor do I know what version of Open MPI
>>> includes that compatibility.
>>>
>>> I have CC'ed Ralph, and hopefully he will be able to offer some
>>> suggestions.
>>>
>>> Regarding your question about supporting the MPI libraries in the same
>>> manner that we are doing the Nvidia libraries, that would be hard. Nvid=
ia
>>> specifically builds their libraries to be as generally compatible as
>>> possible (e.g. the same libraries/binaries work on a large array of Lin=
ux
>>> distributions). Most people do not build host libraries in a manner tha=
t
>>> would be generally compatible as Nvidia does.
>>>
>>> Hope that helps!
>>>
>>> Greg
>>>
>>>
>>>
>>> On Mon, Jul 3, 2017 at 2:07 AM, victor sv <vict...@gmail.com> wrote:
>>>
>>>> Dear Singularity team,
>>>>
>>>> first of all, thanks for the great work with Singularity. It looks
>>>> amazing!
>>>>
>>>> Sorry if this topic is duplicated and for the length of the email, but
>>>> I want to share my experience about Singularity and OpenMPI compatibil=
ity,
>>>> and also ask some questions.
>>>>
>>>> I've being reading a lot about OpenMPI and Singularity compatibility
>>>> because we are trying to find the generic way to run OpenMPI applicati=
ons
>>>> within Singularity containers. It was not so clear (for me) in the
>>>> documentation, forums and mailing lists, and this is why we've perform=
ed an
>>>> OpenMPI empiric compatibility study.
>>>>
>>>> We ran these comparisons in CESGA FinisTerrae II cluster (
>>>> https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2).
>>>>
>>>> We used several versions of OpenMPI. The chosen versions of OpenMPI
>>>> were the versions already installed in the cluster:
>>>>
>>>> - openmpi/1.10.2
>>>> - openmpi/2.0.0
>>>> - openmpi/2.0.1
>>>> - openmpi/2.0.2
>>>> - openmpi/2.1.1
>>>>
>>>> We have created Singularity images containing the same versions of
>>>> OpenMPI and with the basic OpenMPI ring example. I share the bootstrap
>>>> definition file template used below:
>>>>
>>>> ```
>>>> BootStrap: docker
>>>> From: ubuntu:16.04
>>>> IncludeCmd: yes
>>>>
>>>> %post
>>>>         sed -i 's/main/main restricted universe/g' /etc/apt/sources.li=
st
>>>>         apt-get update
>>>>         apt-get install -y bash git wget build-essential gcc time
>>>> libc6-dev libgcc-5-dev
>>>>         apt-get install -y dapl2-utils libdapl-dev libdapl2 libibverbs=
1
>>>> librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 lib=
nes1
>>>> libpmi0 libpmi0-dev libslurm29 libslurm-dev
>>>>
>>>>         ##Install OpenMPI
>>>>         cd /tmp
>>>>         wget 'https://www.open-mpi.org/soft
>>>> ware/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz' -O openmpi-X.X.X.tar.gz
>>>>         tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X
>>>>         mkdir -p /tmp/openmpi-X.X.X/build
>>>>         cd /tmp/openmpi-X.X.X/build
>>>>          ../configure --enable-shared --enable-mpi-thread-multiple
>>>> --with-verbs --enable-mpirun-prefix-by-default --with-hwloc
>>>> --disable-dlopen --with-pmi --prefix=3D/usr
>>>>         make all install
>>>>
>>>>         # Install ring
>>>>         cd /tmp
>>>>         wget https://raw.githubusercontent.
>>>> com/open-mpi/ompi/master/examples/ring_c.c
>>>>         mpicc ring_c.c -o /usr/bin/ring
>>>> ```
>>>>
>>>> Once the containers were created, we ran the ring app with mpirun usin=
g
>>>> 2 cores of 2 different nodes mixing all possible combinations of those
>>>> OpenMPI versions inside and outside the container.
>>>>
>>>> The obtained results shown that we need the same versions of OpenMPI
>>>> inside and outside the container to succesfully run the contained
>>>> application in parallel with mpirun.
>>>>
>>>> Is this the expected behaviour or am I missing something?
>>>>
>>>> Will be this the expected behaviour in the future (with future version=
s
>>>> of OpenMPI)?
>>>>
>>>> Currently, we have slurm 14.11.10-Bull.1.0 installed as job scheduler
>>>> at FinisTerrae II. We found the following tip/trick to use srun as pro=
cess
>>>> manager:
>>>>
>>>> http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls
>>>>
>>>> In order to run whatever Singularity image containing OpenMPI
>>>> applications using Slurm, we've adapted it to our infrastructure and
>>>> checked the same test cases running them with srun. It seems that it's
>>>> working properly (no real world applications were tested yet).
>>>>
>>>> What do you think about this strategy?
>>>> Can you confirm that it provides portability of singularity images
>>>> containing OpenMPI applications?
>>>>
>>>> I think this strategy is similar to the one you are following with
>>>> "--nv" option  for NVidia drivers.
>>>>
>>>> Why not to do the same strategy with MPI, PMI, libibverbs, etc.?
>>>>
>>>> Thanks in advance and congrats again for your great work!
>>>>
>>>> V=C3=ADctor.
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> CEO, SingularityWare, LLC.
>>> Senior Architect, RStor
>>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> CEO, SingularityWare, LLC.
>> Senior Architect, RStor
>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
Gregory M. Kurtzer
CEO, SingularityWare, LLC.
Senior Architect, RStor
Computational Science Advisor, Lawrence Berkeley National Laboratory

--001a1140e3308ffbb705540be599
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Victor,<div><br></div><div>I will let Ralph comment on =
the OMPI versions and compatibilities, but regarding using the MPI host lib=
raries within a container is dangerous for the reason that you are mentioni=
ng. If you are running ABI compatible containers with the host, then things=
 *might* work as expected. But this breaks container portability, and goes =
against the principals of containment.</div><div><br></div><div>We do howev=
er do exactly this for the Nvidia driver libraries, but... Nvidia builds th=
ese libraries with careful attention on ABI compatibility such that these b=
inary libraries are indeed reasonably portable across containers.</div><div=
><br></div><div>The only way to do this portably is with using a launcher o=
n the host, outside the container, to spin up the container and launch the =
MPI within. PMIx is a fantastic approach to solving this.</div><div><br></d=
iv><div>Hope that helps!</div><div><br></div><div>Greg</div><div><br></div>=
<div><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, =
Jul 11, 2017 at 6:03 AM, victor sv <span dir=3D"ltr">&lt;<a href=3D"mailto:=
vict...@gmail.com" target=3D"_blank">vict...@gmail.com</a>&gt;</span> wrote=
:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Greg and Ralph,<br>=
<br>Thank you for your precise and elaborated answers.<br><br>Only for conf=
irmation and to sum up some conclussions (if I understood correctly):<br><b=
r>=C2=A0- OpenMPI process management compatibility depends on PMIx.<br>=C2=
=A0- OpenMPI (and also Slurm) complete=C2=A0 backward/forward compatibility=
 will come (hopefully) in the future by means of PMIx 2.1.<br>=C2=A0- Nowad=
ays, there exists compatibility with OpenMPI 2.X if we compile it with defa=
ult PMIx (1.X) support.<br>=C2=A0- OpenMPI 2.1 must be compiled with --disa=
ble-pmix-dstore due to a compatibility break.<br>=C2=A0- OpenMPI 1.X does n=
ot suppot PMIx and we can ignore it from this thread.<br><br>I&#39;m right?=
<br><br>I&#39;m interested in performing the tests you purpose. I will try =
to build all three OMPI versions (2.0, 2.1 and 3.0) against the same PMIx e=
xternal library to check the compatibility. Which PMIx version (1.2.0, 1.2.=
1 or 1.2.2 ) do you recommend as a start point? <br><br>I will report this =
results ASAP to this thread.<br><br>On the other hand, although we are plan=
ning to add support to PMIx, unfortunately, our Slurm version (14.11.10-Bul=
l.1.0) does not support it yet.<br><br>The second strategy we are testing t=
o get compatibility between OpenMPI inside and outside a Singularity contai=
ner relies on replacing the OpenMPI libraries inside the container by the h=
ost libraries hierarchy.<br><br>This approach rest upon the assumption that=
 OpenMPI symbols and data structures are compatible through several version=
s of OpenMPI. At least combining several releases that share the same major=
 version.<br><br>Although the empirical tests of this approach seem to work=
 properly with some tests, benchmarks and real apps, I&#39;m afraid of gett=
ing unexepected errors/warnings (segfaults, data errors, etc.) in the futur=
e.<br><br>What do you think about this approach?<br><br>Can you confirm tha=
t OpenMPI is compatible in this way?<br><br>Finally, I think this thread co=
uld be very interesting for other users too and I would like to keep it ali=
ve with your help.<br><br>Thank you again for your support!<br><br>BR,<br>V=
=C3=ADctor<br></div><div class=3D"HOEnZb"><div class=3D"h5"><div class=3D"g=
mail_extra"><br><div class=3D"gmail_quote">2017-07-09 23:45 GMT+02:00 Grego=
ry M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailto:gmku...@gmail.com" ta=
rget=3D"_blank">gmku...@gmail.com</a>&gt;</span>:<br><blockquote class=3D"g=
mail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-l=
eft:1ex"><div dir=3D"ltr">Hiya Victor, et al.,=C2=A0<div><br></div><div>I d=
idn&#39;t realize this but Ralph had to drop off of the Singularity list. H=
opefully we will get him back again, as he is a fantastic resource for all =
of the OMPI questions and always a great source of information and ideas (p=
oke, poke Ralph!). Ralph did send me this in response to the previous email=
 hoping it helps to explain things:</div><div><br></div><div><br></div><div=
>On Sun, Jul 9, 2017 at 2:22 PM, <a href=3D"mailto:r...@open-mpi.org" targe=
t=3D"_blank">r...@open-mpi.org</a>=C2=A0<span dir=3D"ltr">&lt;<a href=3D"ma=
ilto:r...@open-mpi.org" target=3D"_blank">rhc@open-mpi<wbr>.org</a>&gt;</sp=
an>=C2=A0wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0p=
x 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"></bloc=
kquote></div><div><div style=3D"font-size:12.8px">...</div><div style=3D"fo=
nt-size:12.8px">You are welcome to forward the following to the list:</div>=
<div style=3D"font-size:12.8px"><br></div><div style=3D"font-size:12.8px">A=
s Greg said, we have been concerned about this since we started looking at =
Singularity support. Just for clarity, the version of PMI OMPI uses is PMIx=
 (<a href=3D"https://pmix.github.io/pmix/" target=3D"_blank">https://pmix.g=
ithub.io/pmix/</a>)<wbr>. While our plan from the beginning was to support =
cross-versions specifically to address this problem, we fell behind on its =
implementation due to priorities. We just committed the code to the PMIx re=
po in the last week, and it won=E2=80=99t be released into production for a=
 few months while we shake it down.</div><div style=3D"font-size:12.8px"><b=
r></div><div style=3D"font-size:12.8px">I fear it will be impossible to get=
 the OMPI 1.10 series to work with anything other than itself as it pre-dat=
es PMIx.</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-=
size:12.8px">The OMPI 2.0 and 2.1 series should work across each other as t=
hey both include PMIx 1.x. However, you probably will need to configure the=
 2.1 series with --disable-pmix-dstore as there was an unintended compatibi=
lity break there (the shared memory store was added during the PMIx 1.x ser=
ies and we didn=E2=80=99t catch the compatibility break it introduced).</di=
v><div style=3D"font-size:12.8px"><br></div><div style=3D"font-size:12.8px"=
>Looking into the future, OMPI 3.0 is about to be released. It includes PMI=
x 2.0, which isn=E2=80=99t backwards compatible at this time, and so it won=
=E2=80=99t cross-version with OMPI 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We=
 haven=E2=80=99t tested this, but one thing you could try is to build all t=
hree OMPI versions against the same PMIx external library (you would probab=
ly have to experiment a bit with PMIx versions to see which works across th=
e different OMPI versions as the glue between the two also changed a bit). =
This will ensure that the shared memory store in PMIx is compatible across =
the versions, and things should work since OMPI doesn=E2=80=99t care how th=
e data is moved across the host-container boundary.</div><div style=3D"font=
-size:12.8px"><br></div><div style=3D"font-size:12.8px">As I said, we will =
be adding cross-version support to the PMIx release series soon, without ch=
anging the API, that will ensure support across all PMIx versions starting =
with v1.2. Thus, you could (once that happens) build OMPI 2.0, 2.1, and 3.0=
 against the new PMIx release (probably PMIx v2.1.0) and the resulting cont=
ainers would be future-proof as OMPI moves ahead. The RMs plan to follow th=
at path as well, so you should be in good shape once this is done if you pr=
efer to =E2=80=9Cdirect launch=E2=80=9D your containers (e.g., =E2=80=9Csru=
n ./mycontainer=E2=80=9D under SLURM).</div><div style=3D"font-size:12.8px"=
><br></div><div style=3D"font-size:12.8px">Sorry if that is all confusing -=
 we sometimes get lost in the numbering schemes between OMPI and PMIx ourse=
lves. Feel free to contact me directly, or on the OMPI or PMIx mailing list=
s, if you have more questions or encounter problems. We definitely want to =
make this work.</div><div style=3D"font-size:12.8px"><br></div><div style=
=3D"font-size:12.8px">Ralph</div></div></div><div class=3D"m_-1840755824492=
384681HOEnZb"><div class=3D"m_-1840755824492384681h5"><div class=3D"gmail_e=
xtra"><br><div class=3D"gmail_quote">On Sun, Jul 9, 2017 at 12:19 PM, Grego=
ry M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailto:gmku...@gmail.com" ta=
rget=3D"_blank">gmku...@gmail.com</a>&gt;</span> wrote:<br><blockquote clas=
s=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pad=
ding-left:1ex"><div dir=3D"ltr">Hi Victor,<div><br></div><div>Sorry for the=
 latency, I&#39;m on email overload.</div><div><br></div><div>Open MPI uses=
 PMI to communicate both inside and outside of the container. Ralph Castain=
 (on this list, but possibly not monitoring actively) is leading the PMI ef=
fort and he is an active Open MPI developer. We have had several talks abou=
t how to achieve &quot;hetero-versionistic&quot; compatibility through the =
PMI handshake. I was under the impression that PMI now supports that, as lo=
ng as you are running equal or newer version on the host (outside the conta=
iner). Also, I don&#39;t know what version of PMI this feature was introduc=
ed in, nor do I know what version of Open MPI includes that compatibility.<=
/div><div><br></div><div>I have CC&#39;ed Ralph, and hopefully he will be a=
ble to offer some suggestions.</div><div><br></div><div>Regarding your ques=
tion about supporting the MPI libraries in the same manner that we are doin=
g the Nvidia libraries, that would be hard. Nvidia specifically builds thei=
r libraries to be as generally compatible as possible (e.g. the same librar=
ies/binaries work on a large array of Linux distributions). Most people do =
not build host libraries in a manner that would be generally compatible as =
Nvidia does.</div><div><br></div><div>Hope that helps!</div><div><br></div>=
<div>Greg</div><div><br></div><div><br></div></div><div class=3D"gmail_extr=
a"><div><div class=3D"m_-1840755824492384681m_6215373137033584522h5"><br><d=
iv class=3D"gmail_quote">On Mon, Jul 3, 2017 at 2:07 AM, victor sv <span di=
r=3D"ltr">&lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict..=
.@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" styl=
e=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div di=
r=3D"ltr">Dear Singularity team,<br><br>first of all, thanks for the great =
work with Singularity. It looks amazing!<br><br>Sorry if this topic is dupl=
icated and for the length of the email, but I want to share my experience a=
bout Singularity and OpenMPI compatibility, and also ask some questions.<br=
><br>I&#39;ve being reading a lot about OpenMPI and Singularity compatibili=
ty because we are trying to find the generic way to run OpenMPI application=
s within Singularity containers. It was not so clear (for me) in the docume=
ntation, forums and mailing lists, and this is why we&#39;ve performed an O=
penMPI empiric compatibility study.<br><br>We ran these comparisons in CESG=
A FinisTerrae II cluster (<a href=3D"https://www.cesga.es/en/infraestructur=
as/computacion/FinisTerrae2" target=3D"_blank">https://www.cesga.es/en/infr=
a<wbr>estructuras/computacion/FinisT<wbr>errae2</a>).<br><br>We used severa=
l versions of OpenMPI. The chosen versions of OpenMPI were the versions alr=
eady installed in the cluster:<br><br>- openmpi/1.10.2<br>- openmpi/2.0.0<b=
r>- openmpi/2.0.1<br>- openmpi/2.0.2<br>- openmpi/2.1.1<br><br>We have crea=
ted Singularity images containing the same versions of OpenMPI and with the=
 basic OpenMPI ring example. I share the bootstrap definition file template=
 used below:<br><br>```<br>BootStrap: docker<br>From: ubuntu:16.04<br>Inclu=
deCmd: yes<br><br>%post<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sed -=
i &#39;s/main/main restricted universe/g&#39; /etc/apt/sources.list<br>=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get update<br>=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get install -y bash git wget build-essentia=
l gcc time libc6-dev libgcc-5-dev<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 apt-get install -y dapl2-utils libdapl-dev libdapl2 libibverbs1 libr=
dmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 libnes1 libp=
mi0 libpmi0-dev libslurm29 libslurm-dev<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 ##Install OpenMPI<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget &#39;<a h=
ref=3D"https://www.open-mpi.org/software/ompi/vX.X/downloads/openmpi-X.X.X.=
tar.gz" target=3D"_blank">https://www.open-mpi.org/soft<wbr>ware/ompi/vX.X/=
downloads/openm<wbr>pi-X.X.X.tar.gz</a>&#39; -O openmpi-X.X.X.tar.gz<br>=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 tar -xzf openmpi-X.X.X.tar.gz -C op=
enmpi-X.X.X<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mkdir -p /tmp/ope=
nmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp/open=
mpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 ../conf=
igure --enable-shared --enable-mpi-thread-multiple --with-verbs --enable-mp=
irun-prefix-by-defa<wbr>ult --with-hwloc --disable-dlopen --with-pmi --pref=
ix=3D/usr<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 make all install<br=
><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 # Install ring<br>=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 wget <a href=3D"https://raw.githubusercontent.com/open-mpi/omp=
i/master/examples/ring_c.c" target=3D"_blank">https://raw.githubusercontent=
.<wbr>com/open-mpi/ompi/master/examp<wbr>les/ring_c.c</a><br>=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mpicc ring_c.c -o /usr/bin/ring<br>```<br><b=
r>Once the containers were created, we ran the ring app with mpirun using 2=
 cores of 2 different nodes mixing all possible combinations of those OpenM=
PI versions inside and outside the container.<br><br>The obtained results s=
hown that we need the same versions of OpenMPI inside and outside the conta=
iner to succesfully run the contained application in parallel with mpirun.<=
br><br>Is this the expected behaviour or am I missing something?<br><br>Wil=
l be this the expected behaviour in the future (with future versions of Ope=
nMPI)?<br><br>Currently, we have slurm 14.11.10-Bull.1.0 installed as job s=
cheduler at FinisTerrae II. We found the following tip/trick to use srun as=
 process manager:<br><br><a href=3D"http://singularity.lbl.gov/tutorial-gpu=
-drivers-open-mpi-mtls" target=3D"_blank">http://singularity.lbl.gov/tut<wb=
r>orial-gpu-drivers-open-mpi-mtl<wbr>s</a><br><br>In order to run whatever =
Singularity image containing OpenMPI applications using Slurm, we&#39;ve ad=
apted it to our infrastructure and checked the same test cases running them=
 with srun. It seems that it&#39;s working properly (no real world applicat=
ions were tested yet).<br><br>What do you think about this strategy?<br>Can=
 you confirm that it provides portability of singularity images containing =
OpenMPI applications?<br><br>I think this strategy is similar to the one yo=
u are following with &quot;--nv&quot; option=C2=A0 for NVidia drivers.<br><=
br>Why not to do the same strategy with MPI, PMI, libibverbs, etc.?<br><br>=
Thanks in advance and congrats again for your great work!<br><br>V=C3=ADcto=
r.<span class=3D"m_-1840755824492384681m_6215373137033584522m_8312868002830=
329868HOEnZb"><font color=3D"#888888"><br></font></span></div><span class=
=3D"m_-1840755824492384681m_6215373137033584522m_8312868002830329868HOEnZb"=
><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div></div=
></div><span class=3D"m_-1840755824492384681m_6215373137033584522HOEnZb"><f=
ont color=3D"#888888">-- <br><div class=3D"m_-1840755824492384681m_62153731=
37033584522m_8312868002830329868gmail_signature" data-smartmail=3D"gmail_si=
gnature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>=
Gregory M. Kurtzer</div><div>CEO, SingularityWare, LLC.</div><div>Senior Ar=
chitect, RStor</div><div><span style=3D"font-size:12.8px">Computational Sci=
ence Advisor, Lawrence Berkeley National Laboratory</span><br></div></div><=
/div></div></div></div></div>
</font></span></div>
</blockquote></div><br><br clear=3D"all"><div><br></div>-- <br><div class=
=3D"m_-1840755824492384681m_6215373137033584522gmail_signature" data-smartm=
ail=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div di=
r=3D"ltr"><div>Gregory M. Kurtzer</div><div>CEO, SingularityWare, LLC.</div=
><div>Senior Architect, RStor</div><div><span style=3D"font-size:12.8px">Co=
mputational Science Advisor, Lawrence Berkeley National Laboratory</span><b=
r></div></div></div></div></div></div></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtze=
r</div><div>CEO, SingularityWare, LLC.</div><div>Senior Architect, RStor</d=
iv><div><span style=3D"font-size:12.8px">Computational Science Advisor, Law=
rence Berkeley National Laboratory</span><br></div></div></div></div></div>=
</div></div>
</div></div></div>

--001a1140e3308ffbb705540be599--
