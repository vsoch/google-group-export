X-Received: by 10.200.49.99 with SMTP id h32mr3098685qtb.71.1488404453538;
        Wed, 01 Mar 2017 13:40:53 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.3.1 with SMTP id e1ls1670070ite.16.gmail; Wed, 01 Mar 2017
 13:40:52 -0800 (PST)
X-Received: by 10.98.87.27 with SMTP id l27mr11274120pfb.169.1488404452684;
        Wed, 01 Mar 2017 13:40:52 -0800 (PST)
Return-Path: <chris...@monash.edu>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id d72si5650523pfb.158.2017.03.01.13.40.52
        for <singu...@lbl.gov>;
        Wed, 01 Mar 2017 13:40:52 -0800 (PST)
Received-SPF: pass (google.com: domain of chris...@monash.edu designates 209.85.223.177 as permitted sender) client-ip=209.85.223.177;
Authentication-Results: mx.google.com;
       dkim=pass (test mode) head...@monash.edu;
       spf=pass (google.com: domain of chris...@monash.edu designates 209.85.223.177 as permitted sender) smtp.mailfrom=chris...@monash.edu
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2GDAACNPrdYhrHfVdFeHAEBBAEBCgEBGAEFAQsBhAaBCQeDTAiKCJFlkAmFLIFKQwEeAQyCQIM2AoIyBz8YAQEBAQEBAQEBAQECEAEBAQgLCwodL4IzBAIDGQUEBD0NLgEBAQEBAQEBAQEBAQEBAQEaAg0GVwEBAQMBIx0BAQwsBAsLCw0qAgIiDwMBBQEcDgcEARwEiVAIBQmlKz+LG2iCJiuCXQEBBYR2AQeDBgEBAQcBAQEBAQEBGQgShjqEb4J2gTARAUmCHzqCX4kjjFWGNYIChHOLPYJOgQmNR5FuFB6BFR+BAjAIIRQhaAWDeioPEQyBdSw1B4c9R4FnAQEB
X-IronPort-AV: E=Sophos;i="5.35,227,1484035200"; 
   d="scan'208,217";a="66275018"
Received: from mail-io0-f177.google.com ([209.85.223.177])
  by fe3.lbl.gov with ESMTP; 01 Mar 2017 13:40:51 -0800
Received: by mail-io0-f177.google.com with SMTP id 90so40809895ios.1
        for <singu...@lbl.gov>; Wed, 01 Mar 2017 13:40:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=monash.edu; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=q8U4INMW4UL8IuVO+1GntrhDtmMcTATXk+/M9rMXY4E=;
        b=kl35l3s65tL6OWO/Jo3UkhDOnofNA9XB2EgBQeezuw6v2eiOdtnmeEcZ39dYKM2g0U
         6wd0fEsFm3o3LCycijSCEBHopXfincPA+x4dsY2dWFEEBa1z5N56J0WFMb2vUQfs2nd2
         A0P30JkcOmQgN736pbEUW9pXNixghZzW1WT2v1yaIWGf/Wcn8baywDCOzNFoyBQeEG12
         XH4b8g7ILr8ZBNB/qLFTGQtJijvvgqj96pTdGd1UmXVYl/aY2w181gOl6MJU9n0VYKaM
         5ccZ1rwb+SpSce49S7vLMh+tLYJdHjbsNUk3VuWK6rKRVd6FGS4rr7iL20vvZjtrIEeS
         ck7g==
X-Gm-Message-State: AMke39mwKNLz+TXN28uGlN2HADzrMQvECw0fuzRComStBNOAKqcrxrQZy9jxEvlqwqF+fVoCy+NN5Br4OcQD+UlD
X-Received: by 10.107.41.21 with SMTP id p21mr11620886iop.13.1488404450482;
 Wed, 01 Mar 2017 13:40:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.79.26.213 with HTTP; Wed, 1 Mar 2017 13:40:49 -0800 (PST)
In-Reply-To: <101f5d04-2933-393c-d27a-7274d014404f@microway.com>
References: <54cc2731-b5b5-4e54-98a0-2c00fa5f9a78@lbl.gov> <101f5d04-2933-393c-d27a-7274d014404f@microway.com>
From: Chris Hines <chris...@monash.edu>
Date: Thu, 2 Mar 2017 08:40:49 +1100
Message-ID: <CADf5cTEQu+gML83=mJMkEitvR0Zj8hB=9PriALVj4zFaqDOWpg@mail.gmail.com>
Subject: Re: [Singularity] GPU Neural simulations in Singularity using Neurokernel
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a1141d0128dce290549b22aac

--001a1141d0128dce290549b22aac
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

We (Monash Uni) have had some success with Caffe in this mode. This is an
ubuntu 16.04 container on Centos7. I think we symlinked on CentOS7 the
relevant GPU libs to an appropriate location, bind mounted that location
into the container and added the LD_LIBRARY_PATH (if you can't tell I
didn't actually do the work). One issue we have come across is the device
"/dev/nvidia-uvm". It wasn't created by CentOS7 during boot, presumably it
would be created after the appropriate CUDA calls. Documentation suggests
that the command nvidia-modprobe should create it, but that binary is SUID
and thus doesn't work once the container is executing. So our biggest
hurdle is ensuring all nvidia devices are created before instantiating
singularity. You can do stuff with mknod to ensure the devices exist, but
that feels like a time machine back to the late 90's. devpts should take
care of that for us these days.

For shrinking, my colleague did the following.

#!/bin/bash

if [[ $EUID -ne 0 ]]; then
   echo "This script must be run as root" 1>&2
   exit 1
fi

if [[ $# -ne 1 ]]; then
   echo "Usage: ./build.sh build_definition.def"
   exit 1
fi

DEF=3D"$1"
IMG=3D"${DEF%.def}.img"
IMG_TMP=3D"$IMG.tmp"
BUILD_SIZE=3D"4096"
EXPORT_SIZE=3D"700"

singularity create -s "$BUILD_SIZE" "$IMG_TMP" &&
singularity bootstrap "$IMG_TMP" "$DEF" &&
singularity create -s "$EXPORT_SIZE" "$IMG" &&
singularity export "$IMG_TMP" | singularity import "$IMG"

rm "$IMG_TM"

with the follow up comment ```You could do something smart with the
`BUILD_SIZE` and `EXPORT_SIZE` variables, but I=E2=80=99m lazy, you=E2=80=
=99re lazy, we=E2=80=99re
all lazy.```
Its true, I am lazy, that's why I like singularity.

In terms of minimising the content of the container, moving the user space
gpu drive out of the container is a good idea (for portability), however I
don't advocate moving CUDA its self out of the container.
Depending on how neurokernel was installed, all the usual tricks for
shrinking images apply, i.e. remove the source code after compilation,
remove any system packages downloaded and cached before installation. See
if there are any packages you can do without (did an X windows environment
get installed as a dependency of anything?)

Cheers,
--
Chris.

On 2 March 2017 at 07:25, Eliot Eshelman <el...@microway.com> wrote:

> The biggest problem with portability of GPU-accelerated Singularity is th=
e
> reliance on specific versions of the NVIDIA driver. Docker was in a simil=
ar
> position a year or so ago, which is why NVIDIA created nvidia-docker. The
> nvidia-docker tool is really just startup scripts which make the host's
> NVIDIA driver files available to the container. Here's a peek into the
> NVIDIA driver files and libraries that matter:
>
> https://github.com/NVIDIA/nvidia-docker/blob/master/src/
> nvidia/volumes.go#L104
>
> I have been thinking about how to make an nvidia-singularity tool, but
> haven't written a single line of code yet. I'd be interested to hear if
> anyone else is also looking to tackle this one?
>
> Eliot
>
>
>
>
> On 03/01/2017 06:49 AM, 'Adam Tomkins' via singularity wrote:
>
> Hello,
>
>
> I'm new to Singularity, and I have built a container for the NeuroKernel
> <https://neurokernel.github.io/> neuron simulator.
>
>
> It currently works locally, assuming you have CUDA installed correctly,
> however it is quite large.
>
>
> I am looking for any feedback on how to make it more portable, or usable
> in general, with the aim of eventually moving it to our HPC clusters.
>
>
> You can find it on github here
> <https://github.com/AdamRTomkins/Neurokernel-singularity-container>
>
>
> Thanks for the great software.
>
>
> Adam
>
>
>
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>
>
> --
> Eliot Eshelman
> Microway, Inc.
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a1141d0128dce290549b22aac
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">We (Monash Uni) have had some success with Caffe in this m=
ode. This is an ubuntu 16.04 container on Centos7. I think we symlinked on =
CentOS7 the relevant GPU libs to an appropriate location, bind mounted that=
 location into the container and added the LD_LIBRARY_PATH (if you can&#39;=
t tell I didn&#39;t actually do the work). One issue we have come across is=
 the device &quot;/dev/nvidia-uvm&quot;. It wasn&#39;t created by CentOS7 d=
uring boot, presumably it would be created after the appropriate CUDA calls=
. Documentation suggests that the command nvidia-modprobe should create it,=
 but that binary is SUID and thus doesn&#39;t work once the container is ex=
ecuting. So our biggest hurdle is ensuring all nvidia devices are created b=
efore instantiating singularity. You can do stuff with mknod to ensure the =
devices exist, but that feels like a time machine back to the late 90&#39;s=
. devpts should take care of that for us these days.<div><br></div><div>For=
 shrinking, my colleague did the following.</div><div><br></div><div><div>#=
!/bin/bash</div><div><br></div><div>if [[ $EUID -ne 0 ]]; then</div><div>=
=C2=A0 =C2=A0echo &quot;This script must be run as root&quot; 1&gt;&amp;2</=
div><div>=C2=A0 =C2=A0exit 1</div><div>fi</div><div><br></div><div>if [[ $#=
 -ne 1 ]]; then</div><div>=C2=A0 =C2=A0echo &quot;Usage: ./build.sh build_d=
efinition.def&quot;</div><div>=C2=A0 =C2=A0exit 1</div><div>fi</div><div><b=
r></div><div>DEF=3D&quot;$1&quot;</div><div>IMG=3D&quot;${DEF%.def}.img&quo=
t;</div><div>IMG_TMP=3D&quot;$IMG.tmp&quot;</div><div>BUILD_SIZE=3D&quot;40=
96&quot;</div><div>EXPORT_SIZE=3D&quot;700&quot;</div><div><br></div><div>s=
ingularity create -s &quot;$BUILD_SIZE&quot; &quot;$IMG_TMP&quot; &amp;&amp=
;</div><div>singularity bootstrap &quot;$IMG_TMP&quot; &quot;$DEF&quot; &am=
p;&amp;</div><div>singularity create -s &quot;$EXPORT_SIZE&quot; &quot;$IMG=
&quot; &amp;&amp;</div><div>singularity export &quot;$IMG_TMP&quot; | singu=
larity import &quot;$IMG&quot;</div><div><br></div><div>rm &quot;$IMG_TM&qu=
ot;</div><div><br></div><div>with the follow up comment ```You could do som=
ething smart with the `BUILD_SIZE` and `EXPORT_SIZE` variables, but I=E2=80=
=99m lazy, you=E2=80=99re lazy, we=E2=80=99re all lazy.```</div><div>Its tr=
ue, I am lazy, that&#39;s why I like singularity.</div><div><br></div><div>=
In terms of minimising the content of the container, moving the user space =
gpu drive out of the container is a good idea (for portability), however I =
don&#39;t advocate moving CUDA its self out of the container.=C2=A0</div><d=
iv>Depending on how neurokernel was installed, all the usual tricks for shr=
inking images apply, i.e. remove the source code after compilation, remove =
any system packages downloaded and cached before installation. See if there=
 are any packages you can do without (did an X windows environment get inst=
alled as a dependency of anything?)</div><div><br></div><div>Cheers,</div><=
div>--</div><div>Chris.</div></div></div><div class=3D"gmail_extra"><br><di=
v class=3D"gmail_quote">On 2 March 2017 at 07:25, Eliot Eshelman <span dir=
=3D"ltr">&lt;<a href=3D"mailto:el...@microway.com" target=3D"_blank">el...@=
microway.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
 =20
   =20
 =20
  <div bgcolor=3D"#FFFFFF" text=3D"#000000">
    <div class=3D"m_-7868827222529392413moz-cite-prefix">The biggest proble=
m with portability of
      GPU-accelerated Singularity is the reliance on specific versions
      of the NVIDIA driver. Docker was in a similar position a year or
      so ago, which is why NVIDIA created nvidia-docker. The
      nvidia-docker tool is really just startup scripts which make the
      host&#39;s NVIDIA driver files available to the container. Here&#39;s=
 a
      peek into the NVIDIA driver files and libraries that matter:<br>
      <br>
      <a href=3D"https://github.com/NVIDIA/nvidia-docker/blob/master/src/nv=
idia/volumes.go#L104" target=3D"_blank">https://github.com/NVIDIA/<wbr>nvid=
ia-docker/blob/master/src/<wbr>nvidia/volumes.go#L104</a><br>
      <br>
      I have been thinking about how to make an nvidia-singularity tool,
      but haven&#39;t written a single line of code yet. I&#39;d be interes=
ted
      to hear if anyone else is also looking to tackle this one?<br>
      <br>
      Eliot<div><div class=3D"h5"><br>
      <br>
      <br>
      <br>
      On 03/01/2017 06:49 AM, &#39;Adam Tomkins&#39; via singularity wrote:=
<br>
    </div></div></div><div><div class=3D"h5">
    <blockquote type=3D"cite">
      <p><u></u>Hello,<u></u></p>
      <p><u></u><br>
        <u></u></p>
      <u></u>
        <p><u></u>I&#39;m new to Singularity, and I have built a
            container for the <a href=3D"https://neurokernel.github.io/" ta=
rget=3D"_blank">NeuroKernel</a>=C2=A0neuron
            simulator.=C2=A0<u></u></p>
        <u></u>
          <p><u></u><br>
            <u></u></p>
          <u></u>
            <p><u></u>It currently works locally, assuming you
                have CUDA installed correctly, however it is quite
                large.=C2=A0<u></u></p>
            <u></u>
              <p><u></u><br>
                <u></u></p>
              <u></u>
                <p><u></u>I am looking for any feedback on how to
                    make it more portable, or usable in general, with
                    the aim of eventually moving it to our HPC clusters.<u>=
</u></p>
                <u></u>
                  <p><u></u><br>
                    <u></u></p>
                  <u></u>
                    <p><u></u>You can find it on github <a href=3D"https://=
github.com/AdamRTomkins/Neurokernel-singularity-container" target=3D"_blank=
">here</a><u></u></p>
                    <u></u>
                      <p><u></u><br>
                        <u></u></p>
                      <u></u>
                        <p><u></u>Thanks for the great software.<u></u></p>
                        <u></u>
                          <p><u></u><br>
                            <u></u></p>
                          <u></u>
                            <p><u></u>Adam<u></u></p>
                            <u></u>
                              <p><u></u><br>
                                <u></u></p>
                              <u></u>
                                <p><u></u><br>
                                  <u></u></p>
                                <u></u>
                                  <p><u></u><br>
                                    <u></u></p>
                                  <u></u>
                                    <p><u></u><br>
                                      <u></u></p>
                                    <u></u><u></u>
                                        -- <br>
                                        You received this message
                                        because you are subscribed to
                                        the Google Groups &quot;singularity=
&quot;
                                        group.<br>
                                        To unsubscribe from this group
                                        and stop receiving emails from
                                        it, send an email to <a href=3D"mai=
lto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscribe@lbl.<wbr>go=
v</a>.<br>
                                      <u></u><u></u><u></u><u></u><u></u><u=
></u><u></u><u></u><u></u><u></u><u></u><u></u><u></u><u></u><u></u><u></u>=
<u></u></blockquote>
    <p><br>
    </p>
    </div></div><span class=3D"HOEnZb"><font color=3D"#888888"><div class=
=3D"m_-7868827222529392413moz-signature">-- <br>
      Eliot Eshelman<br>
      Microway, Inc.
    </div>
  </font></span></div><div class=3D"HOEnZb"><div class=3D"h5">


<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--001a1141d0128dce290549b22aac--
