X-Received: by 10.36.118.79 with SMTP id z76mr3983224itb.1.1466527804878;
        Tue, 21 Jun 2016 09:50:04 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.120.2 with SMTP id p2ls288465itc.30.gmail; Tue, 21 Jun 2016
 09:50:04 -0700 (PDT)
X-Received: by 10.36.60.80 with SMTP id m77mr7136632ita.96.1466527804278;
        Tue, 21 Jun 2016 09:50:04 -0700 (PDT)
Return-Path: <bge...@riken.jp>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id u83si41064143pfa.234.2016.06.21.09.50.04
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 21 Jun 2016 09:50:04 -0700 (PDT)
Received-SPF: pass (google.com: domain of bge...@riken.jp designates 134.160.33.83 as permitted sender) client-ip=134.160.33.83;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of bge...@riken.jp designates 134.160.33.83 as permitted sender) smtp.mailfrom=bge...@riken.jp
X-Ironport-SBRS: 5.2
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A0AAAAAgb2lXmFMhoIZdHAEBglKCIQaDNqs4hwGGcwgXAYV/AoEwQxABAQEBAQEBEgEBAQEBCAsLByEvhEsBAQEBAgESESswCwsLDSoCAiEBDwMBBQEcDgcEARwEAYd0Aw8IBaNJgTE+MYs7hiSGLg2DdQEBCAEBAQEjhieDSoEDgkOBTxEBgx2CWgWOaYlcNIwygXqPI4gLhi4SHoEPNYI4gXdOB4kMgTUBAQE
X-IPAS-Result: A0AAAAAgb2lXmFMhoIZdHAEBglKCIQaDNqs4hwGGcwgXAYV/AoEwQxABAQEBAQEBEgEBAQEBCAsLByEvhEsBAQEBAgESESswCwsLDSoCAiEBDwMBBQEcDgcEARwEAYd0Aw8IBaNJgTE+MYs7hiSGLg2DdQEBCAEBAQEjhieDSoEDgkOBTxEBgx2CWgWOaYlcNIwygXqPI4gLhi4SHoEPNYI4gXdOB4kMgTUBAQE
X-IronPort-AV: E=Sophos;i="5.26,505,1459839600"; 
   d="scan'208,217";a="26998478"
Received: from postman1.riken.jp (HELO postman.riken.jp) ([134.160.33.83])
  by fe4.lbl.gov with ESMTP; 21 Jun 2016 09:50:01 -0700
Received: from postman.riken.jp (localhost.localdomain [127.0.0.1])
	by postman.riken.jp (Postfix) with SMTP id 32D7A68111
	for <singu...@lbl.gov>; Wed, 22 Jun 2016 01:50:01 +0900 (JST)
Received: from mail-it0-f44.google.com (mail-it0-f44.google.com [209.85.214.44])
	by postman.riken.jp (Postfix) with ESMTPA id 85C7E6A02B
	for <singu...@lbl.gov>; Wed, 22 Jun 2016 01:50:00 +0900 (JST)
Received: by mail-it0-f44.google.com with SMTP id f6so18799216ith.0
        for <singu...@lbl.gov>; Tue, 21 Jun 2016 09:50:00 -0700 (PDT)
X-Gm-Message-State: ALyK8tL34gestUQzL8UJ5pReBakXOd+cJAwjNeL70CQBU9tb0dweXAxKkcvFW9KFDgqkbL6kdYTr2NBE4tJ5mg==
X-Received: by 10.36.254.195 with SMTP id w186mr7194583ith.49.1466527799596;
 Tue, 21 Jun 2016 09:49:59 -0700 (PDT)
MIME-Version: 1.0
Reply-To: bge...@riken.jp
Received: by 10.50.18.103 with HTTP; Tue, 21 Jun 2016 09:49:59 -0700 (PDT)
In-Reply-To: <CAHCZMOGATFSE2HmNoi=XocaxC6g=V2yeaiegi04RU07j792U0g@mail.gmail.com>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov> <244D668A-1EB7-412D-9303-E52F5412409C@open-mpi.org>
 <CAPqNE2WY31aOinCS12uqJD-wXQNUGBaEr6irHuSQMGGp=47SDQ@mail.gmail.com> <CAHCZMOGATFSE2HmNoi=XocaxC6g=V2yeaiegi04RU07j792U0g@mail.gmail.com>
From: Balazs Gerofi <bge...@riken.jp>
Date: Tue, 21 Jun 2016 09:49:59 -0700
X-Gmail-Original-Message-ID: <CAPVZOdEgH-K-i...@mail.gmail.com>
Message-ID: <CAPVZOdEgH-K-ieiMaA62TOXqGF2+hRNCexK6v8ZrBfXCcj_F0g@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=94eb2c033d0e8c609c0535cc9ca4
X-PMX-Version: 6.1.1.2430161, Antispam-Engine: 2.7.2.2107409, Antispam-Data: 2016.6.21.164217

--94eb2c033d0e8c609c0535cc9ca4
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hello Greg,

I've tested Intel MPI and it works fine.
One caveat: if you run over IB you will need to add the network drivers
(libdapl* and friends) to the container image.
Unfortunately these don't get displayed just by inspecting your binary with
ldd, but you can figure them out during runtime.

Best,
Balazs

On Tue, Jun 21, 2016 at 9:39 AM, Greg Keller <gregw...@gmail.com> wrote:

> Any chance IntelMPI will, "just work"?
>
> On Tue, Jun 21, 2016 at 10:46 AM, 'John Hearns' via singularity <
> singu...@lbl.gov> wrote:
>
>> > We=E2=80=99ll take care of the rest. Our initial studies showed zero
>> performance degradation by running inside Singularity, and the launch
>> penalty is near-zero as well
>>
>> May I just say - I haz a happee. Lolz.
>> Sorry - normal service will be resumed as soon as possible.  And yes I a=
m
>> a sad person when the thought of running MPI processes in containers mak=
es
>> me happy.
>>
>> On 21 June 2016 at 15:49, Ralph Castain <r...@open-mpi.org> wrote:
>>
>>> Singularity is fully supported by OMPI (and vice versa). If you grab a
>>> copy of the OMPI master and build it
>>> =E2=80=94with-singularity=3D<path-to-singularity> (or have the singular=
ity path in
>>> your default path), then all you have to do is use mpirun as you normal=
ly
>>> do, but provide the container as your =E2=80=9Capp=E2=80=9D.
>>>
>>> We=E2=80=99ll take care of the rest. Our initial studies showed zero pe=
rformance
>>> degradation by running inside Singularity, and the launch penalty is
>>> near-zero as well (and gets better when compared against dl_open=E2=80=
=99d dynamic
>>> jobs running at scale). I=E2=80=99ll let Greg answer the question of ho=
w to address
>>> the running container.
>>>
>>>
>>> On Jun 21, 2016, at 7:37 AM, Raimon Bosch <raimo...@gmail.com>
>>> wrote:
>>>
>>>
>>>
>>> Hi,
>>>
>>> We are trying to run experiments using singularity containers. The idea
>>> is to run OpenMPI among several containers and check performance result=
s.
>>>
>>> How can I communicate with another container? In docker this is clear
>>> because every container gets an assigned IP and you can ping there, but
>>> what is the situation in the case of singularity? Is it possible to ass=
ign
>>> an IP to each container? Can I connect via ssh to them?
>>>
>>> Thanks in advance,
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--94eb2c033d0e8c609c0535cc9ca4
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello Greg,<div><br></div><div>I&#39;ve tested Intel MPI a=
nd it works fine.</div><div>One caveat: if you run over IB you will need to=
 add the network drivers (libdapl* and friends) to the container image.</di=
v><div>Unfortunately these don&#39;t get displayed just by inspecting your =
binary with ldd, but you can figure them out during runtime.=C2=A0</div><di=
v><br></div><div>Best,</div><div>Balazs=C2=A0</div><div class=3D"gmail_extr=
a"><br><div class=3D"gmail_quote">On Tue, Jun 21, 2016 at 9:39 AM, Greg Kel=
ler <span dir=3D"ltr">&lt;<a href=3D"mailto:gregw...@gmail.com" target=3D"_=
blank">gregw...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gma=
il_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-lef=
t:1ex"><div dir=3D"ltr">Any chance IntelMPI will, &quot;just work&quot;?</d=
iv><div class=3D"HOEnZb"><div class=3D"h5"><div class=3D"gmail_extra"><br><=
div class=3D"gmail_quote">On Tue, Jun 21, 2016 at 10:46 AM, &#39;John Hearn=
s&#39; via singularity <span dir=3D"ltr">&lt;<a href=3D"mailto:singu...@lbl=
.gov" target=3D"_blank">singu...@lbl.gov</a>&gt;</span> wrote:<br><blockquo=
te class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc so=
lid;padding-left:1ex"><div dir=3D"ltr"><span><span style=3D"font-size:12.8p=
x">&gt; We=E2=80=99ll take care of the rest. Our initial studies showed zer=
o performance degradation by running inside Singularity, and the launch pen=
alty is near-zero as well</span><br><div><span style=3D"font-size:12.8px"><=
br></span></div></span><div><span style=3D"font-size:12.8px">May I just say=
 - I haz a happee. Lolz.</span></div><div><span style=3D"font-size:12.8px">=
Sorry - normal service will be resumed as soon as possible.=C2=A0 And yes I=
 am a sad person when the thought of running MPI processes in containers ma=
kes me happy.</span><br></div></div><div><div><div class=3D"gmail_extra"><b=
r><div class=3D"gmail_quote">On 21 June 2016 at 15:49, Ralph Castain <span =
dir=3D"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...=
@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" st=
yle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div =
style=3D"word-wrap:break-word">Singularity is fully supported by OMPI (and =
vice versa). If you grab a copy of the OMPI master and build it =E2=80=94wi=
th-singularity=3D&lt;path-to-singularity&gt; (or have the singularity path =
in your default path), then all you have to do is use mpirun as you normall=
y do, but provide the container as your =E2=80=9Capp=E2=80=9D.<div><br></di=
v><div>We=E2=80=99ll take care of the rest. Our initial studies showed zero=
 performance degradation by running inside Singularity, and the launch pena=
lty is near-zero as well (and gets better when compared against dl_open=E2=
=80=99d dynamic jobs running at scale). I=E2=80=99ll let Greg answer the qu=
estion of how to address the running container.</div><div><div><div><br></d=
iv><div><br><div><blockquote type=3D"cite"><div>On Jun 21, 2016, at 7:37 AM=
, Raimon Bosch &lt;<a href=3D"mailto:raimo...@gmail.com" target=3D"_blank">=
raimo...@gmail.com</a>&gt; wrote:</div><br><div><div dir=3D"ltr"><br><br>Hi=
,<br><br>We are trying to run experiments using singularity containers. The=
 idea is to run OpenMPI among several containers and check performance resu=
lts. <br><br>How can I communicate with another container? In docker this i=
s clear because every container gets an assigned IP and you can ping there,=
 but what is the situation in the case of singularity? Is it possible to as=
sign an IP to each container? Can I connect via ssh to them?<br><br>Thanks =
in advance,<br></div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></blockquote></div><br></div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div></div>

--94eb2c033d0e8c609c0535cc9ca4--
