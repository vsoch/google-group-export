X-Received: by 10.13.232.136 with SMTP id r130mr84643948ywe.54.1452045586674;
        Tue, 05 Jan 2016 17:59:46 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.130.34 with SMTP id e34ls1473623iod.7.gmail; Tue, 05 Jan
 2016 17:59:46 -0800 (PST)
X-Received: by 10.98.67.155 with SMTP id l27mr80526813pfi.93.1452045586080;
        Tue, 05 Jan 2016 17:59:46 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id yd1si21931878pab.39.2016.01.05.17.59.45
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 05 Jan 2016 17:59:46 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.48 as permitted sender) client-ip=74.125.82.48;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.48 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 5.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FUAABodIxWmzBSfUpUCoRqDwaDIoERhCCzdwENgWQYAYV2AoEbBzgUAQEBAQEBAQMOAQEBAQEGCwsJIS6ENAEBAQMBEggJKx4CCBMLCw0gCgICIg8DAQUBDg4OAgUEARoCBAGIBQgFo1OBMT4xi0iQcgEBAQcBAQEBAR4Khk2EfoQmBgsBBoM1gUkFjjCEWoN+AYgyhSCBXI0dik6CJxEkgRcgAQGCUxUHgX4dNAeEDwkXBIEnAQEB
X-IronPort-AV: E=Sophos;i="5.20,527,1444719600"; 
   d="scan'208";a="10220252"
Received: from mail-wm0-f48.google.com ([74.125.82.48])
  by fe3.lbl.gov with ESMTP; 05 Jan 2016 17:59:43 -0800
Received: by mail-wm0-f48.google.com with SMTP id b14so55556239wmb.1
        for <singu...@lbl.gov>; Tue, 05 Jan 2016 17:59:43 -0800 (PST)
X-Gm-Message-State: ALoCoQnOye9rd3otZZsp43wD+koSSVyI51dkz2wkqFkuVkJ1LgjfeoMouilUtFXSym/4tnMswn4CyE52BIK+pwKmkTE3rWLUAXnPdimTy1Dusx5lRMiKhphsxo2pQ0M0e4uUwykAHyFu3VGLya+slrg/dwzLiPcymVEm262BAcz7WXwEjGnh7Vg=
X-Received: by 10.28.86.196 with SMTP id k187mr6872308wmb.61.1452045583013;
        Tue, 05 Jan 2016 17:59:43 -0800 (PST)
MIME-Version: 1.0
X-Received: by 10.28.86.196 with SMTP id k187mr6872302wmb.61.1452045582843;
 Tue, 05 Jan 2016 17:59:42 -0800 (PST)
Received: by 10.28.218.71 with HTTP; Tue, 5 Jan 2016 17:59:42 -0800 (PST)
In-Reply-To: <310A83CD-CF76-4575-8D69-883088D8CD89@open-mpi.org>
References: <937ff7ed-30b6-4143-849e-5e1a234ff89b@lbl.gov>
	<4D239990-0E7E-4F15-B5CB-09C8465562F0@lbl.gov>
	<2778d375-2145-4c15-81d5-0c961734993f@lbl.gov>
	<CAN7etTz8O5Vt4x+=oTp9+7MXEGxr8NT3K2u+BLxB5s=a9MKn4A@mail.gmail.com>
	<CAN7etTyafQgmAwAH94z1-3of=NNj9QQmLzJUzwn=mO8VRjJAFQ@mail.gmail.com>
	<D29DD7ED.89FB1%Grigory.Shamov@ad.umanitoba.ca>
	<D33AC0B6-5024-4234-BD25-7878E8E7C417@lbl.gov>
	<0C2E7133-169B-4546-9A8E-DF261094EF6F@open-mpi.org>
	<CAN7etTx+WFj6tQ=6=woiTJCta2z7P9m+hqG-TA8teAyqGXiijA@mail.gmail.com>
	<310A83CD-CF76-4575-8D69-883088D8CD89@open-mpi.org>
Date: Tue, 5 Jan 2016 17:59:42 -0800
Message-ID: <CAN7etTzwfe6Gk9VRBG69my+f7O7jrzWhuxFazTi7VPtZtqmvkw@mail.gmail.com>
Subject: Re: [Singularity] Singularity and HPC support
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a11452ca829d1320528a0b52a

--001a11452ca829d1320528a0b52a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On Mon, Jan 4, 2016 at 8:10 PM, Ralph Castain <r...@open-mpi.org> wrote:

>
> On Jan 4, 2016, at 3:52 PM, Gregory M. Kurtzer <gmku...@lbl.gov> wrote:
>
> Hi Ralph,
>
> Sorry for the delay, the holidays got me a bit behind!
>
>
> No issues - glad you got a chance to get away for a bit
>
>
>
>
> On Sat, Jan 2, 2016 at 9:45 AM, Ralph Castain <r...@open-mpi.org> wrote:
>
>> Hello all
>>
>> I=E2=80=99ve been monitoring this mailing list since it began, but have =
been
>> swamped by too many things to respond. I=E2=80=99d like to specifically =
address one
>> of Greg=E2=80=99s early requests for input from some MPI folks. As some =
of you may
>> know, I=E2=80=99ve been a lead developer of OpenMPI=E2=80=99s runtime si=
nce the project=E2=80=99s
>> inception some 12 years ago, and have spent a lot of time interfacing it=
 to
>> various environments. So I=E2=80=99d like to try and provide some perspe=
ctive in
>> the hopes that Singularity may be able to better support HPC operations.
>>
>
> Excellent, and exactly what I was hoping for! I will answer as much as I
> can, but I feel we may want to have a discussion in realtime to get more
> details as it maybe faster to get on the same page initially (or that I m=
ay
> confuse you with my answers! LOL). Perhaps a google hangout, skype, or
> other means (e.g. we can go old fashioned and use a phone too!) would do
> well.
>
>
> I agree - let=E2=80=99s up the bandwidth with a skype or call. I confess =
that I
> don=E2=80=99t really know much about Singularity, having only tried to fo=
llow what
> was said here, and so I may well be misunderstanding how it works. I was
> expecting that the containers provide some degree of isolation between ea=
ch
> other when sharing a node, and thus some entity (e.g., the host resource
> manager) is responsible for assigning resources to each container.
>

Isolation is one but of several features that containers provide. Another
is application portability (which is where Singularity really focuses on).
While there needs to be some bits of isolation to achieve this, it is a
significantly different model from full isolation in order to replicate a
full operating system or mimicking full virtualization.


>
> Once the container is instantiated, it has to stay within those applied
> resource constraints. At that point, we have two options - the applicatio=
ns
> executing within each container can either:
>
> (a) see themselves as being on the same physical node, which corresponds
> to being under the same OS instance. In this case, the container is
> basically just an application delivery vehicle and has no =E2=80=9Csubsta=
nce=E2=80=9D. It
> isn=E2=80=99t quite clear to me what advantage this provides over any oth=
er binary
> prelocation mechanism such as those we have in today=E2=80=99s RMs as tho=
se already
> position both binaries and any associated/required libraries. However, I
> agree that there is no networking confusion in this case.
>
> (b) see themselves as being under different OS instances - i.e., the
> container provides some level of environment abstraction (e.g., I can
> replace the OS or package app-specific library environments, while
> maintaining cross-container isolation). Here we do have some networking
> confusion as the processes don=E2=80=99t realize where they are relative =
to each
> other when communicating to the local NIC.
>
> Your answers below imply that Singularity is focused on (a) - is that
> correct? If so, then it probably doesn=E2=80=99t really address the use-c=
ases we
> are facing, though I=E2=80=99m sure it provides some functionality that m=
eets your
> needs. On the plus side, I don=E2=80=99t see any reason for that use-case=
 to have
> an issue running MPI applications today.
>

I would say that Singularity sits in a unique position between (a) and (b).
While it is an application delivery vehicle, it also does not run within
the same OS instance or context as the physical host node. The primary
need/use-case that we are seeing time and time again are with application
portability on HPC systems and people using a full container (or worse, a
VM hard disk image) to achieve this portability. Singularity will take a
SPEC file (similar to RPM) and builds a package walking the dependency
chain of the packaged components and including them into the container.
Thus the container not is an extremely minimal subset of an operating
system image, but it is also all completely owned by the user invoking it.

Hope that helps! If you want to email me directly/privately and let me know
a few times that you would be available for a phone call we can schedule a
meeting.

Thanks!

Greg


>
> Ralph
>
>
>
>>
>> First, let me clarify my expected use-case. I=E2=80=99m assuming that a =
user
>> requests an allocation of  some number of containers for running a
>> multi-process application. The user may intend to run one or more
>> application processes in each container. When allocated, the containers
>> will belong to that one user, and will be for their exclusive use. The
>> containers will be returned to the scheduler upon completion of the
>> application.
>>
>
> Singularity really fogs up the idea of containers in terms of allocations=
,
> resources, and contentions because the container runs like a standard
> traditional application... Because it is not like other container platfor=
ms
> you maybe using as a baseline. There are two ways to run Singularity apps=
:
>
> The first is directly executing the container archive file (it is created
> with +x, and via the magic of interpretation Singularity can run it
> seamlessly). When the container is run in this manner, it extracts its
> contents to a temporary directory and will only run in a single usage
> context.
>
> The second manner of running Singularity containers is more inline with
> what you have described. Using the Singularity command, you install the
> singularity container into a user specific cache, and from that cached
> container, you can invoke as many instances of the program(s)/workflows
> within the container at once as you like. If that cache directory is
> shared, you can run processes from within that container on any number of
> nodes (a kin to running an application from an NFS volume).
>
> Again, I want to stress that Singularity containers all run within the
> context of the user. There is no centralized management facility for any =
of
> the Singularity containers (with the exception the application stack of
> Singularity itself).
>
>
>>
>> So when I talk about =E2=80=9Cmultiple containers=E2=80=9D below, I=E2=
=80=99m talking strictly
>> about the above use-case. It is quite possible that a user could have
>> multiple allocations running in parallel - the containers for each
>> allocation would be distinct and have no knowledge of the containers in
>> another allocation, even though they belong to the same user. Allocation=
s
>> given to different users should have no knowledge of each other.
>>
>
> Understood and agreed.
>
>
>>
>> I expect that containers from different users will share a physical node
>> (i.e., that cloud will typically be a multi-tenant environment), which
>> implies there will be some resource contention between them. Thus, we wi=
ll
>> require some kind of QoS control over the networks. Remember, these will=
 be
>> OS-bypass networks, and so the OS in each container will have no control=
 or
>> even visibility over them.
>>
>
> This is where Singularity diverges considerably from other container
> solutions. Singularity containers (or more specifically the applications
> within the containers) are intended to run on the same networks, file
> systems, and environment that the user already has access to. There is no
> mechanism for the user to gain access or change users thus there is no ne=
ed
> to isolate the network stack into a virtualized namespace. Additionally,
> while that maybe beneficially for containers that are mimicking virtual
> machines, there is no need or benefit (that I see) to separate the networ=
k
> namespace. Soooo... Long story short, there is no OS-bypass network or
> additional QOSs that must be managed for Singularity.
>
>
>>
>> Within that context, the container must be given the following knowledge
>> at startup if we really want to provide performance comparable to what i=
s
>> achieved today:
>>
>> * the ID of all other containers in this allocation - a unique =E2=80=9C=
hostname=E2=80=9D
>> or its equivalent. Must include the IP address assigned to each containe=
r
>>
>
> If the container cache is located in a shared location, this should be
> easy to do either within the job script or beforehand by the user. The IP
> address would be the same as the host compute node running the job.
>
>
>>
>> * the =E2=80=9Ctrue=E2=80=9D location of each container in the allocatio=
n - i.e., the
>> physical node where the container is executing (so we can identify which
>> processes are =E2=80=9Csharing=E2=80=9D physical resources) and where it=
 is bound (which
>> physical cpus; what network, memory, and other resources it has been giv=
en;
>> etc.)
>>
>
> I am hoping Singularity will make this easier then other container
> solutions.
>
>
>>
>> * if application processes were started as part of setting up the
>> containers, then which processes (i.e., which ranks) were put on each
>> container, along with what network endpoints were assigned to them and
>> where they are bound
>>
>
> It would be very convenient if the ranks, network ports, and everything
> works just as it work natively from the MPI front. Perhaps I am
> overthinking this, but because the Singularity containers exist on the sa=
me
> physical network as would be expected, things should be easier in this
> context.
>
>
>>
>> * the network topology between the containers - alternatively, you can
>> provide the topology between the involved physical nodes and we can use =
the
>> above location info to get what we need
>>
>
> Same as above.
>
>
>>
>> * a method by which application procs can request any or all of that inf=
o
>> as they need it in order to properly setup their communication
>> infrastructure.
>>
>
> Hopefully environment variables will suffice (they pass through to the
> Singularity container).
>
>
>>
>> * an enhancement would include a shared memory region on each physical
>> node that hosts multiple containers so we could use shared memory
>> communication in that situation. Obviously, there would need to be a
>> dedicated region for each allocation to ensure separation of application=
s.
>>
>
> Shared memory (should) already be managed properly (in theory), but I hav=
e
> not tested this yet.
>
>
>>
>> * a =E2=80=9Cglobal=E2=80=9D key-value exchange server that will allow u=
sers to
>> =E2=80=9Cdiscover=E2=80=9D applications and their allocations, subject t=
o access controls.
>> Thus, an ocean simulator in one allocation could =E2=80=9Cdiscover=E2=80=
=9D and connect to
>> an atmospheric model in another allocation so they can share data. The H=
PC
>> world knows how to build this now - just a case of ensuring adequate
>> knowledge transfer.
>>
>
> I'd be very interested in hearing more about what you are thinking here.
>
>
>>
>> I hope that helps provide some direction. I=E2=80=99d be happy to help c=
onstruct
>> this capability, and interface it to OpenMPI.
>>
>
> Yep, totally!
>
> Additionally, I have a major concern which has to do with environment
> portability in matching the kernel and container API versions (e.g. OFED)=
.
> What I would love to see is that part of the MPI stack that interfaces wi=
th
> the host environment/kernel gets called directly on the host, but the
> processes itself still runs within the container context (possibly with a
> container installed runtime component of the MPI stack). I have no idea h=
ow
> this could work, but perhaps you would have some ideas on this or just te=
ll
> me it is impossible and kick me.
>
> Thank you and let me know if I have indeed made this more complicated or
> confusing and that we should discuss in real time!
>
> Greg
>
>
> --
> Gregory M. Kurtzer
> Technical Lead and HPC Systems Architect
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
Gregory M. Kurtzer
Technical Lead and HPC Systems Architect
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a11452ca829d1320528a0b52a
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Mon, Jan 4, 2016 at 8:10 PM, Ralph Castain <span dir=3D"ltr">&lt;<a =
href=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>&g=
t;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0=
 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:=
break-word"><br><div><span class=3D""><blockquote type=3D"cite"><div>On Jan=
 4, 2016, at 3:52 PM, Gregory M. Kurtzer &lt;<a href=3D"mailto:gmku...@lbl.=
gov" target=3D"_blank">gmku...@lbl.gov</a>&gt; wrote:</div><br><div><div di=
r=3D"ltr">Hi Ralph,<div><br></div><div>Sorry for the delay, the holidays go=
t me a bit behind!</div></div></div></blockquote><div><br></div></span>No i=
ssues - glad you got a chance to get away for a bit</div><div><span class=
=3D""><br><blockquote type=3D"cite"><div><div dir=3D"ltr"><div><br></div><d=
iv><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Sat, Ja=
n 2, 2016 at 9:45 AM, Ralph Castain <span dir=3D"ltr">&lt;<a href=3D"mailto=
:r...@open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>&gt;</span> wrot=
e:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-l=
eft:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word">He=
llo all<div><br></div><div>I=E2=80=99ve been monitoring this mailing list s=
ince it began, but have been swamped by too many things to respond. I=E2=80=
=99d like to specifically address one of Greg=E2=80=99s early requests for =
input from some MPI folks. As some of you may know, I=E2=80=99ve been a lea=
d developer of OpenMPI=E2=80=99s runtime since the project=E2=80=99s incept=
ion some 12 years ago, and have spent a lot of time interfacing it to vario=
us environments. So I=E2=80=99d like to try and provide some perspective in=
 the hopes that Singularity may be able to better support HPC operations.</=
div></div></blockquote><div><br></div><div>Excellent, and exactly what I wa=
s hoping for! I will answer as much as I can, but I feel we may want to hav=
e a discussion in realtime to get more details as it maybe faster to get on=
 the same page initially (or that I may confuse you with my answers! LOL). =
Perhaps a google hangout, skype, or other means (e.g. we can go old fashion=
ed and use a phone too!) would do well.</div></div></div></div></div></div>=
</blockquote><div><br></div></span>I agree - let=E2=80=99s up the bandwidth=
 with a skype or call. I confess that I don=E2=80=99t really know much abou=
t Singularity, having only tried to follow what was said here, and so I may=
 well be misunderstanding how it works. I was expecting that the containers=
 provide some degree of isolation between each other when sharing a node, a=
nd thus some entity (e.g., the host resource manager) is responsible for as=
signing resources to each container.</div></div></blockquote><div><br></div=
><div>Isolation is one but of several features that containers provide. Ano=
ther is application portability (which is where Singularity really focuses =
on). While there needs to be some bits of isolation to achieve this, it is =
a significantly different model from full isolation in order to replicate a=
 full operating system or mimicking full virtualization.</div><div>=C2=A0</=
div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-lef=
t:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><div=
><br></div><div>Once the container is instantiated, it has to stay within t=
hose applied resource constraints. At that point, we have two options - the=
 applications executing within each container can either:</div><div><br></d=
iv><div>(a) see themselves as being on the same physical node, which corres=
ponds to being under the same OS instance. In this case, the container is b=
asically just an application delivery vehicle and has no =E2=80=9Csubstance=
=E2=80=9D. It isn=E2=80=99t quite clear to me what advantage this provides =
over any other binary prelocation mechanism such as those we have in today=
=E2=80=99s RMs as those already position both binaries and any associated/r=
equired libraries. However, I agree that there is no networking confusion i=
n this case.</div><div><br></div><div>(b) see themselves as being under dif=
ferent OS instances - i.e., the container provides some level of environmen=
t abstraction (e.g., I can replace the OS or package app-specific library e=
nvironments, while maintaining cross-container isolation). Here we do have =
some networking confusion as the processes don=E2=80=99t realize where they=
 are relative to each other when communicating to the local NIC.</div><div>=
<br></div><div>Your answers below imply that Singularity is focused on (a) =
- is that correct? If so, then it probably doesn=E2=80=99t really address t=
he use-cases we are facing, though I=E2=80=99m sure it provides some functi=
onality that meets your needs. On the plus side, I don=E2=80=99t see any re=
ason for that use-case to have an issue running MPI applications today.</di=
v></div></blockquote><div><br></div><div>I would say that Singularity sits =
in a unique position between (a) and (b). While it is an application delive=
ry vehicle, it also does not run within the same OS instance or context as =
the physical host node. The primary need/use-case that we are seeing time a=
nd time again are with application portability on HPC systems and people us=
ing a full container (or worse, a VM hard disk image) to achieve this porta=
bility. Singularity will take a SPEC file (similar to RPM) and builds a pac=
kage walking the dependency chain of the packaged components and including =
them into the container. Thus the container not is an extremely minimal sub=
set of an operating system image, but it is also all completely owned by th=
e user invoking it.</div><div><br></div><div>Hope that helps! If you want t=
o email me directly/privately and let me know a few times that you would be=
 available for a phone call we can schedule a meeting.</div><div><br></div>=
<div>Thanks!</div><div><br></div><div>Greg</div><div>=C2=A0</div><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div style=3D"word-wrap:break-word"><span class=3D"HOE=
nZb"><font color=3D"#888888"><div><br></div><div>Ralph</div><div><br></div>=
</font></span><div><blockquote type=3D"cite"><div><div><div class=3D"h5"><d=
iv dir=3D"ltr"><div><div class=3D"gmail_extra"><div class=3D"gmail_quote"><=
div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:brea=
k-word"><div><br></div><div><div>First, let me clarify my expected use-case=
. I=E2=80=99m assuming that a user requests an allocation of =C2=A0some num=
ber of containers for running a multi-process application. The user may int=
end to run one or more application processes in each container. When alloca=
ted, the containers will belong to that one user, and will be for their exc=
lusive use. The containers will be returned to the scheduler upon completio=
n of the application.</div></div></div></blockquote><div><br></div><div>Sin=
gularity really fogs up the idea of containers in terms of allocations, res=
ources, and contentions because the container runs like a standard traditio=
nal application... Because it is not like other container platforms you may=
be using as a baseline. There are two ways to run Singularity apps:</div><d=
iv><br></div><div>The first is directly executing the container archive fil=
e (it is created with +x, and via the magic of interpretation Singularity c=
an run it seamlessly). When the container is run in this manner, it extract=
s its contents to a temporary directory and will only run in a single usage=
 context.</div><div><br></div><div>The second manner of running Singularity=
 containers is more inline with what you have described. Using the Singular=
ity command, you install the singularity container into a user specific cac=
he, and from that cached container, you can invoke as many instances of the=
 program(s)/workflows within the container at once as you like. If that cac=
he directory is shared, you can run processes from within that container on=
 any number of nodes (a kin to running an application from an NFS volume).<=
/div><div><br></div><div>Again, I want to stress that Singularity container=
s all run within the context of the user. There is no centralized managemen=
t facility for any of the Singularity containers (with the exception the ap=
plication stack of Singularity itself).</div><div>=C2=A0</div><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;=
padding-left:1ex"><div style=3D"word-wrap:break-word"><div><div><br></div><=
div>So when I talk about =E2=80=9Cmultiple containers=E2=80=9D below, I=E2=
=80=99m talking strictly about the above use-case. It is quite possible tha=
t a user could have multiple allocations running in parallel - the containe=
rs for each allocation would be distinct and have no knowledge of the conta=
iners in another allocation, even though they belong to the same user. Allo=
cations given to different users should have no knowledge of each other.</d=
iv></div></div></blockquote><div><br></div><div>Understood and agreed.</div=
><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:br=
eak-word"><div><div><br></div><div>I expect that containers from different =
users will share a physical node (i.e., that cloud will typically be a mult=
i-tenant environment), which implies there will be some resource contention=
 between them. Thus, we will require some kind of QoS control over the netw=
orks. Remember, these will be OS-bypass networks, and so the OS in each con=
tainer will have no control or even visibility over them.</div></div></div>=
</blockquote><div><br></div><div>This is where Singularity diverges conside=
rably from other container solutions. Singularity containers (or more speci=
fically the applications within the containers) are intended to run on the =
same networks, file systems, and environment that the user already has acce=
ss to. There is no mechanism for the user to gain access or change users th=
us there is no need to isolate the network stack into a virtualized namespa=
ce. Additionally, while that maybe beneficially for containers that are mim=
icking virtual machines, there is no need or benefit (that I see) to separa=
te the network namespace. Soooo... Long story short, there is no OS-bypass =
network or additional QOSs that must be managed for Singularity.</div><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-wo=
rd"><div><div><br></div><div>Within that context, the container must be giv=
en the following knowledge at startup if we really want to provide performa=
nce comparable to what is achieved today:</div><div><br></div><div>* the ID=
 of all other containers in this allocation - a unique =E2=80=9Chostname=E2=
=80=9D or its equivalent. Must include the IP address assigned to each cont=
ainer</div></div></div></blockquote><div><br></div><div>If the container ca=
che is located in a shared location, this should be easy to do either withi=
n the job script or beforehand by the user. The IP address would be the sam=
e as the host compute node running the job.</div><div>=C2=A0</div><blockquo=
te class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc so=
lid;padding-left:1ex"><div style=3D"word-wrap:break-word"><div><div><br></d=
iv><div>* the =E2=80=9Ctrue=E2=80=9D location of each container in the allo=
cation - i.e., the physical node where the container is executing (so we ca=
n identify which processes are =E2=80=9Csharing=E2=80=9D physical resources=
) and where it is bound (which physical cpus; what network, memory, and oth=
er resources it has been given; etc.)</div></div></div></blockquote><div><b=
r></div><div>I am hoping Singularity will make this easier then other conta=
iner solutions.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div s=
tyle=3D"word-wrap:break-word"><div><div><br></div><div>* if application pro=
cesses were started as part of setting up the containers, then which proces=
ses (i.e., which ranks) were put on each container, along with what network=
 endpoints were assigned to them and where they are bound</div></div></div>=
</blockquote><div><br></div><div>It would be very convenient if the ranks, =
network ports, and everything works just as it work natively from the MPI f=
ront. Perhaps I am overthinking this, but because the Singularity container=
s exist on the same physical network as would be expected, things should be=
 easier in this context.</div><div>=C2=A0</div><blockquote class=3D"gmail_q=
uote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1e=
x"><div style=3D"word-wrap:break-word"><div><div><br></div><div>* the netwo=
rk topology between the containers - alternatively, you can provide the top=
ology between the involved physical nodes and we can use the above location=
 info to get what we need</div></div></div></blockquote><div><br></div><div=
>Same as above.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div s=
tyle=3D"word-wrap:break-word"><div><div><br></div><div>* a method by which =
application procs can request any or all of that info as they need it in or=
der to properly setup their communication infrastructure.</div></div></div>=
</blockquote><div><br></div><div>Hopefully environment variables will suffi=
ce (they pass through to the Singularity container).</div><div>=C2=A0</div>=
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><div><di=
v><br></div><div>* an enhancement would include a shared memory region on e=
ach physical node that hosts multiple containers so we could use shared mem=
ory communication in that situation. Obviously, there would need to be a de=
dicated region for each allocation to ensure separation of applications.</d=
iv></div></div></blockquote><div><br></div><div>Shared memory (should) alre=
ady be managed properly (in theory), but I have not tested this yet.</div><=
div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:brea=
k-word"><div><div><br></div><div>* a =E2=80=9Cglobal=E2=80=9D key-value exc=
hange server that will allow users to =E2=80=9Cdiscover=E2=80=9D applicatio=
ns and their allocations, subject to access controls. Thus, an ocean simula=
tor in one allocation could =E2=80=9Cdiscover=E2=80=9D and connect to an at=
mospheric model in another allocation so they can share data. The HPC world=
 knows how to build this now - just a case of ensuring adequate knowledge t=
ransfer.</div></div></div></blockquote><div><br></div><div>I&#39;d be very =
interested in hearing more about what you are thinking here.</div><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"=
><div><br></div><div>I hope that helps provide some direction. I=E2=80=99d =
be happy to help construct this capability, and interface it to OpenMPI.</d=
iv></div></blockquote><div><br></div><div>Yep, totally!</div><div><br></div=
><div>Additionally, I have a major concern which has to do with environment=
 portability in matching the kernel and container API versions (e.g. OFED).=
 What I would love to see is that part of the MPI stack that interfaces wit=
h the host environment/kernel gets called directly on the host, but the pro=
cesses itself still runs within the container context (possibly with a cont=
ainer installed runtime component of the MPI stack). I have no idea how thi=
s could work, but perhaps you would have some ideas on this or just tell me=
 it is impossible and kick me.</div><div><br></div><div>Thank you and let m=
e know if I have indeed made this more complicated or confusing and that we=
 should discuss in real time!</div><div><br></div><div>Greg</div></div><br =
clear=3D"all"><div><br></div>-- <br><div>Gregory M. Kurtzer<br>Technical Le=
ad and HPC Systems Architect<br>High Performance Computing Services (HPCS)<=
br>University of California<br>Lawrence Berkeley National Laboratory<br>One=
 Cyclotron Road, Berkeley, CA 94720</div>
</div></div></div><div><br></div></div></div><span class=3D"">

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</span></div></blockquote></div><br></div><div class=3D"HOEnZb"><div class=
=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature">Gregory M. Kurtzer<br>Technical Lead and HPC=
 Systems Architect<br>High Performance Computing Services (HPCS)<br>Univers=
ity of California<br>Lawrence Berkeley National Laboratory<br>One Cyclotron=
 Road, Berkeley, CA 94720</div>
</div></div>

--001a11452ca829d1320528a0b52a--
