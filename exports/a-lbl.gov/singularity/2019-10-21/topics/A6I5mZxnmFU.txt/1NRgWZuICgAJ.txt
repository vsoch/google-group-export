X-Received: by 2002:aa7:8143:: with SMTP id d3-v6mr5504915pfn.26.1526908645041;
        Mon, 21 May 2018 06:17:25 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a65:400d:: with SMTP id f13-v6ls3009678pgp.16.gmail; Mon, 21
 May 2018 06:17:24 -0700 (PDT)
X-Received: by 2002:a62:e50d:: with SMTP id n13-v6mr19942820pff.125.1526908643770;
        Mon, 21 May 2018 06:17:23 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1526908643; cv=none;
        d=google.com; s=arc-20160816;
        b=X+Qwhl30o6enpJk5QNuqq6Ni0jp91wzOu3QZFNXZFOWQZwYT1OaFur2bbci9xhgqrk
         PqQ5Jh0+3F3RJtRvSYRIAJDeP5unN9e6/ar4QtI/qDADgFNQowrsNHJTGlL0pYGLly5G
         PIGiaW9mGV7fqx60LF3L4HU/M70+1aTirbe5+PYyUdCIa5AaMaatuY+pmbbFzOaVFK5+
         6cqv0DdyD/FQKbl7+5ine3rTcAFJy/9p1PHT9gfCbcIxwScpX2Mak+4zhaOn9c9Kg6OS
         DfXFBer2xLF3yA7Gk1Df2zMkYzeHVeIvOY7uqOi0//nPPtFg7XS4U8zjd0gTTiDE7Dgy
         vvBA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=Yp5hQ9iPfx0DEDWJ/iXmParrdz+Y9F4qwYwUwnFctzw=;
        b=oEzi9beULkmPvKMBq/+wnCWWbeRwmS7CVZCcdWD1+eKYTG1aSRbzJAiScFhMOZVc/Z
         Nt3zkXB8GNwbOS5TyTC6CR6F4iXuiLjDUiji0Q0Nq1QsxPatnQwVGWafF94yu2JBBmJe
         FTWDGmo/tJBu6b2DrRfFqLN2t0oom2zeJfVHjY7S2S7H3GY2h+t+wCzK0QkkHA7EsDyd
         zJHCM2qCCXhNACGig3ZsT7ZB+ucUffcr6MpW/7XMLh/O2I87hJBO77mltkm7zIu9xZyI
         9RcCrgXX760m2GFxK5v7iq5gu/zc/MKi3syVEJDFSlQ2Ba50ufn1a4SMsmkDjpf9Gxd9
         QBjA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=eRuNyG/W;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.215.43 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id r59-v6si13956956plb.314.2018.05.21.06.17.23
        for <singu...@lbl.gov>;
        Mon, 21 May 2018 06:17:23 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 209.85.215.43 as permitted sender) client-ip=209.85.215.43;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=eRuNyG/W;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.215.43 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HjAgCNxgJbhivXVdFcHAEBAQQBAQoBA?=
 =?us-ascii?q?YMYgQx9KINvBoEdglCRCoF5gQ+TSoEOAxgXIQMjAQwJgUmCL0YCghkhNxUBAgE?=
 =?us-ascii?q?BAQEBAQIBAQIQAQEBCAsLCCgjDII1BQIDAh4FBEspAgEwAQEBAQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBFAIMASAXEgIYAQEBAwEaAQgdAQ0OHgMBCwYDAgsGAwEBAQEVCwE?=
 =?us-ascii?q?JAgIhAQEOAwEFAQsJCA4HBAEaAgSDAQIogT4BAw0IBQqLQJACPIsFgX8FAReCc?=
 =?us-ascii?q?AWDSQoZJg1UV4IHAgYSiCOBVD+BD4JeLoJPKxcBAQKBGQoIARIBKxQNEoJBglQ?=
 =?us-ascii?q?Ch2CJNIZ1FywJhWqFboJ/gTeDbYdZiV9KhAeCODCBBDJWLnFwFTsxggIBAQENC?=
 =?us-ascii?q?YFnJBqDToF/iFU9MAEPjW1HgXABAQ?=
X-IronPort-AV: E=Sophos;i="5.49,426,1520924400"; 
   d="scan'208,217";a="116282873"
Received: from mail-lf0-f43.google.com ([209.85.215.43])
  by fe3.lbl.gov with ESMTP; 21 May 2018 06:16:56 -0700
Received: by mail-lf0-f43.google.com with SMTP id w202-v6so23824813lff.12
        for <singu...@lbl.gov>; Mon, 21 May 2018 06:16:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=Yp5hQ9iPfx0DEDWJ/iXmParrdz+Y9F4qwYwUwnFctzw=;
        b=eRuNyG/WU83jxXkc6dgPAc5oUUV431ZXl/2Z5wc/oMol30KEPvYrwv/LARAqqszUv+
         SCbq1VMOMHnx+9QRkBS4trSkogFuPgWniEwF8MOJS3YkxStKraVhe8cMhmd79EKUegEF
         23X/58BBZAd/Y5GyzpgbRMT7vVagDrCvAiVVCatTSxsro0vhCWV1yYtcOPPSR/EqN2og
         +GKbuQ4zl8Z5hOZvVE3mTArp3P+d4d5QnMzdYaJYeXdbtSISz0V0jFDp1sFp77PUdN26
         xjfVmUowByVlAACqZT7r180GAnfnmTGY23Cv16QccBI7dWrlvCy+vJlW+3r9uTTxBVCy
         E0+Q==
X-Gm-Message-State: ALKqPwdbxIGdXtPGNpSSBbP8wdv5P6zr9yMbxftie8pgHzkuxEf4pWQJ
	Orv8svr/vWB2ceCZtqgljdegkAekaqBg5k4ajLySbg==
X-Received: by 2002:a2e:59d4:: with SMTP id g81-v6mr12826745ljf.4.1526908614263;
 Mon, 21 May 2018 06:16:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 2002:a19:1891:0:0:0:0:0 with HTTP; Mon, 21 May 2018 06:16:53
 -0700 (PDT)
In-Reply-To: <CAB6eJZL+F7DwMcebA6DC+QWHqqsnnqEfJ2uqR2D-6bas6DmnYw@mail.gmail.com>
References: <167671ff-657d-4560-a35f-87091223fe29@lbl.gov> <B58197C146EC324AA00A2A07DC082602C2CBA0C3@XMAIL-MBX-BT1.AD.UCSD.EDU>
 <CAB6eJZ+6PDjs3POSQPNoLdaMys5d7iYDYcUeQFpMyzJ6DEP31w@mail.gmail.com>
 <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com>
 <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com> <CAB6eJZL+F7DwMcebA6DC+QWHqqsnnqEfJ2uqR2D-6bas6DmnYw@mail.gmail.com>
From: victor sv <vict...@gmail.com>
Date: Mon, 21 May 2018 15:16:53 +0200
Message-ID: <CA+Wz_FznUJ9kWSy7qneZsVEYqemqnm6eq0ASo+AoD49VNqU=zw@mail.gmail.com>
Subject: Re: [Singularity] Running an mpi program with mvapitch
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="0000000000008ec767056cb71d70"

--0000000000008ec767056cb71d70
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi George,

please check that you are calling the right program. Is the executable path
in the PATH environment variable? If not you have the prepend the path to
call the executable.

Take a look to this:

https://stackoverflow.com/questions/47472153/mpi-mpirun-execvp-error-no-suc=
h-file-or-directory

BR,
V=C3=ADctor.

2018-05-21 14:58 GMT+02:00 George Zaki <georg...@gmail.com>:

> Hi Jason,
>
> I was not able to run the program even without singularity when I use
> mvapitch. I am in contact with our system admin
>
> Here is the  output I got when I run mpiexec within singularity:
>
> mpiexec -n 1 mpi-pi.o
>
> [mpiexec@cn3137] HYDU_create_process (utils/launch/launch.c:75): execvp
> error on file srun (No such file or directory)
>
> Best regards,
>
> George.
>
> On Mon, May 21, 2018 at 3:04 AM victor sv <vict...@gmail.com> wrote:
>
>> Hi George,
>>
>> not any experience with mvapitch. I think the compiler version has no
>> effect here.
>>
>> Jason solution and is a good starting point to check if the container MP=
I
>> works in a single node (not in several nodes).
>>
>> To run the hybrid MPI approach you should take into account that both
>> version and vendor of MPI and PMI must match. Can you check if PMI
>> libraries match?
>>
>> BR,
>> V=C3=ADctor.
>>
>> 2018-05-18 20:14 GMT+02:00 Jason Stover <jason...@gmail.com>:
>>
>>> Hi George,
>>>
>>>   Can you run it from inside the container? For example:
>>>
>>>     singularity exec
>>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1
>>> mpi-pi.o
>>>
>>> -J
>>>
>>>
>>> On Fri, May 18, 2018 at 12:56 PM, George Zaki <georg...@gmail.com>
>>> wrote:
>>> > Thanks Marty
>>> >
>>> > Below are the values I got, any obvious mismatch? I git this working
>>> fine
>>> > with OpenMPI.
>>> >
>>> > Here is also what I try to run:
>>> >
>>> > singularity exec /data/zakigf/candle/swift-hypervisor-horovod-mvapich=
.
>>> simg
>>> > mpicc mpi-pi.c -o  mpi-pi.o
>>> >
>>> > [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec
>>> > /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o
>>> >
>>> > Then I kill after no response:
>>> >
>>> > ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested
>>> >
>>> > [mpiexec@cn2360] Press Ctrl-C again to force abort
>>> >
>>> > [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error
>>> (Bad
>>> > file descriptor)
>>> >
>>> > [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal
>>> (pm/pmiserv/pmiserv_cb.c:169):
>>> > unable to write data to proxy
>>> >
>>> > [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to
>>> send
>>> > signal downstream
>>> >
>>> > [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event
>>> > (tools/demux/demux_poll.c:76): callback returned error status
>>> >
>>> > [mpiexec@cn2360] HYD_pmci_wait_for_completion
>>> > (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event
>>> >
>>> > [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error
>>> > waiting for completion
>>> >
>>> >
>>> > Now about the versions:
>>> >
>>> > Singularity: Invoking an interactive shell within container...
>>> >
>>> >
>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> mpiexec --versio=
n
>>> >
>>> > HYDRA build details:
>>> >
>>> >     Version:                                 3.1.4
>>> >
>>> >     Release Date:                            Wed Sep  7 14:33:43 EDT
>>> 2016
>>> >
>>> >     CC:                              gcc
>>> >
>>> >     CXX:                             g++
>>> >
>>> >     F77:                             gfortran
>>> >
>>> >     F90:                             gfortran
>>> >
>>> >     Configure options:
>>>  '--disable-option-checking'
>>> > '--prefix=3DNONE' '--cache-file=3D/dev/null' '--srcdir=3D.' 'CC=3Dgcc=
' 'CFLAGS=3D
>>> > -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,-rpath,=
/lib
>>> > -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad -libumad -libve=
rbs
>>> -ldl
>>> > -lrt -lm -lpthread ' 'CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/include
>>> > -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/sr=
c
>>> > -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>> > -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include
>>> -I/include
>>> > -I/include'
>>> >
>>> >     Process Manager:                         pmi
>>> >
>>> >     Launchers available:                     ssh rsh fork slurm ll ls=
f
>>> sge
>>> > manual persist
>>> >
>>> >     Topology libraries available:            hwloc
>>> >
>>> >     Resource management kernels available:   user slurm ll lsf sge pb=
s
>>> > cobalt
>>> >
>>> >     Checkpointing libraries available:
>>> >
>>> >     Demux engines available:                 poll select
>>> >
>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> gcc --version
>>> >
>>> > gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>> >
>>> > Copyright (C) 2015 Free Software Foundation, Inc.
>>> >
>>> > This is free software; see the source for copying conditions.  There
>>> is NO
>>> >
>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>> PURPOSE.
>>> >
>>> >
>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> exit
>>> >
>>> > exit
>>> >
>>> >
>>> > [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0
>>> >
>>> > [+] Loading mvapich2 2.2 for GCC 5.3.0
>>> >
>>> > [zakigf@cn2360 ~]$ gcc --version
>>> >
>>> > gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
>>> >
>>> > Copyright (C) 2010 Free Software Foundation, Inc.
>>> >
>>> > This is free software; see the source for copying conditions.  There
>>> is NO
>>> >
>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>> PURPOSE.
>>> >
>>> >
>>> > [zakigf@cn2360 ~]$ mpiexec --version
>>> >
>>> > HYDRA build details:
>>> >
>>> >     Version:                                 3.1.4
>>> >
>>> >     Release Date:                            Wed Sep  7 14:33:43 EDT
>>> 2016
>>> >
>>> >     CC:                              /usr/local/GCC/5.3.0/bin/gcc
>>> >
>>> >     CXX:                             /usr/local/GCC/5.3.0/bin/g++
>>> >
>>> >     F77:                             /usr/local/GCC/5.3.0/bin/gfortra=
n
>>> >
>>> >     F90:                             /usr/local/GCC/5.3.0/bin/gfortra=
n
>>> >
>>> >     Configure options:
>>>  '--disable-option-checking'
>>> > '--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0'
>>> > '--with-slurm-lib=3D/usr/local/slurm/lib'
>>> > '--with-slurm-include=3D/usr/local/slurm/include' '--enable-debug=3Dn=
one'
>>> > '--enable-timing=3Druntime' 'CC=3D/usr/local/GCC/5.3.0/bin/gcc'
>>> > 'CXX=3D/usr/local/GCC/5.3.0/bin/g++' 'FC=3D/usr/local/GCC/5.3.0/bin/
>>> gfortran'
>>> > 'F77=3D/usr/local/GCC/5.3.0/bin/gfortran' '--cache-file=3D/dev/null'
>>> > '--srcdir=3D.' 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib=
 -L/lib
>>> > -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
>>> 'LIBS=3D-libmad
>>> > -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src
>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include
>>> -I/include
>>> > -I/include -I/include -I/include'
>>> >
>>> >     Process Manager:                         pmi
>>> >
>>> >     Launchers available:                     ssh rsh fork slurm ll ls=
f
>>> sge
>>> > manual persist
>>> >
>>> >     Topology libraries available:            hwloc
>>> >
>>> >     Resource management kernels available:   user slurm ll lsf sge pb=
s
>>> > cobalt
>>> >
>>> >     Checkpointing libraries available:
>>> >
>>> >     Demux engines available:                 poll select
>>> >
>>> >
>>> > On Fri, May 18, 2018 at 1:31 PM Kandes, Martin <mka...@sdsc.edu>
>>> wrote:
>>> >>
>>> >> Hi George,
>>> >>
>>> >> I run with different gcc compiler versions inside and outside my MPI
>>> >> containers. So I would be surprised if that is the issue here. I'm
>>> not sure
>>> >> I have a good recommendation of where to start debugging your
>>> problem. But I
>>> >> might start by double checking the MPI versions match inside and
>>> outside the
>>> >> container. e.g. see [1].
>>> >>
>>> >> Marty
>>> >>
>>> >> [1]
>>> >>
>>> >> [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1
>>> >> --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/ba=
sh
>>> >> srun: job 16364303 queued and waiting for resources
>>> >> srun: job 16364303 has been allocated resources
>>> >> [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/
>>> >> [mkandes@comet-14-06 16364303]$ cp
>>> >> /oasis/scratch/comet/mkandes/temp_project/singularity/
>>> images/ubuntu-mvapich2.img
>>> >> ./
>>> >> [mkandes@comet-14-06 16364303]$ module purge
>>> >> [mkandes@comet-14-06 16364303]$ module load gnu
>>> >> [mkandes@comet-14-06 16364303]$ module load mvapich2_ib
>>> >> [mkandes@comet-14-06 16364303]$ module list
>>> >> Currently Loaded Modulefiles:
>>> >>   1) gnu/4.9.2         2) mvapich2_ib/2.1
>>> >> [mkandes@comet-14-06 16364303]$ module load singularity
>>> >> [mkandes@comet-14-06 16364303]$ gcc --version
>>> >> gcc (GCC) 4.9.2
>>> >> Copyright (C) 2014 Free Software Foundation, Inc.
>>> >> This is free software; see the source for copying conditions.  There
>>> is NO
>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>> >> PURPOSE.
>>> >>
>>> >> [mkandes@comet-14-06 16364303]$ mpirun --version
>>> >> HYDRA build details:
>>> >>     Version:                                 3.1.4
>>> >>     Release Date:                            Thu Apr  2 17:15:15 EDT
>>> 2015
>>> >>     CC:                              gcc  -fPIC -O3
>>> >>     CXX:                             g++  -fPIC -O3
>>> >>     F77:                             gfortran -fPIC -O3
>>> >>     F90:                             gfortran -fPIC -O3
>>> >>     Configure options:
>>>  '--disable-option-checking'
>>> >> '--prefix=3D/opt/mvapich2/gnu/ib' '--enable-shared'
>>> '--enable-sharedlibs=3Dgcc'
>>> >> '--with-hwloc' '--enable-f77' '--enable-fc' '--enable-hybrid'
>>> >> '--with-ib-include=3D/usr/include/infiniband'
>>> '--with-ib-libpath=3D/usr/lib64'
>>> >> '--enable-fast=3DO3'
>>> >> '--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/
>>> sdsc-mvapich2_gnu_ib-2.1/../..//cache/build-limic'
>>> >> '--with-slurm=3D/usr/lib64/slurm' '--with-file-system=3Dlustre' 'CC=
=3Dgcc'
>>> >> 'CFLAGS=3D-fPIC -O3 -O3' 'CXX=3Dg++' 'CXXFLAGS=3D-fPIC -O3 -O3'
>>> 'FC=3Dgfortran'
>>> >> 'FCFLAGS=3D-fPIC -O3 -O3' 'F77=3Dgfortran' 'FFLAGS=3D-L/usr/lib64 -L=
/lib
>>> -L/lib
>>> >> -fPIC -O3 -O3' '--cache-file=3D/dev/null' '--srcdir=3D.'
>>> 'LDFLAGS=3D-L/usr/lib64
>>> >> -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib
>>> -L/usr/lib64
>>> >> -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/../..//cache/build-limic/lib
>>> >> -L/lib -L/lib' 'LIBS=3D-libmad -lrdmacm -libumad -libverbs -ldl -lrt
>>> -llimic2
>>> >> -lm -lpthread ' 'CPPFLAGS=3D-I/usr/include/infiniband
>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/mvapich2-2.1/src/mpl/include
>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/mvapich2-2.1/src/mpl/include
>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/mvapich2-2.1/src/openpa/src
>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/mvapich2-2.1/src/openpa/src
>>> >> -D_REENTRANT
>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/mvapich2-2.1/src/mpi/romio/include
>>> >> -I/include -I/include -I/usr/include/infiniband
>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>> ib-2.1/../..//cache/build-limic/include
>>> >> -I/include -I/include'
>>> >>     Process Manager:                         pmi
>>> >>     Launchers available:                     ssh rsh fork slurm ll
>>> lsf sge
>>> >> manual persist
>>> >>     Topology libraries available:            hwloc
>>> >>     Resource management kernels available:   user slurm ll lsf sge p=
bs
>>> >> cobalt
>>> >>     Checkpointing libraries available:
>>> >>     Demux engines available:                 poll select
>>> >> [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.im=
g
>>> >> Singularity: Invoking an interactive shell within container...
>>> >>
>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> gcc
>>> --version
>>> >> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>> >> Copyright (C) 2015 Free Software Foundation, Inc.
>>> >> This is free software; see the source for copying conditions.  There
>>> is NO
>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>> >> PURPOSE.
>>> >>
>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> mpirun
>>> >> --version
>>> >> HYDRA build details:
>>> >>     Version:                                 3.1.4
>>> >>     Release Date:                            Thu Apr  2 17:15:15 EDT
>>> 2015
>>> >>     CC:                              gcc
>>> >>     CXX:                             g++
>>> >>     F77:                             gfortran
>>> >>     F90:                             gfortran
>>> >>     Configure options:
>>>  '--disable-option-checking'
>>> >> '--prefix=3D/opt/mvapich2' '--cache-file=3D/dev/null' '--srcdir=3D.'
>>> 'CC=3Dgcc'
>>> >> 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib
>>> >> -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libma=
d
>>> -lrdmacm
>>> >> -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>> >> -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/
>>> include
>>> >> -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/sr=
c
>>> >> -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include
>>> -I/include
>>> >> -I/include -I/include'
>>> >>     Process Manager:                         pmi
>>> >>     Launchers available:                     ssh rsh fork slurm ll
>>> lsf sge
>>> >> manual persist
>>> >>     Topology libraries available:            hwloc
>>> >>     Resource management kernels available:   user slurm ll lsf sge p=
bs
>>> >> cobalt
>>> >>     Checkpointing libraries available:
>>> >>     Demux engines available:                 poll select
>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303>
>>> >>
>>> >>
>>> >> ________________________________
>>> >> From: George Zaki [georg...@gmail.com]
>>> >> Sent: Friday, May 18, 2018 6:48 AM
>>> >> To: singularity
>>> >> Subject: [Singularity] Running an mpi program with mvapitch
>>> >>
>>> >> Hi singularity team,
>>> >>
>>> >>
>>> >> I would like to run an MPI program in a singularity container. The
>>> program
>>> >> is compiled using mvapicth2.2 using a gcc version 5.4.
>>> >>
>>> >>
>>> >> I can see that my cluster has a compiled version of mvapitch2.2 with
>>> gcc
>>> >> 5.3
>>> >>
>>> >>
>>> >> When I run:
>>> >>
>>> >>
>>> >> mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o
>>> >>
>>> >> the call does not return.
>>> >>
>>> >>
>>> >>
>>> >> Does the gcc version has to be exactly the same? I tried the switch
>>> the
>>> >> compiler in this image:
>>> >>
>>> >>
>>> >> BootStrap: docker
>>> >> From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04
>>> >>
>>> >>
>>> >> However when gcc 5.3 is used the mvapitch does not build correctly.
>>> >>
>>> >>
>>> >> If that's the problem, Is there a preferred method of switching gcc
>>> >> version in this container singularity container?
>>> >>
>>> >>
>>> >> Thanks,
>>> >> George
>>> >>
>>> >> --
>>> >> You received this message because you are subscribed to the Google
>>> Groups
>>> >> "singularity" group.
>>> >> To unsubscribe from this group and stop receiving emails from it,
>>> send an
>>> >> email to singu...@lbl.gov.
>>> >>
>>> >> --
>>> >> You received this message because you are subscribed to a topic in t=
he
>>> >> Google Groups "singularity" group.
>>> >> To unsubscribe from this topic, visit
>>> >> https://groups.google.com/a/lbl.gov/d/topic/singularity/
>>> A6I5mZxnmFU/unsubscribe.
>>> >> To unsubscribe from this group and all its topics, send an email to
>>> >> singu...@lbl.gov.
>>> >
>>> > --
>>> > You received this message because you are subscribed to the Google
>>> Groups
>>> > "singularity" group.
>>> > To unsubscribe from this group and stop receiving emails from it, sen=
d
>>> an
>>> > email to singu...@lbl.gov.
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to a topic in the
>> Google Groups "singularity" group.
>> To unsubscribe from this topic, visit https://groups.google.com/a/
>> lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe.
>> To unsubscribe from this group and all its topics, send an email to
>> singu...@lbl.gov.
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--0000000000008ec767056cb71d70
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi George,=C2=A0<div><br></div><div>please check that you =
are calling the right program. Is the executable path in the PATH environme=
nt variable? If not you have the prepend the path to call the executable.</=
div><div><br></div><div>Take a look to this:</div><div><br></div><div><a hr=
ef=3D"https://stackoverflow.com/questions/47472153/mpi-mpirun-execvp-error-=
no-such-file-or-directory">https://stackoverflow.com/questions/47472153/mpi=
-mpirun-execvp-error-no-such-file-or-directory</a><br></div><div><br></div>=
<div>BR,</div><div>V=C3=ADctor.</div></div><div class=3D"gmail_extra"><br><=
div class=3D"gmail_quote">2018-05-21 14:58 GMT+02:00 George Zaki <span dir=
=3D"ltr">&lt;<a href=3D"mailto:georg...@gmail.com" target=3D"_blank">georg.=
..@gmail.com</a>&gt;</span>:<br><blockquote class=3D"gmail_quote" style=3D"=
margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"=
ltr">Hi Jason,=C2=A0<div><br></div><div>I was not able to run the program e=
ven without singularity when I use mvapitch. I am in contact with our syste=
m admin</div><div><br></div><div>Here is the=C2=A0 output I got when I run =
mpiexec within singularity:=C2=A0</div><div><br></div><div>mpiexec -n 1 mpi=
-pi.o<span class=3D"m_-6547978355576408365inbox-inbox-Apple-converted-space=
">=C2=A0</span><br></div><div>
<p class=3D"m_-6547978355576408365inbox-inbox-p1"><span class=3D"m_-6547978=
355576408365inbox-inbox-s1">[mpiexec@cn3137] HYDU_create_process (utils/lau=
nch/launch.c:75): execvp error on file srun (No such file or directory)</sp=
an></p><p class=3D"m_-6547978355576408365inbox-inbox-p1">Best regards,</p><=
p class=3D"m_-6547978355576408365inbox-inbox-p1">George.</p></div></div><di=
v class=3D"HOEnZb"><div class=3D"h5"><br><div class=3D"gmail_quote"><div di=
r=3D"ltr">On Mon, May 21, 2018 at 3:04 AM victor sv &lt;<a href=3D"mailto:v=
ict...@gmail.com" target=3D"_blank">vict...@gmail.com</a>&gt; wrote:<br></d=
iv><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left=
:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><div><div><div>Hi G=
eorge,<br><br></div>not any experience with mvapitch. I think the compiler =
version has no effect here. <br><br>Jason solution and is a good starting p=
oint to check if the container MPI works in a single node (not in several n=
odes).<br><br></div>To run the hybrid MPI approach you should take into acc=
ount that both version and vendor of MPI and PMI must match. Can you check =
if PMI libraries match?<br><br></div>BR,<br></div>V=C3=ADctor.<br></div><di=
v class=3D"gmail_extra"><br><div class=3D"gmail_quote">2018-05-18 20:14 GMT=
+02:00 Jason Stover <span dir=3D"ltr">&lt;<a href=3D"mailto:jason...@gmail.=
com" target=3D"_blank">jason...@gmail.com</a>&gt;</span>:<br><blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;p=
adding-left:1ex">Hi George,<br>
<br>
=C2=A0 Can you run it from inside the container? For example:<br>
<br>
=C2=A0 =C2=A0 singularity exec<br>
/data/zakigf/candle/swift-<wbr>hypervisor-horovod-mvapich.<wbr>simg mpiexec=
 -n 1<br>
mpi-pi.o<br>
<span class=3D"m_-6547978355576408365m_-6591493537311087044HOEnZb"><font co=
lor=3D"#888888"><br>
-J<br>
</font></span><div class=3D"m_-6547978355576408365m_-6591493537311087044HOE=
nZb"><div class=3D"m_-6547978355576408365m_-6591493537311087044h5"><br>
<br>
On Fri, May 18, 2018 at 12:56 PM, George Zaki &lt;<a href=3D"mailto:georg..=
.@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt; wrote:<br>
&gt; Thanks Marty<br>
&gt;<br>
&gt; Below are the values I got, any obvious mismatch? I git this working f=
ine<br>
&gt; with OpenMPI.<br>
&gt;<br>
&gt; Here is also what I try to run:<br>
&gt;<br>
&gt; singularity exec /data/zakigf/candle/swift-<wbr>hypervisor-horovod-mva=
pich.<wbr>simg<br>
&gt; mpicc mpi-pi.c -o=C2=A0 mpi-pi.o<br>
&gt;<br>
&gt; [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec<br>
&gt; /data/zakigf/candle/swift-<wbr>hypervisor-horovod-mvapich.<wbr>simg mp=
i-pi.o<br>
&gt;<br>
&gt; Then I kill after no response:<br>
&gt;<br>
&gt; ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested<br>
&gt;<br>
&gt; [mpiexec@cn2360] Press Ctrl-C again to force abort<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error =
(Bad<br>
&gt; file descriptor)<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal (pm/pmiserv/pmiserv_cb.c=
:169):<br>
&gt; unable to write data to proxy<br>
&gt;<br>
&gt; [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79)<wbr>: unable=
 to send<br>
&gt; signal downstream<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event<br>
&gt; (tools/demux/demux_poll.c:76): callback returned error status<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmci_wait_for_completion<br>
&gt; (pm/pmiserv/pmiserv_pmci.c:<wbr>198): error waiting for event<br>
&gt;<br>
&gt; [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error<=
br>
&gt; waiting for completion<br>
&gt;<br>
&gt;<br>
&gt; Now about the versions:<br>
&gt;<br>
&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; mpiexec -=
-version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3DNONE&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--sr=
cdir=3D.&#39; &#39;CC=3Dgcc&#39; &#39;CFLAGS=3D<br>
&gt; -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,=
-rpath,/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -libumad=
 -libverbs -ldl<br>
&gt; -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/=
<wbr>include<br>
&gt; -I/tmp/mvapich2-2.2/src/mpl/<wbr>include -I/tmp/mvapich2-2.2/src/<wbr>=
openpa/src<br>
&gt; -I/tmp/mvapich2-2.2/src/<wbr>openpa/src -D_REENTRANT<br>
&gt; -I/tmp/mvapich2-2.2/src/mpi/<wbr>romio/include -I/include -I/include -=
I/include<br>
&gt; -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; gcc --ver=
sion<br>
&gt;<br>
&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;<br>
&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; exit<br>
&gt;<br>
&gt; exit<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0<br>
&gt;<br>
&gt; [+] Loading mvapich2 2.2 for GCC 5.3.0<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ gcc --version<br>
&gt;<br>
&gt; gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)<br>
&gt;<br>
&gt; Copyright (C) 2010 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ mpiexec --version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 /usr/local/GCC/5.3=
.0/bin/gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/<wbr>gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/<wbr>gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3D/usr/local/MVAPICH2/<wbr>2.2/gcc-5.3.0&#39;<br>
&gt; &#39;--with-slurm-lib=3D/usr/local/<wbr>slurm/lib&#39;<br>
&gt; &#39;--with-slurm-include=3D/usr/<wbr>local/slurm/include&#39; &#39;--=
enable-debug=3Dnone&#39;<br>
&gt; &#39;--enable-timing=3Druntime&#39; &#39;CC=3D/usr/local/GCC/5.3.0/bin=
/<wbr>gcc&#39;<br>
&gt; &#39;CXX=3D/usr/local/GCC/5.3.0/bin/<wbr>g++&#39; &#39;FC=3D/usr/local=
/GCC/5.3.0/bin/<wbr>gfortran&#39;<br>
&gt; &#39;F77=3D/usr/local/GCC/5.3.0/bin/<wbr>gfortran&#39; &#39;--cache-fi=
le=3D/dev/null&#39;<br>
&gt; &#39;--srcdir=3D.&#39; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#=
39;LDFLAGS=3D-L/lib -L/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;=
LIBS=3D-libmad<br>
&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/openpa/src<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/openpa/src -D_REENTRAN=
T<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpi/romio/<wbr>include=
 -I/include<br>
&gt; -I/include -I/include -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt;<br>
&gt; On Fri, May 18, 2018 at 1:31 PM Kandes, Martin &lt;<a href=3D"mailto:m=
ka...@sdsc.edu" target=3D"_blank">mka...@sdsc.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi George,<br>
&gt;&gt;<br>
&gt;&gt; I run with different gcc compiler versions inside and outside my M=
PI<br>
&gt;&gt; containers. So I would be surprised if that is the issue here. I&#=
39;m not sure<br>
&gt;&gt; I have a good recommendation of where to start debugging your prob=
lem. But I<br>
&gt;&gt; might start by double checking the MPI versions match inside and o=
utside the<br>
&gt;&gt; container. e.g. see [1].<br>
&gt;&gt;<br>
&gt;&gt; Marty<br>
&gt;&gt;<br>
&gt;&gt; [1]<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1<=
br>
&gt;&gt; --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash<br>
&gt;&gt; srun: job 16364303 queued and waiting for resources<br>
&gt;&gt; srun: job 16364303 has been allocated resources<br>
&gt;&gt; [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ cp<br>
&gt;&gt; /oasis/scratch/comet/mkandes/<wbr>temp_project/singularity/<wbr>im=
ages/ubuntu-mvapich2.img<br>
&gt;&gt; ./<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module purge<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load gnu<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load mvapich2_ib<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module list<br>
&gt;&gt; Currently Loaded Modulefiles:<br>
&gt;&gt;=C2=A0 =C2=A01) gnu/4.9.2=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A02) mvapi=
ch2_ib/2.1<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load singularity<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ gcc --version<br>
&gt;&gt; gcc (GCC) 4.9.2<br>
&gt;&gt; Copyright (C) 2014 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ mpirun --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc=C2=A0 -f=
PIC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++=C2=A0 -fP=
IC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2/gnu/<wbr>ib&#39; &#39;--enable-share=
d&#39; &#39;--enable-sharedlibs=3Dgcc&#39;<br>
&gt;&gt; &#39;--with-hwloc&#39; &#39;--enable-f77&#39; &#39;--enable-fc&#39=
; &#39;--enable-hybrid&#39;<br>
&gt;&gt; &#39;--with-ib-include=3D/usr/<wbr>include/infiniband&#39; &#39;--=
with-ib-libpath=3D/usr/lib64&#39;<br>
&gt;&gt; &#39;--enable-fast=3DO3&#39;<br>
&gt;&gt; &#39;--with-limic2=3D/state/<wbr>partition1/git/mpi-roll/BUILD/<wb=
r>sdsc-mvapich2_gnu_ib-2.1/../..<wbr>//cache/build-limic&#39;<br>
&gt;&gt; &#39;--with-slurm=3D/usr/lib64/<wbr>slurm&#39; &#39;--with-file-sy=
stem=3Dlustre&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D-fPIC -O3 -O3&#39; &#39;CXX=3Dg++&#39; &#39;CXXFLAGS=
=3D-fPIC -O3 -O3&#39; &#39;FC=3Dgfortran&#39;<br>
&gt;&gt; &#39;FCFLAGS=3D-fPIC -O3 -O3&#39; &#39;F77=3Dgfortran&#39; &#39;FF=
LAGS=3D-L/usr/lib64 -L/lib -L/lib<br>
&gt;&gt; -fPIC -O3 -O3&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--srcdi=
r=3D.&#39; &#39;LDFLAGS=3D-L/usr/lib64<br>
&gt;&gt; -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr=
/lib64<br>
&gt;&gt; -L/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/../..//cache/build-<wbr>limic/lib<br>
&gt;&gt; -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -lrdmacm -libumad -libverbs=
 -ldl -lrt -llimic2<br>
&gt;&gt; -lm -lpthread &#39; &#39;CPPFLAGS=3D-I/usr/include/<wbr>infiniband=
<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpl/<wbr>include<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpl/<wbr>include<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/<wbr>openpa/src<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/<wbr>openpa/src<br>
&gt;&gt; -D_REENTRANT<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpi/<wbr>romio/include<br>
&gt;&gt; -I/include -I/include -I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/../..//cache/build-<wbr>limic/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.=
img<br>
&gt;&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
 gcc --version<br>
&gt;&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
 mpirun<br>
&gt;&gt; --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2&#39; &#39;--cache-file=3D/dev/null&#=
39; &#39;--srcdir=3D.&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib=
 -L/lib -L/lib<br>
&gt;&gt; -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIB=
S=3D-libmad -lrdmacm<br>
&gt;&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<=
br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/mpl/<wbr>include -I/tmp/mvapich2-2.1/src/m=
pl/<wbr>include<br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/<wbr>openpa/src -I/tmp/mvapich2-2.1/src/<w=
br>openpa/src<br>
&gt;&gt; -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/<wbr>romio/include -I/inc=
lude -I/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ______________________________<wbr>__<br>
&gt;&gt; From: George Zaki [<a href=3D"mailto:georg...@gmail.com" target=3D=
"_blank">georg...@gmail.com</a>]<br>
&gt;&gt; Sent: Friday, May 18, 2018 6:48 AM<br>
&gt;&gt; To: singularity<br>
&gt;&gt; Subject: [Singularity] Running an mpi program with mvapitch<br>
&gt;&gt;<br>
&gt;&gt; Hi singularity team,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I would like to run an MPI program in a singularity container. The=
 program<br>
&gt;&gt; is compiled using mvapicth2.2 using a gcc version 5.4.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I can see that my cluster has a compiled version of mvapitch2.2 wi=
th gcc<br>
&gt;&gt; 5.3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; When I run:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o<br>
&gt;&gt;<br>
&gt;&gt; the call does not return.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Does the gcc version has to be exactly the same? I tried the switc=
h the<br>
&gt;&gt; compiler in this image:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; BootStrap: docker<br>
&gt;&gt; From: nvidia/cuda:8.0-cudnn6-devel-<wbr>ubuntu16.04<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However when gcc 5.3 is used the mvapitch does not build correctly=
.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; If that&#39;s the problem, Is there a preferred method of switchin=
g gcc<br>
&gt;&gt; version in this container singularity container?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; George<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to a topic in=
 the<br>
&gt;&gt; Google Groups &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this topic, visit<br>
&gt;&gt; <a href=3D"https://groups.google.com/a/lbl.gov/d/topic/singularity=
/A6I5mZxnmFU/unsubscribe" rel=3D"noreferrer" target=3D"_blank">https://grou=
ps.google.com/a/<wbr>lbl.gov/d/topic/singularity/<wbr>A6I5mZxnmFU/unsubscri=
be</a>.<br>
&gt;&gt; To unsubscribe from this group and all its topics, send an email t=
o<br>
&gt;&gt; <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu=
larity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singul=
arity+unsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/<wbr>lbl.gov/d/topic/singularity/<wbr>A6I5mZxnmFU=
/unsubscribe</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscribe@lbl.=
<wbr>gov</a>.<br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--0000000000008ec767056cb71d70--
