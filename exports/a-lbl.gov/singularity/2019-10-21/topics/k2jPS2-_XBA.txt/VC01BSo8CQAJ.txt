Date: Fri, 18 Aug 2017 07:24:38 -0700 (PDT)
From: Balazs <blaur...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <1b276d14-5fef-4fa0-9ee2-160aceaa4e3d@lbl.gov>
In-Reply-To: <30715e7a-2b3c-4193-b0b4-501a001b5c3f@lbl.gov>
References: <30715e7a-2b3c-4193-b0b4-501a001b5c3f@lbl.gov>
Subject: Re: ompi_rte_init returns (-43) error code when running from a
 container
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_110_64384600.1503066278162"

------=_Part_110_64384600.1503066278162
Content-Type: multipart/alternative; 
	boundary="----=_Part_111_1499890730.1503066278162"

------=_Part_111_1499890730.1503066278162
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

I experienced the very same problem described above, with the very same=20
tutorial test. We are also running openmpi 2.1.0 on our cluster.
The same example *without* container works perfectly fine.

Any help would also be appreciated.

Bests,

Balazs

Le mercredi 16 ao=C3=BBt 2017 15:44:09 UTC+2, Peleg Bar Sapir a =C3=A9crit =
:
>
> Hello,
>
> I'm a student helper in a HPC service, and was asked to check out=20
> Singularity and help decide if we provided to our users. We run openmpi=
=20
> 2.1.0 on our cluster.
>
> Following this <http://singularity.lbl.gov/docs-hpc> tutorial I was able=
=20
> to form a container and use it on our cluster, but running=20
>
>  mpirun -np 20 singularity exec centos7_mpi_test.img /usr/bin/ring
> generates an error (I copied it to the end of this message).
>
> I tried to look for a clue about what is wrong, but couldn't find any use=
ful help. I would appreciate any help provided.
>
> *Error message*:
> =20
> -------------------------------------------------------------------------=
-
> It looks like MPI_INIT failed for some reason; your parallel process is
> likely to abort.  There are many reasons that a parallel process can
> fail during MPI_INIT; some of which are due to configuration or environme=
nt
> problems.  This failure appears to be an internal failure; here's some
> additional information (which may only be relevant to an Open MPI
> developer):
>
>   ompi_mpi_init: ompi_rte_init failed
>   --> Returned "(null)" (-43) instead of "Success" (0)
> -------------------------------------------------------------------------=
-
> -------------------------------------------------------------------------=
-
> It looks like MPI_INIT failed for some reason; your parallel process is
> likely to abort.  There are many reasons that a parallel process can
> fail during MPI_INIT; some of which are due to configuration or environme=
nt
> problems.  This failure appears to be an internal failure; here's some
> additional information (which may only be relevant to an Open MPI
> developer):
>
>   ompi_mpi_init: ompi_rte_init failed
>   --> Returned "(null)" (-43) instead of "Success" (0)
> -------------------------------------------------------------------------=
-
> *** An error occurred in MPI_Init
> *** on a NULL communicator
> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
> ***    and potentially your MPI job)
> *** An error occurred in MPI_Init
> *** on a NULL communicator
> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
> ***    and potentially your MPI job)
> [(server name):6349] Local abort before MPI_INIT completed completed succ=
essfully, but am not able to aggregate error messages, and not able to guar=
antee that all other processes were killed!
> [(server name):6347] Local abort before MPI_INIT completed completed succ=
essfully, but am not able to aggregate error messages, and not able to guar=
antee that all other processes were killed!
> -------------------------------------------------------
> Primary job  terminated normally, but 1 process returned
> a non-zero exit code.. Per user-direction, the job has been aborted.
> -------------------------------------------------------
> -------------------------------------------------------------------------=
-
> mpirun detected that one or more processes exited with non-zero status, t=
hus causing
> the job to be terminated. The first process to do so was:
>
>   Process name: [[12088,1],0]
>   Exit code:    1
> -------------------------------------------------------------------------=
-
>
>
------=_Part_111_1499890730.1503066278162
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi,<div><br></div><div>I experienced the very same problem=
 described above, with the very same tutorial test. We are also running ope=
nmpi 2.1.0 on our cluster.</div><div>The same example *without* container w=
orks perfectly fine.</div><div><br></div><div>Any help would also be apprec=
iated.</div><div><br></div><div>Bests,</div><div><br></div><div>Balazs<br><=
br>Le mercredi 16 ao=C3=BBt 2017 15:44:09 UTC+2, Peleg Bar Sapir a =C3=A9cr=
it=C2=A0:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left: =
0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">Hell=
o,<br><br>I&#39;m a student helper in a HPC service, and was asked to check=
 out Singularity and help decide if we provided to our users. We run openmp=
i 2.1.0 on our cluster.<br><br>Following <a href=3D"http://singularity.lbl.=
gov/docs-hpc" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=
=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2Fd=
ocs-hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNG49Dg5CcQfD0NvgYVHqlE6MDAtR=
w&#39;;return true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?=
q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2Fdocs-hpc\x26sa\x3dD\x26sntz\x3d1\x2=
6usg\x3dAFQjCNG49Dg5CcQfD0NvgYVHqlE6MDAtRw&#39;;return true;">this</a> tuto=
rial I was able to form a container and use it on our cluster, but running =
<br><pre><code><span> </span>mpirun -np 20 singularity <span>exec</span> ce=
ntos7_mpi_test.img /usr/bin/ring<br>generates an error (I copied it to the =
end of this message).<br><br></code><code>I tried</code><code> to look for =
a clue about what is wrong, but couldn&#39;t find any useful help. I would =
appreciate any help provided.<br><br><u>Error message</u>:<br>=C2=A0<br></c=
ode><code>------------------------------<wbr>------------------------------=
<wbr>--------------</code><br><code>It looks like MPI_INIT failed for some =
reason; your parallel process is</code><br><code>likely to abort.  There ar=
e many reasons that a parallel process can</code><br><code>fail during MPI_=
INIT; some of which are due to configuration or environment</code><br><code=
>problems.  This failure appears to be an internal failure; here&#39;s some=
</code><br><code>additional information (which may only be relevant to an O=
pen MPI</code><br><code>developer):</code><br><code></code><br><code>  ompi=
_mpi_init: ompi_rte_init failed</code><br><code>  --&gt; Returned &quot;(nu=
ll)&quot; (-43) instead of &quot;Success&quot; (0)</code><br><code>--------=
----------------------<wbr>------------------------------<wbr>-------------=
-</code><br><code>------------------------------<wbr>----------------------=
--------<wbr>--------------</code><br><code>It looks like MPI_INIT failed f=
or some reason; your parallel process is</code><br><code>likely to abort.  =
There are many reasons that a parallel process can</code><br><code>fail dur=
ing MPI_INIT; some of which are due to configuration or environment</code><=
br><code>problems.  This failure appears to be an internal failure; here&#3=
9;s some</code><br><code>additional information (which may only be relevant=
 to an Open MPI</code><br><code>developer):</code><br><code></code><br><cod=
e>  ompi_mpi_init: ompi_rte_init failed</code><br><code>  --&gt; Returned &=
quot;(null)&quot; (-43) instead of &quot;Success&quot; (0)</code><br><code>=
------------------------------<wbr>------------------------------<wbr>-----=
---------</code><br><code>*** An error occurred in MPI_Init</code><br><code=
>*** on a NULL communicator</code><br><code>*** MPI_ERRORS_ARE_FATAL (proce=
sses in this communicator will now abort,</code><br><code>***    and potent=
ially your MPI job)</code><br><code>*** An error occurred in MPI_Init</code=
><br><code>*** on a NULL communicator</code><br><code>*** MPI_ERRORS_ARE_FA=
TAL (processes in this communicator will now abort,</code><br><code>***    =
and potentially your MPI job)</code><br><code>[</code><code><code>(server n=
ame)</code>:6349] Local abort before MPI_INIT completed completed successfu=
lly, but am not able to aggregate error messages, and not able to guarantee=
 that all other processes were killed!<br>[</code><code><code>(server name)=
</code>:6347] Local abort before MPI_INIT completed completed successfully,=
 but am not able to aggregate error messages, and not able to guarantee tha=
t all other processes were killed!<br>------------------------------<wbr>--=
-----------------------</code><br><code>Primary job  terminated normally, b=
ut 1 process returned</code><br><code>a non-zero exit code.. Per user-direc=
tion, the job has been aborted.</code><br><code>---------------------------=
---<wbr>-------------------------</code><br><code>-------------------------=
-----<wbr>------------------------------<wbr>--------------</code><br><code=
>mpirun detected that one or more processes exited with non-zero status, th=
us causing</code><br><code>the job to be terminated. The first process to d=
o so was:</code><br><code></code><br><code>  Process name: [[12088,1],0]</c=
ode><br><code>  Exit code:    1</code><br><code>---------------------------=
---<wbr>------------------------------<wbr>--------------<br></code><code><=
/code></pre></div></blockquote></div></div>
------=_Part_111_1499890730.1503066278162--

------=_Part_110_64384600.1503066278162--
