Date: Thu, 12 Oct 2017 11:41:08 -0700 (PDT)
From: Matt Kijowski <matthew...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <43e70f0f-e4c3-4876-aff5-ed8255ebbae4@lbl.gov>
Subject: Beginner Singularity question (StarCCM++)
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_15614_1141722903.1507833668990"

------=_Part_15614_1141722903.1507833668990
Content-Type: multipart/alternative; 
	boundary="----=_Part_15615_2041351784.1507833668990"

------=_Part_15615_2041351784.1507833668990
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hello all, we're just starting to use Singularity on our HPC system in 
conjunction with Slurm and I am having some difficulties understanding how 
to set up one of our containers (but I might be setting things up wrong).

So I understand why we would use mpirun outside of a container, but what is 
a good configuration for a program (StarCCM++) that wraps its own MPI and 
remote commands into its execution?

Example.  I have a 64 node cluster and starccm++ installed into a 
container.  If I want to run my job inside the container I execute: 
*srun singularity exec starccm.img starccm+ options simfile*where srun 
requests a compute node from slurm and executes the rest of the command on 
that compute node.  The problem I have is with parallel jobs.  starccm+ 
wants to start its processes on the compute nodes itself (I can specify 
what sort of remote shell to use and which compute nodes I have requested 
via a machine list file in the options) but starccm+ is not aware that it 
needs to launch a singularity container on the compute nodes first.

My first thought is I need to launch the containers on the compute nodes 
before executing the starccm++ parallel job so that when it uses SSH to log 
in to the compute nodes the container is already running but I'm not sure 
on how to accomplish this.  Any thoughts?

------=_Part_15615_2041351784.1507833668990
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello all, we&#39;re just starting to use Singularity on o=
ur HPC system in conjunction with Slurm and I am having some difficulties u=
nderstanding how to set up one of our containers (but I might be setting th=
ings up wrong).<br><br>So I understand why we would use mpirun outside of a=
 container, but what is a good configuration for a program (StarCCM++) that=
 wraps its own MPI and remote commands into its execution?<br><br>Example.=
=C2=A0 I have a 64 node cluster and starccm++ installed into a container.=
=C2=A0 If I want to run my job inside the container I execute: <i>srun sing=
ularity exec starccm.img starccm+ options simfile<br></i>where srun request=
s a compute node from slurm and executes the rest of the command on that co=
mpute node.=C2=A0 The problem I have is with parallel jobs.=C2=A0 starccm+ =
wants to start its processes on the compute nodes itself (I can specify wha=
t sort of remote shell to use and which compute nodes I have requested via =
a machine list file in the options) but starccm+ is not aware that it needs=
 to launch a singularity container on the compute nodes first.<br><br>My fi=
rst thought is I need to launch the containers on the compute nodes before =
executing the starccm++ parallel job so that when it uses SSH to log in to =
the compute nodes the container is already running but I&#39;m not sure on =
how to accomplish this.=C2=A0 Any thoughts?<br><i></i></div>
------=_Part_15615_2041351784.1507833668990--

------=_Part_15614_1141722903.1507833668990--
