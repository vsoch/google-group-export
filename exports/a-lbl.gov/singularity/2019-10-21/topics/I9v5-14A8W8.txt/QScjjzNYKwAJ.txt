X-Received: by 10.66.76.10 with SMTP id g10mr21738410paw.20.1463406039665;
        Mon, 16 May 2016 06:40:39 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.36.69 with SMTP id k66ls1109348iok.22.gmail; Mon, 16 May
 2016 06:40:39 -0700 (PDT)
X-Received: by 10.66.253.194 with SMTP id ac2mr44564379pad.55.1463406039155;
        Mon, 16 May 2016 06:40:39 -0700 (PDT)
Return-Path: <shapov...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id 21si45841586pfi.15.2016.05.16.06.40.39
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 May 2016 06:40:39 -0700 (PDT)
Received-SPF: pass (google.com: domain of shapov...@gmail.com designates 74.125.82.50 as permitted sender) client-ip=74.125.82.50;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of shapov...@gmail.com designates 74.125.82.50 as permitted sender) smtp.mailfrom=shapov...@gmail.com
X-Ironport-SBRS: 4.3
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EhAQAOzTlXjzJSfUpdgmyCHga0bIZ7hhECgRsHOxEBAQEBAQEBAw8BAQEBBwsLCSEvgjc5EFUCK0ABAQEDARIRHQEbHgMMBgULDSoCAiEBAREBBQEcGSKHcgEDDwgFoFyBMT4xizuBaoJYBYcgChknDVKDVSkCBhCJX4EDgkOBaIMUglkFjk+EYYRGMYwlgXmBaYRPiGGHXIYnEh6BDjaCJR6BWDkyhkmBPQEBAQ
X-IronPort-AV: E=Sophos;i="5.24,627,1455004800"; 
   d="scan'208,217";a="24113692"
Received: from mail-wm0-f50.google.com ([74.125.82.50])
  by fe3.lbl.gov with ESMTP; 16 May 2016 06:40:37 -0700
Received: by mail-wm0-f50.google.com with SMTP id a17so136804153wme.0
        for <singu...@lbl.gov>; Mon, 16 May 2016 06:40:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=vO1uDmqjzP+2NZtkjDwRHVtRLFi2gAEusRQ8F/z4vsM=;
        b=sBu5yqMxYx5HC2HeZOLTx1GnZ5IWq22iK3vXem8ELHCi9ilApf3fdBU/c4EIxt4cnT
         GZkWyyVHovW6QbKQexdof8ugt20o5QySoMg1m6JfmJ/5rOsWTQWP2ALm4xlUy7b10EiB
         btUQIs0aV7Sg1N4vN4KPEKs7BzkLn3MLF+wvvpPEEnzFETiYH+dPe+eMV5ghgdWY2gO5
         mWLFheLCHfqtwN376unwNF9zhoMbQNnnDMJ560DorcpUh4K423tEYQ8DhLOXW5BvmgO9
         4xsiJNz0svUSJwZYyZ+ytdAPUWM17MFR8Y5vQ+ETq2dfpB4Eb27xDtSYIO9RSsX19gXP
         Si0w==
X-Gm-Message-State: AOPr4FUSZjcPGvXMXmXCM0EGieixE+NjEDIYkwWle7BiuVwQzohQ0C1R+W2GTeaKFaDPpVKxomI2MpjmTeBdfA==
X-Received: by 10.28.31.73 with SMTP id f70mr19054902wmf.77.1463406036964;
 Mon, 16 May 2016 06:40:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.37.230 with HTTP; Mon, 16 May 2016 06:39:57 -0700 (PDT)
In-Reply-To: <46A137E8-DB00-4564-A4CA-AC1B9FE62784@open-mpi.org>
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov> <CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com>
 <CAAS-_CBb3sy393W1Bga5Wnr9-EvSHC6NPNaOx58Mpdw-LiFq8g@mail.gmail.com>
 <414C0039-42A3-4C2E-89F6-3D97D082C742@open-mpi.org> <CAN7etTxQkXe6uEDZgt+kkAex0Hzt9UWYE6+V4qS4vnKJAMgFzQ@mail.gmail.com>
 <CAAS-_CBsKM3=d_OhVknqcF2De29UiQ-cjOv5imCyR=jv=4Rh7g@mail.gmail.com>
 <CAN7etTxJkbqv3mdujx0JziZo7y9fTrRkk3eE0GpXnkmKhpTc=g@mail.gmail.com> <46A137E8-DB00-4564-A4CA-AC1B9FE62784@open-mpi.org>
From: Taras Shapovalov <shapov...@gmail.com>
Date: Mon, 16 May 2016 16:39:57 +0300
Message-ID: <CAAS-_CA7JGbAuGJCwM4iQbg6ObfVQXrB_1uNLNY780T5+kuTjA@mail.gmail.com>
Subject: Re: [Singularity] SIngularity and MPI implementations
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a114b415cfedb480532f5c4d2

--001a114b415cfedb480532f5c4d2
Content-Type: text/plain; charset=UTF-8

Thank you guys,

You've really helped to understand how it works with MPI!
No more questions, at least for now.

Best regards,

Taras

On Mon, May 16, 2016 at 4:05 PM, Ralph Castain <r...@open-mpi.org> wrote:

>
> On May 16, 2016, at 5:54 AM, Gregory M. Kurtzer <gmku...@lbl.gov> wrote:
>
>
>
> On Mon, May 16, 2016 at 1:17 AM, Taras Shapovalov <shapov...@gmail.com>
>  wrote:
>
>> Hi guys,
>>
>> Thanks for the great answers! Now it is more or less clear how it works.
>> To be absolutely sure, can you please confirm also these statements (got
>> from your answers):
>>
>> 1. Ralph's answer mentions mpiexec, but Gregory's answer is about mpirun.
>> So, all the discussed here can be applied to the both utilities included in
>> Open MPI distribution.
>>
>
> Ralph can speak definitively here, but I believe my answer applies to both.
>
>
> The two names are for the identical binary - in the MPI world, folks use
> both names interchangeably
>
>
>
>>
>> 2. Running Open MPI processes in a single container is impleneted only in
>> Singularity v2. In v1 each Open MPI process still will be executed in
>> different containers.
>>
>
> For technical Q&A we should probably use the word namespaces in addition
> to containers, I'll explain.
>
> Singularity v1 will cache the container on each node, so processes within
> a node will share the container cache but operate in some different
> namespaces (the specific namespaces are somewhat application/necessity
> dependent).
>
> Singularity v2 has no need to cache the container, but it does need to
> bind it to a loop device. This happens once per node, but again there is no
> cache so all nodes are sharing the same container image and also operate in
> some separate namespaces (again dependent on need).
>
>
>>
>> 3. Lets compare these 2 scenarios: Singularity runs child processes in a
>> single container agains scenario when each child runs in a separate
>> container each. The optimization with dlopen call happens in the first
>> scenario, because the opened library is loaded into the memory per
>> Singularity container, then dlopen magically returns the same handler for
>> each child process inside the container, which should be faster. Or there
>> is some other low level optimization occurs in the first scenario regarding
>> dlopen?
>>
>
> I am not sure I follow completely, but if you are asking what I think
> you're asking... Singularity v2 will optimize all calls to open()
> (including dlopen()) within the container because what is within the
> container all exist within a single image (there is no need to make
> additional metadata requests to files that exist within the container
> image). Additionally there is no launch penalty taken because there is no
> need to cache the image. On average, launch time when using this method is
> about .020s on my test system and writes/changes never require a rebuild.
>
> With Singularity v1, files are pulled out of the container archive (SAPP)
> and spilled out to the storage. If the storage is local to nodes, then
> calls to open() and thus the required metadata will not goto shared
> storage. By default the container is cached to shared storage (unless
> launching a SAPP file directly through Open MPI). Launch time for v1 is
> about .050s after the image has been cached, and caching of the image
> usually takes anywhere from .5s to as high as you want to go depending on
> image size (I've seen in my testing upwards of 10 seconds).
>
> Hopefully that helps!
>
>

--001a114b415cfedb480532f5c4d2
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div>Thank you guys,<br><br></div>You&#39;ve rea=
lly helped to understand how it works with MPI!<br>No more questions, at le=
ast for now.<br><br></div>Best regards,<br><br></div>Taras<br><div class=3D=
"gmail_extra"><br><div class=3D"gmail_quote">On Mon, May 16, 2016 at 4:05 P=
M, Ralph Castain <span dir=3D"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org"=
 target=3D"_blank">r...@open-mpi.org</a>&gt;</span> wrote:<br><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;=
padding-left:1ex"><div style=3D"word-wrap:break-word"><br><div><span><block=
quote type=3D"cite"><div>On May 16, 2016, at 5:54 AM, Gregory M. Kurtzer &l=
t;<a href=3D"mailto:gmku...@lbl.gov" target=3D"_blank">gmku...@lbl.gov</a>&=
gt; wrote:</div><br><div><div dir=3D"ltr" style=3D"font-family:Helvetica;fo=
nt-size:12px;font-style:normal;font-weight:normal;letter-spacing:normal;tex=
t-align:start;text-indent:0px;text-transform:none;white-space:normal;word-s=
pacing:0px"><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">O=
n Mon, May 16, 2016 at 1:17 AM, Taras Shapovalov<span>=C2=A0</span><span di=
r=3D"ltr">&lt;<a href=3D"mailto:shapov...@gmail.com" target=3D"_blank">shap=
ov...@gmail.com</a>&gt;</span><span>=C2=A0</span>wrote:<br><blockquote clas=
s=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;b=
order-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"=
><div dir=3D"ltr"><div><div><div><div><div>Hi guys,<br><br></div>Thanks for=
 the great answers! Now it is more or less clear how it works. To be absolu=
tely sure, can you please confirm also these statements (got from your answ=
ers):<br><br></div><div>1. Ralph&#39;s answer mentions mpiexec, but Gregory=
&#39;s answer is about mpirun. So, all the discussed here can be applied to=
 the both utilities included in Open MPI distribution.<br></div></div></div=
></div></div></blockquote><div><br></div><div>Ralph can speak definitively =
here, but I believe my answer applies to both.</div></div></div></div></div=
></blockquote><div><br></div></span>The two names are for the identical bin=
ary - in the MPI world, folks use both names interchangeably</div><div><div=
><div><br><blockquote type=3D"cite"><div><div dir=3D"ltr" style=3D"font-fam=
ily:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-sp=
acing:normal;text-align:start;text-indent:0px;text-transform:none;white-spa=
ce:normal;word-spacing:0px"><div class=3D"gmail_extra"><div class=3D"gmail_=
quote"><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0=
px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);b=
order-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><div><div><div><d=
iv></div><div><br></div>2. Running Open MPI processes in a single container=
 is impleneted only in Singularity v2. In v1 each Open MPI process still wi=
ll be executed in different containers.<br></div></div></div></div></blockq=
uote><div><br></div><div>For technical Q&amp;A we should probably use the w=
ord namespaces in addition to containers, I&#39;ll explain.</div><div><br><=
/div><div>Singularity v1 will cache the container on each node, so processe=
s within a node will share the container cache but operate in some differen=
t namespaces (the specific namespaces are somewhat application/necessity de=
pendent).</div><div><br></div><div>Singularity v2 has no need to cache the =
container, but it does need to bind it to a loop device. This happens once =
per node, but again there is no cache so all nodes are sharing the same con=
tainer image and also operate in some separate namespaces (again dependent =
on need).</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"=
margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,20=
4,204);border-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><div><div=
><div><br></div>3. Lets compare these 2 scenarios: Singularity runs child p=
rocesses in a single container agains scenario when each child runs in a se=
parate container each. The optimization with dlopen call happens in the fir=
st scenario, because the opened library is loaded into the memory per Singu=
larity container, then dlopen magically returns the same handler for each c=
hild process inside the container, which should be faster. Or there is some=
 other low level optimization occurs in the first scenario regarding dlopen=
?<br></div></div></div></blockquote><div><br></div><div>I am not sure I fol=
low completely, but if you are asking what I think you&#39;re asking... Sin=
gularity v2 will optimize all calls to open() (including dlopen()) within t=
he container because what is within the container all exist within a single=
 image (there is no need to make additional metadata requests to files that=
 exist within the container image). Additionally there is no launch penalty=
 taken because there is no need to cache the image. On average, launch time=
 when using this method is about .020s on my test system and writes/changes=
 never require a rebuild.<br></div><div><br></div><div>With Singularity v1,=
 files are pulled out of the container archive (SAPP) and spilled out to th=
e storage. If the storage is local to nodes, then calls to open() and thus =
the required metadata will not goto shared storage. By default the containe=
r is cached to shared storage (unless launching a SAPP file directly throug=
h Open MPI). Launch time for v1 is about .050s after the image has been cac=
hed, and caching of the image usually takes anywhere from .5s to as high as=
 you want to go depending on image size (I&#39;ve seen in my testing upward=
s of 10 seconds).</div><div><br></div><div>Hopefully that helps!</div><div>=
<br></div></div></div></div></div></blockquote></div></div></div></div></bl=
ockquote></div><br></div></div>

--001a114b415cfedb480532f5c4d2--
