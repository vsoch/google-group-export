Date: Mon, 18 Feb 2019 08:33:02 -0800 (PST)
From: Justin Cook <jus...@sylabs.io>
To: singularity <singu...@lbl.gov>
Message-Id: <5ca92c63-5ef0-4074-b14c-7dba51726cce@lbl.gov>
In-Reply-To: <bb9e3d9f-c17b-4a07-8733-3b5dc4a5ecd3@lbl.gov>
References: <71fec772-2642-4b6e-98a3-8801b2ae0cf7@lbl.gov> <59709108-447E-491F-BB5A-C04D3F5AB654@gmail.com>
 <7A55B410-CD7D-4360-A1F3-44E1EF1ECA1B@gmail.com> <cbd878fc-f1a6-4548-8dd2-3ac380da2ecd@lbl.gov>
 <CAJk3+YVedLNPT6=hxrQtAR6xJG3NtERLRFRBCO3M4JL6YXS9Yg@mail.gmail.com>
 <2d3a29da-6518-4643-aef6-6664a21b2dc3@lbl.gov> <8a6e555b-8763-469f-aa4d-6f1b07521f93@lbl.gov>
 <CAJk3+YV6_KmOJoo+7gJ50izAxe_zAYQWhvrWxR8+5GfhGskcNw@mail.gmail.com>
 <bb9e3d9f-c17b-4a07-8733-3b5dc4a5ecd3@lbl.gov>
Subject: Re: [Singularity] Error when running OpenFoam over Singularity
 using Slurm
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_680_569452469.1550507582843"

------=_Part_680_569452469.1550507582843
Content-Type: multipart/alternative; 
	boundary="----=_Part_681_889960819.1550507582844"

------=_Part_681_889960819.1550507582844
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Carmen,

It looks like this has a history going now. Can we move this to a github=20
issue?

Can you show us the definition of your hello world container that is not=20
working? Let's start there and see where we get.

Thanks,

Justin

On Wednesday, February 13, 2019 at 2:16:36 AM UTC-6, ccvera wrote:
>
> I was doing more tests and nothings works, I'm a little desperate with=20
> this problem... :(
>
> Thanks Samy for your answer but my problem exists with on single node as=
=20
> well as 4 nodes (64 cores). Running:
>
> singularity exec -B /home ../../of6/openfoam6.MPIx.img mpiexec -n 16=20
> simpleFoam -case /home/carmen/test_singularity/OpenFOAM/pruebaOF5_16cores
>
>
> I can work on 16 cores correctly, but I understand that I am running the=
=20
> container's OMPI, likewise, I want to launch it on 64 cores for performan=
ce=20
> and for that (I understand) that I should launch it like this (as I have=
=20
> done with SGE):
>
> mpirun -n $SLURM_NTASKS singularity exec ../../of6/openfoam6.x.img=20
> simpleFoam -parallel -case=20
> /home/carmen/test_singularity/OpenFOAM/pruebaOF5_16cores
>
>
> And it doesn't work.
>
> Fatih :)
>
> xx True
> xx Not at all. I can run basic programs in parallel using slurm without=
=20
> singularity
> xx True. Only fails with Slurm and Singularity
>
> I've tried so many combinations and cases, so I'm sure I'm forgetting som=
e=20
> of them.
>
> Background:
> - I was working with SGE and everything worked perfectly (recently the=20
> cluster in which I work has migrated to Slurm, -is a supercomputing cente=
r,=20
> not a particular mini-cluster-)
> - This same container worked with up to 64 cores without problems.
>
> Some tests:
> - Connecting directly to a node by ssh, OF works correctly with 16 cores=
=20
> (w/ 32 too, but having only 16 cores creates oversubscription)
> - Doing salloc -N1 -n16 (or salloc -N2 -n32), and then run the program,=
=20
> fails.
> - Both with 16 cores and with 32, executing my script (sbatch=20
> myscript.sh), fails.
> - A simple "hello world" with Slurm doesn't work either, e.g. running:
> mpirun -n $SLURM_NTASKS singularity exec -B /home container.img=20
> /home/carmen/test_singularity/mpi_slurm/hello_world
>
> I attach the recipe of the last container that I created.
>
> Thanx,
>
> Carmen
> ----
> El s=C3=A1bado, 9 de febrero de 2019, 5:19:16 (UTC+1), Fatih Ertinaz escr=
ibi=C3=B3:
>>
>> Ok, this is really interesting to me. So as a summary -- if I'm not=20
>> mistaken:
>>
>> xx You can run parallel OF on a single node using OMPI through=20
>> Singularity without Slurm=20
>> xx You can run basic parallel MPI tasks using Slurm with and without=20
>> Singularity
>> xx You cannot run multi-node basic parallel jobs using Singularity -- I=
=20
>> don't know if you used Slurm, so maybe fails with both
>>
>> Should Slurm have a special configuration to run mpi programs in paralle=
l=20
>>> with singularity, apart from OpenMPI and PMIx?
>>
>>
>> I don't think Slurm is the problematic part because of the item 2 above.=
=20
>> For sure Slurm needs information about compute nodes and user account, b=
ut=20
>> if that was the issue you wouldn't be able to run any tasks even on a=20
>> single node. With that being said, I never configured Slurm from scratch=
,=20
>> so I am not a Slurm expert.
>>
>> Also, I don't think this is an OF issue because of the item 1. However=
=20
>> you might want to make sure if OF is built with "SYSTEMOPENMPI" option. =
But=20
>> again, even though restricted to 1 node, you managed to run it without=
=20
>> Slurm so OF should be fine imho.=20
>>
>> If you cannot run multi-node jobs, I guess that's a clear indication of =
a=20
>> potential OMPI problem. Check how OMPI is installed, which fabrics are=
=20
>> being used etc. Additionally, you can also check if Slurm flag is=20
>> explicitly defined, sth. like "--with-slurm=3D/opt/slurm".=20
>>
>> Moreover can you give some information about the cluster you're working=
=20
>> on? I mean, is this a typical cluster with many users running their=20
>> simulations? If that's the case, then I think Slurm or OMPI should be qu=
ite=20
>> reliable. If this is a cloud cluster that you're experimenting, I bet it=
 is=20
>> OMPI :)
>>
>> Hope this helps=20
>>
>> On Fri, Feb 8, 2019 at 2:51 PM Samy <sma...@gmail.com> wrote:
>>
>>> Can you try to run the same command on single node and see? like: mpiru=
n=20
>>> -n *1* singularity exec .....
>>> Also, if you have access to interactive mode nodes, it would be a good=
=20
>>> test to run OF with mpirun interactively on 2 or more nodes. It sounds =
to=20
>>> me that it's an issue running your OF on multinode not a slurm problem.
>>>
>>> Good luck,
>>>
>>>
>>> On Friday, February 8, 2019 at 12:09:26 AM UTC-8, ccvera wrote:
>>>>
>>>> Hi!
>>>>
>>>> I didn't say it in my first post (sorry) but, in case it serves as=20
>>>> information, the problem appears only when I execute OF in parallel (u=
sing=20
>>>> the -parallel option, that is what I need).
>>>>
>>>> Regarding the options you mention to me, Fatih:
>>>> xx I don`t have problems executing simple works (and even some more=20
>>>> complicated) e.g. variable printing and information (all without=20
>>>> singularity). Also, I run singularity basic programs and, normally, I =
use=20
>>>> then to train CNN (no need MPI) and all work fine.
>>>> xx I have replicated the OpenMPI and PMIx host installation in the=20
>>>> container, so they have the same versions and libraries were copied.=
=20
>>>>
>>>> In the logs, both slurmd and slurmctl or the nodes logs I'm not seeing=
=20
>>>> nothing that gives me light.=20
>>>> I think you're right when you tell me that it can be an openmpi=20
>>>> problem. I'm trying again to execute a "hello world" on singularity an=
d=20
>>>> when requesting several nodes I have the same problems :(
>>>>
>>>> Should Slurm have a special configuration to run mpi programs in=20
>>>> parallel with singularity, apart from OpenMPI and PMIx? Also, should I=
=20
>>>> include other configuration in my container?=20
>>>>
>>>> Thax for your help.
>>>>
>>>> Carmen.
>>>>
>>>> El mi=C3=A9rcoles, 6 de febrero de 2019, 15:33:36 (UTC+1), Fatih Ertin=
az=20
>>>> escribi=C3=B3:
>>>>>
>>>>> Hello Carmen
>>>>>
>>>>> To me this looks like an OpenMPI & Slurm issue rather than an OF &=20
>>>>> Slurm problem.=20
>>>>>
>>>>> Few things you can check;
>>>>> xx Try to execute simple jobs using Slurm, e.g. printing hostnames or=
=20
>>>>> mpi ping-pong stuff.=20
>>>>> xx Do you know how OpenMPI is installed in the host? Maybe it is buil=
t=20
>>>>> with some other underlying libraries regarding IB that you don't have=
 in=20
>>>>> your container.
>>>>>
>>>>> I'd say if the first one works with hostnames then I'd say focus on=
=20
>>>>> the OpenMPI installation.
>>>>>
>>>>> On Wed, Feb 6, 2019 at 4:44 AM 'ccvera' via singularity <
>>>>> si...@lbl.gov> wrote:
>>>>>
>>>>>> Thanks a lot for your quickly reply :)
>>>>>>
>>>>>> This solution doesn't work for me. I tried to unset all SLURM=20
>>>>>> environment variables (first SLURM_JOBID, then SLURM_NODELIST and fi=
nally=20
>>>>>> all as you told me) and i obtain the same MPI error.=20
>>>>>>
>>>>>> Carmen
>>>>>>
>>>>>> El martes, 5 de febrero de 2019, 17:56:27 (UTC+1), Shenglong Wang=20
>>>>>> escribi=C3=B3:
>>>>>>>
>>>>>>> It seems=20
>>>>>>>
>>>>>>> unset SLURM_JOBID
>>>>>>>
>>>>>>> is enough to cheat mpiexec
>>>>>>>
>>>>>>> Shenglong
>>>>>>>
>>>>>>> On Feb 5, 2019, at 11:37 AM, Shenglong Wang <wa...@gmail.com>=20
>>>>>>> wrote:
>>>>>>>
>>>>>>> Can you try to unset all SLURM environment variables?
>>>>>>>
>>>>>>> for e in $(env | egrep ^SLURM_ | cut -d=3D -f1); do unset $e; done
>>>>>>>
>>>>>>> or
>>>>>>>
>>>>>>> unset SLURM_NODELIST
>>>>>>>
>>>>>>> But you=E2=80=99ll have to manually generate host file.
>>>>>>>
>>>>>>> Best,
>>>>>>> Shenglong
>>>>>>>
>>>>>>>
>>>>>>> On Feb 5, 2019, at 11:30 AM, 'ccvera' via singularity <
>>>>>>> si...@lbl.gov> wrote:
>>>>>>>
>>>>>>> Dear all,
>>>>>>>
>>>>>>> I'm experiencing some issues running OpenFOAM over singularity with=
=20
>>>>>>> slurm.=20
>>>>>>>
>>>>>>> I've several images based on Ubuntu and within several versions of=
=20
>>>>>>> OpenMPI and PMIx and i'm able to run OpenFOAM correctly without use=
 slurm=20
>>>>>>> (directly on the node) using next command:
>>>>>>>
>>>>>>> $ mpirun -n 16 singularity exec -B /home ../../of6/openfoam6.x.img=
=20
>>>>>>> simpleFoam -parallel -case=20
>>>>>>> /home/carmen/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/prue=
baOF6_16cores
>>>>>>>
>>>>>>> My problem comes when I run my program with slurm. Whether I make=
=20
>>>>>>> salloc or execute a script with sbatch, it shows me the following e=
rror:
>>>>>>>
>>>>>>> It looks like MPI_INIT failed for some reason; your parallel proces=
s=20
>>>>>>> is
>>>>>>>
>>>>>>> likely to abort.  There are many reasons that a parallel process ca=
n
>>>>>>>
>>>>>>> fail during MPI_INIT; some of which are due to configuration or=20
>>>>>>> environment
>>>>>>>
>>>>>>> problems.  This failure appears to be an internal failure; here's=
=20
>>>>>>> some
>>>>>>>
>>>>>>> additional information (which may only be relevant to an Open MPI
>>>>>>>
>>>>>>> developer):
>>>>>>>
>>>>>>>
>>>>>>>   ompi_mpi_init: ompi_rte_init failed
>>>>>>>
>>>>>>>   --> Returned "(null)" (-43) instead of "Success" (0)
>>>>>>>
>>>>>>>
>>>>>>> and
>>>>>>>
>>>>>>> *** An error occurred in MPI_Init_thread
>>>>>>>
>>>>>>> *** on a NULL communicator
>>>>>>>
>>>>>>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now=
=20
>>>>>>> abort,
>>>>>>>
>>>>>>> ***    and potentially your MPI job)
>>>>>>>
>>>>>>> [cn3045:369] Local abort before MPI_INIT completed completed=20
>>>>>>> successfully, but am not able to aggregate error messages, and not =
able to=20
>>>>>>> guarantee that all other processes were killed!
>>>>>>>
>>>>>>>
>>>>>>> I know I must have the same openMPI versions on both (host and=20
>>>>>>> container) and I have also tried other versions of OpenMPI (2.X.X) =
and in=20
>>>>>>> all cases OpenFOAM works correctly, but at the moment I want to run=
 it with=20
>>>>>>> slurm it show me the errors.
>>>>>>>
>>>>>>> I have also tried other ways to run the program with srun using the=
=20
>>>>>>> option --mpi=3Dpmi2 (among others) but I always find the same probl=
em.
>>>>>>>
>>>>>>> I use the following script to run OpenFoam:
>>>>>>>
>>>>>>> ----------------------------
>>>>>>>
>>>>>>> #!/bin/bash
>>>>>>>
>>>>>>>
>>>>>>> #SBATCH -N 1
>>>>>>>
>>>>>>> #SBATCH -p haswell=20
>>>>>>>
>>>>>>> #SBATCH -J test_OpenFOAM
>>>>>>>
>>>>>>> #SBATCH --output=3D"singularity.%j.o"=20
>>>>>>>
>>>>>>> #SBATCH --error=3D"singularity.%j.e"
>>>>>>>
>>>>>>>
>>>>>>> module load haswell/singularity_2.6.0
>>>>>>>
>>>>>>> module load haswell/openmpi_3.1.2_gcc8.2.0_pmix
>>>>>>>
>>>>>>>
>>>>>>> ulimit -s unlimited
>>>>>>>
>>>>>>>
>>>>>>> mpirun -n 16 singularity exec ../../of6/openfoam6.x.img simpleFoam=
=20
>>>>>>> -parallel -case=20
>>>>>>> /home/software/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pr=
uebaOF6_16cores
>>>>>>>
>>>>>>> ----------------------------
>>>>>>>
>>>>>>>
>>>>>>> The versions that I'm using are:
>>>>>>>
>>>>>>> *Host: *
>>>>>>>
>>>>>>> OS: CentOS7.5
>>>>>>>
>>>>>>> OpenMPI: 3.1.2
>>>>>>>
>>>>>>> PMIx: 2.1.4
>>>>>>>
>>>>>>>
>>>>>>> *Container:*
>>>>>>>
>>>>>>> OS: Ubuntu16.04
>>>>>>>
>>>>>>> OpenMPI: 3.1.2
>>>>>>>
>>>>>>> PMIx: 2.1.4
>>>>>>>
>>>>>>>
>>>>>>> Can it be a configuration problem of SLURM? Is there any limitation=
=20
>>>>>>> of SLURM that is affecting OpenFOAM?
>>>>>>>
>>>>>>> Some info about slurm:
>>>>>>>
>>>>>>> # srun --version
>>>>>>> slurm 18.08.3
>>>>>>> # srun --mpi=3Dlist
>>>>>>> srun: MPI types are...
>>>>>>> srun: pmi2
>>>>>>> srun: openmpi
>>>>>>> srun: none
>>>>>>>
>>>>>>>
>>>>>>> I'm a little bit lost with this issue :(
>>>>>>> Can someone give me some lights?
>>>>>>>
>>>>>>> Thanks a lot in advance,
>>>>>>> Carmen
>>>>>>>
>>>>>>>
>>>>>>> --=20
>>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,=
=20
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --=20
>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,=20
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>> --=20
>>> You received this message because you are subscribed to the Google=20
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>> an email to singu...@lbl.gov.
>>>
>>
------=_Part_681_889960819.1550507582844
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>Carmen,</div><div><br></div><div>It looks like this h=
as a history going now. Can we move this to a github issue?</div><div><br><=
/div><div>Can you show us the definition of your hello world container that=
 is not working? Let&#39;s start there and see where we get.<br></div><div>=
<br></div><div>Thanks,</div><div><br></div><div>Justin<br></div><br>On Wedn=
esday, February 13, 2019 at 2:16:36 AM UTC-6, ccvera wrote:<blockquote clas=
s=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left: 1px #c=
cc solid;padding-left: 1ex;"><div dir=3D"ltr"><div>I was doing more tests a=
nd nothings works, I&#39;m a little desperate with this problem... :(</div>=
<div><br></div><div>Thanks Samy for your answer but my problem exists with =
on single node as well as 4 nodes (64 cores). Running:</div><div><br></div>=
<blockquote style=3D"margin:0 0 0 40px;border:none;padding:0px"><div>singul=
arity exec -B /home ../../of6/openfoam6.MPIx.img mpiexec -n 16 simpleFoam -=
case /home/carmen/test_singularity/<wbr>OpenFOAM/pruebaOF5_16cores</div></b=
lockquote><div><br></div><div>I can work on 16 cores correctly, but I under=
stand that I am running the container&#39;s OMPI, likewise, I want to launc=
h it on 64 cores for performance and for that (I understand) that I should =
launch it like this (as I have done with SGE):</div><div><br></div><blockqu=
ote style=3D"margin:0 0 0 40px;border:none;padding:0px"><div>mpirun -n $SLU=
RM_NTASKS singularity exec ../../of6/openfoam6.x.img simpleFoam -parallel -=
case /home/carmen/test_singularity/<wbr>OpenFOAM/pruebaOF5_16cores</div></b=
lockquote><div><br></div><div>And it doesn&#39;t work.</div><div><br></div>=
<div>Fatih :)</div><div><br></div><div>xx True</div><div>xx Not at all. I c=
an run basic programs in parallel using slurm without singularity</div><div=
>xx True. Only fails with Slurm and Singularity</div><div><br></div><div>I&=
#39;ve tried so many combinations and cases, so I&#39;m sure I&#39;m forget=
ting some of them.</div><div><br></div><div>Background:</div><div>- I was w=
orking with SGE and everything worked perfectly (recently the cluster in wh=
ich I work has migrated to Slurm, -is a supercomputing center, not a partic=
ular mini-cluster-)</div><div>- This same container worked with up to 64 co=
res without problems.</div><div><br></div><div>Some tests:</div><div>- Conn=
ecting directly to a node by ssh, OF works correctly with 16 cores (w/ 32 t=
oo, but having only 16 cores creates oversubscription)</div><div>- Doing sa=
lloc -N1 -n16 (or salloc -N2 -n32), and then run the program, fails.</div><=
div>- Both with 16 cores and with 32, executing my script (sbatch myscript.=
sh), fails.</div><div>- A simple &quot;hello world&quot; with Slurm doesn&#=
39;t work either, e.g. running:</div><div>mpirun -n $SLURM_NTASKS singulari=
ty exec -B /home container.img /home/carmen/test_singularity/<wbr>mpi_slurm=
/hello_world</div><div><br></div><div>I attach the recipe of the last conta=
iner that I created.</div><div><br></div><div>Thanx,</div><div><br></div><d=
iv>Carmen</div>----<br>El s=C3=A1bado, 9 de febrero de 2019, 5:19:16 (UTC+1=
), Fatih Ertinaz  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"=
margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><di=
v dir=3D"ltr"><div dir=3D"ltr">Ok, this is really interesting to me. So as =
a summary -- if I&#39;m not mistaken:<div><br><div>xx You can run parallel =
OF on a single node using OMPI through Singularity without Slurm=C2=A0</div=
><div>xx You can run basic parallel MPI tasks using Slurm with and without =
Singularity</div><div>xx You cannot run multi-node basic parallel jobs usin=
g Singularity -- I don&#39;t know if you used Slurm, so maybe fails with bo=
th</div><div><div><br></div></div><blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding=
-left:1ex">Should Slurm have a special configuration to run mpi programs in=
 parallel with singularity, apart from OpenMPI and PMIx?</blockquote><div><=
br></div><div>I don&#39;t think Slurm is the problematic part because of th=
e item 2 above. For sure Slurm needs information about compute nodes and us=
er account, but if that was the issue you wouldn&#39;t be able to run any t=
asks even on a single node. With that being said, I never configured Slurm =
from scratch, so I am not a Slurm expert.</div><div><br></div><div>Also, I =
don&#39;t think this is an OF issue because of the item 1. However you migh=
t want to make sure if OF is built with &quot;SYSTEMOPENMPI&quot; option. B=
ut again, even though restricted to 1 node, you managed to run it without S=
lurm so OF should be fine imho.=C2=A0</div><div><br></div><div>If you canno=
t run multi-node jobs, I guess that&#39;s a clear indication of a potential=
 OMPI problem. Check how OMPI is installed, which fabrics are being used et=
c. Additionally, you can also check if Slurm flag is explicitly defined, st=
h. like &quot;--with-slurm=3D/opt/slurm&quot;.=C2=A0</div><div><br></div><d=
iv>Moreover can you give some information about the cluster you&#39;re work=
ing on? I mean, is this a typical cluster with many users running their sim=
ulations? If that&#39;s the case, then I think Slurm or OMPI should be quit=
e reliable. If this is a cloud cluster that you&#39;re experimenting, I bet=
 it is OMPI :)</div><div><br></div><div>Hope this helps=C2=A0</div></div></=
div></div><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Fri, Feb 8, 20=
19 at 2:51 PM Samy &lt;<a rel=3D"nofollow">sma...@gmail.com</a>&gt; wrote:<=
br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8e=
x;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"=
>Can you try to run the same command on single node and see? like: mpirun -=
n <b>1</b> singularity exec .....<div>Also, if you have access to interacti=
ve mode nodes, it would be a good test to run OF with mpirun interactively =
on 2 or more nodes. It sounds to me that it&#39;s an issue running your OF =
on multinode not a slurm problem.</div><div><br></div><div>Good luck,</div>=
<div><br></div><div><br>On Friday, February 8, 2019 at 12:09:26 AM UTC-8, c=
cvera wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0=
.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"l=
tr"><div>Hi!</div><div><br></div><div>I didn&#39;t say it in my first post =
(sorry) but, in case it serves as information, the problem appears only whe=
n I execute OF in parallel (using the -parallel option, that is what I need=
).</div><div><br></div><div>Regarding the options you mention to me, Fatih:=
</div><div>xx I don`t have problems executing simple works (and even some m=
ore complicated) e.g. variable printing and information (all without singul=
arity). Also, I run singularity basic programs and, normally, I use then to=
 train CNN (no need MPI) and all work fine.</div><div>xx I have replicated =
the OpenMPI and PMIx host installation in the container, so they have the s=
ame versions and libraries were copied.=C2=A0</div><div><br></div><div>In t=
he logs, both slurmd and slurmctl or the nodes logs I&#39;m not seeing noth=
ing that gives me light.=C2=A0</div><div>I think you&#39;re right when you =
tell me that it can be an openmpi problem. I&#39;m trying again to execute =
a &quot;hello world&quot; on singularity and when requesting several nodes =
I have the same problems :(</div><div><br></div><div>Should Slurm have a sp=
ecial configuration to run mpi programs in parallel with singularity, apart=
 from OpenMPI and PMIx? Also, should I include other configuration in my co=
ntainer?=C2=A0</div><div><br></div><div>Thax for your help.</div><div><br><=
/div><div>Carmen.</div><br>El mi=C3=A9rcoles, 6 de febrero de 2019, 15:33:3=
6 (UTC+1), Fatih Ertinaz  escribi=C3=B3:<blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);pad=
ding-left:1ex"><div dir=3D"ltr">Hello Carmen<div><br></div><div>To me this =
looks like an OpenMPI &amp; Slurm issue rather than an OF &amp; Slurm probl=
em.=C2=A0</div><div><br></div><div>Few things you can check;</div><div>xx T=
ry to execute simple jobs using Slurm, e.g. printing hostnames or mpi ping-=
pong stuff.=C2=A0</div><div>xx Do you know how OpenMPI is installed in the =
host? Maybe it is built with some other underlying libraries regarding IB t=
hat you don&#39;t have in your container.</div><div><br></div><div>I&#39;d =
say if the first one works with hostnames then I&#39;d say focus on the Ope=
nMPI installation.</div></div><br><div class=3D"gmail_quote"><div dir=3D"lt=
r">On Wed, Feb 6, 2019 at 4:44 AM &#39;ccvera&#39; via singularity &lt;<a r=
el=3D"nofollow">si...@lbl.gov</a>&gt; wrote:<br></div><blockquote class=3D"=
gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(20=
4,204,204);padding-left:1ex"><div dir=3D"ltr"><div>Thanks a lot for your qu=
ickly reply :)</div><div><br></div><div>This solution doesn&#39;t work for =
me. I tried to unset all SLURM environment variables (first SLURM_JOBID, th=
en SLURM_NODELIST and finally all as you told me) and i obtain the same MPI=
 error.=C2=A0</div><div><br></div><div>Carmen</div><br>El martes, 5 de febr=
ero de 2019, 17:56:27 (UTC+1), Shenglong Wang  escribi=C3=B3:<blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid=
 rgb(204,204,204);padding-left:1ex"><div>It seems=C2=A0<div><br></div><div>=
unset SLURM_JOBID</div><div><br></div><div>is enough to cheat mpiexec</div>=
<div><br></div><div>Shenglong<br><div><br><blockquote type=3D"cite"><div>On=
 Feb 5, 2019, at 11:37 AM, Shenglong Wang &lt;<a rel=3D"nofollow">wa...@gma=
il.com</a>&gt; wrote:</div><br><div><div>Can you try to unset all SLURM env=
ironment variables?<div><br></div><div><div>for e in $(env | egrep ^SLURM_ =
| cut -d=3D -f1); do unset $e; done</div><div><br></div><div>or</div><div><=
br></div><div>unset SLURM_NODELIST</div><div><br></div><div>But you=E2=80=
=99ll have to manually generate host file.</div><div><br></div><div>Best,</=
div><div>Shenglong</div><div><br><div><br><blockquote type=3D"cite"><div>On=
 Feb 5, 2019, at 11:30 AM, &#39;ccvera&#39; via singularity &lt;<a rel=3D"n=
ofollow">si...@lbl.gov</a>&gt; wrote:</div><br><div><div dir=3D"ltr"><div><=
div>Dear all,</div><div><br></div><div>I&#39;m experiencing some issues run=
ning OpenFOAM over singularity with slurm.=C2=A0</div><div><br></div><div>I=
&#39;ve several images based on Ubuntu and within several versions of OpenM=
PI and PMIx and i&#39;m able to run OpenFOAM correctly without use slurm (d=
irectly on the node) using next command:</div><div><br></div><div>$ mpirun =
-n 16 singularity exec -B /home ../../of6/openfoam6.x.img simpleFoam -paral=
lel -case /home/carmen/test_singularity/<wbr>OpenFOAM/pruebaOF6_16cores_<wb=
r>SLURM/pruebaOF6_16cores</div><div><br></div><div>My problem comes when I =
run my program with slurm. Whether I make salloc or execute a script with s=
batch, it shows me the following error:</div><div><br></div></div><blockquo=
te style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><blockquote st=
yle=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><font col=
or=3D"#666666">It looks like MPI_INIT failed for some reason; your parallel=
 process is</font></div></div></blockquote><blockquote style=3D"margin:0px =
0px 0px 40px;border:none;padding:0px"><div><div><font color=3D"#666666">lik=
ely to abort.=C2=A0 There are many reasons that a parallel process can</fon=
t></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bor=
der:none;padding:0px"><div><div><font color=3D"#666666">fail during MPI_INI=
T; some of which are due to configuration or environment</font></div></div>=
</blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;paddi=
ng:0px"><div><div><font color=3D"#666666">problems.=C2=A0 This failure appe=
ars to be an internal failure; here&#39;s some</font></div></div></blockquo=
te><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><d=
iv><div><font color=3D"#666666">additional information (which may only be r=
elevant to an Open MPI</font></div></div></blockquote><blockquote style=3D"=
margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><font color=3D"#=
666666">developer):</font></div></div></blockquote><blockquote style=3D"mar=
gin:0px 0px 0px 40px;border:none;padding:0px"><div><div><font color=3D"#666=
666"><br></font></div></div></blockquote><blockquote style=3D"margin:0px 0p=
x 0px 40px;border:none;padding:0px"><div><div><font color=3D"#666666">=C2=
=A0 ompi_mpi_init: ompi_rte_init failed</font></div></div></blockquote><blo=
ckquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div=
><font color=3D"#666666">=C2=A0 --&gt; Returned &quot;(null)&quot; (-43) in=
stead of &quot;Success&quot; (0)</font></div></div></blockquote></blockquot=
e><div><div><br></div><div>and</div><div><br></div></div><blockquote style=
=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><blockquote style=3D"m=
argin:0px 0px 0px 40px;border:none;padding:0px"><div><div><font color=3D"#6=
66666">*** An error occurred in MPI_Init_thread</font></div></div></blockqu=
ote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><=
div><div><font color=3D"#666666">*** on a NULL communicator</font></div></d=
iv></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;pa=
dding:0px"><div><div><font color=3D"#666666">*** MPI_ERRORS_ARE_FATAL (proc=
esses in this communicator will now abort,</font></div></div></blockquote><=
blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><=
div><font color=3D"#666666">***=C2=A0 =C2=A0 and potentially your MPI job)<=
/font></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px=
;border:none;padding:0px"><div><div><font color=3D"#666666">[cn3045:369] Lo=
cal abort before MPI_INIT completed completed successfully, but am not able=
 to aggregate error messages, and not able to guarantee that all other proc=
esses were killed!</font></div></div></blockquote></blockquote><div><div><b=
r></div><div>I know I must have the same openMPI versions on both (host and=
 container) and I have also tried other versions of OpenMPI (2.X.X) and in =
all cases OpenFOAM works correctly, but at the moment I want to run it with=
 slurm it show me the errors.</div><div><br></div><div>I have also tried ot=
her ways to run the program with srun using the option --mpi=3Dpmi2 (among =
others) but I always find the same problem.</div><div><br></div><div>I use =
the following script to run OpenFoam:</div></div><blockquote style=3D"margi=
n:0px 0px 0px 40px;border:none;padding:0px"><div><div>---------------------=
-------</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40p=
x;border:none;padding:0px"><blockquote style=3D"margin:0px 0px 0px 40px;bor=
der:none;padding:0px"><div><div>#!/bin/bash</div></div></blockquote><blockq=
uote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><b=
r></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bor=
der:none;padding:0px"><div><div>#SBATCH -N 1</div></div></blockquote><block=
quote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>#=
SBATCH -p haswell=C2=A0</div></div></blockquote><blockquote style=3D"margin=
:0px 0px 0px 40px;border:none;padding:0px"><div><div>#SBATCH -J test_OpenFO=
AM</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bor=
der:none;padding:0px"><div><div>#SBATCH --output=3D&quot;singularity.%j.o&q=
uot;=C2=A0</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px =
40px;border:none;padding:0px"><div><div>#SBATCH --error=3D&quot;singularity=
.%j.e&quot;</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px=
 40px;border:none;padding:0px"><div><div><br></div></div></blockquote><bloc=
kquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>=
module load haswell/singularity_2.6.0</div></div></blockquote><blockquote s=
tyle=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>module l=
oad haswell/openmpi_3.1.2_gcc8.2.<wbr>0_pmix</div></div></blockquote><block=
quote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><=
br></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bo=
rder:none;padding:0px"><div><div>ulimit -s unlimited</div></div></blockquot=
e><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><di=
v><div><br></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px=
 40px;border:none;padding:0px"><div><div>mpirun -n 16=C2=A0singularity exec=
 ../../of6/openfoam6.x.img simpleFoam -parallel -case /home/software/test_<=
wbr>singularity/OpenFOAM/<wbr>pruebaOF6_16cores_SLURM/<wbr>pruebaOF6_16core=
s</div></div></blockquote></blockquote><blockquote style=3D"margin:0px 0px =
0px 40px;border:none;padding:0px"><div><div>----------------------------</d=
iv></div></blockquote><div><div><br></div><div>The versions that I&#39;m us=
ing are:</div></div><blockquote style=3D"margin:0px 0px 0px 40px;border:non=
e;padding:0px"><div><div><b>Host:=C2=A0</b></div></div></blockquote><blockq=
uote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><blockquote =
style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>OS: Cen=
tOS7.5</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px=
;border:none;padding:0px"><div><div>OpenMPI: 3.1.2</div></div></blockquote>=
<blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div>=
<div>PMIx: 2.1.4</div></div></blockquote></blockquote><blockquote style=3D"=
margin:0px 0px 0px 40px;border:none;padding:0px"><div><div><br></div></div>=
<div><div><b>Container:</b></div></div></blockquote><blockquote style=3D"ma=
rgin:0px 0px 0px 40px;border:none;padding:0px"><blockquote style=3D"margin:=
0px 0px 0px 40px;border:none;padding:0px"><div><div>OS: Ubuntu16.04</div></=
div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:none;p=
adding:0px"><div><div>OpenMPI: 3.1.2</div></div></blockquote><blockquote st=
yle=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><div><div>PMIx: 2.1=
.4</div></div></blockquote></blockquote><div><div><br></div><div>Can it be =
a configuration problem of SLURM? Is there any limitation of SLURM that is =
affecting OpenFOAM?</div><div><br></div><div>Some info about slurm:</div></=
div><blockquote style=3D"margin:0px 0px 0px 40px;border:none;padding:0px"><=
div><div># srun --version</div></div><div><div>slurm 18.08.3</div></div><di=
v><div># srun --mpi=3Dlist</div></div><div><div>srun: MPI types are...</div=
></div><div><div>srun: pmi2</div></div><div><div>srun: openmpi</div></div><=
div><div>srun: none</div></div></blockquote><div><div><br></div><div>I&#39;=
m a little bit lost with this issue :(</div><div>Can someone give me some l=
ights?</div><div><br></div><div>Thanks a lot in advance,</div><div>Carmen</=
div></div><div><br></div></div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></blockquote></div><br></div></div></div></div></blockquote></div><br=
></div></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</blockquote></div>
</blockquote></div></blockquote></div></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</blockquote></div>
</blockquote></div></blockquote></div>
------=_Part_681_889960819.1550507582844--

------=_Part_680_569452469.1550507582843--
