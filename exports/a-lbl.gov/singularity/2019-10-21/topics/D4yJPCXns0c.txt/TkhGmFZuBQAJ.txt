X-Received: by 2002:a17:902:9a8a:: with SMTP id w10-v6mr442623plp.19.1542142685066;
        Tue, 13 Nov 2018 12:58:05 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a63:ba43:: with SMTP id l3ls2922296pgu.7.gmail; Tue, 13 Nov
 2018 12:58:04 -0800 (PST)
X-Received: by 2002:a65:4ccb:: with SMTP id n11mr6263805pgt.257.1542142683929;
        Tue, 13 Nov 2018 12:58:03 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1542142683; cv=none;
        d=google.com; s=arc-20160816;
        b=t7yS5zA4LD7CC955aJHvr2L933K4vZzgc2rF4qamXWQfakvn602LjSqutmIc1Pp3G3
         A+Zjxqwn+LL8Wb4xl8ZB09dV6WOdompMlMX8vROrc7IZGb/CNPcqwjBYxe2zx2Otixmr
         Y0twY6yBvH7hrEBGScOKH3tvvXrSHX1xUoCWcQdnm5f8u4cTPHoeukvufKGWIPV1bzZf
         L3EUhB+8kw/EfMeN4lM0dMiYGsERJnSg6TCYmAT8NhOqy++VstR+nBILJzEVty+1d5R/
         vYV7xx0sVVht6EUQrZ4tgfoCeC67tN41OUBRPh2e7Je2ONJDi6bJ1JiTEpm/HY8IiCUF
         qtzQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature;
        bh=E+RXEoWA3gk7k2kvBkHnpfSTj/C2FwhpajF55LmZdgQ=;
        b=NFy+RuzqP1FffSN0259WIPzi0564m7T7EPDD3tEOoA8jFGk9oj7em7sBh5QTkw5xAH
         PTBmICIQhjA7YW70KnhgjlM41vGCFG4eWXkI/O5bK4qr2i+nPF2OlJNb5znguEcmMYHA
         dm/n/IPM8un2YpCGVeI+XqRQI6SDmrnf8Vougi/QHA08KTt3oWiqmtReQtdSJwd7KDh6
         c9Tbifaf7LrbbmDv92IWxgjz5jnkv3wd4PKFSLTQIYfiVtj+a/Q3CA/sKFwKoyPucx2f
         iHHc8D76q/ipm7axVNNOrogffqnyO57eClJuusUjTeGB0yjYNabiDGqxDcBLRjpEq3/0
         a00w==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=D7FXT7xu;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.166.196 as permitted sender) smtp.mailfrom=vso...@gmail.com
Return-Path: <vso...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id k35si7424621pgm.225.2018.11.13.12.58.03
        for <singu...@lbl.gov>;
        Tue, 13 Nov 2018 12:58:03 -0800 (PST)
Received-SPF: pass (google.com: domain of vso...@gmail.com designates 209.85.166.196 as permitted sender) client-ip=209.85.166.196;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=D7FXT7xu;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.166.196 as permitted sender) smtp.mailfrom=vso...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2FmBADDOetbf8SmVdFgA4NyTkGBAieDc?=
 =?us-ascii?q?gaBHYJekieRYYVogSs7JQEVhD4Cgz0iOBYBAwEBAgEBAgEBAhABAQkLCwgbDCU?=
 =?us-ascii?q?MgjYFAgMaBwlLOzABAQEBAQEBAQEBHwItKQEaAQICASMEGQENDh4DAQsGAwILN?=
 =?us-ascii?q?wICIQEBDgMBBQEcDgcEARMHAgSDASgBgT8BAw0IBY0YkAc8iw18FgUBF4J3BYQ?=
 =?us-ascii?q?4ChknDVqBNwIGEotwF4F/gRGCFH6CVoFiEwEMBgEJAjUmgj2CVwKPe48GJy4Jj?=
 =?us-ascii?q?XeDKxiQc44vgwCGPzCBOVYNI3FwFWyCO4YIinAkMBCLRQ4XMIF3AQE?=
X-IronPort-AV: E=Sophos;i="5.56,229,1539673200"; 
   d="scan'208,217";a="42406618"
Received: from mail-it1-f196.google.com ([209.85.166.196])
  by fe4.lbl.gov with ESMTP; 13 Nov 2018 12:58:02 -0800
Received: by mail-it1-f196.google.com with SMTP id t190-v6so20534054itb.2
        for <singu...@lbl.gov>; Tue, 13 Nov 2018 12:58:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=E+RXEoWA3gk7k2kvBkHnpfSTj/C2FwhpajF55LmZdgQ=;
        b=D7FXT7xu5WEDx07mImcwjEPURY+J2OEe8jDWWJLZSERSlNFS8Af5hwTpVKhn5CuEVj
         gZT7ARNlbvpVNEDwZHZ91qQJAvdBHgq+xtSsdg8BImcQpUjqX2aqeh23aHhXlg+7Xs54
         ZWzM6F1EldSjVpYpZKYlFMP5NAYVX5i/XLItsrcPOyZddI0ESBsJT8s9wmiTg0fSFeSb
         /+4B2/aNhh6b8+3IWwCtx/Pjl/jS20F/z2Iq5rxgh+Xy5kdDyfRgTvWcXUN1ZSGF2u+d
         qL77w7KN6+N8QEfwadP5dp+pyRlkb5eQxO1gDpExiECBn4iELGprBZAAEnNRApSbtrF6
         XsAQ==
X-Gm-Message-State: AGRZ1gLTPKwafXsquVmAI697vkPgHuvSHXFg4z52uZUZdGr//gy5Dcqo
	MSL06ETxBBOCs5/5DTmFWlRYp3E6o5c5GKM+GhRpmqeR
X-Received: by 2002:a02:5953:: with SMTP id p80-v6mr6455933jab.111.1542142681311;
 Tue, 13 Nov 2018 12:58:01 -0800 (PST)
MIME-Version: 1.0
References: <f7003a30-6d9a-4df1-a1dc-9d9ec5912580@lbl.gov>
In-Reply-To: <f7003a30-6d9a-4df1-a1dc-9d9ec5912580@lbl.gov>
From: v <vso...@gmail.com>
Date: Tue, 13 Nov 2018 15:57:49 -0500
Message-ID: <CAM=pu+LEGC2fh2Z=vT_TeryF+rck1i5Y2nJ+6RyGkznx=pmFgQ@mail.gmail.com>
Subject: Re: [Singularity] Sregistry serving performance tweaks
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000b66f9a057a921249"

--000000000000b66f9a057a921249
Content-Type: text/plain; charset="UTF-8"

I think the performance issues for scaling are logical given that we have a
single nginx, uwgi application, and file system storage. Typically when you
scale these "apps" in some kind of cloud or cluster, you are generating
several instances of the components that scale with demand. For Singularity
Hub, for example, we don't rely on a filesystem to serve images, we use an
object storage API that the registry server can easily direct the user to.
The web interface doesn't need to do much other than provide a human
friendly interface to that. You are right that tweaking the config will
only take you so far - you would actually need to mimic a container cluster
and somehow deploy more than one set of servers for accessing the same
database and filesystem. This sounds messy to me.

I think what we should do here is to put our heads together and think about
how the images can better be served at scale, for Singularity Registry
Server. This comes down to the kind of file storage (or object storage)
that you are able to use. For each solution we can make a storage plugin.
For example, you could weigh the costs and benefits of using Google
Storage, and then your "upload" to the registry would be to push there.
That's how SingularityHub rolls :) For the user (you) it would come down to
enabling the plugin, and exporting some credential as a secret. That's
probably it :)

And when I say "we can make" I mean "I" - I would be happy to implement
these for you - you can rely on this dinosaur! What I'd want to do is get a
solid description of your (and the community needs) so the SRegistry server
can be all that it needs to be!)

Best,

Vanessa

On Tue, Nov 13, 2018 at 3:39 PM Mike Moore <wxdu...@gmail.com> wrote:

> Hi everyone,
>
>   Our POC with sregistry has been moving along well.  I am trying to
> address reliability issues when running multiple simultaneous pulls from
> sregistry.  I would see timeouts and disconnects on the compute node side
> when trying to run only 5 simultaneous pulls.  With changes to the number
> of processes and worker threads in the nginx and sregistry uwsgi
> containers, I have been able to resolve these timeouts for the most part.
> What I am running into is a significant throughput limit on a per-pull
> basis.  So, here is a little background on our POC infrastructure.
>
> Sregistry is running in a virtual machine with 16 GB of RAM and 8 Cores.
> The VM is running CentOS 7, hosted by KVM hypervisor, on a HP Blade with 2
> x 2.2GHz AMD Opteron 6174 (12 Core) processors.  Due to limitations in the
> network hardware, the hypervisor has a 6 Gbit Ethernet connection out to
> our network. The VM's disk is a single qcow2 image running on a GPFS file
> system.  The disk will provide easily 100+MB/s of throughput.
>
> The problem I am having is that there is something throttling the ability
> for sregistry to push the container image out to the clients. For a single
> client pull of a 1.5GB image, I only see about ~32MB/s of throughput (256
> Mbit/s).  With multiple simultaneous pulls, I can get up to about 230-240
> MB/s sustained (2+ Gbit/s).  Using dstat to monitor cpu, disk, network, and
> interrupts, I have ruled out the disk and CPU being a limit.  CPU
> utilization for the single pull cases is almost nothing, and when really
> cranking (at least 40+ simultaneous pulls) it is busy, but nothing is
> blocked on disk IO.  There is no disk activity that I can see.  It appears
> that everything is being served from memory.
>
> I first thought this was an issue with the network configuration so I ran
> iperf between the VM and the compute nodes to test the throughput. I get
> between 4-5 Gbit/s throughput between the VM to the compute nodes. So next,
> I shelled into the containers to see if the docker proxy between the
> container network and the outside world was causing problems. Using the
> uwsgi container, I can get again 4-5 Gbit/s throughput.  So it is not the
> proxy causing the limit.
>
> If I understand the traffic flow I am seeing using "docker stats",  when
> singularity pulls from sregistry, the traffic flow looks like
>
>        uwsgi/sregistry container------->nginx container------>docker
> proxy------>VM eth0------->network------>compute node--->singularity client
>
> The nginx container has a lot of traffic coming and going as everything
> does go through it.
>
> I am at a loss as where to look next to improve performance.  Here is what
> I have done so far:
>
> In uwsgi.ini, I replaced the original processes and threads with the
> following settings:
> http-timeout = 3600000
> socket-timeout = 120
> processes = 128
> threads = 1
> ignore-sigpipe = true
> ignore-write-errors = true
> disable-write-exception = true
>
> In nginx, I made the following changes:
> Upped worker_processes to 32, but keeping it lower didn't make an
> appreciable difference.
> Disabled access logging
> Upped worker_connections from 1024 to 2048
> Added settings to do asynchronous IO, disabled sendfile.
>
> I still see the occasional failure on a pull, and I think that is because
> the registry isn't pushing data out as fast as it could.  Does anyone have
> any further suggestions on settings to try to relieve the bottleneck(s)?
>
> Thank you
> -Mike
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>


-- 
Vanessa Villamia Sochat
Stanford University '16
(603) 321-0676

--000000000000b66f9a057a921249
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">I think the performance issues for scaling are logical giv=
en that we have a single nginx, uwgi application, and file system storage. =
Typically when you scale these &quot;apps&quot; in some kind of cloud or cl=
uster, you are generating several instances of the components that scale wi=
th demand. For Singularity Hub, for example, we don&#39;t rely on a filesys=
tem to serve images, we use an object storage API that the registry server =
can easily direct the user to. The web interface doesn&#39;t need to do muc=
h other than provide a human friendly interface to that. You are right that=
 tweaking the config will only take you so far - you would actually need to=
 mimic a container cluster and somehow deploy more than one set of servers =
for accessing the same database and filesystem. This sounds messy to me.<di=
v><br></div><div>I think what we should do here is to put our heads togethe=
r and think about how the images can better be served at scale, for Singula=
rity Registry Server. This comes down to the kind of file storage (or objec=
t storage) that you are able to use. For each solution we can make a storag=
e plugin. For example, you could weigh the costs and benefits of using Goog=
le Storage, and then your &quot;upload&quot; to the registry would be to pu=
sh there. That&#39;s how SingularityHub rolls :) For the user (you) it woul=
d come down to enabling the plugin, and exporting some credential as a secr=
et. That&#39;s probably it :)</div><div><br></div><div>And when I say &quot=
;we can make&quot; I mean &quot;I&quot; - I would be happy to implement the=
se for you - you can rely on this dinosaur! What I&#39;d want to do is get =
a solid description of your (and the community needs) so the SRegistry serv=
er can be all that it needs to be!)</div><div><br></div><div>Best,</div><di=
v><br></div><div>Vanessa</div></div><br><div class=3D"gmail_quote"><div dir=
=3D"ltr">On Tue, Nov 13, 2018 at 3:39 PM Mike Moore &lt;<a href=3D"mailto:w=
xdu...@gmail.com">wxdu...@gmail.com</a>&gt; wrote:<br></div><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pa=
dding-left:1ex"><div dir=3D"ltr">Hi everyone,<br><br>=C2=A0 Our POC with sr=
egistry has been moving along well.=C2=A0 I am trying to address reliabilit=
y issues when running multiple simultaneous pulls from sregistry.=C2=A0 I w=
ould see timeouts and disconnects on the compute node side when trying to r=
un only 5 simultaneous pulls.=C2=A0 With changes to the number of processes=
 and worker threads in the nginx and sregistry uwsgi containers, I have bee=
n able to resolve these timeouts for the most part.=C2=A0 What I am running=
 into is a significant throughput limit on a per-pull basis.=C2=A0 So, here=
 is a little background on our POC infrastructure.=C2=A0 <br><br>Sregistry =
is running in a virtual machine with 16 GB of RAM and 8 Cores.=C2=A0 The VM=
 is running CentOS 7, hosted by KVM hypervisor, on a HP Blade with 2 x 2.2G=
Hz AMD Opteron 6174 (12 Core) processors.=C2=A0 Due to limitations in the n=
etwork hardware, the hypervisor has a 6 Gbit Ethernet connection out to our=
 network. The VM&#39;s disk is a single qcow2 image running on a GPFS file =
system.=C2=A0 The disk will provide easily 100+MB/s of throughput.=C2=A0 <b=
r><br>The problem I am having is that there is something throttling the abi=
lity for sregistry to push the container image out to the clients. For a si=
ngle client pull of a 1.5GB image, I only see about ~32MB/s of throughput (=
256 Mbit/s).=C2=A0 With multiple simultaneous pulls, I can get up to about =
230-240 MB/s sustained (2+ Gbit/s).=C2=A0 Using dstat to monitor cpu, disk,=
 network, and interrupts, I have ruled out the disk and CPU being a limit.=
=C2=A0 CPU utilization for the single pull cases is almost nothing, and whe=
n really cranking (at least 40+ simultaneous pulls) it is busy, but nothing=
 is blocked on disk IO.=C2=A0 There is no disk activity that I can see.=C2=
=A0 It appears that everything is being served from memory. <br><br>I first=
 thought this was an issue with the network configuration so I ran iperf be=
tween the VM and the compute nodes to test the throughput. I get between 4-=
5 Gbit/s throughput between the VM to the compute nodes. So next, I shelled=
 into the containers to see if the docker proxy between the container netwo=
rk and the outside world was causing problems. Using the uwsgi container, I=
 can get again 4-5 Gbit/s throughput.=C2=A0 So it is not the proxy causing =
the limit.<br><br>If I understand the traffic flow I am seeing using &quot;=
docker stats&quot;,=C2=A0 when singularity pulls from sregistry, the traffi=
c flow looks like<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 uwsgi/sregist=
ry container-------&gt;nginx container------&gt;docker proxy------&gt;VM et=
h0-------&gt;network------&gt;compute node---&gt;singularity client<br><br>=
The nginx container has a lot of traffic coming and going as everything doe=
s go through it. <br><br>I am at a loss as where to look next to improve pe=
rformance.=C2=A0 Here is what I have done so far:<br><br>In uwsgi.ini, I re=
placed the original processes and threads with the following settings:<br>h=
ttp-timeout =3D 3600000<br>socket-timeout =3D 120<br>processes =3D 128<br>t=
hreads =3D 1<br>ignore-sigpipe =3D true<br>ignore-write-errors =3D true<br>=
disable-write-exception =3D true<br><br>In nginx, I made the following chan=
ges:<br>Upped worker_processes to 32, but keeping it lower didn&#39;t make =
an appreciable difference.<br>Disabled access logging<br>Upped worker_conne=
ctions from 1024 to 2048<br>Added settings to do asynchronous IO, disabled =
sendfile.<br><br>I still see the occasional failure on a pull, and I think =
that is because the registry isn&#39;t pushing data out as fast as it could=
.=C2=A0 Does anyone have any further suggestions on settings to try to reli=
eve the bottleneck(s)?<br><br>Thank you<br>-Mike<br><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"gmail_signature" data-smartmail=3D"gmail_signature">Vanessa Villa=
mia Sochat<br>Stanford University &#39;16<br><div><div><div>(603) 321-0676<=
/div></div></div></div>

--000000000000b66f9a057a921249--
