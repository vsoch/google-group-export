X-Received: by 10.107.140.142 with SMTP id o136mr9710303iod.3.1468597915717;
        Fri, 15 Jul 2016 08:51:55 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.167.3 with SMTP id q3ls1674225ioe.49.gmail; Fri, 15 Jul
 2016 08:51:55 -0700 (PDT)
X-Received: by 10.66.170.44 with SMTP id aj12mr33075804pac.131.1468597915065;
        Fri, 15 Jul 2016 08:51:55 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id h3si2776881pfg.65.2016.07.15.08.51.54
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 15 Jul 2016 08:51:55 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) client-ip=209.85.215.70;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EyAQC8BYlXf0bXVdEZAzgIgnCBJHwGgzaBDKgQjBqBe4YaAoEoBzgUAQEBAQEBAQMPAQEJCwsJHzGEXAEBBAESCAEIKzALCQILDRYBCQEJAgIhAQ8DAQUBCxEGCAcEARwEAYd0Aw8IBZQVI49CgTE+MYs7igINhCMBAQgBAQEBASIQimeCQ4FPDAUBZAGCOII9HQWGCYIFCAdfhQx1P4QkhQ00AYtxD0OCF4FrF4dxhUKGXYFFhjoSHoEPHoJBHIFsHDIHhQoNF4EeAQEB
X-IronPort-AV: E=Sophos;i="5.28,368,1464678000"; 
   d="scan'208,217";a="29763856"
Received: from mail-lf0-f70.google.com ([209.85.215.70])
  by fe4.lbl.gov with ESMTP; 15 Jul 2016 08:51:51 -0700
Received: by mail-lf0-f70.google.com with SMTP id 33so75906263lfw.1
        for <singu...@lbl.gov>; Fri, 15 Jul 2016 08:51:51 -0700 (PDT)
X-Gm-Message-State: ALyK8tJ6D++rKF2YcJxeKZtgpbd9WwxawrlU7AMizNOp3SVlg4vLo/fQUP0up5+g+0MUwJu7+kLQkgMPaAq4v2B04FVbRmPRSmfAHWk2D5l1VS+RN7ahBbCHOdVQB49yK/bePvgVV7s+F4Td6EWCiUimvGo=
X-Received: by 10.25.19.105 with SMTP id j102mr9478514lfi.44.1468597910285;
        Fri, 15 Jul 2016 08:51:50 -0700 (PDT)
X-Received: by 10.25.19.105 with SMTP id j102mr9478503lfi.44.1468597909940;
 Fri, 15 Jul 2016 08:51:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.158 with HTTP; Fri, 15 Jul 2016 08:51:48 -0700 (PDT)
In-Reply-To: <f942d15c-1e7f-401a-8f6b-01ecfec1c7e9@lbl.gov>
References: <03a19fb0-27ce-43c4-9400-8e58cf726500@lbl.gov> <CAN7etTwRbSe1MMh9wdQAMYoKVJTb_SGJeHto+WrZ=aU7NoBmhQ@mail.gmail.com>
 <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov> <CAN7etTyqGkWy1P57-cVJgyru5BT_DvnhwDzLe1p38BV8z_PPww@mail.gmail.com>
 <66b82c74-2778-44f2-ae5c-87e01ec8885d@lbl.gov> <CAN7etTz09uzfP-NpZ3-+HnijfrC+u+=pOuBQDTShXG+unxgOVg@mail.gmail.com>
 <c4e5864c-cbaa-4269-9522-db61d63d7cff@lbl.gov> <CAN7etTztBSE1YXY3etq9ipMNnPSKh2Eatz5i-QQOH=ecdNDVCg@mail.gmail.com>
 <0686e644-e7d6-45d7-a371-bf17bead57a4@lbl.gov> <CAN7etTy7cLFaG_NK8-izwc4V-WiU6tsGqNwyVY79TPND9eZnrw@mail.gmail.com>
 <f942d15c-1e7f-401a-8f6b-01ecfec1c7e9@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Fri, 15 Jul 2016 08:51:48 -0700
Message-ID: <CAN7etTz8Sj3CvHsn6ywyLVxfk3g5+kPy021sBbvXCZTtjsyi4w@mail.gmail.com>
Subject: Re: [Singularity] Image is locked by another process
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a11407c56bd81a40537ae986d

--001a11407c56bd81a40537ae986d
Content-Type: text/plain; charset=UTF-8

On Fri, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <sgmeh...@gmail.com>
wrote:

> Hello,
>
> I went back and recreated everything from scratch using v2.1.  When I
> moved the image file to the mach1 and mach2 nodes I used tar with the -S
> option.  I used /tmp for the image files.  After everything was set I ran
> mpirun on each individual system (only starting jobs on that system) with
> no errors.  For some reason when I ran mpirun from mach0 and asked for jobs
> to be run on mach0 and another system I get the error.  The mpirun
> complains about not being able to create a file (already exists) and
> hangs.  The singularity image file on mach0 and the other system selected
> are locked with pids that are no longer running.
>
> Saying all that, I decided to go back and put --debug on the singularity
> command in the mpirun to get you better diagnostics.  I rebooted mach0 and
> mach2, then reran the command and it worked correctly.  I then removed the
> --debug command and it worked.  I added mach1 to the mix and everything
> worked fine.  What? I guess the bottom line is that if the mpirun aborts
> (and it seems to hang) then you can get this condition.
>

So does the problem seem to arise only when there is a failure on one of
the processes, and MPI kills the remaining processes?

Ralph, do you know how OMPI kills the leftover processes in a job abort and
what signal it uses?

(additional comments inline)


> Steve
>
>
> [root@mach0 ompi]# mpirun --allow-run-as-root -np 2 -H mach2 singularity
> exec /tmp/c7new.img /usr/bin/ring
> Process 1 exiting
> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
> Process 0 sent to 1
> Process 0 decremented value: 9
> Process 0 decremented value: 8
> Process 0 decremented value: 7
> Process 0 decremented value: 6
> Process 0 decremented value: 5
> Process 0 decremented value: 4
> Process 0 decremented value: 3
> Process 0 decremented value: 2
> Process 0 decremented value: 1
> Process 0 decremented value: 0
> Process 0 exiting
>

Is /tmp NFS mounted or shared?


> [root@mach0 ompi]# mpirun --allow-run-as-root -np 4 -H mach0,mach2
> singularity exec /tmp/c7new.img /usr/bin/ring
> ERROR  : Could not create directory
> /tmp/.singularity-session-0.64768.158221: File exists
> ERROR  : Failed creating session directory:
> /tmp/.singularity-session-0.64768.158221
> -------------------------------------------------------
> Primary job  terminated normally, but 1 process returned
> a non-zero exit code. Per user-direction, the job has been aborted.
> -------------------------------------------------------
> ^CKilled by signal 2.
> mpirun: abort is already in progress...hit ctrl-c again to forcibly
> terminate
>

Ahhhhh, I think you may have found a race condition in Singularity which
killed one of the processes. Then the MPI killed the rest.


> [root@mach0 ompi]# singularity shell -w /tmp/c7new.img
> ERROR  : Image is locked by another process
> [root@mach0 ompi]# lslocks
> COMMAND           PID  TYPE SIZE MODE  M START END PATH
> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
> crond             606 FLOCK   4B WRITE 0     0   0 /run/crond.pid
> master            820 FLOCK  33B WRITE 0     0   0
> /var/spool/postfix/pid/master.pid
> master            820 FLOCK  33B WRITE 0     0   0
> /var/lib/postfix/master.lock
> slurmctld         924 POSIX   4B WRITE 0     0   0 /run/slurmctld.pid
> slurmd           1103 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
> (unknown)        2477 FLOCK   0B READ  0     0   0 /
> [root@mach0 ompi]# ls /proc/2477
> ls: cannot access /proc/2477: No such file or directory
>

OK, that is weird. If process 2477 is dead, there should be no active lock.
I would guess this might be a kernel bug? What distribution and kernel are
you running?


> [root@mach0 ompi]# ssh mach2
> Last login: Fri Jul 15 07:52:56 2016 from mach0
> [root@mach2 ~]# singularity shell -w /tmp/c7new.img
> ERROR  : Image is locked by another process
> [root@mach2 ~]# lslocks
> COMMAND           PID  TYPE SIZE MODE  M START END PATH
> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
> master            851 FLOCK  33B WRITE 0     0   0
> /var/spool/postfix/pid/master.pid
> master            851 FLOCK  33B WRITE 0     0   0
> /var/lib/postfix/master.lock
> (unknown)       10110 FLOCK   0B READ  0     0   0 /
> crond             668 FLOCK   4B WRITE 0     0   0 /run/crond.pid
> slurmd           1610 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
> [root@mach2 ~]# ls /proc/10110
> ls: cannot access /proc/10110: No such file or directory
>

Very weird. Seems both nodes have the same issue..?

I wonder if I can replicate this by kill -9 the outer parent of a
Singularity run. I will test when I get to my development box.


>
> Good Run:
>
> [root@mach0 ompi]# mpirun --allow-run-as-root -np 6 -H mach0,mach1,mach2
> singularity exec /tmp/c7new.img /usr/bin/ring
> Process 0 sending 10 to 1, tag 201 (6 processes in ring)
> Process 0 sent to 1
> Process 1 exiting
> Process 0 decremented value: 9
> Process 0 decremented value: 8
> Process 0 decremented value: 7
> Process 0 decremented value: 6
> Process 0 decremented value: 5
> Process 0 decremented value: 4
> Process 0 decremented value: 3
> Process 0 decremented value: 2
> Process 0 decremented value: 1
> Process 0 decremented value: 0
> Process 0 exiting
> Process 2 exiting
> Process 5 exiting
> Process 3 exiting
> Process 4 exiting
>

So as long as we don't crash (or get killed), all seems fine... Interesting.

I will try and replicate the error condition.

Greg


>
>
>
> On Thursday, July 14, 2016 at 3:56:48 PM UTC-6, Gregory M. Kurtzer wrote:
>
>> I am wondering if some aspect of the sparse file has caused this. Sparse
>> files are weird, and do require some attention when copying around. With
>> that said, I do not think it should ever corrupt an image, but it might
>> take along garbage data with it. One way to retest this is to go back to
>> the original image, and tar it before sending using the -S/--sparse tar
>> option when packaging it up. Then bring it to the other test system and see
>> if it still does that.
>>
>> Aside from that, I'm a bit confused as to what is happening. There should
>> be no issue with 2.0 vs. 2.1, but it might be worth checking with some
>> images also created with 2.1 on the same target system where you are seeing
>> the problem.
>>
>> On Thu, Jul 14, 2016 at 2:46 PM, Steve Mehlberg <sg...@gmail.com>
>> wrote:
>>
>>> It is very possible that I did copy it from another system - maybe
>>> created it with singularity 2.0 and now running it with 2.1??  I will start
>>> over and recreate from scratch and see if I still have the problem.
>>>
>>> Sorry, I checked there is no /proc/2299
>>>
>>> The only way I've found to get the file unlocked is to reboot.  Not an
>>> issue with the little VM.
>>>
>>> Steve
>>>
>>> On Thursday, July 14, 2016 at 3:30:12 PM UTC-6, Gregory M. Kurtzer wrote:
>>>
>>>> Very odd...
>>>>
>>>> I'm concerned about the Buffer I/O errors. It is almost like the image
>>>> itself has a problem with it. Did you copy this image from another system?
>>>> I wonder if it is the sparseness...
>>>>
>>>> Does the directory exist for /proc/2299/ ?
>>>>
>>>> Can you kill it with a -9?
>>>>
>>>> On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlberg <sg...@gmail.com>
>>>> wrote:
>>>>
>>>>> Seems that process isn't running any more.  I did find this in the
>>>>> /var/log/messages file:
>>>>>
>>>>> [root@mach0 ~]# cat /var/log/messages | grep 2299
>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> Command=exec,
>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1,
>>>>> logical block 22299
>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1,
>>>>> logical block 22990
>>>>> ...
>>>>>
>>>>> [root@mach0 ~]# ps -ef |grep 2299
>>>>> root     10545  2002  0 14:15 pts/0    00:00:00 grep --color=auto 2299
>>>>> [root@mach0 ~]# ps |grep 2299
>>>>>
>>>>> On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer
>>>>> wrote:
>>>>>
>>>>>> What is running at PID 2299?
>>>>>>
>>>>>> On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <sg...@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Ok, I see my error with shell vs exec.  I'm running on a VM with my
>>>>>>> own space, no NFS involved.  Here is the debug command run and lsofl
>>>>>>>
>>>>>>> [root@mach0 ~]# df -h
>>>>>>> Filesystem             Size  Used Avail Use% Mounted on
>>>>>>> /dev/mapper/vg1-lv001  5.7G  4.3G  1.1G  80% /
>>>>>>> devtmpfs               911M     0  911M   0% /dev
>>>>>>> tmpfs                  920M     0  920M   0% /dev/shm
>>>>>>> tmpfs                  920M   41M  880M   5% /run
>>>>>>> tmpfs                  920M     0  920M   0% /sys/fs/cgroup
>>>>>>> /dev/vda1              190M  110M   67M  63% /boot
>>>>>>> /dev/mapper/vg1-lv002   12G  203M   11G   2% /var
>>>>>>> tmpfs                  184M     0  184M   0% /run/user/0
>>>>>>> [root@mach0 ~]# singularity --debug shell -w c7
>>>>>>> enabling debugging
>>>>>>> ending argument loop
>>>>>>> Exec'ing: /usr/local/libexec/singularity/cli/shell.exec -w+ '[' -f
>>>>>>> /usr/local/etc/singularity/init ']'
>>>>>>> + . /usr/local/etc/singularity/init
>>>>>>> ++ unset module
>>>>>>> ++
>>>>>>> PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin
>>>>>>> ++ HISTFILE=/dev/null
>>>>>>> ++ export PATH HISTFILE
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> ++ '[' -n '' ']'
>>>>>>> + true
>>>>>>> + case ${1:-} in
>>>>>>> + shift
>>>>>>> + SINGULARITY_WRITABLE=1
>>>>>>> + export SINGULARITY_WRITABLE
>>>>>>> + true
>>>>>>> + case ${1:-} in
>>>>>>> + break
>>>>>>> + '[' -z c7 ']'
>>>>>>> + SINGULARITY_IMAGE=c7
>>>>>>> + export SINGULARITY_IMAGE
>>>>>>> + shift
>>>>>>> + exec /usr/local/libexec/singularity/sexec
>>>>>>> VERBOSE [U=0,P=8045]
>>>>>>> message.c:52:init()                        : Set messagelevel to: 5
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:127:main()                         : Gathering and caching user
>>>>>>> info.
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:43:get_user_privs()            : Called get_user_privs(struct
>>>>>>> s_privinfo *uinfo)
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:54:get_user_privs()            : Returning
>>>>>>> get_user_privs(struct s_privinfo *uinfo) = 0
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:134:main()                         : Checking if we can escalate
>>>>>>> privs properly.
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:61:escalate_privs()            : Called escalate_privs(void)
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:73:escalate_privs()            : Returning escalate_privs(void)
>>>>>>> = 0
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:141:main()                         : Setting privs to calling user
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:79:drop_privs()                : Called drop_privs(struct
>>>>>>> s_privinfo *uinfo)
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:87:drop_privs()                : Dropping privileges to GID =
>>>>>>> '0'
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:93:drop_privs()                : Dropping privileges to UID =
>>>>>>> '0'
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:103:drop_privs()               : Confirming we have correct GID
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:109:drop_privs()               : Confirming we have correct UID
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> privilege.c:115:drop_privs()               : Returning drop_privs(struct
>>>>>>> s_privinfo *uinfo) = 0
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:146:main()                         : Obtaining user's homedir
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:150:main()                         : Obtaining file descriptor to
>>>>>>> current directory
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:155:main()                         : Getting current working
>>>>>>> directory path string
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:161:main()                         : Obtaining SINGULARITY_COMMAND
>>>>>>> from environment
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:168:main()                         : Obtaining SINGULARITY_IMAGE
>>>>>>> from environment
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:174:main()                         : Checking container image is a
>>>>>>> file: c7
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:180:main()                         : Building configuration file
>>>>>>> location
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:183:main()                         : Config location:
>>>>>>> /usr/local/etc/singularity/singularity.conf
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:185:main()                         : Checking Singularity
>>>>>>> configuration is a file: /usr/local/etc/singularity/singularity.conf
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:191:main()                         : Checking Singularity
>>>>>>> configuration file is owned by root
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:197:main()                         : Opening Singularity
>>>>>>> configuration file
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:210:main()                         : Checking Singularity
>>>>>>> configuration for 'sessiondir prefix'
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> config_parser.c:47:config_get_key_value()  : Called
>>>>>>> config_get_key_value(fp, sessiondir prefix)
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> config_parser.c:66:config_get_key_value()  : Return
>>>>>>> config_get_key_value(fp, sessiondir prefix) = NULL
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> file.c:48:file_id()                        : Called file_id(c7)
>>>>>>> VERBOSE [U=0,P=8045]
>>>>>>> file.c:58:file_id()                        : Generated file_id:
>>>>>>> 0.64768.26052
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> file.c:60:file_id()                        : Returning file_id(c7) =
>>>>>>> 0.64768.26052
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:217:main()                         : Set sessiondir to:
>>>>>>> /tmp/.singularity-session-0.64768.26052
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:221:main()                         : Set containername to: c7
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:223:main()                         : Setting loop_dev_* paths
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> config_parser.c:47:config_get_key_value()  : Called
>>>>>>> config_get_key_value(fp, container dir)
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> config_parser.c:58:config_get_key_value()  : Return
>>>>>>> config_get_key_value(fp, container dir) = /var/singularity/mnt
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:232:main()                         : Set image mount path to:
>>>>>>> /var/singularity/mnt
>>>>>>> LOG     [U=0,P=8045]
>>>>>>> sexec.c:234:main()                         : Command=shell, Container=c7,
>>>>>>> CWD=/root, Arg1=(null)
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:247:main()                         : Set prompt to: Singularity.c7>
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:249:main()                         : Checking if we are opening
>>>>>>> image as read/write
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:264:main()                         : Opening image as read/write
>>>>>>> only: c7
>>>>>>> DEBUG   [U=0,P=8045]
>>>>>>> sexec.c:271:main()                         : Setting exclusive lock on file
>>>>>>> descriptor: 6
>>>>>>> ERROR   [U=0,P=8045]
>>>>>>> sexec.c:273:main()                         : Image is locked by another
>>>>>>> process
>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>> [root@mach0 ~]# lslocks
>>>>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>>>>> crond             601 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>>>>> master            826 FLOCK  33B WRITE 0     0   0
>>>>>>> /var/spool/postfix/pid/master.pid
>>>>>>> master            826 FLOCK  33B WRITE 0     0   0
>>>>>>> /var/lib/postfix/master.lock
>>>>>>> lvmetad           478 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>>>>>> slurmctld         940 POSIX   4B WRITE 0     0   0 /run/slurmctld.pid
>>>>>>> slurmd            966 POSIX   4B WRITE 0     0   0 /run/slurmd.pid
>>>>>>> (unknown)        2299 FLOCK   0B READ  0     0   0 /
>>>>>>>
>>>>>>>
>>>>>>> On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gregory M. Kurtzer
>>>>>>> wrote:
>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <
>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>
>>>>>>>>> Gregory,
>>>>>>>>>
>>>>>>>>> Thanks for the quick suggestions.  There don't seem to be any
>>>>>>>>> processes attached to the container and I can't seem to run other commands
>>>>>>>>> (in read mode). I'm not sure what's going on.
>>>>>>>>>
>>>>>>>>> Just lazy with root right now, I need to create a normal uid.
>>>>>>>>>
>>>>>>>>
>>>>>>>> Ahh, ok. Been there, done that! lol
>>>>>>>>
>>>>>>>>
>>>>>>>>>
>>>>>>>>> Steve
>>>>>>>>>
>>>>>>>>> [root@mach0 ~]# ls -la c7
>>>>>>>>> -rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7
>>>>>>>>> [root@mach0 ~]# singularity shell -w c7
>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>
>>>>>>>>
>>>>>>>> Can you run this command again in --debug mode (singularity --debug
>>>>>>>> ....)
>>>>>>>>
>>>>>>>>
>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>>
>>>>>>>>
>>>>>>>> What about the command "lslocks"?
>>>>>>>>
>>>>>>>>
>>>>>>>>> [root@mach0 ~]# singularity shell c7 whoami
>>>>>>>>> /usr/bin/whoami: /usr/bin/whoami: cannot execute binary file
>>>>>>>>>
>>>>>>>>
>>>>>>>> Ahh, this is normal. You are asking the shell script to read in
>>>>>>>> /usr/bin/whoami. If you want to use shell to run whoami, you must prefix it
>>>>>>>> with the -c (e.g. -c "whoami [args]"), or use the 'exec' Singularity
>>>>>>>> subcommand: "singularity exec c7 whoami"
>>>>>>>>
>>>>>>>>
>>>>>>>>> [root@mach0 ~]#
>>>>>>>>> [root@mach0 ~]# ps -ef |grep c7
>>>>>>>>> root      7479  2002  0 11:49 pts/0    00:00:00 grep --color=auto
>>>>>>>>> c7
>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>
>>>>>>>>
>>>>>>>> What kind of file system does your image exist on? Is it NFS by
>>>>>>>> chance? I wonder if there is a host issue with a locking daemon or
>>>>>>>> something else weird going on where it is not giving the exclusive lock
>>>>>>>> properly. If this is NFS or other non local file system, can you copy the
>>>>>>>> image to /tmp, rerun the MPI command to get it to fail again, and try
>>>>>>>> again?
>>>>>>>>
>>>>>>>> Thanks!
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M.
>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>
>>>>>>>>>> Hi Steve,
>>>>>>>>>>
>>>>>>>>>> That means there is an active file descriptor/process still
>>>>>>>>>> running and attached to the container maintaining a shared lock. You can
>>>>>>>>>> run other commands against the container as long as long as the container
>>>>>>>>>> is not being requested as --writable(-w), because that will try and obtain
>>>>>>>>>> an exclusive lock and it will fail if there are any active shared locks.
>>>>>>>>>> Try an "lsof /path/to/c7" to see what processes are attached to it. You may
>>>>>>>>>> see a list like:
>>>>>>>>>>
>>>>>>>>>> # lsof /tmp/Demo-2.img
>>>>>>>>>> COMMAND    PID USER   FD   TYPE DEVICE   SIZE/OFF      NODE NAME
>>>>>>>>>> sexec   107975 root    6rR  REG  253,0 1073741856 202112247
>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>> sexec   107977 root    6r   REG  253,0 1073741856 202112247
>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>> bash    107982 root    6r   REG  253,0 1073741856 202112247
>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>
>>>>>>>>>> Notice the two top ones are 'sexec' which are part of the
>>>>>>>>>> Singularity process stack. Kill the bottom one, and those should go away
>>>>>>>>>> naturally.
>>>>>>>>>>
>>>>>>>>>> BTW, as long as you have installed Singularity as root, there is
>>>>>>>>>> no need to run Singularity commands as root (unless you want to make system
>>>>>>>>>> changes within the container).
>>>>>>>>>>
>>>>>>>>>> Hope that helps!
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <
>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>>> Running mpirun tests, when an abort occurs, my image ends up
>>>>>>>>>>> locked.  Is there a way to clear the lock without rebooting?  I looked for
>>>>>>>>>>> processes that I could kill, but didn't see anything worthy.
>>>>>>>>>>>
>>>>>>>>>>> I'm using singularity v2.1 on Centos 7.2 (both host and
>>>>>>>>>>> container).
>>>>>>>>>>>
>>>>>>>>>>> Regards,
>>>>>>>>>>>
>>>>>>>>>>> Steve
>>>>>>>>>>>
>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H mach1,mach2
>>>>>>>>>>> singularity exec c7 /usr/bin/ring
>>>>>>>>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>>>>>>>>> Process 0 sent to 1
>>>>>>>>>>> Process 0 decremented value: 9
>>>>>>>>>>> Process 0 decremented value: 8
>>>>>>>>>>> Process 0 decremented value: 7
>>>>>>>>>>> Process 0 decremented value: 6
>>>>>>>>>>> Process 0 decremented value: 5
>>>>>>>>>>> Process 0 decremented value: 4
>>>>>>>>>>> Process 0 decremented value: 3
>>>>>>>>>>> Process 0 decremented value: 2
>>>>>>>>>>> Process 0 decremented value: 1
>>>>>>>>>>> Process 0 decremented value: 0
>>>>>>>>>>> Process 0 exiting
>>>>>>>>>>> Process 1 exiting
>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H
>>>>>>>>>>> mach0,mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>
>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>> It appears as if there is not enough space for
>>>>>>>>>>> /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory
>>>>>>>>>>> backing
>>>>>>>>>>> file). It is likely that your MPI job will now either abort or
>>>>>>>>>>> experience
>>>>>>>>>>> performance degradation.
>>>>>>>>>>>
>>>>>>>>>>>   Local host:  mach0
>>>>>>>>>>>   Space Requested: 4194312 B
>>>>>>>>>>>   Space Available: 0 B
>>>>>>>>>>>
>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>> [mach0:02308] create_and_attach: unable to create shared memory
>>>>>>>>>>> BTL coordinating structure :: size 134217728
>>>>>>>>>>> [mach0:02291] 2 more processes have sent help message
>>>>>>>>>>> help-opal-shmem-mmap.txt / target full
>>>>>>>>>>> [mach0:02291] Set MCA parameter "orte_base_help_aggregate" to 0
>>>>>>>>>>> to see all help / error messages
>>>>>>>>>>> ^CKilled by signal 2.
>>>>>>>>>>> Killed by signal 2.
>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2308
>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2309
>>>>>>>>>>> [warn] Epoll ADD(4) on fd 31 failed.  Old events were 0; read
>>>>>>>>>>> change was 0 (none); write change was 1 (add): Bad file descriptor
>>>>>>>>>>> ^C[root@mach0 ~]singularity shell -w c7
>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>> [root@mach0 ~]# tail -30 /var/log/messages
>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon
>>>>>>>>>>> management.
>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Reached target Multi-User System.
>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.
>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about System
>>>>>>>>>>> Runlevel Changes...
>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data
>>>>>>>>>>> Collection 10s After Completed Startup.
>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Update UTMP about System
>>>>>>>>>>> Runlevel Changes.
>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel
>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]
>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel
>>>>>>>>>>> arming.
>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms
>>>>>>>>>>> (kernel) + 1.100s (initrd) + 4.931s (userspace) = 6.446s.
>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.
>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting user-0.slice.
>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user root.
>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Started Session 1 of user root.
>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user root.
>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2023)>
>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)>
>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Image is
>>>>>>>>>>> locked by another process
>>>>>>>>>>> Jul 14 10:42:36 mach0 kernel: loop: module loaded
>>>>>>>>>>> Jul 14 10:43:38 mach0 Singularity: sexec (U=0,P=2050)>
>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>> Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>> Jul 14 10:49:17 mach0 Singularity: sexec (U=0,P=2203)>
>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>> Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>> Jul 14 10:50:39 mach0 Singularity: sexec (U=0,P=2244)>
>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>> Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)>
>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2300)>
>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>> Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted
>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)>
>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Image is
>>>>>>>>>>> locked by another process
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> --
>>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from
>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> --
>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>> University of California
>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>>> Groups "singularity" group.
>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> Gregory M. Kurtzer
>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>> University of California
>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>
>>>>>>> --
>>>>>>> You received this message because you are subscribed to the Google
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a11407c56bd81a40537ae986d
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">=
On Fri, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:sgmeh...@gmail.com" target=3D"_blank">sgmeh...@gmail.com</a>&g=
t;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0=
px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-colo=
r:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div>Hello,</div><div=
><br></div><div>I went back and recreated everything from scratch using v2.=
1.=C2=A0 When I moved the image file to the mach1 and mach2 nodes I used ta=
r with the -S option.=C2=A0 I used /tmp for the image files.=C2=A0 After ev=
erything was set I ran mpirun on each individual system (only starting jobs=
 on that system)=C2=A0with no errors.=C2=A0 For some reason when I ran mpir=
un from mach0 and asked for jobs to be run on mach0 and another system I ge=
t the error.=C2=A0 The mpirun complains about not being able to create a fi=
le (already exists) and hangs.=C2=A0 The singularity image file on mach0 an=
d the other system selected are locked with pids that are no longer running=
.</div><div><br></div><div>Saying all that, I decided to go back and put --=
debug on the singularity command in the mpirun to get you better diagnostic=
s.=C2=A0 I rebooted mach0 and mach2, then reran the command and it worked c=
orrectly.=C2=A0 I then removed the --debug command and it worked.=C2=A0 I a=
dded mach1 to the mix and everything worked fine.=C2=A0 What? I guess the b=
ottom line is that if the mpirun aborts (and it seems to hang) then you can=
 get this condition.</div></div></blockquote><div><br></div><div>So does th=
e problem seem to arise only when there is a failure on one of the processe=
s, and MPI kills the remaining processes?<br></div><div><br></div><div>Ralp=
h, do you know how OMPI kills the leftover processes in a job abort and wha=
t signal it uses?</div><div><br></div><div>(additional comments inline)</di=
v><div><br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px =
0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:r=
gb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><br>Steve</div><div=
><br></div><div><br></div><div><font face=3D"courier new,monospace">[root@m=
ach0 ompi]# mpirun --allow-run-as-root -np 2 -H mach2 singularity exec /tmp=
/c7new.img /usr/bin/ring<br>Process 1 exiting<span class=3D""><br>Process 0=
 sending 10 to 1, tag 201 (2 processes in ring)<br>Process 0 sent to 1<br>P=
rocess 0 decremented value: 9<br>Process 0 decremented value: 8<br>Process =
0 decremented value: 7<br>Process 0 decremented value: 6<br>Process 0 decre=
mented value: 5<br>Process 0 decremented value: 4<br>Process 0 decremented =
value: 3<br>Process 0 decremented value: 2<br>Process 0 decremented value: =
1<br>Process 0 decremented value: 0<br>Process 0 exiting<br></span></font><=
/div></div></blockquote><div><br></div><div>Is /tmp NFS mounted or shared?<=
/div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px=
 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-co=
lor:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><font face=3D"=
courier new,monospace"><span class=3D""></span>[root@mach0 ompi]# mpirun --=
allow-run-as-root -np 4 -H mach0,mach2 singularity exec /tmp/c7new.img /usr=
/bin/ring<br><font color=3D"#ff0000">ERROR=C2=A0 : Could not create directo=
ry /tmp/.singularity-session-0.64768.158221: File exists<br>ERROR=C2=A0 : F=
ailed creating session directory: /tmp/.singularity-session-0.64768.158221<=
/font><br>-------------------------------------------------------<br>Primar=
y job=C2=A0 terminated normally, but 1 process returned<br>a non-zero exit =
code. Per user-direction, the job has been aborted.<br>--------------------=
-----------------------------------<br><font color=3D"#ff0000">^C</font>Kil=
led by signal 2.<br>mpirun: abort is already in progress...hit ctrl-c again=
 to forcibly terminate</font></div></div></blockquote><div><br></div><div>A=
hhhhh, I think you may have found a race condition in Singularity which kil=
led one of the processes. Then the MPI killed the rest.</div><div>=C2=A0</d=
iv><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;bord=
er-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,204=
);padding-left:1ex"><div dir=3D"ltr"><div><font face=3D"courier new,monospa=
ce">[root@mach0 ompi]# singularity shell -w /tmp/c7new.img<span class=3D"">=
<br>ERROR=C2=A0 : Image is locked by another process<br></span>[root@mach0 =
ompi]# lslocks<span class=3D""><br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START END PATH=
<br></span>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 473 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 =
0 /run/lvmetad.pid<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 606 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 820 FLOCK=C2=A0 33B WRITE 0=C2=A0=
=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.pid<br>mas=
ter=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 820 F=
LOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/pos=
tfix/master.lock<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 924 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 =
0 /run/slurmctld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 1103 POSIX=C2=A0=C2=A0 5B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=
=C2=A0=C2=A0 0 /run/slurmd.pid<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 <font color=3D"#ff0000">2477</font> FLOCK=C2=A0=C2=A0 0B READ=
=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>[root@mach0 ompi]# ls=
 /proc/2477<br>ls: cannot access /proc/2477: No such file or directory<br><=
/font></div></div></blockquote><div><br></div><div>OK, that is weird. If pr=
ocess 2477 is dead, there should be no active lock. I would guess this migh=
t be a kernel bug? What distribution and kernel are you running?</div><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0=
.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(20=
4,204,204);padding-left:1ex"><div dir=3D"ltr"><div><font face=3D"courier ne=
w,monospace">[root@mach0 ompi]# ssh mach2<br>Last login: Fri Jul 15 07:52:5=
6 2016 from mach0<br>[root@mach2 ~]# singularity shell -w /tmp/c7new.img<sp=
an class=3D""><br>ERROR=C2=A0 : Image is locked by another process<br></spa=
n>[root@mach2 ~]# lslocks<span class=3D""><br>COMMAND=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M ST=
ART END PATH<br></span>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 473 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=
=C2=A0=C2=A0 0 /run/lvmetad.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 851 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.pid<br>master=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 851 FLOCK=
=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix=
/master.lock<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <font color=
=3D"#ff0000">10110</font> FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=
=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 668 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=
=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br>slurmd=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1610 POSIX=C2=A0=C2=A0 5B WRITE =
0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmd.pid<br>[root@mach2 ~]=
# ls /proc/10110<br>ls: cannot access /proc/10110: No such file or director=
y</font></div></div></blockquote><div><br></div><div>Very weird. Seems both=
 nodes have the same issue..?</div><div><br></div><div>I wonder if I can re=
plicate this by kill -9 the outer parent of a Singularity run. I will test =
when I get to my development box.</div><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex">=
<div dir=3D"ltr"><div><br></div><div>Good Run:</div><div><br></div><div>[<f=
ont face=3D"courier new,monospace">root@mach0 ompi]# mpirun --allow-run-as-=
root -np 6 -H mach0,mach1,mach2 singularity exec /tmp/c7new.img /usr/bin/ri=
ng<br>Process 0 sending 10 to 1, tag 201 (6 processes in ring)<span class=
=3D""><br>Process 0 sent to 1<br></span>Process 1 exiting<span class=3D""><=
br>Process 0 decremented value: 9<br>Process 0 decremented value: 8<br>Proc=
ess 0 decremented value: 7<br>Process 0 decremented value: 6<br>Process 0 d=
ecremented value: 5<br>Process 0 decremented value: 4<br>Process 0 decremen=
ted value: 3<br>Process 0 decremented value: 2<br>Process 0 decremented val=
ue: 1<br>Process 0 decremented value: 0<br>Process 0 exiting<br></span>Proc=
ess 2 exiting<br>Process 5 exiting<br>Process 3 exiting<br>Process 4 exitin=
g</font></div></div></blockquote><div><br></div><div>So as long as we don&#=
39;t crash (or get killed), all seems fine... Interesting.</div><div><br></=
div><div>I will try and replicate the error condition.</div><div><br></div>=
<div>Greg</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"=
margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;bord=
er-left-color:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><br>=
</div><span class=3D""><div><br></div><div><br>On Thursday, July 14, 2016 a=
t 3:56:48 PM UTC-6, Gregory M. Kurtzer wrote:</div></span><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-=
left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid">=
<span class=3D""><div dir=3D"ltr">I am wondering if some aspect of the spar=
se file has caused this. Sparse files are weird, and do require some attent=
ion when copying around. With that said, I do not think it should ever corr=
upt an image, but it might take along garbage data with it. One way to rete=
st this is to go back to the original image, and tar it before sending usin=
g the -S/--sparse tar option when packaging it up. Then bring it to the oth=
er test system and see if it still does that.<div><br></div><div>Aside from=
 that, I&#39;m a bit confused as to what is happening. There should be no i=
ssue with 2.0 vs. 2.1, but it might be worth checking with some images also=
 created with 2.1 on the same target system where you are seeing the proble=
m.</div></div></span><div><div class=3D"h5"><div><br><div class=3D"gmail_qu=
ote">On Thu, Jul 14, 2016 at 2:46 PM, Steve Mehlberg <span dir=3D"ltr">&lt;=
<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;bord=
er-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:soli=
d"><div dir=3D"ltr"><div>It is very possible that I did copy it from anothe=
r system - maybe created it with singularity 2.0 and now running it with 2.=
1??=C2=A0 I will start over and recreate from scratch and see if I still ha=
ve the problem.</div><div><br></div><div>Sorry, I checked there is no /proc=
/2299</div><div><br></div><div>The only way I&#39;ve found to get the file =
unlocked is to reboot.=C2=A0 Not an issue with the little VM.</div><span><f=
ont color=3D"#888888"><div><br></div></font></span><div><span><font color=
=3D"#888888">Steve</font></span><span><br><br>On Thursday, July 14, 2016 at=
 3:30:12 PM UTC-6, Gregory M. Kurtzer wrote:</span></div><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-=
left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid">=
<span><div dir=3D"ltr">Very odd...<div><br></div><div>I&#39;m concerned abo=
ut the Buffer I/O errors. It is almost like the image itself has a problem =
with it. Did you copy this image from another system? I wonder if it is the=
 sparseness...</div><div><br></div><div>Does the directory exist for /proc/=
2299/ ?</div><div><br></div><div>Can you kill it with a -9?</div></div></sp=
an><div><div><div><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 2:=
27 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail=
.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);=
border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div>Seems =
that process isn&#39;t running any more.=C2=A0 I did find this in the /var/=
log/messages file:</div><div><br></div><div><font face=3D"courier new,monos=
pace">[root@mach0 ~]# cat /var/log/messages | grep 2299<span><br>Jul 14 10:=
51:34 mach0 Singularity: sexec (U=3D0,P=3D2299)&gt; Command=3Dexec, Contain=
er=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br></span>Jul 14 11:37:17 mach0 =
kernel: Buffer I/O error on device loop1, logical block 22299<br>Jul 14 11:=
37:17 mach0 kernel: Buffer I/O error on device loop1, logical block 22990<b=
r>...</font></div><div><font face=3D"courier new,monospace"><br></font></di=
v><div><font face=3D"courier new,monospace">[root@mach0 ~]# ps -ef |grep 22=
99<br>root=C2=A0=C2=A0=C2=A0=C2=A0 10545=C2=A0 2002=C2=A0 0 14:15 pts/0=C2=
=A0=C2=A0=C2=A0 00:00:00 grep --color=3Dauto 2299<br>[root@mach0 ~]# ps |gr=
ep 2299</font><br></div><span><div><br>On Thursday, July 14, 2016 at 2:59:2=
6 PM UTC-6, Gregory M. Kurtzer wrote:</div></span><blockquote class=3D"gmai=
l_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-col=
or:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><span><d=
iv dir=3D"ltr">What is running at PID=C2=A0<span style=3D"font-size:12.8px"=
>2299?</span></div></span><div><div><div><br><div class=3D"gmail_quote">On =
Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=
=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D=
"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-lef=
t-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><di=
v dir=3D"ltr">Ok, I see my error with shell vs exec.=C2=A0 I&#39;m running =
on a VM with my own space, no NFS involved.=C2=A0 Here is the debug command=
 run and lsofl<br><br>[root@mach0 ~]# df -h<br>Filesystem=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 Size=C2=A0 Used Avai=
l Use% Mounted on<br>/dev/mapper/vg1-lv001=C2=A0 5.7G=C2=A0 4.3G=C2=A0 1.1G=
=C2=A0 80% /<br>devtmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 911M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 911M=
=C2=A0=C2=A0 0% /dev<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0=
=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=C2=A0 0% /dev/shm<br>tmpfs=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 920M=C2=A0=C2=A0 41M=C2=A0 880M=C2=A0=C2=A0 5% /run<br>tmpfs=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=
=C2=A0 0% /sys/fs/cgroup<br>/dev/vda1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 190M=C2=A0 110M=C2=A0=C2=A0 67M=
=C2=A0 63% /boot<br>/dev/mapper/vg1-lv002=C2=A0=C2=A0 12G=C2=A0 203M=C2=A0=
=C2=A0 11G=C2=A0=C2=A0 2% /var<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 184M=C2=
=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 184M=C2=A0=C2=A0 0% /run/user/0<br>[root@mach=
0 ~]# singularity --debug shell -w c7<br>enabling debugging<br>ending argum=
ent loop<br>Exec&#39;ing: /usr/local/libexec/singularity/cli/shell.exec -w+=
 &#39;[&#39; -f /usr/local/etc/singularity/init &#39;]&#39;<br>+ . /usr/loc=
al/etc/singularity/init<br>++ unset module<br>++ PATH=3D/usr/local/sbin:/us=
r/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin<br>+=
+ HISTFILE=3D/dev/null<br>++ export PATH HISTFILE<br>++ &#39;[&#39; -n &#39=
;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[=
&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;=
<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39=
; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39;=
 -n &#39;&#39; &#39;]&#39;<br>+ true<br>+ case ${1:-} in<br>+ shift<br>+ SI=
NGULARITY_WRITABLE=3D1<br>+ export SINGULARITY_WRITABLE<br>+ true<br>+ case=
 ${1:-} in<br>+ break<br>+ &#39;[&#39; -z c7 &#39;]&#39;<br>+ SINGULARITY_I=
MAGE=3Dc7<br>+ export SINGULARITY_IMAGE<br>+ shift<br>+ exec /usr/local/lib=
exec/singularity/sexec<br>VERBOSE [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 message.c:52:init()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Set messagelevel to: 5<br>DEBUG=C2=A0=C2=A0 [U=3D0,=
P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:127:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Gathering an=
d caching user info.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 privilege.c:43:get_user_privs()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called get_user_privs(struc=
t s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 privilege.c:54:get_user_privs()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning get_user_privs(st=
ruct s_privinfo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:134:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we can escalate=
 privs properly.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 privilege.c:61:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called escalate_privs(void)<br>DE=
BUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privil=
ege.c:73:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Returning escalate_privs(void) =3D 0<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:141:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set=
ting privs to calling user<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:79:drop_privs()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Called drop_privs(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_privs()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Dropping privileges to GID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:93:dro=
p_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping privileges to UID =3D &#39;0&#39;<br>DE=
BUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privil=
ege.c:103:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct GID<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege=
.c:109:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct UID<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:=
115:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning drop_privs(struct s_privinfo *uinfo=
) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:146:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Obtaining user&#39;s homedir<br>DEBUG=C2=A0=C2=A0 [=
U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:150:main()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtain=
ing file descriptor to current directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D80=
45]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:155:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Getting current wo=
rking directory path string<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:161:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_COMMAND =
from environment<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:168:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_IMAGE from environme=
nt<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:174:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Checking container image is a file: c7<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:180:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Building configuration file location<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:183:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Config location: /usr=
/local/etc/singularity/singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D804=
5]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:185:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity =
configuration is a file: /usr/local/etc/singularity/singularity.conf<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c=
:191:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Checking Singularity configuration file is owned by root<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:1=
97:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Opening Singularity configuration file<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:210:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singul=
arity configuration for &#39;sessiondir prefix&#39;<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:47:confi=
g_get_key_value()=C2=A0 : Called config_get_key_value(fp, sessiondir prefix=
)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 config_parser.c:66:config_get_key_value()=C2=A0 : Return config_get_key_va=
lue(fp, sessiondir prefix) =3D NULL<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:48:file_id()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called file_id(c7)<br>VERBO=
SE [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:58:file_id()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Generated=
 file_id: 0.64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 file.c:60:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning file_id(c7) =3D 0.64768.26052=
<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
sexec.c:217:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Set sessiondir to: /tmp/.singularity-session-0.64768.26052<b=
r>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 se=
xec.c:221:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Set containername to: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D804=
5]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:223:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting loop_dev_* pa=
ths<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 config_parser.c:47:config_get_key_value()=C2=A0 : Called config_get_key=
_value(fp, container dir)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:58:config_get_key_value()=C2=A0 : =
Return config_get_key_value(fp, container dir) =3D /var/singularity/mnt<br>=
DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexe=
c.c:232:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Set image mount path to: /var/singularity/mnt<br>LOG=C2=A0=C2=
=A0=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.=
c:234:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Command=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c=
:247:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Set prompt to: Singularity.c7&gt;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:249:main()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we are=
 opening image as read/write<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:264:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening image as read/write on=
ly: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:271:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Setting exclusive lock on file descriptor: 6<br>ERR=
OR=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c=
:273:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Image is locked by another process<span><br>[root@mach0 ~]# lsof c=
7<br></span>[root@mach0 ~]# lslocks<br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START EN=
D PATH<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 601 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=
=A0=C2=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.pid<br>master=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 3=
3B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix/master.=
lock<br>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 478 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /=
run/lvmetad.pid<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 940 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 =
0 /run/slurmctld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 966 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0=C2=A0 0 /run/slurmd.pid<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 2299 FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0=C2=A0 0 /<br><br><br>On Thursday, July 14, 2016 at 1:03:50 PM U=
TC-6, Gregory M. Kurtzer wrote:<div><div><blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(20=
4,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">=
<br><div><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 11:50 AM, S=
teve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>=
&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px=
 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-l=
eft-width:1px;border-left-style:solid"><div dir=3D"ltr">Gregory,<br><br>Tha=
nks for the quick suggestions.=C2=A0 There don&#39;t seem to be any process=
es attached to the container and I can&#39;t seem to run other commands (in=
 read mode). I&#39;m not sure what&#39;s going on.<br><br>Just lazy with ro=
ot right now, I need to create a normal uid.<br></div></blockquote><div><br=
></div><div>Ahh, ok. Been there, done that! lol</div><div>=C2=A0</div><bloc=
kquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left=
:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-s=
tyle:solid"><div dir=3D"ltr"><br>Steve<br><br><span style=3D"font-family:&q=
uot;courier new&quot;,monospace">[root@mach0 ~]# ls -la c7<br>-rwxr-xr-x 1 =
root root 1610612769 Jul 14 10:49 c7<br>[root@mach0 ~]# singularity shell -=
w c7<span><br>ERROR=C2=A0 : Image is locked by another process<br></span></=
span></div></blockquote><div><br></div><div>Can you run this command again =
in --debug mode (singularity --debug ....)</div><div>=C2=A0</div><blockquot=
e class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;=
border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:=
solid"><div dir=3D"ltr"><span style=3D"font-family:&quot;courier new&quot;,=
monospace"><span></span>[root@mach0 ~]# lsof c7<br></span></div></blockquot=
e><div><br></div><div>What about the command &quot;lslocks&quot;?</div><div=
>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width=
:1px;border-left-style:solid"><div dir=3D"ltr"><span style=3D"font-family:&=
quot;courier new&quot;,monospace">[root@mach0 ~]# singularity shell c7 whoa=
mi<br>/usr/bin/whoami: /usr/bin/whoami: cannot execute binary file<br></spa=
n></div></blockquote><div><br></div><div>Ahh, this is normal. You are askin=
g the shell script to read in /usr/bin/whoami. If you want to use shell to =
run whoami, you must prefix it with the -c (e.g. -c &quot;whoami [args]&quo=
t;), or use the &#39;exec&#39; Singularity subcommand: &quot;singularity ex=
ec c7 whoami&quot;</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(20=
4,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">=
<span style=3D"font-family:&quot;courier new&quot;,monospace">[root@mach0 ~=
]#<br>[root@mach0 ~]# ps -ef |grep c7<br>root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 7479=C2=A0 2002=C2=A0 0 11:49 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --colo=
r=3Dauto c7<br>[root@mach0 ~]#</span></div></blockquote><div><br></div><div=
>What kind of file system does your image exist on? Is it NFS by chance? I =
wonder if there is a host issue with a locking daemon or something else wei=
rd going on where it is not giving the exclusive lock properly. If this is =
NFS or other non local file system, can you copy the image to /tmp, rerun t=
he MPI command to get it to fail again, and try again?=C2=A0</div><div><br>=
</div><div>Thanks!</div><div><br></div><div><br></div><div>=C2=A0</div><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-lef=
t:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-=
style:solid"><div dir=3D"ltr"><span><br><br><br><br>On Thursday, July 14, 2=
016 at 12:30:01 PM UTC-6, Gregory M. Kurtzer wrote:</span><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-=
left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid">=
<span><div dir=3D"ltr">Hi Steve,<div><br></div><div>That means there is an =
active file descriptor/process still running and attached to the container =
maintaining a shared lock. You can run other commands against the container=
 as long as long as the container is not being requested as --writable(-w),=
 because that will try and obtain an exclusive lock and it will fail if the=
re are any active shared locks. Try an &quot;lsof /path/to/c7&quot; to see =
what processes are attached to it. You may see a list like:</div><div><br><=
/div><div><div># lsof /tmp/Demo-2.img=C2=A0</div><div>COMMAND =C2=A0 =C2=A0=
PID USER =C2=A0 FD =C2=A0 TYPE DEVICE =C2=A0 SIZE/OFF =C2=A0 =C2=A0 =C2=A0N=
ODE NAME</div><div>sexec =C2=A0 107975 root =C2=A0 =C2=A06rR =C2=A0REG =C2=
=A0253,0 1073741856 202112247 /tmp/Demo-2.img</div><div>sexec =C2=A0 107977=
 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 1073741856 202112247 /tmp/Demo=
-2.img</div><div>bash =C2=A0 =C2=A0107982 root =C2=A0 =C2=A06r =C2=A0 REG =
=C2=A0253,0 1073741856 202112247 /tmp/Demo-2.img</div></div><div><br></div>=
<div>Notice the two top ones are &#39;sexec&#39; which are part of the Sing=
ularity process stack. Kill the bottom one, and those should go away natura=
lly.</div><div><br></div><div>BTW, as long as you have installed Singularit=
y as root, there is no need to run Singularity commands as root (unless you=
 want to make system changes within the container).</div><div><br></div><di=
v>Hope that helps!</div><div><br></div></div></span><div><br><div class=3D"=
gmail_quote"><div><div>On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <sp=
an dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote=
:<br></div></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px =
0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-w=
idth:1px;border-left-style:solid"><div><div><div dir=3D"ltr">Running mpirun=
 tests, when an abort occurs, my image ends up locked.=C2=A0 Is there a way=
 to clear the lock without rebooting?=C2=A0 I looked for processes that I c=
ould kill, but didn&#39;t see anything worthy.<br><br>I&#39;m using singula=
rity v2.1 on Centos 7.2 (both host and container).<br><br>Regards,<br><br>S=
teve<br><br><span style=3D"font-family:&quot;courier new&quot;,monospace">[=
root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H mach1,mach2 singularity e=
xec c7 /usr/bin/ring<br>Process 0 sending 10 to 1, tag 201 (2 processes in =
ring)<br>Process 0 sent to 1<br>Process 0 decremented value: 9<br>Process 0=
 decremented value: 8<br>Process 0 decremented value: 7<br>Process 0 decrem=
ented value: 6<br>Process 0 decremented value: 5<br>Process 0 decremented v=
alue: 4<br>Process 0 decremented value: 3<br>Process 0 decremented value: 2=
<br>Process 0 decremented value: 1<br>Process 0 decremented value: 0<br>Pro=
cess 0 exiting<br>Process 1 exiting<br>[root@mach0 ~]# mpirun --allow-run-a=
s-root -n 3 -H mach0,mach1,mach2 singularity exec c7 /usr/bin/ring<br>-----=
---------------------------------------------------------------------<br>It=
 appears as if there is not enough space for /tmp/ompi.mach0.2291/54935/1/0=
/vader_segment.mach0.0 (the shared-memory backing<br>file). It is likely th=
at your MPI job will now either abort or experience<br>performance degradat=
ion.<br><br>=C2=A0 Local host:=C2=A0 mach0<br>=C2=A0 Space Requested: 41943=
12 B<br>=C2=A0 Space Available: 0 B<br>------------------------------------=
--------------------------------------<br>[mach0:02308] create_and_attach: =
unable to create shared memory BTL coordinating structure :: size 134217728=
<br>[mach0:02291] 2 more processes have sent help message help-opal-shmem-m=
map.txt / target full<br>[mach0:02291] Set MCA parameter &quot;orte_base_he=
lp_aggregate&quot; to 0 to see all help / error messages<br>^CKilled by sig=
nal 2.<br>Killed by signal 2.<br>Singularity is sending SIGKILL to child pi=
d: 2308<br>Singularity is sending SIGKILL to child pid: 2309<br>[warn] Epol=
l ADD(4) on fd 31 failed.=C2=A0 Old events were 0; read change was 0 (none)=
; write change was 1 (add): Bad file descriptor<br>^C[root@mach0 ~]singular=
ity shell -w c7<br>ERROR=C2=A0 : Image is locked by another process<br>[roo=
t@mach0 ~]# tail -30 /var/log/messages<br>Jul 14 10:42:17 mach0 systemd: St=
arted LSB: slurm daemon management.<br>Jul 14 10:42:17 mach0 systemd: Reach=
ed target Multi-User System.<br>Jul 14 10:42:17 mach0 systemd: Starting Mul=
ti-User System.<br>Jul 14 10:42:17 mach0 systemd: Starting Update UTMP abou=
t System Runlevel Changes...<br>Jul 14 10:42:17 mach0 systemd: Started Stop=
 Read-Ahead Data Collection 10s After Completed Startup.<br>Jul 14 10:42:17=
 mach0 systemd: Started Update UTMP about System Runlevel Changes.<br>Jul 1=
4 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel<br>Jul 14 10:42:18 ma=
ch0 kdumpctl: Starting kdump: [OK]<br>Jul 14 10:42:18 mach0 systemd: Starte=
d Crash recovery kernel arming.<br>Jul 14 10:42:18 mach0 systemd: Startup f=
inished in 415ms (kernel) + 1.100s (initrd) + 4.931s (userspace) =3D 6.446s=
.<br>Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.<br>Jul 14 1=
0:42:34 mach0 systemd: Starting user-0.slice.<br>Jul 14 10:42:34 mach0 syst=
emd-logind: New session 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: S=
tarted Session 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: Starting S=
ession 1 of user root.<br>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=
=3D2023)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/r=
ing<br>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D2024)&gt; Comman=
d=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42=
:36 mach0 Singularity: sexec (U=3D0,P=3D2024)&gt; Image is locked by anothe=
r process<br>Jul 14 10:42:36 mach0 kernel: loop: module loaded<br>Jul 14 10=
:43:38 mach0 Singularity: sexec (U=3D0,P=3D2050)&gt; Command=3Dshell, Conta=
iner=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:43:38 mach0 kernel: EXT4=
-fs (loop0): mounted filesystem with ordered data mode. Opts: discard<br>Ju=
l 14 10:49:17 mach0 Singularity: sexec (U=3D0,P=3D2203)&gt; Command=3Dshell=
, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:49:17 mach0 kerne=
l: EXT4-fs (loop0): mounted filesystem with ordered data mode. Opts: discar=
d<br>Jul 14 10:50:39 mach0 Singularity: sexec (U=3D0,P=3D2244)&gt; Command=
=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:50:=
39 mach0 kernel: EXT4-fs (loop0): mounted filesystem with ordered data mode=
. Opts: discard<br>Jul 14 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2299=
)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>=
Jul 14 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2300)&gt; Command=3Dexe=
c, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mac=
h0 kernel: EXT4-fs (loop0): mounted filesystem with ordered data mode. Opts=
: discard<br>Jul 14 10:51:57 mach0 Singularity: sexec (U=3D0,P=3D2322)&gt; =
Command=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:51=
:57 mach0 Singularity: sexec (U=3D0,P=3D2322)&gt; Image is locked by anothe=
r process</span><span><font color=3D"#888888"><br><br><br></font></span></d=
iv></div></div><span><font color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance C=
omputing Services (HPCS)<br>University of California<br>Lawrence Berkeley N=
ational Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></d=
iv>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div class=3D""><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>

--001a11407c56bd81a40537ae986d--
