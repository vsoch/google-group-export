X-Received: by 2002:a17:902:1347:: with SMTP id r7mr1742251ple.133.1551325354691;
        Wed, 27 Feb 2019 19:42:34 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a62:b50c:: with SMTP id y12ls1367707pfe.0.gmail; Wed, 27 Feb
 2019 19:42:33 -0800 (PST)
X-Received: by 2002:a63:7c07:: with SMTP id x7mr6392760pgc.284.1551325353312;
        Wed, 27 Feb 2019 19:42:33 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1551325353; cv=none;
        d=google.com; s=arc-20160816;
        b=LMeho0OHOxovUB3PoC3aGI0vhwLYPQ7SPJbZT7ibd0sGwj9eZVqS1jTDZyRfx567FJ
         NTiZYMKqEK7KDOkq5SU7XUrxVUQ+HKQzeooBh2OmbWhkmZviW6GuwG51XagdiR2NpS8s
         PxRjTSCmFwriYDk1edP0AuXLFv0GXVSbNVGnBDGx+EjHhg0dsoVmLwNiu1IfWk7CCe9u
         LYXdb16D/AYMxVG3HrS+tsb5478mPGiOau9czNyPCgflmTbkdE1ieGyNyGSnzEpkDFN2
         4vhlPugsvkmjDz/jZCELugkMP+nrjU/gLqzvZbNUEheXxnvT/LGP7EUkpDf9NJaLl31a
         iAhg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature;
        bh=4L0O84HHgb+BO1upH3F19f/7KO3M+OK7SBE5NRJGt4o=;
        b=IdQDYP9pZHsVN6uAruR9XVh4Mk897EYAnzpjPvuKcF6SzOVswlUEr1ivJ5dL7wK/0f
         ldmS9dO67X6GZ5LuEgrApywskByaUfLla5Fbd3xOaFqEHz8fMgNu3CtCR/OkhCcT4MQF
         Nnf0SCiZmbUhhlParWNUyC8jAFy7El7yf37HZmJoLMWSKAK3VeegflTDR549uHenzTNe
         dXp2C4OUhRqUhVMJPtYDqHaweCLTD8QOwEC2W+OQSD8Z/rWJzU7VS5hv0fq8PbBzi90W
         3+8VzKItg5Dm+j9MRut1EEzS7gEFc9+ORY00CT37DlL6ob6k9EHiZnIDsM2EjhKtG/Yx
         OrKw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Wkzk5eIv;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.160.179 as permitted sender) smtp.mailfrom=gmku...@gmail.com
Return-Path: <gmku...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [131.243.228.52])
        by mx.google.com with ESMTP id l10si15221163pgp.25.2019.02.27.19.42.33
        for <singu...@lbl.gov>;
        Wed, 27 Feb 2019 19:42:33 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@gmail.com designates 209.85.160.179 as permitted sender) client-ip=209.85.160.179;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Wkzk5eIv;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.160.179 as permitted sender) smtp.mailfrom=gmku...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2GBBgC7V3dchrOgVdFkgheBWlCBETMnh?=
 =?us-ascii?q?AEHgR2CXo9sgg2JP4hvhgaBKzwlARWEPgKEDiI3Bg0BAwEBAgEBAgEBAhABAQE?=
 =?us-ascii?q?ICwsIGw4jDII6BQIDHwcOTTswAQEBAQEBAQEBAR8CLSkBGgECAgEjHQENDh4DA?=
 =?us-ascii?q?QsGBQQHNwICIQEBDgMBBQEcBggHBAEcBIJ/JAGBNQEDDQgFoFU8iVsagSaBEgU?=
 =?us-ascii?q?BF4J5BYQ5ChknDV+BNwIGEoMfiRcXPoFBg3UugleBdwESAQuDHoI1IgKRXpEwK?=
 =?us-ascii?q?TMHAgKPK4M9GZMckWGLKzCBPFgwcXCBAYI7gjYcgziKdCEwEI9wDhcwgXcBAQ?=
X-IronPort-AV: E=Sophos;i="5.58,421,1544515200"; 
   d="scan'208,217";a="145313458"
Received: from mail-qt1-f179.google.com ([209.85.160.179])
  by fe3.lbl.gov with ESMTP; 27 Feb 2019 19:42:32 -0800
Received: by mail-qt1-f179.google.com with SMTP id p25so22008591qtb.3
        for <singu...@lbl.gov>; Wed, 27 Feb 2019 19:42:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=4L0O84HHgb+BO1upH3F19f/7KO3M+OK7SBE5NRJGt4o=;
        b=Wkzk5eIv0k7G6gMcl+83bTSLDhtchFE4iizTmAHfHqGdMW7EkNxf1v/2sEK+y7vyem
         lZBtT7Ki3DcFLAhgcNO7TQthAShpZ4r2y5VPcB97JgE5aNIPeTVnF1Lls/wjU7kUuuqv
         UgajOXXWAAu4pBZef8OHAjlyGW3K80cTYYFUl4LyzbDqInoQVnZZ05Dfy9mCgfw+4mH6
         ShfRYL3B5QQoBbTL4cBEZcKDhcasiws3XQGDouPAP9Zay4jnaHYgyc+W1p56mKUYxrgD
         CM0IJPGNjp/vxNpiobejauO/gr6FKwedE0w0WeJkFCO3s5tsyR3yk/mbEY85MqlhLQOf
         t3bA==
X-Gm-Message-State: APjAAAVwqPzgIjZfV+1WClmNXxCNy5GdsTmw50zowDUCqZluEuP27dio
	hrB9zOytK82a2ds2YEagc8HU7lXq1UXE9ft1UVcrFw==
X-Received: by 2002:ac8:2ff1:: with SMTP id m46mr4537318qta.267.1551325350989;
 Wed, 27 Feb 2019 19:42:30 -0800 (PST)
MIME-Version: 1.0
References: <a94c4768-8cd7-43aa-8bb9-b21b3ccf950c@lbl.gov>
In-Reply-To: <a94c4768-8cd7-43aa-8bb9-b21b3ccf950c@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@gmail.com>
Date: Wed, 27 Feb 2019 19:42:20 -0800
Message-ID: <CAApQTTh9E=oNwfTLFmjmH6rAXDH2Q+X29D0Ur_Y3VMXvGvsDtg@mail.gmail.com>
Subject: Re: [Singularity] Singularity image download syncronization during
 multiple job startup
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary="0000000000007a14bb0582ec14ea"

--0000000000007a14bb0582ec14ea
Content-Type: text/plain; charset="UTF-8"

How are you running the job in question? Is it an MPI job and are you
providing the Docker URL to run it?

If so, can you cache (or rather build) the image beforehand, put it on
parallel storage, and run it from there? For example:

$ singularity build /scratch/container.sif docker://container
$ srun -np X singularity exec /scratch/container.sif
/path/to/containerized/mpi_program

Greg

On Tue, Feb 26, 2019 at 1:30 PM Mike Moore <wxdu...@gmail.com> wrote:

> Hi Everyone,
>
>   I've been trying to address some startup failures we have in our HPC
> environment when starting multiple jobs on the same node in Singularity. We
> have a run rate of ~30-50 million jobs per month. There are a lot of jobs
> that start simultaneously in the various workflows. During my testing, I am
> seeing situations where while the first job on a node is downloading the
> container from the registry, a second job is released to the same node.
> The second job tries to use the downloading image. Obviously, singularity
> fails because the image is incomplete, causing the second job to fail.
> After the image is downloaded and stored in the cache, multiple jobs can
> start in parallel without failing.
>
> So, my questions...
>
> Anyone dealing with a similar situation?
> What have you done to address this conflict?
> Is anyone using a container registry that is being accessed in parallel by
> your cluster or object store?
> Are you storing your simg/sif files in a shared file system and running
> them directly?
> Job prolog check for the existence of the container locally before
> starting?
>
> I can see the finish line for deploying containers to our environment,
> this is the last big hill we need to climb...
>
> Thanks
> -Mike
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>


-- 
Gregory M. Kurtzer
CEO, Sylabs Inc.

--0000000000007a14bb0582ec14ea
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">How are you running the job in question? Is it an MPI job =
and are you providing the Docker URL to run it?<div><br></div><div>If so, c=
an you cache (or rather build) the image beforehand, put it on parallel sto=
rage, and run it from there? For example:</div><div><br></div><div>$ singul=
arity build /scratch/container.sif docker://container</div><div>$ srun -np =
X singularity exec /scratch/container.sif /path/to/containerized/mpi_progra=
m</div><div><br></div><div>Greg</div></div><br><div class=3D"gmail_quote"><=
div dir=3D"ltr" class=3D"gmail_attr">On Tue, Feb 26, 2019 at 1:30 PM Mike M=
oore &lt;<a href=3D"mailto:wxdu...@gmail.com">wxdu...@gmail.com</a>&gt; wro=
te:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"=
ltr">Hi Everyone,<br><br>=C2=A0 I&#39;ve been trying to address some startu=
p failures we have in our HPC environment when starting multiple jobs on th=
e same node in Singularity. We have a run rate of ~30-50 million jobs per m=
onth. There are a lot of jobs that start simultaneously in the various work=
flows. During my testing, I am seeing situations where while the first job =
on a node is downloading the container from the registry, a second job is r=
eleased to the same node.=C2=A0 The second job tries to use the downloading=
 image. Obviously, singularity fails because the image is incomplete, causi=
ng the second job to fail. After the image is downloaded and stored in the =
cache, multiple jobs can start in parallel without failing.<br><br>So, my q=
uestions...<br><br>Anyone dealing with a similar situation?<br>What have yo=
u done to address this conflict?<br>Is anyone using a container registry th=
at is being accessed in parallel by your cluster or object store?<br>Are yo=
u storing your simg/sif files in a shared file system and running them dire=
ctly?<br>Job prolog check for the existence of the container locally before=
 starting?<br><br>I can see the finish line for deploying containers to our=
 environment, this is the last big hill we need to climb...<br><br>Thanks<b=
r>-Mike<br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Ku=
rtzer</div><div>CEO, Sylabs Inc.</div></div></div></div></div></div></div><=
/div></div></div></div>

--0000000000007a14bb0582ec14ea--
