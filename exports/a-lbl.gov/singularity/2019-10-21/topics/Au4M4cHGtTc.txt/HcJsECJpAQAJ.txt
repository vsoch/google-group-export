Date: Thu, 12 Oct 2017 11:53:07 -0700 (PDT)
From: Matt Kijowski <matthew...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <942d5c3f-57a6-4d81-a4d9-5755b03fdc60@lbl.gov>
In-Reply-To: <CAHGBjRwXGr50wCTgknTD6TAhxoFc-hOvLr07qPw7YHeW3eqwdA@mail.gmail.com>
References: <43e70f0f-e4c3-4876-aff5-ed8255ebbae4@lbl.gov>
 <CAHGBjRwXGr50wCTgknTD6TAhxoFc-hOvLr07qPw7YHeW3eqwdA@mail.gmail.com>
Subject: Re: [Singularity] Beginner Singularity question (StarCCM++)
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_14088_569821842.1507834387278"

------=_Part_14088_569821842.1507834387278
Content-Type: multipart/alternative; 
	boundary="----=_Part_14089_1883955386.1507834387279"

------=_Part_14089_1883955386.1507834387279
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

yes (ish).  I dont really care whether mpi runs inside the container or 
outside.  But given that starccm+ seems to only work with its own built in 
mpi I think I need it running within the container itself.


On Thursday, October 12, 2017 at 2:48:04 PM UTC-4, Andrew Rokitka wrote:
>
> Just to clarify - you want mpi to run within the container itself?
>
> On Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski <mat...@gmail.com 
> <javascript:>> wrote:
>
>> Hello all, we're just starting to use Singularity on our HPC system in 
>> conjunction with Slurm and I am having some difficulties understanding how 
>> to set up one of our containers (but I might be setting things up wrong).
>>
>> So I understand why we would use mpirun outside of a container, but what 
>> is a good configuration for a program (StarCCM++) that wraps its own MPI 
>> and remote commands into its execution?
>>
>> Example.  I have a 64 node cluster and starccm++ installed into a 
>> container.  If I want to run my job inside the container I execute: 
>> *srun singularity exec starccm.img starccm+ options simfile*where srun 
>> requests a compute node from slurm and executes the rest of the command on 
>> that compute node.  The problem I have is with parallel jobs.  starccm+ 
>> wants to start its processes on the compute nodes itself (I can specify 
>> what sort of remote shell to use and which compute nodes I have requested 
>> via a machine list file in the options) but starccm+ is not aware that it 
>> needs to launch a singularity container on the compute nodes first.
>>
>> My first thought is I need to launch the containers on the compute nodes 
>> before executing the starccm++ parallel job so that when it uses SSH to log 
>> in to the compute nodes the container is already running but I'm not sure 
>> on how to accomplish this.  Any thoughts?
>>
>> -- 
>> You received this message because you are subscribed to the Google Groups 
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an 
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
------=_Part_14089_1883955386.1507834387279
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">yes (ish).=C2=A0 I dont really care whether mpi runs insid=
e the container or outside.=C2=A0 But given that starccm+ seems to only wor=
k with its own built in mpi I think I need it running within the container =
itself.<br><br><br>On Thursday, October 12, 2017 at 2:48:04 PM UTC-4, Andre=
w Rokitka wrote:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin=
-left: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"lt=
r">Just to clarify - you want mpi to run within the container itself?</div>=
<div><br><div class=3D"gmail_quote">On Thu, Oct 12, 2017 at 2:41 PM, Matt K=
ijowski <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_blank" gdf=
-obfuscated-mailto=3D"TS1DHHqkAQAJ" rel=3D"nofollow" onmousedown=3D"this.hr=
ef=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D&#39;javasc=
ript:&#39;;return true;">mat...@gmail.com</a>&gt;</span> wrote:<br><blockqu=
ote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div dir=3D"ltr">Hello all, we&#39;re just starting =
to use Singularity on our HPC system in conjunction with Slurm and I am hav=
ing some difficulties understanding how to set up one of our containers (bu=
t I might be setting things up wrong).<br><br>So I understand why we would =
use mpirun outside of a container, but what is a good configuration for a p=
rogram (StarCCM++) that wraps its own MPI and remote commands into its exec=
ution?<br><br>Example.=C2=A0 I have a 64 node cluster and starccm++ install=
ed into a container.=C2=A0 If I want to run my job inside the container I e=
xecute: <i>srun singularity exec starccm.img starccm+ options simfile<br></=
i>where srun requests a compute node from slurm and executes the rest of th=
e command on that compute node.=C2=A0 The problem I have is with parallel j=
obs.=C2=A0 starccm+ wants to start its processes on the compute nodes itsel=
f (I can specify what sort of remote shell to use and which compute nodes I=
 have requested via a machine list file in the options) but starccm+ is not=
 aware that it needs to launch a singularity container on the compute nodes=
 first.<br><br>My first thought is I need to launch the containers on the c=
ompute nodes before executing the starccm++ parallel job so that when it us=
es SSH to log in to the compute nodes the container is already running but =
I&#39;m not sure on how to accomplish this.=C2=A0 Any thoughts?<span><font =
color=3D"#888888"><br><i></i></font></span></div><span><font color=3D"#8888=
88">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
TS1DHHqkAQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br></div>
</blockquote></div>
------=_Part_14089_1883955386.1507834387279--

------=_Part_14088_569821842.1507834387278--
