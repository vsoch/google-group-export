X-Received: by 2002:a63:f65:: with SMTP id 37-v6mr8540155pgp.110.1536175379311;
        Wed, 05 Sep 2018 12:22:59 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a63:9149:: with SMTP id l70-v6ls842148pge.18.gmail; Wed, 05
 Sep 2018 12:22:58 -0700 (PDT)
X-Received: by 2002:a62:8c8c:: with SMTP id m134-v6mr42027843pfd.130.1536175378265;
        Wed, 05 Sep 2018 12:22:58 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1536175378; cv=none;
        d=google.com; s=arc-20160816;
        b=q0DVD3ZGwgpEsqr/+QmMvi31I+fm7unn/Fnjf1SpE8PtBkahnGtwwuAtVif0nG4wU9
         u/Mvf5O7LA0hTctvrWDXlbASfy+VgF/GoVCMdeMFKOrul+0IxokN34IP1H+cl9wLc1Uh
         wHuw61p9AhkbmdR2QW+y3qfZTGyLLC3ZgJ6XlGrSzhJ36du2z1eu0mydEA+g2rlO+nPM
         GDltw3RqvhGD/tjN7JrZcmaaPYhKO6gjjxyODqyLFOqXQARNWqn8aDoajXgojB570aGX
         lZfqDEsEXKfyj+gxpm8pLpMumC9HjJ2Ipx7ZooEICLdxLk/S4mYpMIMK7EVFsV3BsEkh
         loXA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:in-reply-to:references
         :mime-version:dkim-signature;
        bh=Ftz8tpR873JuSK0vK8tTrxG2ijT8gmh8cq+ymxCLt8o=;
        b=PHMRuEBEt9u/UXFMYFLpt2cH0LSg3ifXRkXzH+2aQsxmBi6H1GT3XaEbW3p/aOaQVr
         DKq4uADNdDlxY0iMxpWlSNCGQd20NfVuTmj8Kyvkho44iaFIY1koDtiIypTn4tHYmX0V
         hZt3Yfycw72wsyfNxe/9lH73cp9TYaGu6pdpzSrxpxi7SqATcpBzXa6aMnTSOH05uYxJ
         bBO2Bv1vCkLzK3L22LYUrRLde5qM/q01UIJtEA3mGYskgopmhpGWWE0MQ+kxvGBcVoA+
         OV6By4QWxPrZzTHb8uoHzVCGewu81v9kSTe34Ay3TZCywsk5+2bt9ZcLSsLYnZWBLB2T
         xp3Q==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=oWuhJTtl;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.214.52 as permitted sender) smtp.mailfrom=vso...@gmail.com
Return-Path: <vso...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id t1-v6si3127558pfb.208.2018.09.05.12.22.58
        for <singu...@lbl.gov>;
        Wed, 05 Sep 2018 12:22:58 -0700 (PDT)
Received-SPF: pass (google.com: domain of vso...@gmail.com designates 209.85.214.52 as permitted sender) client-ip=209.85.214.52;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=oWuhJTtl;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.214.52 as permitted sender) smtp.mailfrom=vso...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EWAgCuLJBbfzTWVdFQBwMeAQYMgySCD?=
 =?us-ascii?q?iiDbAaBHYJakEyCDYlQhyyFR4ErOyMBFYQ+AoNRITgUAQIBAQIBAQIBAQIQAQE?=
 =?us-ascii?q?JCwsIGwwlDII1BQIDGgEFCUs6MAEBAQEBAQEBAQEfAi0pARkBAQEBAgEjHQEND?=
 =?us-ascii?q?g8OAQMBCwYFCw0qAgIiAQ4DAQUBHA4HBAEaAgSDKQGBPwEDDQgFlxo8iVgRgSK?=
 =?us-ascii?q?BEQUBF4J2BYNoChkmDVeBbwIGEopGF4IAhCSESwEHCwEJAjUmgjqCVwKIQIRzh?=
 =?us-ascii?q?UiIYQmJMYZNF0V7hzeFa5NlMIE3Vg0jcXAVbII7ghkMF3oBCIJCim4jMBCLKA4?=
 =?us-ascii?q?XMIF1AQE?=
X-IronPort-AV: E=Sophos;i="5.53,334,1531810800"; 
   d="scan'208,217";a="127652209"
Received: from mail-it0-f52.google.com ([209.85.214.52])
  by fe3.lbl.gov with ESMTP; 05 Sep 2018 12:22:54 -0700
Received: by mail-it0-f52.google.com with SMTP id h1-v6so10961678itj.4
        for <singu...@lbl.gov>; Wed, 05 Sep 2018 12:22:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc;
        bh=Ftz8tpR873JuSK0vK8tTrxG2ijT8gmh8cq+ymxCLt8o=;
        b=oWuhJTtleoWRFKHdtXmfU8tRVSrFbfTRyYTvplYaZVH8AOUOOqKUKFnusbxurT3RT4
         LD0lkP0mna18kjmulFVV2DPbhfxgvbP1h91WgGOu0h55kxSmabwe73u5MrvRp6T6fNy+
         vuYAa7Ci1AZ9ktmethMpYIh21bpQW/Y/mt3FF2JJGmdmPQSsrqrMib+txuDMpkNEZTx+
         IIeB0Pn4doMeb9VL4Rw8BjgWMeIBzucLeHXPN8dfGBGmCpsIunmLkLXOg+Nypl79KSTi
         0FrAW6+v1+jOGh4RmxtfgOvU2Xn/3IRBYwzD9/qG7SoNAkuPyXN8FpkJKDzYKEQh41JY
         hlGg==
X-Gm-Message-State: APzg51D55zq4rNVYh+Quj9zuxEup/DMYqh6MtLLL0t8IPusMB1Q1B85u
	nVScRGGwrqoNx5rcgvc/5nCBcyD9ghrhvyeX5XdvZkUz
X-Received: by 2002:a24:6b0d:: with SMTP id v13-v6mr1725212itc.16.1536175373767;
 Wed, 05 Sep 2018 12:22:53 -0700 (PDT)
MIME-Version: 1.0
References: <b6d5848a-790a-419b-9a93-adfdf3c4f01f@lbl.gov> <7ee00bc3-63d6-4bc8-ba19-cbf9e641767a@lbl.gov>
In-Reply-To: <7ee00bc3-63d6-4bc8-ba19-cbf9e641767a@lbl.gov>
From: v <vso...@gmail.com>
Date: Wed, 5 Sep 2018 15:22:40 -0400
Message-ID: <CAM=pu+Kbo6x1+KiSgfWYbo-anHouVEPisWpKN8Zd0Cs8LpohVA@mail.gmail.com>
Subject: Re: [Singularity] Re: Sregistry tuning and multi-site deployments
To: singularity@lbl.gov
Cc: Ruth Marinshaw <ru...@stanford.edu>
Content-Type: multipart/alternative; boundary="0000000000007757e2057524b39a"

--0000000000007757e2057524b39a
Content-Type: text/plain; charset="UTF-8"

Hey Mike,

It's awesome that you want to use Singularity Registry Server! To give you
some background, it's an open source project that is intended for the kind
of deployments that Dave hinted at - usually single centers that want to
manage (and push) their own images. To the best of my knowledge, there
aren't any centers like yours using it. And you hit the nail on the head
that it's not developed for this (yet to be discussed) use case, and this
is because I typically develop it based on what is needed and asked for,
and you are the first to ask.

The multi-site use case sounds more akin to Singularity Hub, which
essentially serves as a central API to fire off instances to build images,
and then the actual interaction between some resource and Singularity Hub
is to Google Storage. This way, the server mostly is for browsing around,
and acts as a gateway to get the paths to images in storage. We would want,
in a sense, for sregistry to work more like that for your use case.

My vision for Singularity Registry server is that it continues development
and serves (pun!) as an open source, container registry. Unlike Singularity
Hub that sort of has drank the koolaid for Google Cloud, my development
strategy for stregisy has been based around plugins. Dave's LDAP is a good
example - institutions that want LDAP can use it, and others don't have an
extra feature that just sits there doing nothing.  The same should be true
of the way that sregistry builds and stores its images. Right now that is
again very simple - you push, and the server uploads to its file storage.
We would ideally have this same modular plugin setup for builders and
storage. You should be able to easily configure cloud or local HPC. The
uploads with nginx directly is also not suited for a scaled operation. It's
actually a very bad idea :) For a scaled operation, you would want the main
server to just serve as more of a router, with minimal burden of things
pushing /pulling, and asking things of it.

Moving forward, if you would like to pursue these builder and storage
plugins in an open source manner, I would love to take this on with you. I
intuit that it's needed, but nobody has asked. We would want to:identify
the difference resources (local? cloud?) that you use for storage, and then
develop plugins from there. I think the strongest solutions probably use
the provider APIs and don't try to implement something from scratch. Most
of the burden of download and upload should not be handled by the same
server that has friendly pictures of robots for you to visit :)

Here are some quick answers to your points:

   1. Has anyone run sregistry in an environment this big?

Not to my knowledge.

   1. If not, any alternative suggestions for a container repository or
   image storage that is compatible with Singularity?

I think we can do some additional development with sregistry to fit your
needs. I can imagine many companies will come out of the woodwork and offer
you a product you can pay for too. I push for the first because there are a
lot of academic institutions that want to "roll their own" and prefer the
open source (and free) option.

   1. If you don't mind sharing, what does your sregistry infrastructure
   look like?

Stanford runs Singularity Hub instead, which is like sregistry hard coded
with Google Cloud. So, in other words, it's a beautiful thing :)

   1. I've started testing an sregistry VM setup following the direction on
   the sregistry page.  However, it does not seem to handle simultaneous
   connections well (1 upload and 1 download).  Are there any recommendations
   for parameters for nginx?

This is likely because of the nginx upload module, and we would remove this
with the addition of more proper methods (for example, upload and download
to Singularity Hub bootstraps the Google Storage API with added exponential
backoff.)

Let me know your thoughts! Thanks for reaching out (and to my robot team at
Stanford as well!)

Best,

Vanessa

On Wed, Sep 5, 2018 at 2:55 PM David Trudgian <dtr...@sylabs.io> wrote:

> Hi Mike,
>
> I can give a little bit of info, as I deployed sregistry in my previous
> position at an HPC center and was involved a bit in the development,
> contributing the LDAP plugin. Hopefully Vanessa Sochat will be able to
> chime in on this too as she is the main developer and might be able to give
> information about any future plans.
>
> The deployment I set up was a single install on an ESXi VM, keeping the
> images on SSD storage. We were a single cluster with everything local, very
> fast Infiniband networking, and it would have kept up with usage on the
> 200-300 nodes. I left before it went into production though, so can't
> confirm the usage.
>
> I'm not aware that anyone has deployed sregistry multi-site. In the
> current codebase there isn't any implementation of syncing between multiple
> installs. I could potentially see running a master install somewhere, and
> using external tools to synchronize the database and files to read-only
> mirrors at other sites. That could allow the heavy download operations to
> be performed locally, but getting tokens etc. would have to be done on the
> (possibly remote) master. Also, I'm not really sure how it would handle lag
> on filesystem replication of containers. I don't think there is anything
> that would check for consistency before serving an image - but I may be
> wrong there.
>
> With regard to playing around the VM - to get useful performance with
> simultaneous connections I had to tweak both UWSGI and NGINX configuration.
> I believe I was allowing 48 UWSGI procs and 48 NGINX worker processes on a
> fairly beefy machine. This was prior to Vanessa changing the uploads to be
> offloaded to NGINX. Large uploads/downloads are going to block a uwsgi
> and/or nginx process, so you will need the max numbers for those set high
> enough.
>
> Regarding cryptographic signing of images - this is coming in Singularity
> 3.0 with the new SIF format. If you are adventurous you can try it out in
> the master branch of the GitHub repo which is the 3.0 development area.
>
> Cheers,
>
> DT
>
>
> On Wednesday, September 5, 2018 at 11:15:10 AM UTC-5, Mike Moore wrote:
>>
>> Hello everyone,
>>
>>   My name is Mike Moore.  I am an HPC Systems Engineer at Nuance
>> Communications.  I am running a project to introduce container technology
>> into our HPC environments and guide our users/developers to containerize
>> their HPC/AI workloads using Singularity.  We are deploying several
>> sregistry VM's into HPC Grids as a POC to enable our users to work with
>> Singularity.  I have several questions about sregistry in regards to
>> multi-site deployment and how to tune it for scalability.  I'm hoping the
>> experts here can share some of the lessons they have learned when blazing
>> the path.
>>
>> Multi-server sregistry deployment:
>>
>> We have multiple, separate HPC grids in different regions around the
>> world for various regulatory compliance needs.  We run similar workloads in
>> these environment, just with different datasets.  We will have at least one
>> sregistry server running in each environment to host the container images
>> and serve the out fast enough for the jobs.  Our WAN backbone isn't
>> sufficient to support a single centralized sregistry server
>> infrastructure.  My questions are:
>>
>>    1. Has anyone deployed a similar model with sregistry?
>>    2. If so, how have you kept the separate instances in-sync?
>>    3. Is there any built-in replication functionality that we could take
>>    advantage of to mirror the container images/metadata to the other regions?
>>    4. If you are not running sregistry, what did you do for hosting your
>>    container images?
>>    5. We are using LDAP for authentication within Singularity.  Is there
>>    any way to keep the per-user tokens in sync between instances of sregistry?
>>
>> Tuning/Scalability:
>>
>> So, currently we have ~2000 nodes in our various HPC environments.  Our
>> smaller environments are on the order of 100-200 nodes.  Our largest is
>> north of 1200.  We have a mix of CPU and GPU compute nodes.  CPU nodes have
>> a max capacity of 8 to 20 jobs based on CPU type.  Our GPU nodes have a max
>> capacity of 2 to 16 jobs, again based on GPU type.  We have a run-rate of
>> ~30-50 million jobs per month in our largest environment.  I am wondering
>> if:
>>
>>
>>    1. Has anyone run sregistry in an environment this big?
>>    2. If not, any alternative suggestions for a container repository or
>>    image storage that is compatible with Singularity?
>>    3. If you don't mind sharing, what does your sregistry infrastructure
>>    look like?
>>    4. I've started testing an sregistry VM setup following the direction
>>    on the sregistry page.  However, it does not seem to handle simultaneous
>>    connections well (1 upload and 1 download).  Are there any recommendations
>>    for parameters for nginx?
>>
>> One final question..  Does Singularity support the cryptographic signing
>> of images yet?  If not, any plans/timeframe for when it will?
>>
>>
>> Thank you,
>>
>>
>> -Mike Moore
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>


-- 
Vanessa Villamia Sochat
Stanford University '16
(603) 321-0676

--0000000000007757e2057524b39a
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hey Mike,<div><br></div><div>It&#39;s awesome that you wan=
t to use Singularity Registry Server! To give you some background, it&#39;s=
 an open source project that is intended for the kind of deployments that D=
ave hinted at - usually single centers that want to manage (and push) their=
 own images. To the best of my knowledge, there aren&#39;t any centers like=
 yours using it. And you hit the nail on the head that it&#39;s not develop=
ed for this (yet to be discussed) use case, and this is because I typically=
 develop it based on what is needed and asked for, and you are the first to=
 ask.</div><div><br></div><div>The multi-site use case sounds more akin to =
Singularity Hub, which essentially serves as a central API to fire off inst=
ances to build images, and then the actual interaction between some resourc=
e and Singularity Hub is to Google Storage. This way, the server mostly is =
for browsing around, and acts as a gateway to get the paths to images in st=
orage. We would want, in a sense, for sregistry to work more like that for =
your use case.</div><div><br></div><div>My vision for Singularity Registry =
server is that it continues development and serves (pun!) as an open source=
, container registry. Unlike Singularity Hub that sort of has drank the koo=
laid for Google Cloud, my development strategy for stregisy has been based =
around plugins. Dave&#39;s LDAP is a good example - institutions that want =
LDAP can use it, and others don&#39;t have an extra feature that just sits =
there doing nothing.=C2=A0 The same should be true of the way that sregistr=
y builds and stores its images. Right now that is again very simple - you p=
ush, and the server uploads to its file storage. We would ideally have this=
 same modular plugin setup for builders and storage. You should be able to =
easily configure cloud or local HPC. The uploads with nginx directly is als=
o not suited for a scaled operation. It&#39;s actually a very bad idea :) F=
or a scaled operation, you would want the main server to just serve as more=
 of a router, with minimal burden of things pushing /pulling, and asking th=
ings of it.</div><div><br></div><div>Moving forward, if you would like to p=
ursue these builder and storage plugins in an open source manner, I would l=
ove to take this on with you. I intuit that it&#39;s needed, but nobody has=
 asked. We would want to:identify the difference resources (local? cloud?) =
that you use for storage, and then develop plugins from there. I think the =
strongest solutions probably use the provider APIs and don&#39;t try to imp=
lement something from scratch. Most of the burden of download and upload sh=
ould not be handled by the same server that has friendly pictures of robots=
 for you to visit :)</div><div><br></div><div>Here are some quick answers t=
o your points:</div><div><ol><li style=3D"margin-left:15px">Has anyone run =
sregistry in an environment this big?</li></ol>Not to my knowledge.=C2=A0=
=C2=A0<br><ol><li style=3D"margin-left:15px">If not, any alternative sugges=
tions for a container repository or image storage that is compatible with S=
ingularity?<br></li></ol>I think we can do some additional development with=
 sregistry to fit your needs. I can imagine many companies will come out of=
 the woodwork and offer you a product you can pay for too. I push for the f=
irst because there are a lot of academic institutions that want to &quot;ro=
ll their own&quot; and prefer the open source (and free) option.<br><ol><li=
 style=3D"margin-left:15px">If you don&#39;t mind sharing, what does your s=
registry infrastructure look like?=C2=A0=C2=A0<br></li></ol>Stanford runs S=
ingularity Hub instead, which is like sregistry hard coded with Google Clou=
d. So, in other words, it&#39;s a beautiful thing :)<br><ol><li style=3D"ma=
rgin-left:15px">I&#39;ve started testing an sregistry VM setup following th=
e direction on the sregistry page.=C2=A0 However, it does not seem to handl=
e simultaneous connections well (1 upload and 1 download).=C2=A0 Are there =
any recommendations for parameters for nginx?</li></ol>This is likely becau=
se of the nginx upload module, and we would remove this with the addition o=
f more proper methods (for example, upload and download to Singularity Hub =
bootstraps the Google Storage API with added exponential backoff.)<br><br><=
/div><div>Let me know your thoughts! Thanks for reaching out (and to my rob=
ot team at Stanford as well!)</div><div><br></div><div>Best,</div><div><br>=
</div><div>Vanessa</div></div><br><div class=3D"gmail_quote"><div dir=3D"lt=
r">On Wed, Sep 5, 2018 at 2:55 PM David Trudgian &lt;<a href=3D"mailto:dtr.=
..@sylabs.io">dtr...@sylabs.io</a>&gt; wrote:<br></div><blockquote class=3D=
"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding=
-left:1ex"><div dir=3D"ltr"><div>Hi Mike,</div><div><br></div><div>I can gi=
ve a little bit of info, as I deployed sregistry in my previous position at=
 an HPC center and was involved a bit in the development, contributing the =
LDAP plugin. Hopefully Vanessa Sochat will be able to chime in on this too =
as she is the main developer and might be able to give information about an=
y future plans.</div><div><br></div><div>The deployment I set up was a sing=
le install on an ESXi VM, keeping the images on SSD storage. We were a sing=
le cluster with everything local, very fast Infiniband networking, and it w=
ould have kept up with usage on the 200-300 nodes. I left before it went in=
to production though, so can&#39;t confirm the usage.<br></div><div><br></d=
iv><div>I&#39;m not aware that anyone has deployed sregistry multi-site. In=
 the current codebase there isn&#39;t any implementation of syncing between=
 multiple installs. I could potentially see running a master install somewh=
ere, and using external tools to synchronize the database and files to read=
-only mirrors at other sites. That could allow the heavy download operation=
s to be performed locally, but getting tokens etc. would have to be done on=
 the (possibly remote) master. Also, I&#39;m not really sure how it would h=
andle lag on filesystem replication of containers. I don&#39;t think there =
is anything that would check for consistency before serving an image - but =
I may be wrong there.<br></div><div><br></div><div>With regard to playing a=
round the VM - to get useful performance with simultaneous connections I ha=
d to tweak both UWSGI and NGINX configuration. I believe I was allowing 48 =
UWSGI procs and 48 NGINX worker processes on a fairly beefy machine. This w=
as prior to Vanessa changing the uploads to be offloaded to NGINX. Large up=
loads/downloads are going to block a uwsgi and/or nginx process, so you wil=
l need the max numbers for those set high enough. <br></div><div><br></div>=
<div>Regarding cryptographic signing of images - this is coming in Singular=
ity 3.0 with the new SIF format. If you are adventurous you can try it out =
in the master branch of the GitHub repo which is the 3.0 development area.<=
/div><div><br></div><div>Cheers,</div><div><br>DT<br></div><div><br></div><=
div><br></div>On Wednesday, September 5, 2018 at 11:15:10 AM UTC-5, Mike Mo=
ore wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0=
.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hello ev=
eryone,<br><br>=C2=A0 My name is Mike Moore.=C2=A0 I am an HPC Systems Engi=
neer at Nuance Communications.=C2=A0 I am running a project to introduce co=
ntainer technology into our HPC environments and guide our users/developers=
 to containerize their HPC/AI workloads using Singularity.=C2=A0 We are dep=
loying several sregistry VM&#39;s into HPC Grids as a POC to enable our use=
rs to work with Singularity.=C2=A0 I have several questions about sregistry=
 in regards to multi-site deployment and how to tune it for scalability.=C2=
=A0 I&#39;m hoping the experts here can share some of the lessons they have=
 learned when blazing the path.<br><br>Multi-server sregistry deployment:<b=
r><br>We have multiple, separate HPC grids in different regions around the =
world for various regulatory compliance needs.=C2=A0 We run similar workloa=
ds in these environment, just with different datasets.=C2=A0 We will have a=
t least one sregistry server running in each environment to host the contai=
ner images and serve the out fast enough for the jobs.=C2=A0 Our WAN backbo=
ne isn&#39;t sufficient to support a single centralized sregistry server in=
frastructure.=C2=A0 My questions are:<br><ol><li>Has anyone deployed a simi=
lar model with sregistry?</li><li>If so, how have you kept the separate ins=
tances in-sync?=C2=A0 <br></li><li>Is there any built-in replication functi=
onality that we could take advantage of to mirror the container images/meta=
data to the other regions?</li><li>If you are not running sregistry, what d=
id you do for hosting your container images?</li><li>We are using LDAP for =
authentication within Singularity.=C2=A0 Is there any way to keep the per-u=
ser tokens in sync between instances of sregistry?<br></li></ol>Tuning/Scal=
ability:<br><br>So, currently we have ~2000 nodes in our various HPC enviro=
nments.=C2=A0 Our smaller environments are on the order of 100-200 nodes.=
=C2=A0 Our largest is north of 1200.=C2=A0 We have a mix of CPU and GPU com=
pute nodes.=C2=A0 CPU nodes have a max capacity of 8 to 20 jobs based on CP=
U type.=C2=A0 Our GPU nodes have a max capacity of 2 to 16 jobs, again base=
d on GPU type.=C2=A0 We have a run-rate of ~30-50 million jobs per month in=
 our largest environment.=C2=A0 I am wondering if:<br><br><ol><li>Has anyon=
e run sregistry in an environment this big?=C2=A0 <br></li><li>If not, any =
alternative suggestions for a container repository or image storage that is=
 compatible with Singularity?<br></li><li>If you don&#39;t mind sharing, wh=
at does your sregistry infrastructure look like?=C2=A0 <br></li><li>I&#39;v=
e started testing an sregistry VM setup following the direction on the sreg=
istry page.=C2=A0 However, it does not seem to handle simultaneous connecti=
ons well (1 upload and 1 download).=C2=A0 Are there any recommendations for=
 parameters for nginx?</li></ol><p>One final question..=C2=A0 Does Singular=
ity support the cryptographic signing of images yet?=C2=A0 If not, any plan=
s/timeframe for when it will?</p><p><br></p><p>Thank you,</p><p><br></p><p>=
-Mike Moore<br></p></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"gmail_signature" data-smartmail=3D"gmail_signature">Vanessa Villa=
mia Sochat<br>Stanford University &#39;16<br><div><div><div>(603) 321-0676<=
/div></div></div></div>

--0000000000007757e2057524b39a--
