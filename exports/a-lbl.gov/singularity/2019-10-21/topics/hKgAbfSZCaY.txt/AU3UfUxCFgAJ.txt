Date: Tue, 5 Feb 2019 08:30:01 -0800 (PST)
From: ccvera <mc...@unileon.es>
To: singularity <singu...@lbl.gov>
Message-Id: <71fec772-2642-4b6e-98a3-8801b2ae0cf7@lbl.gov>
Subject: Error when running OpenFoam over Singularity using Slurm
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_1795_2059052280.1549384201838"

------=_Part_1795_2059052280.1549384201838
Content-Type: multipart/alternative; 
	boundary="----=_Part_1796_496423709.1549384201838"

------=_Part_1796_496423709.1549384201838
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Dear all,

I'm experiencing some issues running OpenFOAM over singularity with slurm. 

I've several images based on Ubuntu and within several versions of OpenMPI 
and PMIx and i'm able to run OpenFOAM correctly without use slurm (directly 
on the node) using next command:

$ mpirun -n 16 singularity exec -B /home ../../of6/openfoam6.x.img 
simpleFoam -parallel -case 
/home/carmen/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pruebaOF6_16cores

My problem comes when I run my program with slurm. Whether I make salloc or 
execute a script with sbatch, it shows me the following error:

It looks like MPI_INIT failed for some reason; your parallel process is

likely to abort.  There are many reasons that a parallel process can

fail during MPI_INIT; some of which are due to configuration or environment

problems.  This failure appears to be an internal failure; here's some

additional information (which may only be relevant to an Open MPI

developer):


  ompi_mpi_init: ompi_rte_init failed

  --> Returned "(null)" (-43) instead of "Success" (0)


and

*** An error occurred in MPI_Init_thread

*** on a NULL communicator

*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,

***    and potentially your MPI job)

[cn3045:369] Local abort before MPI_INIT completed completed successfully, 
but am not able to aggregate error messages, and not able to guarantee that 
all other processes were killed!


I know I must have the same openMPI versions on both (host and container) 
and I have also tried other versions of OpenMPI (2.X.X) and in all cases 
OpenFOAM works correctly, but at the moment I want to run it with slurm it 
show me the errors.

I have also tried other ways to run the program with srun using the option 
--mpi=pmi2 (among others) but I always find the same problem.

I use the following script to run OpenFoam:

----------------------------

#!/bin/bash


#SBATCH -N 1

#SBATCH -p haswell 

#SBATCH -J test_OpenFOAM

#SBATCH --output="singularity.%j.o" 

#SBATCH --error="singularity.%j.e"


module load haswell/singularity_2.6.0

module load haswell/openmpi_3.1.2_gcc8.2.0_pmix


ulimit -s unlimited


mpirun -n 16 singularity exec ../../of6/openfoam6.x.img simpleFoam 
-parallel -case 
/home/software/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pruebaOF6_16cores

----------------------------


The versions that I'm using are:

*Host: *

OS: CentOS7.5

OpenMPI: 3.1.2

PMIx: 2.1.4


*Container:*

OS: Ubuntu16.04

OpenMPI: 3.1.2

PMIx: 2.1.4


Can it be a configuration problem of SLURM? Is there any limitation of 
SLURM that is affecting OpenFOAM?

Some info about slurm:

# srun --version
slurm 18.08.3
# srun --mpi=list
srun: MPI types are...
srun: pmi2
srun: openmpi
srun: none


I'm a little bit lost with this issue :(
Can someone give me some lights?

Thanks a lot in advance,
Carmen


------=_Part_1796_496423709.1549384201838
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div>Dear all,</div><div><br></div><div>I&#39;m exper=
iencing some issues running OpenFOAM over singularity with slurm.=C2=A0</di=
v><div><br></div><div>I&#39;ve several images based on Ubuntu and within se=
veral versions of OpenMPI and PMIx and i&#39;m able to run OpenFOAM correct=
ly without use slurm (directly on the node) using next command:</div><div><=
br></div><div>$ mpirun -n 16 singularity exec -B /home ../../of6/openfoam6.=
x.img simpleFoam -parallel -case /home/carmen/test_singularity/OpenFOAM/pru=
ebaOF6_16cores_SLURM/pruebaOF6_16cores</div><div><br></div><div>My problem =
comes when I run my program with slurm. Whether I make salloc or execute a =
script with sbatch, it shows me the following error:</div><div><br></div></=
div><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><=
blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><=
div><font color=3D"#666666">It looks like MPI_INIT failed for some reason; =
your parallel process is</font></div></div></blockquote><blockquote style=
=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div><font color=
=3D"#666666">likely to abort.=C2=A0 There are many reasons that a parallel =
process can</font></div></div></blockquote><blockquote style=3D"margin: 0 0=
 0 40px; border: none; padding: 0px;"><div><div><font color=3D"#666666">fai=
l during MPI_INIT; some of which are due to configuration or environment</f=
ont></div></div></blockquote><blockquote style=3D"margin: 0 0 0 40px; borde=
r: none; padding: 0px;"><div><div><font color=3D"#666666">problems.=C2=A0 T=
his failure appears to be an internal failure; here&#39;s some</font></div>=
</div></blockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; p=
adding: 0px;"><div><div><font color=3D"#666666">additional information (whi=
ch may only be relevant to an Open MPI</font></div></div></blockquote><bloc=
kquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div>=
<font color=3D"#666666">developer):</font></div></div></blockquote><blockqu=
ote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div><fo=
nt color=3D"#666666"><br></font></div></div></blockquote><blockquote style=
=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div><font color=
=3D"#666666">=C2=A0 ompi_mpi_init: ompi_rte_init failed</font></div></div><=
/blockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding:=
 0px;"><div><div><font color=3D"#666666">=C2=A0 --&gt; Returned &quot;(null=
)&quot; (-43) instead of &quot;Success&quot; (0)</font></div></div></blockq=
uote></blockquote><div><div><br></div><div>and</div><div><br></div></div><b=
lockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><blockq=
uote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div><f=
ont color=3D"#666666">*** An error occurred in MPI_Init_thread</font></div>=
</div></blockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; p=
adding: 0px;"><div><div><font color=3D"#666666">*** on a NULL communicator<=
/font></div></div></blockquote><blockquote style=3D"margin: 0 0 0 40px; bor=
der: none; padding: 0px;"><div><div><font color=3D"#666666">*** MPI_ERRORS_=
ARE_FATAL (processes in this communicator will now abort,</font></div></div=
></blockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; paddin=
g: 0px;"><div><div><font color=3D"#666666">***=C2=A0 =C2=A0 and potentially=
 your MPI job)</font></div></div></blockquote><blockquote style=3D"margin: =
0 0 0 40px; border: none; padding: 0px;"><div><div><font color=3D"#666666">=
[cn3045:369] Local abort before MPI_INIT completed completed successfully, =
but am not able to aggregate error messages, and not able to guarantee that=
 all other processes were killed!</font></div></div></blockquote></blockquo=
te><div><div><br></div><div>I know I must have the same openMPI versions on=
 both (host and container) and I have also tried other versions of OpenMPI =
(2.X.X) and in all cases OpenFOAM works correctly, but at the moment I want=
 to run it with slurm it show me the errors.</div><div><br></div><div>I hav=
e also tried other ways to run the program with srun using the option --mpi=
=3Dpmi2 (among others) but I always find the same problem.</div><div><br></=
div><div>I use the following script to run OpenFoam:</div></div><blockquote=
 style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div>------=
----------------------</div></div></blockquote><blockquote style=3D"margin:=
 0 0 0 40px; border: none; padding: 0px;"><blockquote style=3D"margin: 0 0 =
0 40px; border: none; padding: 0px;"><div><div>#!/bin/bash</div></div></blo=
ckquote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px=
;"><div><div><br></div></div></blockquote><blockquote style=3D"margin: 0 0 =
0 40px; border: none; padding: 0px;"><div><div>#SBATCH -N 1</div></div></bl=
ockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0p=
x;"><div><div>#SBATCH -p haswell=C2=A0</div></div></blockquote><blockquote =
style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div>#SBATCH=
 -J test_OpenFOAM</div></div></blockquote><blockquote style=3D"margin: 0 0 =
0 40px; border: none; padding: 0px;"><div><div>#SBATCH --output=3D&quot;sin=
gularity.%j.o&quot;=C2=A0</div></div></blockquote><blockquote style=3D"marg=
in: 0 0 0 40px; border: none; padding: 0px;"><div><div>#SBATCH --error=3D&q=
uot;singularity.%j.e&quot;</div></div></blockquote><blockquote style=3D"mar=
gin: 0 0 0 40px; border: none; padding: 0px;"><div><div><br></div></div></b=
lockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0=
px;"><div><div>module load haswell/singularity_2.6.0</div></div></blockquot=
e><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><di=
v><div>module load haswell/openmpi_3.1.2_gcc8.2.0_pmix</div></div></blockqu=
ote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><=
div><div><br></div></div></blockquote><blockquote style=3D"margin: 0 0 0 40=
px; border: none; padding: 0px;"><div><div>ulimit -s unlimited</div></div><=
/blockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding:=
 0px;"><div><div><br></div></div></blockquote><blockquote style=3D"margin: =
0 0 0 40px; border: none; padding: 0px;"><div><div>mpirun -n 16=C2=A0singul=
arity exec ../../of6/openfoam6.x.img simpleFoam -parallel -case /home/softw=
are/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pruebaOF6_16cores</di=
v></div></blockquote></blockquote><blockquote style=3D"margin: 0 0 0 40px; =
border: none; padding: 0px;"><div><div>----------------------------</div></=
div></blockquote><div><div><br></div><div>The versions that I&#39;m using a=
re:</div></div><blockquote style=3D"margin: 0 0 0 40px; border: none; paddi=
ng: 0px;"><div><div><b>Host:=C2=A0</b></div></div></blockquote><blockquote =
style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><blockquote style=
=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div>OS: CentOS7.=
5</div></div></blockquote><blockquote style=3D"margin: 0 0 0 40px; border: =
none; padding: 0px;"><div><div>OpenMPI: 3.1.2</div></div></blockquote><bloc=
kquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div>=
PMIx: 2.1.4</div></div></blockquote></blockquote><blockquote style=3D"margi=
n: 0 0 0 40px; border: none; padding: 0px;"><div><div><br></div></div><div>=
<div><b>Container:</b></div></div></blockquote><blockquote style=3D"margin:=
 0 0 0 40px; border: none; padding: 0px;"><blockquote style=3D"margin: 0 0 =
0 40px; border: none; padding: 0px;"><div><div>OS: Ubuntu16.04</div></div><=
/blockquote><blockquote style=3D"margin: 0 0 0 40px; border: none; padding:=
 0px;"><div><div>OpenMPI: 3.1.2</div></div></blockquote><blockquote style=
=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div><div>PMIx: 2.1.4<=
/div></div></blockquote></blockquote><div><div><br></div><div>Can it be a c=
onfiguration problem of SLURM? Is there any limitation of SLURM that is aff=
ecting OpenFOAM?</div><div><br></div><div>Some info about slurm:</div></div=
><blockquote style=3D"margin: 0 0 0 40px; border: none; padding: 0px;"><div=
><div># srun --version</div></div><div><div>slurm 18.08.3</div></div><div><=
div># srun --mpi=3Dlist</div></div><div><div>srun: MPI types are...</div></=
div><div><div>srun: pmi2</div></div><div><div>srun: openmpi</div></div><div=
><div>srun: none</div></div></blockquote><div><div><br></div><div>I&#39;m a=
 little bit lost with this issue :(</div><div>Can someone give me some ligh=
ts?</div><div><br></div><div>Thanks a lot in advance,</div><div>Carmen</div=
></div><div><br></div></div>
------=_Part_1796_496423709.1549384201838--

------=_Part_1795_2059052280.1549384201838--
