X-Received: by 10.98.66.150 with SMTP id h22mr7430227pfd.50.1523270998901;
        Mon, 09 Apr 2018 03:49:58 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:68cb:: with SMTP id x11-v6ls1689929plm.3.gmail; Mon,
 09 Apr 2018 03:49:57 -0700 (PDT)
X-Received: by 2002:a17:902:2943:: with SMTP id g61-v6mr38765633plb.238.1523270997663;
        Mon, 09 Apr 2018 03:49:57 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1523270997; cv=none;
        d=google.com; s=arc-20160816;
        b=ifY0i6qyOgEzcAsKqX7P6ZnWrcnkpc52RJsCJOsAWm/Uz99DRy9x+krcYfBzieKmAE
         b0fCv1+YS2FLP69BzXSMlbDetf/3omtXVh2NsFIFG54LqhRfQEhu/lCA2s2qTpnu7osd
         Jy2hCD97wHrwo3EKxJhOGvbKnCgG734fKRiit44M7l+Q38rU8JEh2ptkuFjkkuNsJOee
         bgxhJcagK0bIGfGNI4Qsrnx5qaExW/gP5neec67wJdCx1ISl/7VAgCCldD1FgEe8B88b
         nuP3II3vjJIYhXP69yqPwWscC+I2tJtpeFDCwqZ3OhtjrrJlKPef+/KVMy2q+kw+bsho
         dIiA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature:arc-authentication-results;
        bh=riSNiUqr1S9t4UxQtpgKZwxk6l3VSPaeaKCsMum0QiY=;
        b=Gqbw86lhvZ/Ulai/u4Jqw2HhOE2CAsgii2R83/0l9Sg2DHnTl95oTu26FAGumEacGz
         epqf4CejpsgGitbFbk9Ml1zNvrN+yJZgRtgr5N5zdcY8gzlBlmlZzITLkFAqBskoQ1Mv
         TIShWLwEbmGSuL/bEan92gjIHU3Om9VCLK8fY0wgL/VvSXqSqkfXFjLzxdJUOOpGiDxe
         saczNgkqsImC/jJRFccvoTJSmDN+lrR2yXbZME6lWVbUS6iyhNae5LmDLzfMnbovinfH
         qIbWvXeSRiuEFmLT3pxMzzuo03HkkSH+jFP2v79b9MuBLh9YfyEL4WlBEyHtxMj7KQy3
         vzPg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=bGm07vHd;
       spf=pass (google.com: domain of shervi...@gmail.com designates 209.85.223.179 as permitted sender) smtp.mailfrom=shervi...@gmail.com
Return-Path: <shervi...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id g20si51279pfh.202.2018.04.09.03.49.57
        for <singu...@lbl.gov>;
        Mon, 09 Apr 2018 03:49:57 -0700 (PDT)
Received-SPF: pass (google.com: domain of shervi...@gmail.com designates 209.85.223.179 as permitted sender) client-ip=209.85.223.179;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=bGm07vHd;
       spf=pass (google.com: domain of shervi...@gmail.com designates 209.85.223.179 as permitted sender) smtp.mailfrom=shervi...@gmail.com
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HTAAAURctagLPfVdFbHgEGDIMXgQxvK?=
 =?us-ascii?q?INaBoEdk26BdIEPjXmEeIEsOiMBBgyBT4MJAoI6ITcVAQIBAQEBAQECAQECEAE?=
 =?us-ascii?q?BCQ0JCCYlDII1BQIDGgYJSywvAQEBAQEBAQEBAR8CExglARkBAQEBAgEjHQEIB?=
 =?us-ascii?q?Q4eAwELBgULDSoCAiEBAQ4DAQUBHA4HBAEcBIMVgT4BAw0IBZkhPIsFgX8FARe?=
 =?us-ascii?q?CbwWDSAoZJg1UV4IWAgYSh1mBVD+BDIMEgk9CBIEnARIBgx+CVAKHJoRTiyIsC?=
 =?us-ascii?q?IFgiViCfYxDiViGHzCBBDJWLnEzGiOBAIISCYILDBeDRWqJaD8wAQ8gjGNHgW8?=
 =?us-ascii?q?BAQ?=
X-IronPort-AV: E=Sophos;i="5.48,427,1517904000"; 
   d="scan'208,217";a="19205287"
Received: from mail-io0-f179.google.com ([209.85.223.179])
  by fe4.lbl.gov with ESMTP; 09 Apr 2018 03:49:34 -0700
Received: by mail-io0-f179.google.com with SMTP id p139so9090640iod.0
        for <singu...@lbl.gov>; Mon, 09 Apr 2018 03:49:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=riSNiUqr1S9t4UxQtpgKZwxk6l3VSPaeaKCsMum0QiY=;
        b=bGm07vHdPbr51yxjEsRk39TEHFjAhwvm7XEjOIFp2I0gSzfj1vy5l241jY1LMPB7kF
         Oo0P7uhFhcYup9bQTJ0JzlqpRHtrue9y+41ylA6tXKqFic/x0/q37e5XWtAH2/PJAWq3
         C5RVvaM+y62TOUe1Wlmbw5ZNxAx0G485j1Xca1scSmgKoXLq8qgJotN5wQtexrq5k8/z
         dYknGqIfFuWbADHAOE/uvHdIIrNlmNtBVvcpQHC0GYtbzlNFDVXjADMZWYKF5ZXuPLAT
         s5un66R7woeOR/WhRaams0fqWjpM9IzJ6Fwq282WzQtavk6iZwb9HYpUf4TcELTRm9YS
         u2/Q==
X-Gm-Message-State: AElRT7GVtOV544gOSnQ9I8+dkcCfJNN1mVCjdCMdcaWcMW55FLtZm0eW
	/e3taYvQUxMe7QFdUCcN4OTd4SsyxRELxH7PHyk=
X-Received: by 10.107.173.41 with SMTP id w41mr33698582ioe.126.1523270973924;
 Mon, 09 Apr 2018 03:49:33 -0700 (PDT)
MIME-Version: 1.0
References: <03f78c74-88eb-476a-9f1f-2c8c050407be@lbl.gov> <CAGfAqt9ExLMf23wAT3+BPmHLYDw_4k+6q9ydY4+SZYQhmGLhgw@mail.gmail.com>
 <CAHhcXSaM=qA92y-BUqA021yfpJScjQ9+i4u3sfCw1XjuJ+wqow@mail.gmail.com> <CAGfAqt_fjphOAhrbvkKZ2mat5Oj7BY57r=Xi4yZDVX1y5PZ6SQ@mail.gmail.com>
In-Reply-To: <CAGfAqt_fjphOAhrbvkKZ2mat5Oj7BY57r=Xi4yZDVX1y5PZ6SQ@mail.gmail.com>
From: Shervin Sammak <shervi...@gmail.com>
Date: Mon, 09 Apr 2018 10:49:23 +0000
Message-ID: <CAHhcXSYW3kQJ9Hd78ixK5hYT4R-0+5-+okDN+Wjp6aJqeEyrMw@mail.gmail.com>
Subject: Re: [Singularity] Running OpenFOAM in parallel
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="001a114467ca4c2a3305696829ec"

--001a114467ca4c2a3305696829ec
Content-Type: text/plain; charset="UTF-8"

Hi Jason,

It makes sence. Since I needed across the node implementation, I installed
the same openmpi on the host. It is working very nice. Thanks for your help
on this.

Shervin

On Mon, Apr 9, 2018, 1:32 AM Jason Stover <jason...@gmail.com> wrote:

> Hi Shervin,
>
>   Beyond what Alan has said, yes the MPI needs to be "the same" on the
> host as well as in the container. You're starting MPI on the host, but
> your MPI command is running singularity which is then using that host
> initiated MPI session to do the MPI work that happens in the program
> started inside the container. For OpenMPI if the versions are
> different, but they're ABI compatible then it _should_ still work...
> still the possibility of something popping up where the two versions
> don't talk to each other right exists.
>
>   > what is the benefits of putting the software in a container in the
> first place
>
>   Say you have a program that requires an old library version that no
> longer exists in a recent distribution. You could build a container
> based on the old distribution, and still run it on new systems since
> to the program being executed it is running in the older distribution.
> For serial/threaded programs, you shouldn't see an issue no matter
> where you run it. You get more restrictions once you start needing to
> talk between multiple hosts. If you're running an MPI job on a single
> node, you could try starting MPI from within the container. So, in
> your example since you're running with '-n 4' (four processes), you
> could try running:
>
>     singularity exec of4.img mpiexec -n 4 simplefoam -parallel
>
>   As long as your MPI install in the container will work on the host,
> that should work and be portable. You can test by running hostname, or
> similar, to see if it would work. But, once you go multinode that
> _will not work_ as you will be *outside* the container when mpiexec
> goes to start the binary on another host.
>
>   So, in that case we need to start singularity through MPI itself as
> you have done, and that gives us the restriction that the host MPI and
> container MPI need to be compatible. Us saying "the same version" is
> an easy way to cut off any possible issues and is usually pretty easy
> to accomplish.
>
>   Does that make sense?
>
> -J
>
>
> On Sun, Apr 8, 2018 at 6:58 PM, Shervin Sammak <shervi...@gmail.com>
> wrote:
> > Hi Jason,
> >
> > Appreciate your help. It solved a part of the problem. On the system
> (ubuntu
> > 14.04) that I build the container, I can run openfoam in parallel via
> > "mpirun -n 4 singularity exec of4.img simpleFoam -parallel". However, on
> > another machine (RHEL 7), this gives me an error and that is because of
> MPI
> > incompatibility between two systems. This actually confuses me. If I
> need to
> > install the same openmpi version on the RHEL7 machine to run OpenFOAM in
> > parallel (I did it and it works), what is the benefits of putting the
> > software in a container in the first place?!
> >
> > -----------------------------------------------------------
> > Shervin Sammak
> > Research Assistant Professor
> > Center for Research Computing
> > University of Pittsburgh
> > 4420 Bayard St
> > Pittsburgh, PA 15213
> > E-mail: shervi...@pitt.edu
> > Website: www.crc.pitt.edu
> >
> > ~ You chase quality and quantity will chase you.
> >
> > On Sun, Apr 8, 2018 at 1:20 AM, Jason Stover <jason...@gmail.com>
> wrote:
> >>
> >> Hi Shervin,
> >>
> >> Try in your Def file ... change the /bin/sh symlink to bash instead of
> >> dash
> >>
> >> So in %post have:
> >>
> >> /bin/rm /bin/sh
> >> /bin/ln -s /bin/bash /bin/sh
> >>
> >> I'm betting dash doesn't have the '-n' option to export which bash
> >> has. The 'exec' script uses /bin/sh as the shell, so everything needs
> >> to be posix. The openfoam bashrc most definitely has bashism's in it.
> >>
> >> -J
> >>
> >>
> >>
> >> On Sat, Apr 7, 2018 at 10:56 PM, Shervin Sammak
> >> <shervi...@gmail.com> wrote:
> >> > Hi all,
> >> >
> >> > I created an ubuntu image with openfoam installation. Within the
> >> > container
> >> > (run command), I can run openfoam in parallel. Outside the container
> >> > (exec
> >> > command), running in parallel is not possible but I still can run
> >> > openfoam
> >> > in serial. I tried
> >> > $ mpirun -n 4 singularity  exec of4.img simpleFoam -parallel
> >> > which errors out
> >> > /.singularity.d/actions/exec: 146: export: -parallel: bad variable
> name
> >> > /.singularity.d/actions/exec: 146: export: -parallel: bad variable
> name
> >> > /.singularity.d/actions/exec: 146: export: -parallel: bad variable
> name
> >> > /.singularity.d/actions/exec: 146: export: -parallel: bad variable
> name
> >> > and
> >> > $singularity  exec of4.img mpirun -n 4 simpleFoam -parallel
> >> > which results in
> >> > /.singularity.d/actions/exec: 146: export: -n: bad variable name
> >> >
> >> > Altough, I put " echo '. /opt/openfoam4/etc/bashrc'
> >> >>>$SINGULARITY_ENVIRONMENT" in my build recipe, this sounds like an
> >> > environment variable issue.  Any help on this would be appreciated.
> >> >
> >> > --
> >> > You received this message because you are subscribed to the Google
> >> > Groups
> >> > "singularity" group.
> >> > To unsubscribe from this group and stop receiving emails from it, send
> >> > an
> >> > email to singu...@lbl.gov.
> >>
> >> --
> >> You received this message because you are subscribed to the Google
> Groups
> >> "singularity" group.
> >> To unsubscribe from this group and stop receiving emails from it, send
> an
> >> email to singu...@lbl.gov.
> >
> >
> > --
> > You received this message because you are subscribed to the Google Groups
> > "singularity" group.
> > To unsubscribe from this group and stop receiving emails from it, send an
> > email to singu...@lbl.gov.
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a114467ca4c2a3305696829ec
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Jason,<div><br></div><div>It makes sence. Since I needed across the node=
 implementation, I installed the same openmpi on the host. It is working ve=
ry nice. Thanks for your help on this.</div><div><br></div><div>Shervin<br>=
<br><div class=3D"gmail_quote"><div dir=3D"ltr">On Mon, Apr 9, 2018, 1:32 A=
M Jason Stover &lt;<a href=3D"mailto:jason...@gmail.com">jason...@gmail.com=
</a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:=
0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi Shervin,<br>
<br>
=C2=A0 Beyond what Alan has said, yes the MPI needs to be &quot;the same&qu=
ot; on the<br>
host as well as in the container. You&#39;re starting MPI on the host, but<=
br>
your MPI command is running singularity which is then using that host<br>
initiated MPI session to do the MPI work that happens in the program<br>
started inside the container. For OpenMPI if the versions are<br>
different, but they&#39;re ABI compatible then it _should_ still work...<br=
>
still the possibility of something popping up where the two versions<br>
don&#39;t talk to each other right exists.<br>
<br>
=C2=A0 &gt; what is the benefits of putting the software in a container in =
the<br>
first place<br>
<br>
=C2=A0 Say you have a program that requires an old library version that no<=
br>
longer exists in a recent distribution. You could build a container<br>
based on the old distribution, and still run it on new systems since<br>
to the program being executed it is running in the older distribution.<br>
For serial/threaded programs, you shouldn&#39;t see an issue no matter<br>
where you run it. You get more restrictions once you start needing to<br>
talk between multiple hosts. If you&#39;re running an MPI job on a single<b=
r>
node, you could try starting MPI from within the container. So, in<br>
your example since you&#39;re running with &#39;-n 4&#39; (four processes),=
 you<br>
could try running:<br>
<br>
=C2=A0 =C2=A0 singularity exec of4.img mpiexec -n 4 simplefoam -parallel<br=
>
<br>
=C2=A0 As long as your MPI install in the container will work on the host,<=
br>
that should work and be portable. You can test by running hostname, or<br>
similar, to see if it would work. But, once you go multinode that<br>
_will not work_ as you will be *outside* the container when mpiexec<br>
goes to start the binary on another host.<br>
<br>
=C2=A0 So, in that case we need to start singularity through MPI itself as<=
br>
you have done, and that gives us the restriction that the host MPI and<br>
container MPI need to be compatible. Us saying &quot;the same version&quot;=
 is<br>
an easy way to cut off any possible issues and is usually pretty easy<br>
to accomplish.<br>
<br>
=C2=A0 Does that make sense?<br>
<br>
-J<br>
<br>
<br>
On Sun, Apr 8, 2018 at 6:58 PM, Shervin Sammak &lt;<a href=3D"mailto:shervi=
...@gmail.com" target=3D"_blank">shervi...@gmail.com</a>&gt; wrote:<br>
&gt; Hi Jason,<br>
&gt;<br>
&gt; Appreciate your help. It solved a part of the problem. On the system (=
ubuntu<br>
&gt; 14.04) that I build the container, I can run openfoam in parallel via<=
br>
&gt; &quot;mpirun -n 4 singularity exec of4.img simpleFoam -parallel&quot;.=
 However, on<br>
&gt; another machine (RHEL 7), this gives me an error and that is because o=
f MPI<br>
&gt; incompatibility between two systems. This actually confuses me. If I n=
eed to<br>
&gt; install the same openmpi version on the RHEL7 machine to run OpenFOAM =
in<br>
&gt; parallel (I did it and it works), what is the benefits of putting the<=
br>
&gt; software in a container in the first place?!<br>
&gt;<br>
&gt; -----------------------------------------------------------<br>
&gt; Shervin Sammak<br>
&gt; Research Assistant Professor<br>
&gt; Center for Research Computing<br>
&gt; University of Pittsburgh<br>
&gt; 4420 Bayard St<br>
&gt; Pittsburgh, PA 15213<br>
&gt; E-mail: <a href=3D"mailto:shervi...@pitt.edu" target=3D"_blank">shervi=
...@pitt.edu</a><br>
&gt; Website: <a href=3D"http://www.crc.pitt.edu" rel=3D"noreferrer" target=
=3D"_blank">www.crc.pitt.edu</a><br>
&gt;<br>
&gt; ~ You chase quality and quantity will chase you.<br>
&gt;<br>
&gt; On Sun, Apr 8, 2018 at 1:20 AM, Jason Stover &lt;<a href=3D"mailto:jas=
on...@gmail.com" target=3D"_blank">jason...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi Shervin,<br>
&gt;&gt;<br>
&gt;&gt; Try in your Def file ... change the /bin/sh symlink to bash instea=
d of<br>
&gt;&gt; dash<br>
&gt;&gt;<br>
&gt;&gt; So in %post have:<br>
&gt;&gt;<br>
&gt;&gt; /bin/rm /bin/sh<br>
&gt;&gt; /bin/ln -s /bin/bash /bin/sh<br>
&gt;&gt;<br>
&gt;&gt; I&#39;m betting dash doesn&#39;t have the &#39;-n&#39; option to e=
xport which bash<br>
&gt;&gt; has. The &#39;exec&#39; script uses /bin/sh as the shell, so every=
thing needs<br>
&gt;&gt; to be posix. The openfoam bashrc most definitely has bashism&#39;s=
 in it.<br>
&gt;&gt;<br>
&gt;&gt; -J<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Sat, Apr 7, 2018 at 10:56 PM, Shervin Sammak<br>
&gt;&gt; &lt;<a href=3D"mailto:shervi...@gmail.com" target=3D"_blank">sherv=
i...@gmail.com</a>&gt; wrote:<br>
&gt;&gt; &gt; Hi all,<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; I created an ubuntu image with openfoam installation. Within =
the<br>
&gt;&gt; &gt; container<br>
&gt;&gt; &gt; (run command), I can run openfoam in parallel. Outside the co=
ntainer<br>
&gt;&gt; &gt; (exec<br>
&gt;&gt; &gt; command), running in parallel is not possible but I still can=
 run<br>
&gt;&gt; &gt; openfoam<br>
&gt;&gt; &gt; in serial. I tried<br>
&gt;&gt; &gt; $ mpirun -n 4 singularity=C2=A0 exec of4.img simpleFoam -para=
llel<br>
&gt;&gt; &gt; which errors out<br>
&gt;&gt; &gt; /.singularity.d/actions/exec: 146: export: -parallel: bad var=
iable name<br>
&gt;&gt; &gt; /.singularity.d/actions/exec: 146: export: -parallel: bad var=
iable name<br>
&gt;&gt; &gt; /.singularity.d/actions/exec: 146: export: -parallel: bad var=
iable name<br>
&gt;&gt; &gt; /.singularity.d/actions/exec: 146: export: -parallel: bad var=
iable name<br>
&gt;&gt; &gt; and<br>
&gt;&gt; &gt; $singularity=C2=A0 exec of4.img mpirun -n 4 simpleFoam -paral=
lel<br>
&gt;&gt; &gt; which results in<br>
&gt;&gt; &gt; /.singularity.d/actions/exec: 146: export: -n: bad variable n=
ame<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Altough, I put &quot; echo &#39;. /opt/openfoam4/etc/bashrc&#=
39;<br>
&gt;&gt; &gt;&gt;&gt;$SINGULARITY_ENVIRONMENT&quot; in my build recipe, thi=
s sounds like an<br>
&gt;&gt; &gt; environment variable issue.=C2=A0 Any help on this would be a=
ppreciated.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; --<br>
&gt;&gt; &gt; You received this message because you are subscribed to the G=
oogle<br>
&gt;&gt; &gt; Groups<br>
&gt;&gt; &gt; &quot;singularity&quot; group.<br>
&gt;&gt; &gt; To unsubscribe from this group and stop receiving emails from=
 it, send<br>
&gt;&gt; &gt; an<br>
&gt;&gt; &gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D=
"_blank">singu...@lbl.gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singu...@lbl.gov</a>.<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singu...@lbl.gov</a>.<br>
<br>
--<br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu.=
..@lbl.gov</a>.<br>
</blockquote></div></div>

--001a114467ca4c2a3305696829ec--
