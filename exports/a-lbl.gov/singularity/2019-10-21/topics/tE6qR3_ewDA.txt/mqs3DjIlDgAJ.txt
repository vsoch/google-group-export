X-Received: by 10.129.157.215 with SMTP id u206mr69427191ywg.36.1451756718262;
        Sat, 02 Jan 2016 09:45:18 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.41.199 with SMTP id p190ls1866781iop.89.gmail; Sat, 02 Jan
 2016 09:45:18 -0800 (PST)
X-Received: by 10.98.18.11 with SMTP id a11mr113494177pfj.7.1451756717864;
        Sat, 02 Jan 2016 09:45:17 -0800 (PST)
Return-Path: <rhc.o...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id bx1si16777781pab.57.2016.01.02.09.45.17
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sat, 02 Jan 2016 09:45:17 -0800 (PST)
Received-SPF: pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.68 as permitted sender) client-ip=209.85.220.68;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.68 as permitted sender) smtp.mailfrom=rhc.o...@gmail.com;
       dkim=pass head...@gmail.com
X-Ironport-SBRS: 2.6
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FLCQB/C4hWfkTcVdFUCoJugguIWaBGAQEBBZMviQY6EgEBAQEBAQEDDgEBCQsMCCEugmI8AQEBAQEBIwIrQQEBBBIRHQEtDAMMAQVHAgI0AQUBDg4QK4d4AxIFoRqBMT4xi0iEY4gzJw2DKhsBBQ6GAAYFgkwIhxQSgzUugRsFjjCIVoEOhyOOOQ6FVIxuNYEXKAF1g3FRhRABAQE
X-IronPort-AV: E=Sophos;i="5.20,513,1444719600"; 
   d="scan'208,217";a="9911388"
Received: from mail-pa0-f68.google.com ([209.85.220.68])
  by fe3.lbl.gov with ESMTP; 02 Jan 2016 09:45:16 -0800
Received: by mail-pa0-f68.google.com with SMTP id a20so7825280pag.3
        for <singu...@lbl.gov>; Sat, 02 Jan 2016 09:45:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=sender:from:content-type:message-id:mime-version:subject:date
         :references:to:in-reply-to;
        bh=B+3Zf2usJPXulovImZss5KGf4MdKQIxIDQOgQARSpbU=;
        b=nEVp01oCisSm47oLQe0zYckurCAWb2G9umQV1yghi+vJ57cq47EcUrr1/qX/UUB4iT
         c+gdN1ZLPOvKACTKp6/eSJu6FUGyWfKCqifICfXNoCnV86mKVKmRUfpRVCiWl1rjN5ki
         O5/dX53iWbsN1RxoT9YvTSW0iNaotECbNbDFYMqqM8YfPt9mlqNY+lv8WrBnugjCUy9g
         79Y8jyrq//G7OUrMxEyyC5eGWhhgoKc8DsHjY7luRNxg9SWqTNTRa1RGuoNY2SBfmn0n
         Qcp1ug5p5UTLpBzhWTm4dXCW7Sd/VBa5yMwxooDFkxg3xajV/PWB3IJvzEYmTxEaeNzC
         DmTA==
X-Received: by 10.66.252.6 with SMTP id zo6mr48506418pac.154.1451756716391;
        Sat, 02 Jan 2016 09:45:16 -0800 (PST)
Return-Path: <rhc.o...@gmail.com>
Received: from [192.168.0.8] ([208.100.172.177])
        by smtp.gmail.com with ESMTPSA id p17sm81156673pfi.54.2016.01.02.09.45.14
        for <singu...@lbl.gov>
        (version=TLSv1/SSLv3 cipher=OTHER);
        Sat, 02 Jan 2016 09:45:15 -0800 (PST)
Sender: Ralph Castain <rhc.o...@gmail.com>
From: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary="Apple-Mail=_50305344-BC8B-4071-9C56-297F20BF1B9B"
Message-Id: <0C2E7133-169B-4546-9A8E-DF261094EF6F@open-mpi.org>
Mime-Version: 1.0 (Mac OS X Mail 9.2 \(3112\))
Subject: Singularity and HPC support
Date: Sat, 2 Jan 2016 09:45:14 -0800
References: <937ff7ed-30b6-4143-849e-5e1a234ff89b@lbl.gov> <4D239990-0E7E-4F15-B5CB-09C8465562F0@lbl.gov> <2778d375-2145-4c15-81d5-0c961734993f@lbl.gov> <CAN7etTz8O5Vt4x+=oTp9+7MXEGxr8NT3K2u+BLxB5s=a9MKn4A@mail.gmail.com> <CAN7etTyafQgmAwAH94z1-3of=NNj9QQmLzJUzwn=mO8VRjJAFQ@mail.gmail.com> <D29DD7ED.89FB1%Grigory.Shamov@ad.umanitoba.ca> <D33AC0B6-5024-4234-BD25-7878E8E7C417@lbl.gov>
To: singularity@lbl.gov
In-Reply-To: <D33AC0B6-5024-4234-BD25-7878E8E7C417@lbl.gov>
X-Mailer: Apple Mail (2.3112)

--Apple-Mail=_50305344-BC8B-4071-9C56-297F20BF1B9B
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Hello all

I=E2=80=99ve been monitoring this mailing list since it began, but have bee=
n swamped by too many things to respond. I=E2=80=99d like to specifically a=
ddress one of Greg=E2=80=99s early requests for input from some MPI folks. =
As some of you may know, I=E2=80=99ve been a lead developer of OpenMPI=E2=
=80=99s runtime since the project=E2=80=99s inception some 12 years ago, an=
d have spent a lot of time interfacing it to various environments. So I=E2=
=80=99d like to try and provide some perspective in the hopes that Singular=
ity may be able to better support HPC operations.

First, let me clarify my expected use-case. I=E2=80=99m assuming that a use=
r requests an allocation of  some number of containers for running a multi-=
process application. The user may intend to run one or more application pro=
cesses in each container. When allocated, the containers will belong to tha=
t one user, and will be for their exclusive use. The containers will be ret=
urned to the scheduler upon completion of the application.

So when I talk about =E2=80=9Cmultiple containers=E2=80=9D below, I=E2=80=
=99m talking strictly about the above use-case. It is quite possible that a=
 user could have multiple allocations running in parallel - the containers =
for each allocation would be distinct and have no knowledge of the containe=
rs in another allocation, even though they belong to the same user. Allocat=
ions given to different users should have no knowledge of each other.

I expect that containers from different users will share a physical node (i=
.e., that cloud will typically be a multi-tenant environment), which implie=
s there will be some resource contention between them. Thus, we will requir=
e some kind of QoS control over the networks. Remember, these will be OS-by=
pass networks, and so the OS in each container will have no control or even=
 visibility over them.

Within that context, the container must be given the following knowledge at=
 startup if we really want to provide performance comparable to what is ach=
ieved today:

* the ID of all other containers in this allocation - a unique =E2=80=9Chos=
tname=E2=80=9D or its equivalent. Must include the IP address assigned to e=
ach container

* the =E2=80=9Ctrue=E2=80=9D location of each container in the allocation -=
 i.e., the physical node where the container is executing (so we can identi=
fy which processes are =E2=80=9Csharing=E2=80=9D physical resources) and wh=
ere it is bound (which physical cpus; what network, memory, and other resou=
rces it has been given; etc.)

* if application processes were started as part of setting up the container=
s, then which processes (i.e., which ranks) were put on each container, alo=
ng with what network endpoints were assigned to them and where they are bou=
nd

* the network topology between the containers - alternatively, you can prov=
ide the topology between the involved physical nodes and we can use the abo=
ve location info to get what we need

* a method by which application procs can request any or all of that info a=
s they need it in order to properly setup their communication infrastructur=
e.

* an enhancement would include a shared memory region on each physical node=
 that hosts multiple containers so we could use shared memory communication=
 in that situation. Obviously, there would need to be a dedicated region fo=
r each allocation to ensure separation of applications.

* a =E2=80=9Cglobal=E2=80=9D key-value exchange server that will allow user=
s to =E2=80=9Cdiscover=E2=80=9D applications and their allocations, subject=
 to access controls. Thus, an ocean simulator in one allocation could =E2=
=80=9Cdiscover=E2=80=9D and connect to an atmospheric model in another allo=
cation so they can share data. The HPC world knows how to build this now - =
just a case of ensuring adequate knowledge transfer.

I hope that helps provide some direction. I=E2=80=99d be happy to help cons=
truct this capability, and interface it to OpenMPI.

Ralph


--Apple-Mail=_50305344-BC8B-4071-9C56-297F20BF1B9B
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html charset=
=3Dutf-8"><meta http-equiv=3D"Content-Type" content=3D"text/html charset=3D=
utf-8"></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: spac=
e; -webkit-line-break: after-white-space;" class=3D"">Hello all<div class=
=3D""><br class=3D""></div><div class=3D"">I=E2=80=99ve been monitoring thi=
s mailing list since it began, but have been swamped by too many things to =
respond. I=E2=80=99d like to specifically address one of Greg=E2=80=99s ear=
ly requests for input from some MPI folks. As some of you may know, I=E2=80=
=99ve been a lead developer of OpenMPI=E2=80=99s runtime since the project=
=E2=80=99s inception some 12 years ago, and have spent a lot of time interf=
acing it to various environments. So I=E2=80=99d like to try and provide so=
me perspective in the hopes that Singularity may be able to better support =
HPC operations.</div><div class=3D""><br class=3D""></div><div class=3D""><=
div class=3D"">First, let me clarify my expected use-case. I=E2=80=99m assu=
ming that a user requests an allocation of &nbsp;some number of containers =
for running a multi-process application. The user may intend to run one or =
more application processes in each container. When allocated, the container=
s will belong to that one user, and will be for their exclusive use. The co=
ntainers will be returned to the scheduler upon completion of the applicati=
on.</div><div class=3D""><br class=3D""></div><div class=3D"">So when I tal=
k about =E2=80=9Cmultiple containers=E2=80=9D below, I=E2=80=99m talking st=
rictly about the above use-case. It is quite possible that a user could hav=
e multiple allocations running in parallel - the containers for each alloca=
tion would be distinct and have no knowledge of the containers in another a=
llocation, even though they belong to the same user. Allocations given to d=
ifferent users should have no knowledge of each other.</div><div class=3D""=
><br class=3D""></div><div class=3D"">I expect that containers from differe=
nt users will share a physical node (i.e., that cloud will typically be a m=
ulti-tenant environment), which implies there will be some resource content=
ion between them. Thus, we will require some kind of QoS control over the n=
etworks. Remember, these will be OS-bypass networks, and so the OS in each =
container will have no control or even visibility over them.</div><div clas=
s=3D""><br class=3D""></div><div class=3D"">Within that context, the contai=
ner must be given the following knowledge at startup if we really want to p=
rovide performance comparable to what is achieved today:</div><div class=3D=
""><br class=3D""></div><div class=3D"">* the ID of all other containers in=
 this allocation - a unique =E2=80=9Chostname=E2=80=9D or its equivalent. M=
ust include the IP address assigned to each container</div><div class=3D"">=
<br class=3D""></div><div class=3D"">* the =E2=80=9Ctrue=E2=80=9D location =
of each container in the allocation - i.e., the physical node where the con=
tainer is executing (so we can identify which processes are =E2=80=9Csharin=
g=E2=80=9D physical resources) and where it is bound (which physical cpus; =
what network, memory, and other resources it has been given; etc.)</div><di=
v class=3D""><br class=3D""></div><div class=3D"">* if application processe=
s were started as part of setting up the containers, then which processes (=
i.e., which ranks) were put on each container, along with what network endp=
oints were assigned to them and where they are bound</div><div class=3D""><=
br class=3D""></div><div class=3D"">* the network topology between the cont=
ainers - alternatively, you can provide the topology between the involved p=
hysical nodes and we can use the above location info to get what we need</d=
iv><div class=3D""><br class=3D""></div><div class=3D"">* a method by which=
 application procs can request any or all of that info as they need it in o=
rder to properly setup their communication infrastructure.</div><div class=
=3D""><br class=3D""></div><div class=3D"">* an enhancement would include a=
 shared memory region on each physical node that hosts multiple containers =
so we could use shared memory communication in that situation. Obviously, t=
here would need to be a dedicated region for each allocation to ensure sepa=
ration of applications.</div><div class=3D""><br class=3D""></div><div clas=
s=3D"">* a =E2=80=9Cglobal=E2=80=9D key-value exchange server that will all=
ow users to =E2=80=9Cdiscover=E2=80=9D applications and their allocations, =
subject to access controls. Thus, an ocean simulator in one allocation coul=
d =E2=80=9Cdiscover=E2=80=9D and connect to an atmospheric model in another=
 allocation so they can share data. The HPC world knows how to build this n=
ow - just a case of ensuring adequate knowledge transfer.</div></div><div c=
lass=3D""><br class=3D""></div><div class=3D"">I hope that helps provide so=
me direction. I=E2=80=99d be happy to help construct this capability, and i=
nterface it to OpenMPI.</div><div class=3D""><br class=3D""></div><div clas=
s=3D"">Ralph</div><div style=3D"font-family: Calibri, sans-serif; font-size=
: 14px;" class=3D""><br class=3D""></div></body></html>
--Apple-Mail=_50305344-BC8B-4071-9C56-297F20BF1B9B--
