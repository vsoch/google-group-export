X-Received: by 10.157.27.188 with SMTP id z57mr3865730otd.123.1476974995419;
        Thu, 20 Oct 2016 07:49:55 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.135.133 with SMTP id f127ls4781389ite.18.canary; Thu, 20
 Oct 2016 07:49:54 -0700 (PDT)
X-Received: by 10.99.131.66 with SMTP id h63mr1604842pge.103.1476974993849;
        Thu, 20 Oct 2016 07:49:53 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id sm3si37860809pac.261.2016.10.20.07.49.53
        for <singu...@lbl.gov>;
        Thu, 20 Oct 2016 07:49:53 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) client-ip=209.85.215.70;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
IronPort-PHdr: 9a23:n5nVkB04VSu9op6JsmDT+DRfVm0co7zxezQtwd8ZsekSKvad9pjvdHbS+e9qxAeQG96KsbQZ26GJ4ujJYi8p2d65qncMcZhBBVcuqP49uEgeOvODElDxN/XwbiY3T4xoXV5h+GynYwAOQJ6tL2PbrnD61zMOABK3bVMzfbWvXNOCxJvmn8mJuLTrKz1SgzS8Zb4gZD6Xli728vcsvI15N6wqwQHIqHYbM85fxGdvOE7B102kvpT4wYRnuxh0l7phspABAu3Heb8lR+laBSovKGsv5dX35y7ODRiaojQXTGAS3V4HGAnD4wz+V43wrjq5q+xmxSOBNtf3R70cXT6mqahsVlCp23Q7MGtz62DRhdF0hbhavAOJoxZ7hYHTfsvdYOF/eKzAedoARHZQdsJbU2pOBZ3qPKUVCO9Ud8lCoob6vUpGiF32JhS2GPHi0HUA0nDz3rAgz/YsCynC1ghmENUQ5ieH5O7pPbsfBLjmhJLDyi/OOrYI1A==
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2GDAADY2AhYf0bXVdEZA0AaAQEBAQIBAQEBCAEBAQEVAQEBAQIBAQEBCAEBAQGCXTYBAQEBAXRtEAeDOIl1lnyCV4UHjF+BRhskAxwBBoF0gjCBWgKBbwc/FAEBAQEBAQEBAQEBAhABAQkLCwkbJwuCMwQCAQIRBQQBOQoGKwEBAQEBAQEBAQEBAQEBAQEaAggFHgQPAw8CGQEBAQMBEggBCCsdDgULCQILDSABCQICIQEPAwEFAQsRBggHBAEcAgIBh0xKAw8IBZh4MY9NgTI+MotDiQMNg3ABAQEBAQEBAQEBAQEBAQEBAQEBAQEYBAsFiwKCR4FSEQGDIIJbBYhAZIYZhHWFJzUBhimGT4MUgW5OhBuDN4VthxOBVYQXgj4THoERDw9dgn87HIFzHjQHhhpHMYEoAQEB
X-IronPort-AV: E=Sophos;i="5.31,371,1473145200"; 
   d="scan'208,217";a="53313065"
Received: from mail-lf0-f70.google.com ([209.85.215.70])
  by fe3.lbl.gov with ESMTP; 20 Oct 2016 07:49:47 -0700
Received: by mail-lf0-f70.google.com with SMTP id m193so6078636lfm.7
        for <singu...@lbl.gov>; Thu, 20 Oct 2016 07:49:47 -0700 (PDT)
X-Gm-Message-State: AA6/9Rkm1pIi5kaSwgANmVIZPaUo1xPGNyygjdSDFUehR0y5Y+DM3GeLzvyR0QsVYwb7nU+7I+8JQIO9YjEmmacddcpyS1GBIN4BJmxpkba9JZr8llmm7ZNxXzXOZIC2DY/EO5cotzxC+ez6OoN9cSr5aQk=
X-Received: by 10.25.28.70 with SMTP id c67mr2129476lfc.93.1476974985717;
        Thu, 20 Oct 2016 07:49:45 -0700 (PDT)
X-Received: by 10.25.28.70 with SMTP id c67mr2129429lfc.93.1476974985283; Thu,
 20 Oct 2016 07:49:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.167.67 with HTTP; Thu, 20 Oct 2016 07:49:44 -0700 (PDT)
In-Reply-To: <4e7e818e-e472-4acd-a2de-c5274ea89241@lbl.gov>
References: <04cccf38-72fb-49ef-b010-15fb80e71e8e@lbl.gov> <CAN7etTyY_+ytGXpY4Te-=xED4zHNzz4Pg7xkB4ULj2sZ=gk2WA@mail.gmail.com>
 <6e2a6338-f64e-4f2a-894b-d40f1f646113@lbl.gov> <CAN7etTzk47mf9bCo2S6Wp-9sYpS6u3JfwVgN2koRpKcamDtRWw@mail.gmail.com>
 <8689886f-9da7-4f40-8769-06dbfc0a547f@lbl.gov> <CAN7etTz0wBEjGweZXC_pcOzEULW7hmeBr7zY5ndtsXOL88jSdA@mail.gmail.com>
 <5cd8d8d2-44f5-47ab-8e02-838942890090@lbl.gov> <CAN7etTyWS+UGZa_EHc-J5at6ULUtFe=Jh3qPXksJNs4-dS5dVg@mail.gmail.com>
 <CAN7etTwDa1oYx6K7sRxhATxT-4w+Z5A5rmpZrO7MPSFmdv2GPA@mail.gmail.com>
 <dc6c4870-0f65-4f97-b38c-180052ea1020@lbl.gov> <CAN7etTyR-NKepOEGf3YAASPTZERTLNC-RP4vpHO+QZxpsT7HGQ@mail.gmail.com>
 <17D943D8-2CF1-4B6D-A0A3-AA9479256ED0@open-mpi.org> <4e7e818e-e472-4acd-a2de-c5274ea89241@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 20 Oct 2016 07:49:44 -0700
Message-ID: <CAN7etTx53h8AfggsTN3Hqma1TK6h2Bnsvc2wbpNqVbA+tSGvDw@mail.gmail.com>
Subject: Re: [Singularity] Using singularity with MPI jobs
To: "singu...@lbl.gov" <singu...@lbl.gov>
Cc: "r...@open-mpi.org" <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary=001a114022585738f5053f4d09a8

--001a114022585738f5053f4d09a8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Steve,

While this is outside my personal area of expertise, I believe Ralph was
mentioning that the Slurm PMI enabled libraries needs to also be installed
inside the container along with a properly built MPI to link against those
libraries with PMI enabled to implement the type of job you are running.

With that said, why not just call mpirun/mpiexec instead of using srun over
PMI?

Greg


On Thursday, October 20, 2016, Steve Mehlberg <sgmeh...@gmail.com> wrote:

> Thanks for all the suggestions.  Here is an update of where I'm at:
>
> 1) First I tried running the newest version of singularity (2.2) and I
> still experienced the problem.
> 2) I finally was able to compile the NEMO application without using the
> Intel compilers and MPI.  I am now able to get singularity to run with
> slurm srun if I use --mpi=3Dpmix.  So I can do my comparisons.
> 3) Using --mpi=3Dpmi2 still gets the exit error 6.  I'm going to rebuild =
the
> container with the newest version of singularity and try again.
> 4) I'm using slurm 16.05.4 which has the mpi plugins and support.
>
> On Friday, October 14, 2016 at 5:14:45 PM UTC-6, r...@open-mpi.org
> <javascript:_e(%7B%7D,'cvml','r...@open-mpi.org');> wrote:
>>
>>
>> On Oct 13, 2016, at 3:48 PM, Gregory M. Kurtzer <gm...@lbl.gov> wrote:
>>
>>
>>
>> On Thu, Oct 13, 2016 at 8:56 AM, Steve Mehlberg <sg...@gmail.com>
>> wrote:
>>
>>> Gregory,
>>>
>>> I didn't set anything concerning /dev/shm, so I'm not sure why the
>>> openmpi stuff gets there.
>>>
>>
>> I did a bit of checking, and I think openmpi conditionally uses /dev/shm=
/
>> based on local configuration of /tmp.
>>
>>
>> This is correct - if /tmp is a shared file system, for example, or too
>> small to hold the backing file
>>
>>
>>
>>>
>>> Our group (Atos/Bull) is doing development on the slurm product so that
>>> is why we are interested in sbatch/srun vs mpirun.  We haven't found
>>> anything amiss with the invocation using slurm - but something is diffe=
rent
>>> from mpirun that is causing this issue.
>>>
>>
>> I am not an expert on PMIx, but as I understand it, if you are invoking
>> using PMIx via `srun`, you need to have the SLURM PMIx implementation al=
so
>> installed within the container, or that the OMPI build itself has to
>> include the PMIx support.
>>
>> Just to reiterate, does it work as expected when executing via mpirun?
>>
>>
>> Are you using the latest version of SLURM that has PMIx in it? If not,
>> then did you build OMPI --with-pmi so the PMI support was built, and did
>> you include Slurm=E2=80=99s PMI libraries in your container? Otherwise, =
your MPI
>> application won=E2=80=99t find the PMI support, and there is no way it c=
an run
>> using srun as the launcher.
>>
>>
>>
>>>
>>> I'm interested in your comment about singularity support for openmpi.
>>> Are you saying there are changes in openmpi for singularity that are no=
t in
>>> the stable released versions but are in the master branch?  Are any of
>>> these changes specific to pmi2 or pmix?
>>>
>>
>> Yes, there are but I'm not sure if those changes are critical to the
>> failure you are seeing now.
>>
>>
>> Correct, on both counts - the changes make things easier/more transparen=
t
>> for a user to run a Singularity job, but don=E2=80=99t affect the basic
>> functionality.
>>
>>
>>
>>>
>>> How can I make sure I'm running with an openmpi that has the "required"
>>> singularity changes?
>>>
>>
>> I believe currently, my schizo/personality fixes are only in the master
>> branch of OMPI on GitHub at present, and it will be included in the next
>> release... But, again, I don't think this is the cause of what you are
>> seeing. I think it is a PMIx issue in that the PMI support is lacking
>> inside the container. I am CC'ing Ralph with the hope that he can chime =
in.
>>
>>
>> There is nothing =E2=80=9Crequired=E2=80=9D in those changes - as I said=
 above, they only
>> make things more convenient. For example, we automatically detect that a=
n
>> application is actually a Singularity container, and invoke =E2=80=9Csin=
gularity=E2=80=9D
>> to execute it (with the appropriate envars set).
>>
>> So Singularity will work with OMPI as-is - you just have to manually do
>> the cmd line.
>>
>>
>> Greg
>>
>>
>>
>>
>>>
>>> -Steve
>>>
>>> On Wednesday, October 12, 2016 at 8:46:29 PM UTC-6, Gregory M. Kurtzer
>>> wrote:
>>>>
>>>> Hi Steve,
>>>>
>>>> Did you mention that it works if you call it via mpirun? If so, why
>>>> don't you just launch with mpirun/mpiexec? I'm not sure the startup
>>>> invocation is the same for srun even via pmi.
>>>>
>>>> Additionally, you may need to use OMPI from the master branch from
>>>> GitHub. I just heard from Ralph that proper Singularity support has no=
t
>>>> been part of an OMPI release yet.
>>>>
>>>> Thanks and hope that helps!
>>>>
>>>> On Wed, Oct 12, 2016 at 2:37 PM, Gregory M. Kurtzer <gm...@lbl.gov>
>>>> wrote:
>>>>
>>>>> Weird how openmpi is actually throwing the session directory in
>>>>> /dev/shm. I thought it usually uses /tmp.
>>>>>
>>>>> Did you set that somewhere or am I confused?
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Wednesday, October 12, 2016, Steve Mehlberg <sg...@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> Gregory,
>>>>>>
>>>>>> Yes, I was able to create a file on the host (non-root uid) in
>>>>>> /dev/shm/test.it and then view it in the singularity shell.
>>>>>>
>>>>>> And, there is some stuff there too, is that normal?
>>>>>>
>>>>>> bash-4.2$ ls /dev/shm -la
>>>>>> total 4
>>>>>> drwxrwxrwt   4 root      root   100 Oct 12 17:34 .
>>>>>> drwxr-xr-x  20 root      root  3580 Oct  7 22:04 ..
>>>>>> -rwxr-xr-x   1 mehlberg  user   880 Oct 12 17:34 test.it
>>>>>> drwx------  47 root      root   940 Sep 30 17:19
>>>>>> openmpi-sessions-0@node9_0
>>>>>> drwx------ 561 mehlberg  user 11220 Oct 12 15:39
>>>>>> openmpi-sessions-50342@node9_0
>>>>>>
>>>>>>
>>>>>> Steve
>>>>>>
>>>>>> On Wednesday, October 12, 2016 at 11:18:20 AM UTC-6, Gregory M.
>>>>>> Kurtzer wrote:
>>>>>>>
>>>>>>> Can you create a file in /dev/shm/... on the host, and then start a
>>>>>>> Singularity container and confirm that you can see that file from w=
ithin
>>>>>>> the container please?
>>>>>>>
>>>>>>> Thanks!
>>>>>>>
>>>>>>> On Wed, Oct 12, 2016 at 9:45 AM, Steve Mehlberg <sg...@gmail.com>
>>>>>>>  wrote:
>>>>>>>
>>>>>>>> Wow, that was very interesting.  I indeed get the same problem wit=
h
>>>>>>>> the singularity -n1 (srun - one task).  I created the strace, then=
 wanted
>>>>>>>> to compare the output to a non-singularity run.  But when I change=
 the
>>>>>>>> non-singularity run to use anything other than the required number=
 of tasks
>>>>>>>> I get the same error!  That seems to indicate that in the singular=
ity run
>>>>>>>> (srun with the correct number of tasks) for some reason the MPI pr=
ocesses
>>>>>>>> can't communicate with one another.
>>>>>>>>
>>>>>>>> The strace doesn't show much - or at least not much that means
>>>>>>>> something to me.  The program seems to be going along outputting d=
ata then
>>>>>>>> aborts with exit 6:
>>>>>>>>
>>>>>>>> [pid 13573] write(27, "                    suppress iso"..., 56) =
=3D
>>>>>>>> 56
>>>>>>>> [pid 13573] write(27, "                    ------------"..., 56) =
=3D
>>>>>>>> 56
>>>>>>>> [pid 13573] write(27, "      no isolated ocean grid poi"..., 36) =
=3D
>>>>>>>> 36
>>>>>>>> [pid 13573] open("/opt/mpi/openmpi-icc/2.0
>>>>>>>> .0/share/openmpi/help-mpi-errors.txt", O_RDONLY) =3D 28
>>>>>>>> [pid 13573] ioctl(28, SNDCTL_TMR_TIMEBASE or
>>>>>>>> SNDRV_TIMER_IOCTL_NEXT_DEVICE or TCGETS, 0x7ffec31d4d80) =3D -1 EN=
OTTY
>>>>>>>> (Inappropriate ioctl for device)
>>>>>>>> [pid 13573] fstat(28, {st_mode=3DS_IFREG|0644, st_size=3D1506, ...=
}) =3D 0
>>>>>>>> [pid 13573] mmap(NULL, 4096, PROT_READ|PROT_WRITE,
>>>>>>>> MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) =3D 0x7f4b65f45000
>>>>>>>> [pid 13573] read(28, "# -*- text -*-\n#\n# Copyright (c)"..., 8192=
)
>>>>>>>> =3D 1506
>>>>>>>> [pid 13573] read(28, "", 4096)          =3D 0
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] munmap(0x7f4b65f45000, 4096) =3D 0
>>>>>>>> [pid 13573] write(1, "[node9:13573] *** An error occ"..., 361) =3D=
 361
>>>>>>>> [pid 13573] stat("/dev/shm/openmpi-sessions-50342@node9_0/37255/1/=
0",
>>>>>>>> {st_mode=3DS_IFDIR|0700, st_size=3D40, ...}) =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0/37255/1/0", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC)
>>>>>>>> =3D 28
>>>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) =3D 48
>>>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0/37255/1/0", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC)
>>>>>>>> =3D 28
>>>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) =3D 48
>>>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] rmdir("/dev/shm/openmpi-sessions-50342@node9_0/37255/1=
/0")
>>>>>>>> =3D 0
>>>>>>>> [pid 13573] stat("/dev/shm/openmpi-sessions-50342@node9_0/37255/1"=
,
>>>>>>>> {st_mode=3DS_IFDIR|0700, st_size=3D40, ...}) =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0/37255/1", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) =
=3D
>>>>>>>> 28
>>>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) =3D 48
>>>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0/37255/1", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) =
=3D
>>>>>>>> 28
>>>>>>>> [pid 13573] getdents(28, /* 2 entries */, 32768) =3D 48
>>>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] rmdir("/dev/shm/openmpi-sessions-50342@node9_0/37255/1=
")
>>>>>>>> =3D 0
>>>>>>>> [pid 13573] stat("/dev/shm/openmpi-sessions-50342@node9_0",
>>>>>>>> {st_mode=3DS_IFDIR|0700, st_size=3D11080, ...}) =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) =3D 28
>>>>>>>> [pid 13573] getdents(28, /* 554 entries */, 32768) =3D 17424
>>>>>>>> [pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) =3D 28
>>>>>>>> [pid 13573] getdents(28, /* 554 entries */, 32768) =3D 17424
>>>>>>>> [pid 13573] close(28)                   =3D 0
>>>>>>>> [pid 13573] openat(AT_FDCWD, "/dev/shm/openmpi-sessions-503
>>>>>>>> 42@node9_0/37255/1/0", O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC)
>>>>>>>> =3D -1 ENOENT (No such file or directory)
>>>>>>>> [pid 13573] exit_group(6)               =3D ?
>>>>>>>> [pid 13574] +++ exited with 6 +++
>>>>>>>> +++ exited with 6 +++
>>>>>>>> srun: error: node9: task 0: Exited with exit code 6
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> On Wednesday, October 12, 2016 at 8:53:12 AM UTC-6, Gregory M.
>>>>>>>> Kurtzer wrote:
>>>>>>>>>
>>>>>>>>> Can you replicate the problem with a -np 1? If so can you strace
>>>>>>>>> it from within the container:
>>>>>>>>>
>>>>>>>>> mpirun -np 1 singularity exec container.img strace -ff
>>>>>>>>> /path/to/mpi.exe (opts)
>>>>>>>>>
>>>>>>>>> Yes you can try Singularity 2.2. Please install it to a different
>>>>>>>>> path so we can test side by side if you don't mind (if really lik=
e to debug
>>>>>>>>> this).
>>>>>>>>>
>>>>>>>>> Thanks!
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Wednesday, October 12, 2016, Steve Mehlberg <sg...@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>> Greg,
>>>>>>>>>>
>>>>>>>>>> I put a bind to /opt in the singularity.conf file so that
>>>>>>>>>> /opt/intel is available in the container.
>>>>>>>>>>
>>>>>>>>>> All the tasks (16) immediately exit code 6.  The job exits after
>>>>>>>>>> about 4 seconds.  It normally takes about 16 minutes to run with=
 the
>>>>>>>>>> configuration I'm using and I do see the start of some output.
>>>>>>>>>>
>>>>>>>>>> I am using openmpi 2.0.0.
>>>>>>>>>>
>>>>>>>>>> I tried an "export SINGULARITY_NO_NAMESPACE_PID=3D1" in the bash
>>>>>>>>>> script that runs all of this for each process and I still get th=
e problem.
>>>>>>>>>>
>>>>>>>>>> [node12:9779] *** An error occurred in MPI_Isend
>>>>>>>>>> [node12:9779] *** reported by process [3025076225,0]
>>>>>>>>>> [node12:9779] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
>>>>>>>>>> [node12:9779] *** MPI_ERR_RANK: invalid rank
>>>>>>>>>> [node12:9779] *** MPI_ERRORS_ARE_FATAL (processes in this
>>>>>>>>>> communicator will now abort,
>>>>>>>>>> [node12:9779] ***    and potentially your MPI job)
>>>>>>>>>>
>>>>>>>>>> I can try 2.2 - do you think it might behave differently?
>>>>>>>>>>
>>>>>>>>>> Thanks for the ideas and help.
>>>>>>>>>>
>>>>>>>>>> Regards,
>>>>>>>>>>
>>>>>>>>>> Steve
>>>>>>>>>>
>>>>>>>>>> On Tuesday, October 11, 2016 at 8:19:47 PM UTC-6, Gregory M.
>>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>>
>>>>>>>>>>> Hi Steve,
>>>>>>>>>>>
>>>>>>>>>>> I'm not sure at first glance, but just to touch on the basics..=
.
>>>>>>>>>>> Is /opt/intel available from within the container? Do all tasks=
 exit code
>>>>>>>>>>> 6, or just some of them?
>>>>>>>>>>>
>>>>>>>>>>> What version of OMPI are you using?
>>>>>>>>>>>
>>>>>>>>>>> I wonder if the PID namespace is causing a problem here... I'm
>>>>>>>>>>> not sure it gets effectively disabled when running via srun and=
 pmi. Can
>>>>>>>>>>> you export the environment variable "SINGULARITY_NO_NAMESPACE_
>>>>>>>>>>> PID=3D1" in a place where Singularity will pick it up
>>>>>>>>>>> definitively by all ranks? That will ensure that the PID namesp=
ace is not
>>>>>>>>>>> being exported.
>>>>>>>>>>>
>>>>>>>>>>> Additionally, you could try version 2.2. I just released it, an=
d
>>>>>>>>>>> by default it does not unshare() out the PID namespace. But... =
It is the
>>>>>>>>>>> first release in the 2.2 series so it may bring with it other i=
ssues that
>>>>>>>>>>> still need resolving.... But we should debug those too! :)
>>>>>>>>>>>
>>>>>>>>>>> Greg
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Tue, Oct 11, 2016 at 2:40 PM, Steve Mehlberg <
>>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>>
>>>>>>>>>>>> Does singularity support MPI PMI-2 jobs?  I've had mixed
>>>>>>>>>>>> success testing benchmark applications using a singularity con=
tainer.
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> Currently I'm struggling to get the NEMO benchmark to run usin=
g
>>>>>>>>>>>> slurm 16.05 and pmi2.  I can run the exact same executable on =
bare metal
>>>>>>>>>>>> with the same slurm, but I get Rank errors when I run using "s=
run
>>>>>>>>>>>> --mpi=3Dpmi2 singularity...".  The application aborts with an =
exit code 6.
>>>>>>>>>>>>
>>>>>>>>>>>> I tried pmix too, but that gets mpi aborts for both bare metal
>>>>>>>>>>>> and singularity.
>>>>>>>>>>>>
>>>>>>>>>>>> The only way I could get the NEMO application to compile was t=
o
>>>>>>>>>>>> use the intel compilers and mpi:
>>>>>>>>>>>>
>>>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin=
/compilervars.sh
>>>>>>>>>>>> intel64
>>>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin=
/ifortvars.sh
>>>>>>>>>>>> intel64
>>>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin=
/iccvars.sh
>>>>>>>>>>>> intel64
>>>>>>>>>>>> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>>>>>>>>>>>>
>>>>>>>>>>>> It runs fine when I use mpirun with or without singularity.
>>>>>>>>>>>>
>>>>>>>>>>>> Example run/error:
>>>>>>>>>>>>
>>>>>>>>>>>> sbatch ...
>>>>>>>>>>>> srun --mpi=3Dpmi2 -n16 singularity exec c7.img run.it > out_no=
w
>>>>>>>>>>>>
>>>>>>>>>>>> .......
>>>>>>>>>>>> srun: error: node11: tasks 0-7: Exited with exit code 6
>>>>>>>>>>>> srun: error: node12: tasks 8-15: Exited with exit code 6
>>>>>>>>>>>>
>>>>>>>>>>>> $ cat run.it
>>>>>>>>>>>> #!/bin/sh
>>>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin=
/compilervars.sh
>>>>>>>>>>>> intel64
>>>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin=
/ifortvars.sh
>>>>>>>>>>>> intel64
>>>>>>>>>>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin=
/iccvars.sh
>>>>>>>>>>>> intel64
>>>>>>>>>>>> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>>>>>>>>>>>> source env_bench
>>>>>>>>>>>> export PATH=3D/opt/mpi/openmpi-icc/2.0.
>>>>>>>>>>>> 0/bin:/opt/pmix/1.1.5/bin:$PATH
>>>>>>>>>>>> export LD_LIBRARY_PATH=3D/opt/openmpi-i
>>>>>>>>>>>> cc/2.0.0/lib:/opt/pmix/1.1.5/lib:$LD_LIBRARY_PATH
>>>>>>>>>>>> export OMPI_MCA_btl=3Dself,sm,openib
>>>>>>>>>>>>
>>>>>>>>>>>> ./opa_8_2 namelist >out_now
>>>>>>>>>>>>
>>>>>>>>>>>> $ cat out_now
>>>>>>>>>>>> [node12:29725] *** An error occurred in MPI_Isend
>>>>>>>>>>>> [node12:29725] *** reported by process [3865116673,0]
>>>>>>>>>>>> [node12:29725] *** on communicator MPI COMMUNICATOR 3 DUP FROM=
 0
>>>>>>>>>>>> [node12:29725] *** MPI_ERR_RANK: invalid rank
>>>>>>>>>>>> [node12:29725] *** MPI_ERRORS_ARE_FATAL (processes in this
>>>>>>>>>>>> communicator will now abort,
>>>>>>>>>>>> [node12:29725] ***    and potentially your MPI job)
>>>>>>>>>>>>
>>>>>>>>>>>> I am running singularity 2.1 - any ideas?
>>>>>>>>>>>>
>>>>>>>>>>>> -Steve
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> --
>>>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from
>>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> --
>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>>>>>> University of California Berkeley Research IT
>>>>>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>>>>>> er.com/gmkurtzer
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> --
>>>>>>>>>> You received this message because you are subscribed to the
>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>> To unsubscribe from this group and stop receiving emails from it=
,
>>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>>>> University of California Berkeley Research IT
>>>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>>>> er.com/gmkurtzer
>>>>>>>>>
>>>>>>>>>
>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Gregory M. Kurtzer
>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>> University of California Berkeley Research IT
>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>> er.com/gmkurtzer
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> HPC Systems Architect and Technology Developer
>>>> Lawrence Berkeley National Laboratory HPCS
>>>> University of California Berkeley Research IT
>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>> er.com/gmkurtzer
>>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>


--=20
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtze=
r

--001a114022585738f5053f4d09a8
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Steve,<div><br></div><div>While this is outside my personal area of expe=
rtise, I believe Ralph was mentioning that the Slurm PMI enabled=C2=A0libra=
ries needs to also be installed inside the container along with a properly =
built<span></span> MPI to link against those libraries with PMI enabled to =
implement the type of job you are running.=C2=A0</div><div><br></div><div>W=
ith that said, why not just call mpirun/mpiexec instead of using srun over =
PMI?</div><div><br></div><div>Greg</div><div><br><br>On Thursday, October 2=
0, 2016, Steve Mehlberg &lt;<a href=3D"mailto:sgmeh...@gmail.com">sgmeh...@=
gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"marg=
in:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
>Thanks for all the suggestions.=C2=A0 Here is an update of where I&#39;m a=
t:<br><br>1) First I tried running the newest version of singularity (2.2) =
and I still experienced the problem.<br>2) I finally was able to compile th=
e NEMO application without using the Intel compilers and MPI.=C2=A0 I am no=
w able to get singularity to run with slurm srun if I use --mpi=3Dpmix.=C2=
=A0 So I can do my comparisons.<br>3) Using --mpi=3Dpmi2 still gets the exi=
t error 6.=C2=A0 I&#39;m going to rebuild the container with the newest ver=
sion of singularity and try again.<br>4) I&#39;m using slurm 16.05.4 which =
has the mpi plugins and support.<br><br>On Friday, October 14, 2016 at 5:14=
:45 PM UTC-6, <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39...@open-m=
pi.org&#39;);" target=3D"_blank">r...@open-mpi.org</a> wrote:<blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #cc=
c solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><br><div><blo=
ckquote type=3D"cite"><div>On Oct 13, 2016, at 3:48 PM, Gregory M. Kurtzer =
&lt;<a rel=3D"nofollow">gm...@lbl.gov</a>&gt; wrote:</div><br><div><br><br =
style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight=
:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transfo=
rm:none;white-space:normal;word-spacing:0px"><div class=3D"gmail_quote" sty=
le=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight:no=
rmal;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:=
none;white-space:normal;word-spacing:0px">On Thu, Oct 13, 2016 at 8:56 AM, =
Steve Mehlberg<span>=C2=A0</span><span dir=3D"ltr">&lt;<a rel=3D"nofollow">=
sg...@gmail.com</a>&gt;</span><span>=C2=A0</span><wbr>wrote:<br><blockquote=
 class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:=
1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left=
:1ex"><div dir=3D"ltr">Gregory,<br><br>I didn&#39;t set anything concerning=
 /dev/shm, so I&#39;m not sure why the openmpi stuff gets there.<br></div><=
/blockquote><div><br></div><div>I did a bit of checking, and I think openmp=
i conditionally uses /dev/shm/ based on local configuration of /tmp.=C2=A0<=
/div></div></div></blockquote><div><br></div>This is correct - if /tmp is a=
 shared file system, for example, or too small to hold the backing file</di=
v><div><br><blockquote type=3D"cite"><div><div class=3D"gmail_quote" style=
=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight:norm=
al;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:no=
ne;white-space:normal;word-spacing:0px"><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">=
<div dir=3D"ltr"><br>Our group (Atos/Bull) is doing development on the slur=
m product so that is why we are interested in sbatch/srun vs mpirun.=C2=A0 =
We haven&#39;t found anything amiss with the invocation using slurm - but s=
omething is different from mpirun that is causing this issue.<br></div></bl=
ockquote><div><br></div><div>I am not an expert on PMIx, but as I understan=
d it, if you are invoking using PMIx via `srun`, you need to have the SLURM=
 PMIx implementation also installed within the container, or that the OMPI =
build itself has to include the PMIx support.</div><div><br></div><div>Just=
 to reiterate, does it work as expected when executing via mpirun?</div></d=
iv></div></blockquote><div><br></div>Are you using the latest version of SL=
URM that has PMIx in it? If not, then did you build OMPI --with-pmi so the =
PMI support was built, and did you include Slurm=E2=80=99s PMI libraries in=
 your container? Otherwise, your MPI application won=E2=80=99t find the PMI=
 support, and there is no way it can run using srun as the launcher.</div><=
div><br><blockquote type=3D"cite"><div><div class=3D"gmail_quote" style=3D"=
font-family:Helvetica;font-size:12px;font-style:normal;font-weight:normal;l=
etter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;w=
hite-space:normal;word-spacing:0px"><div>=C2=A0</div><blockquote class=3D"g=
mail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-=
left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div =
dir=3D"ltr"><br>I&#39;m interested in your comment about singularity suppor=
t for openmpi.=C2=A0 Are you saying there are changes in openmpi for singul=
arity that are not in the stable released versions but are in the master br=
anch?=C2=A0 Are any of these changes specific to pmi2 or pmix?<br></div></b=
lockquote><div><br></div><div>Yes, there are but I&#39;m not sure if those =
changes are critical to the failure you are seeing now.</div></div></div></=
blockquote><div><br></div>Correct, on both counts - the changes make things=
 easier/more transparent for a user to run a Singularity job, but don=E2=80=
=99t affect the basic functionality.</div><div><br><blockquote type=3D"cite=
"><div><div class=3D"gmail_quote" style=3D"font-family:Helvetica;font-size:=
12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align:=
start;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0=
px"><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px =
0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);bord=
er-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><br>How can I make s=
ure I&#39;m running with an openmpi that has the &quot;required&quot; singu=
larity changes?<br></div></blockquote><div><br></div><div>I believe current=
ly, my schizo/personality fixes are only in the master branch of OMPI on Gi=
tHub at present, and it will be included in the next release... But, again,=
 I don&#39;t think this is the cause of what you are seeing. I think it is =
a PMIx issue in that the PMI support is lacking inside the container. I am =
CC&#39;ing Ralph with the hope that he can chime in.</div></div></div></blo=
ckquote><div><br></div>There is nothing =E2=80=9Crequired=E2=80=9D in those=
 changes - as I said above, they only make things more convenient. For exam=
ple, we automatically detect that an application is actually a Singularity =
container, and invoke =E2=80=9Csingularity=E2=80=9D to execute it (with the=
 appropriate envars set).</div><div><br></div><div>So Singularity will work=
 with OMPI as-is - you just have to manually do the cmd line.</div><div><br=
><blockquote type=3D"cite"><div><div class=3D"gmail_quote" style=3D"font-fa=
mily:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-s=
pacing:normal;text-align:start;text-indent:0px;text-transform:none;white-sp=
ace:normal;word-spacing:0px"><div><br></div><div>Greg</div><div><br></div><=
div><br></div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"m=
argin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204=
,204);border-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><br>-Steve=
<span><br><br>On Wednesday, October 12, 2016 at 8:46:29 PM UTC-6, Gregory M=
. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" style=3D"margin:0p=
x 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);bo=
rder-left-style:solid;padding-left:1ex"><div dir=3D"ltr">Hi Steve,<div><br>=
</div><span><div>Did you mention that it works if you call it via mpirun? I=
f so, why don&#39;t you just launch with mpirun/mpiexec? I&#39;m not sure t=
he startup invocation is the same for srun even via pmi.</div><div><br></di=
v><div>Additionally, you may need to use OMPI from the master branch from G=
itHub. I just heard from Ralph that proper Singularity support has not been=
 part of an OMPI release yet.</div><div><br></div><div>Thanks and hope that=
 helps!</div></span></div><div><br><div class=3D"gmail_quote"><span>On Wed,=
 Oct 12, 2016 at 2:37 PM, Gregory M. Kurtzer<span>=C2=A0</span><span dir=3D=
"ltr">&lt;<a rel=3D"nofollow">gmku...@</a><a href=3D"http://lbl.gov/" rel=
=3D"nofollow" target=3D"_blank">lbl.gov</a>&gt;</span><span>=C2=A0</span>wr=
ot<wbr>e:<br></span><blockquote class=3D"gmail_quote" style=3D"margin:0px 0=
px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);borde=
r-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><span>Weird how openm=
pi is actually throwing the session directory in /dev/shm. I thought it usu=
ally uses /tmp.=C2=A0<div><br></div>Did you set that somewhere or am I conf=
used?</span><div><div><div><div><br><div><br></div><div><br><br>On Wednesda=
y, October 12, 2016, Steve Mehlberg &lt;<a rel=3D"nofollow">sgmeh...@</a><a=
 href=3D"http://gmail.com/" rel=3D"nofollow" target=3D"_blank">gmail.com</a=
>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0=
px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-le=
ft-style:solid;padding-left:1ex"><div dir=3D"ltr">Gregory,<br><br>Yes, I wa=
s able to create a file on the host (non-root uid) in /dev/shm/<a href=3D"h=
ttp://test.it/" rel=3D"nofollow" target=3D"_blank">test.it</a><span>=C2=A0<=
/span>and then view it in the singularity shell.<br><br>And, there is some =
stuff there too, is that normal?<br><br><span style=3D"font-family:&#39;cou=
rier new&#39;,monospace">bash-4.2$ ls /dev/shm -la<br>total 4<br>drwxrwxrwt=
=C2=A0=C2=A0 4 root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 root=C2=A0=C2=A0 100 Oct =
12 17:34 .<br>drwxr-xr-x=C2=A0 20 root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 root=
=C2=A0 3580 Oct=C2=A0 7 22:04 ..<br>-rwxr-xr-x=C2=A0=C2=A0 1 mehlberg=C2=A0=
 user =C2=A0 880 Oct 12 17:34<span>=C2=A0</span><a href=3D"http://test.it/"=
 rel=3D"nofollow" target=3D"_blank">test.it</a><br>drwx------=C2=A0 47 root=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 root=C2=A0=C2=A0 940 Sep 30 17:19 openmpi-se=
ssions-0@node9_0<br>drwx------ 561 mehlberg=C2=A0 user 11220 Oct 12 15:39 o=
penmpi-sessions-50342@node9_0</span><br><br><br>Steve<br><br>On Wednesday, =
October 12, 2016 at 11:18:20 AM UTC-6, Gregory M. Kurtzer wrote:<blockquote=
 class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:=
1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left=
:1ex"><div dir=3D"ltr">Can you create a file in /dev/shm/... on the host, a=
nd then start a Singularity container and confirm that you can see that fil=
e from within the container please?<div><br></div><div>Thanks!</div></div><=
div><br><div class=3D"gmail_quote">On Wed, Oct 12, 2016 at 9:45 AM, Steve M=
ehlberg<span>=C2=A0</span><span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@g=
mail.com</a>&gt;</span><span>=C2=A0</span><wbr>wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">=
<div dir=3D"ltr">Wow, that was very interesting.=C2=A0 I indeed get the sam=
e problem with the singularity -n1 (srun - one task).=C2=A0 I created the s=
trace, then wanted to compare the output to a non-singularity run.=C2=A0 Bu=
t when I change the non-singularity run to use anything other than the requ=
ired number of tasks I get the same error!=C2=A0 That seems to indicate tha=
t in the singularity run (srun with the correct number of tasks) for some r=
eason the MPI processes can&#39;t communicate with one another.<span>=C2=A0=
</span><br><br>The strace doesn&#39;t show much - or at least not much that=
 means something to me.=C2=A0 The program seems to be going along outputtin=
g data then aborts with exit 6:<br><br><span style=3D"font-family:&#39;cour=
ier new&#39;,monospace">[pid 13573] write(27, &quot;=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 suppress iso&quot;..., 56) =3D 56<br>[pid 13573] write(2=
7, &quot;=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 ------------&quot;..., 56)=
 =3D 56<br>[pid 13573] write(27, &quot;=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 no is=
olated ocean grid poi&quot;..., 36) =3D 36<br>[pid 13573] open(&quot;/opt/m=
pi/openmpi-icc/2.0<wbr>.0/share/openmpi/help-mpi-erro<wbr>rs.txt&quot;, O_R=
DONLY) =3D 28<br>[pid 13573] ioctl(28, SNDCTL_TMR_TIMEBASE or SNDRV_TIMER_I=
OCTL_NEXT_DEVICE or TCGETS, 0x7ffec31d4d80) =3D -1 ENOTTY (Inappropriate io=
ctl for device)<br>[pid 13573] fstat(28, {st_mode=3DS_IFREG|0644, st_size=
=3D1506, ...}) =3D 0<br>[pid 13573] mmap(NULL, 4096, PROT_READ|PROT_WRITE, =
MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) =3D 0x7f4b65f45000<br>[pid 13573] read(28=
, &quot;# -*- text -*-\n#\n# Copyright (c)&quot;..., 8192) =3D 1506<br>[pid=
 13573] read(28, &quot;&quot;, 4096)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 =3D 0<br>[pid 13573] munmap(0x7f4b65f45000, 4096) =3D 0<br>[pid 1357=
3] write(1, &quot;[node9:13573] *** An error occ&quot;..., 361) =3D 361<br>=
[pid 13573] stat(&quot;/dev/shm/openmpi-session<wbr>s-50342@node9_0/37255/1=
/0&quot;, {st_mode=3DS_IFDIR|0700, st_size=3D40, ...}) =3D 0<br>[pid 13573]=
 openat(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0/37255/=
1/0&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 1=
3573] getdents(28, /* 2 entries */, 32768) =3D 48<br>[pid 13573] getdents(2=
8, /* 0 entries */, 32768) =3D 0<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/openmp=
i-sessions-503<wbr>42@node9_0/37255/1/0&quot;, O_RDONLY|O_NONBLOCK|O_DIRECT=
OR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 13573] getdents(28, /* 2 entries */, 327=
68) =3D 48<br>[pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0<br>[pi=
d 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] rm=
dir(&quot;/dev/shm/openmpi-sessio<wbr>ns-50342@node9_0/37255/1/0&quot;) =3D=
 0<br>[pid 13573] stat(&quot;/dev/shm/openmpi-session<wbr>s-50342@node9_0/3=
7255/1&quot;, {st_mode=3DS_IFDIR|0700, st_size=3D40, ...}) =3D 0<br>[pid 13=
573] openat(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0/37=
255/1&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid=
 13573] getdents(28, /* 2 entries */, 32768) =3D 48<br>[pid 13573] getdents=
(28, /* 0 entries */, 32768) =3D 0<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/ope=
nmpi-sessions-503<wbr>42@node9_0/37255/1&quot;, O_RDONLY|O_NONBLOCK|O_DIREC=
TOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 13573] getdents(28, /* 2 entries */, 32=
768) =3D 48<br>[pid 13573] getdents(28, /* 0 entries */, 32768) =3D 0<br>[p=
id 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid 13573]=
 rmdir(&quot;/dev/shm/openmpi-sessio<wbr>ns-50342@node9_0/37255/1&quot;) =
=3D 0<br>[pid 13573] stat(&quot;/dev/shm/openmpi-session<wbr>s-50342@node9_=
0&quot;, {st_mode=3DS_IFDIR|0700, st_size=3D11080, ...}) =3D 0<br>[pid 1357=
3] openat(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0&quot=
;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D 28<br>[pid 13573] ge=
tdents(28, /* 554 entries */, 32768) =3D 17424<br>[pid 13573] getdents(28, =
/* 0 entries */, 32768) =3D 0<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 =3D 0<br>[pid 13573] openat(AT_FDCWD, &quot;/dev/shm/openmpi-s=
essions-503<wbr>42@node9_0&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CL=
OEXEC) =3D 28<br>[pid 13573] getdents(28, /* 554 entries */, 32768) =3D 174=
24<br>[pid 13573] close(28)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D 0<br>[pid =
13573] openat(AT_FDCWD, &quot;/dev/shm/openmpi-sessions-503<wbr>42@node9_0/=
37255/1/0&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTOR<wbr>Y|O_CLOEXEC) =3D -1 ENO=
ENT (No such file or directory)<br>[pid 13573] exit_group(6)=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =3D ?=
<br>[pid 13574] +++ exited with 6 +++<br>+++ exited with 6 +++<br>srun: err=
or: node9: task 0: Exited with exit code 6</span><span><br><br><br><br>On W=
ednesday, October 12, 2016 at 8:53:12 AM UTC-6, Gregory M. Kurtzer wrote:</=
span><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;bo=
rder-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:so=
lid;padding-left:1ex"><span>Can you replicate the problem with a -np 1? If =
so can you strace it from within the container:<div><br></div><div>mpirun -=
np 1 singularity exec container.img strace -ff /path/to/mpi.exe (opts)<span=
></span></div><div><br></div><div>Yes you can try Singularity 2.2. Please i=
nstall it to a different path so we can test side by side if you don&#39;t =
mind (if really like to debug this).=C2=A0</div><div><br></div><div>Thanks!=
</div><div><br></div></span><div><div><div><br><br>On Wednesday, October 12=
, 2016, Steve Mehlberg &lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt; wrot=
e:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;b=
order-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:s=
olid;padding-left:1ex"><div dir=3D"ltr">Greg,<br><br>I put a bind to /opt i=
n the singularity.conf file so that /opt/intel is available in the containe=
r.<br><br>All the tasks (16) immediately exit code 6.=C2=A0 The job exits a=
fter about 4 seconds.=C2=A0 It normally takes about 16 minutes to run with =
the configuration I&#39;m using and I do see the start of some output.<br><=
br>I am using openmpi 2.0.0.<br><br>I tried an &quot;export<span>=C2=A0</sp=
an><span>SINGULARITY_NO_NAMESPA<wbr>CE_</span><span>PID=3D1&quot; in the ba=
sh script that runs all of this for each process and I still get the proble=
m.</span><br><br>[node12:9779] *** An error occurred in MPI_Isend<br>[node1=
2:9779] *** reported by process<span>=C2=A0</span><a value=3D"+13025076225"=
>[3025076225</a>,0]<br>[node12:9779] *** on communicator MPI COMMUNICATOR 3=
 DUP FROM 0<br>[node12:9779] *** MPI_ERR_RANK: invalid rank<br>[node12:9779=
] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<=
br>[node12:9779] ***=C2=A0=C2=A0=C2=A0 and potentially your MPI job)<br><br=
>I can try 2.2 - do you think it might behave differently?<br><br>Thanks fo=
r the ideas and help.<br><br>Regards,<br><br>Steve<br><br>On Tuesday, Octob=
er 11, 2016 at 8:19:47 PM UTC-6, Gregory M. Kurtzer wrote:<blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;bo=
rder-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex">=
<div dir=3D"ltr">Hi Steve,<div><br></div><div>I&#39;m not sure at first gla=
nce, but just to touch on the basics... Is /opt/intel available from within=
 the container? Do all tasks exit code 6, or just some of them?</div><div><=
br></div><div>What version of OMPI are you using?</div><div><br></div><div>=
I wonder if the PID namespace is causing a problem here... I&#39;m not sure=
 it gets effectively disabled when running via srun and pmi. Can you export=
 the environment variable &quot;<span>SINGULARITY_NO_NAMESPACE_</span><span=
>PID=3D<wbr>1&quot; in a place where Singularity will pick it up definitive=
ly by all ranks? That will ensure that the PID namespace is not being expor=
ted.</span></div><div><span><br></span></div><div><span>Additionally, you c=
ould try version 2.2. I just released it, and by default it does not unshar=
e() out the PID namespace. But... It is the first release in the 2.2 series=
 so it may bring with it other issues that still need resolving.... But we =
should debug those too! :)</span></div><div><span><br></span></div><div>Gre=
g</div><div><br></div><div><br></div></div><div><br><div class=3D"gmail_quo=
te">On Tue, Oct 11, 2016 at 2:40 PM, Steve Mehlberg<span>=C2=A0</span><span=
 dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span><span>=
=C2=A0</span><wbr>wrote:<br><blockquote class=3D"gmail_quote" style=3D"marg=
in:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,20=
4);border-left-style:solid;padding-left:1ex"><div dir=3D"ltr">Does singular=
ity support MPI PMI-2 jobs?=C2=A0 I&#39;ve had mixed success testing benchm=
ark applications using a singularity container.=C2=A0<span>=C2=A0</span><br=
><br>Currently I&#39;m struggling to get the NEMO benchmark to run using sl=
urm 16.05 and pmi2.=C2=A0 I can run the exact same executable on bare metal=
 with the same slurm, but I get Rank errors when I run using &quot;srun --m=
pi=3Dpmi2 singularity...&quot;.=C2=A0 The application aborts with an exit c=
ode 6.<br><br>I tried pmix too, but that gets mpi aborts for both bare meta=
l and singularity.<br><br>The only way I could get the NEMO application to =
compile was to use the intel compilers and mpi:<br><br><span style=3D"font-=
size:12pt;font-family:&#39;Times New Roman&#39;,serif">source /opt/intel/co=
mpilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>compilervars.sh intel6=
4<br>source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<w=
br>ifortvars.sh intel64<br>source /opt/intel/compilers_and_libra<wbr>ries_2=
016.3.210/linux/bin/<wbr>iccvars.sh intel64<br>source /opt/mpi/openmpi-icc/=
2.0.0/bin<wbr>/mpivars.sh<br><br></span>It runs fine when I use mpirun with=
 or without singularity.<br><br>Example run/error:<br><br>sbatch ...<br>sru=
n --mpi=3Dpmi2 -n16 singularity exec c7.img<span>=C2=A0</span><a href=3D"ht=
tp://run.it/" rel=3D"nofollow" target=3D"_blank">run.it</a><span>=C2=A0</sp=
an>&gt; out_now<br><br>.......<br>srun: error: node11: tasks 0-7: Exited wi=
th exit code 6<br>srun: error: node12: tasks 8-15: Exited with exit code 6<=
br><br>$ cat<span>=C2=A0</span><a href=3D"http://run.it/" rel=3D"nofollow" =
target=3D"_blank">run.it</a><br>#!/bin/sh<br>source /opt/intel/compilers_an=
d_libra<wbr>ries_2016.3.210/linux/bin/<wbr>compilervars.sh intel64<br>sourc=
e /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>ifortva=
rs.sh intel64<br>source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/=
linux/bin/<wbr>iccvars.sh intel64<br>source /opt/mpi/openmpi-icc/2.0.0/bin<=
wbr>/mpivars.sh<br>source env_bench<br>export PATH=3D/opt/mpi/openmpi-icc/2=
.0.<wbr>0/bin:/opt/pmix/1.1.5/bin:$PAT<wbr>H<br>export LD_LIBRARY_PATH=3D/o=
pt/openmpi-i<wbr>cc/2.0.0/lib:/opt/pmix/1.1.5/l<wbr>ib:$LD_LIBRARY_PATH<br>=
export OMPI_MCA_btl=3Dself,sm,openib<br><br>./opa_8_2 namelist &gt;out_now<=
br><br>$ cat out_now<br>[node12:29725] *** An error occurred in MPI_Isend<b=
r>[node12:29725] *** reported by process<span>=C2=A0</span><a value=3D"+138=
65116673">[3865116673</a>,0]<br>[node12:29725] *** on communicator MPI COMM=
UNICATOR 3 DUP FROM 0<br>[node12:29725] *** MPI_ERR_RANK: invalid rank<br>[=
node12:29725] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will=
 now abort,<br>[node12:29725] ***=C2=A0=C2=A0=C2=A0 and potentially your MP=
I job)<br><br>I am running singularity 2.1 - any ideas?<span><font color=3D=
"#888888"><br><br>-Steve<br><br></font></span></div><span><font color=3D"#8=
88888"><div><br></div>--<span>=C2=A0</span><br>You received this message be=
cause you are subscribed to the Google Groups &quot;singularity&quot; group=
.<br>To unsubscribe from this group and stop receiving emails from it, send=
 an email to<span>=C2=A0</span><a rel=3D"nofollow">singu...@lbl.gov</a>.<br=
></font></span></blockquote></div><br><br clear=3D"all"><div><br></div>--<s=
pan>=C2=A0</span><br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div =
dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gr=
egory M. Kurtzer</div><div>HPC Systems Architect and Technology Developer</=
div><div>Lawrence Berkeley National Laboratory HPCS<br>University of Califo=
rnia Berkeley Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"=
http://singularity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://sing=
ularity<wbr>.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a h=
ref=3D"http://warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://=
warewulf.lb<wbr>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://githu=
b.com/gmkurtzer" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<=
wbr>urtzer</a>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span>=
<a href=3D"https://twitter.com/gmkurtzer" rel=3D"nofollow" style=3D"font-si=
ze:12.8px" target=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></=
div></div></div></div></div></div></div></div></div></div></div></blockquot=
e></div><div><br></div>--<span>=C2=A0</span><br>You received this message b=
ecause you are subscribed to the Google Groups &quot;singularity&quot; grou=
p.<br>To unsubscribe from this group and stop receiving emails from it, sen=
d an email to<span>=C2=A0</span><a>singularity+unsubscribe@lbl<wbr>.gov</a>=
.<br></blockquote></div><br><br>--<span>=C2=A0</span><br><div dir=3D"ltr"><=
div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=
=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Systems Arc=
hitect and Technology Developer</div><div>Lawrence Berkeley National Labora=
tory HPCS<br>University of California Berkeley Research IT<br>Singularity L=
inux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" rel=3D"nofoll=
ow" target=3D"_blank">http://singularity<wbr>.lbl.gov/</a>)</div><div>Warew=
ulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lbl.gov/" rel=3D"no=
follow" target=3D"_blank">http://warewulf.lb<wbr>l.gov/</a>)</div><div>GitH=
ub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"nofollow" target=
=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=A0<span style=3D"font=
-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurtzer=
" rel=3D"nofollow" style=3D"font-size:12.8px" target=3D"_blank">https://twi=
tt<wbr>er.com/gmkurtzer</a></div></div></div></div></div></div></div></div>=
</div></div><br></div></div></blockquote></div><div><div><div><br></div>--<=
span>=C2=A0</span><br>You received this message because you are subscribed =
to the Google Groups &quot;singularity&quot; group.<br>To unsubscribe from =
this group and stop receiving emails from it, send an email to<span>=C2=A0<=
/span><a rel=3D"nofollow">singu...@lbl.gov</a>.<br></div></div></blockquote=
></div><br><br clear=3D"all"><div><br></div>--<span>=C2=A0</span><br><div><=
div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=
=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><di=
v>HPC Systems Architect and Technology Developer</div><div>Lawrence Berkele=
y National Laboratory HPCS<br>University of California Berkeley Research IT=
<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.go=
v/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.gov/</a>=
)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.gov/</a=
>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"n=
ofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=A0<spa=
n style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitte=
r.com/gmkurtzer" rel=3D"nofollow" style=3D"font-size:12.8px" target=3D"_bla=
nk">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div></div></d=
iv></div></div></div></div></div></div></blockquote></div><div><br></div>--=
<span>=C2=A0</span><br>You received this message because you are subscribed=
 to the Google Groups &quot;singularity&quot; group.<br>To unsubscribe from=
 this group and stop receiving emails from it, send an email to<span>=C2=A0=
</span><a>singularity+unsubscribe@lbl<wbr>.gov</a>.<br></blockquote></div><=
/div></div></div></div></div></blockquote></div><div><div><br><br clear=3D"=
all"><div><br></div>--<span>=C2=A0</span><br><div><div dir=3D"ltr"><div><di=
v dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr">=
<div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Systems Architect an=
d Technology Developer</div><div>Lawrence Berkeley National Laboratory HPCS=
<br>University of California Berkeley Research IT<br>Singularity Linux Cont=
ainers=C2=A0(<a href=3D"http://singularity.lbl.gov/" rel=3D"nofollow" targe=
t=3D"_blank">http://singularity<wbr>.lbl.gov/</a>)</div><div>Warewulf Clust=
er Management=C2=A0(<a href=3D"http://warewulf.lbl.gov/" rel=3D"nofollow" t=
arget=3D"_blank">http://warewulf.lb<wbr>l.gov/</a>)</div><div>GitHub:=C2=A0=
<a href=3D"https://github.com/gmkurtzer" rel=3D"nofollow" target=3D"_blank"=
>https://github.com/gmk<wbr>urtzer</a>,=C2=A0<span style=3D"font-size:12.8p=
x">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurtzer" rel=3D"no=
follow" style=3D"font-size:12.8px" target=3D"_blank">https://twitt<wbr>er.c=
om/gmkurtzer</a></div></div></div></div></div></div></div></div></div></div=
></div></div></div></div></blockquote></div><div><div><div><br></div>--<spa=
n>=C2=A0</span><br>You received this message because you are subscribed to =
the Google Groups &quot;singularity&quot; group.<br>To unsubscribe from thi=
s group and stop receiving emails from it, send an email to<span>=C2=A0</sp=
an><a rel=3D"nofollow">singu...@lbl.gov</a>.<br></div></div></blockquote></=
div><br style=3D"font-family:Helvetica;font-size:12px;font-style:normal;fon=
t-weight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text=
-transform:none;white-space:normal;word-spacing:0px"><br style=3D"font-fami=
ly:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-spa=
cing:normal;text-align:start;text-indent:0px;text-transform:none;white-spac=
e:normal;word-spacing:0px" clear=3D"all"><div style=3D"font-family:Helvetic=
a;font-size:12px;font-style:normal;font-weight:normal;letter-spacing:normal=
;text-align:start;text-indent:0px;text-transform:none;white-space:normal;wo=
rd-spacing:0px"><br></div><span style=3D"font-family:Helvetica;font-size:12=
px;font-style:normal;font-weight:normal;letter-spacing:normal;text-align:st=
art;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px=
;float:none;display:inline!important">--<span>=C2=A0</span></span><br style=
=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weight:norm=
al;letter-spacing:normal;text-align:start;text-indent:0px;text-transform:no=
ne;white-space:normal;word-spacing:0px"><div style=3D"font-family:Helvetica=
;font-size:12px;font-style:normal;font-weight:normal;letter-spacing:normal;=
text-align:start;text-indent:0px;text-transform:none;white-space:normal;wor=
d-spacing:0px"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"=
><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Ku=
rtzer</div><div>HPC Systems Architect and Technology Developer</div><div>La=
wrence Berkeley National Laboratory HPCS<br>University of California Berkel=
ey Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://sing=
ularity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr=
>.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http=
://warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb=
<wbr>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkur=
tzer" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer<=
/a>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"=
https://twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" =
target=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div><=
/div></div></div></div></div></div></div></div></div></blockquote></div><br=
></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singularity+unsubscribe@lbl.<wbr>gov</a>.=
<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div><div dir=3D"ltr"><d=
iv><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"=
><div>Gregory M. Kurtzer</div><div>HPC Systems Architect and Technology Dev=
eloper</div><div>Lawrence Berkeley National Laboratory HPCS<br>University o=
f California Berkeley Research IT<br>Singularity Linux Containers=C2=A0(<a =
href=3D"http://singularity.lbl.gov/" target=3D"_blank">http://singularity.l=
bl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://=
warewulf.lbl.gov/" target=3D"_blank">http://warewulf.lbl.gov/</a>)</div><di=
v>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" target=3D"_blank">h=
ttps://github.com/gmkurtzer</a>,=C2=A0<span style=3D"font-size:12.8px">Twit=
ter:=C2=A0</span><a href=3D"https://twitter.com/gmkurtzer" style=3D"font-si=
ze:12.8px" target=3D"_blank">https://twitter.com/gmkurtzer</a></div></div><=
/div></div></div></div></div></div></div></div><br>

--001a114022585738f5053f4d09a8--
