Date: Sat, 6 Aug 2016 02:25:55 -0700 (PDT)
From: Stefan Kombrink <stefan....@googlemail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <94debf38-e0a1-40a8-92b3-26000a8ea6d6@lbl.gov>
In-Reply-To: <CAN7etTyOparWNu7=2vLk4=Adzyt5=3DkU4aDO2HgtEvML_j22g@mail.gmail.com>
References: <5397fc9b-cc20-4813-8eaf-c806f8e27771@lbl.gov> <CAN7etTxYNne=gysZyGC535ygutWn_WBX_xqHf5RVE9kYN=mSNw@mail.gmail.com>
 <54ad52db-a96b-42af-abc1-380b704edfca@lbl.gov>
 <CAN7etTyOparWNu7=2vLk4=Adzyt5=3DkU4aDO2HgtEvML_j22g@mail.gmail.com>
Subject: Re: [Singularity] Archiving research experiments using singularity
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_2715_1272376246.1470475555796"

------=_Part_2715_1272376246.1470475555796
Content-Type: multipart/alternative; 
	boundary="----=_Part_2716_1137509792.1470475555797"

------=_Part_2716_1137509792.1470475555797
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi Gregory,

 sorry that I still won't let go off this topic :) ...to me this is the 
most exciting thing about singularity.

But what are the advantages of that hybrid MPI usage in your opinion?
To me it is not fully clear...

As I get it I can run MPI inside the container fine with just any version 
of MPI:
singularity mpirun <OPTS> myMPIapp

Is it like this is just going to work with OpenMPI 2.1+?

mpirun <OPTS> singularity exec myMPIapp

Actually we are interested in that case since I can't see how multi-node 
usage and torque/moab integration can be achieved otherwise...
Or can this be done with arbitrary MPI versions even?

thanks for clarifying
Steven

Am Freitag, 5. August 2016 21:55:56 UTC+2 schrieb Gregory M. Kurtzer:
>
> Hi Steven,
>
> Thank you for pointing the confusing wording out! I will fix that.
>
> It is intended to say that to use Open MPI in a hybrid manner (like this) 
> you must be using at least 2.1 (or maybe 2.0.1) as previous versions of 
> OMPI do not work.
>
> Hope that helps and sorry for the confusion!
>
> Greg
>
>
>
> On Fri, Aug 5, 2016 at 8:55 AM, 'Stefan Kombrink' via singularity <
> si...@lbl.gov <javascript:>> wrote:
>
>> Hi Gregory,
>>
>>  thanks for your responses!
>>
>> Am Freitag, 5. August 2016 16:27:01 UTC+2 schrieb Gregory M. Kurtzer:
>>>
>>>
>>> -Is trunk OpenMPI really a requirement or can I use older versions, too? 
>>>> What about IMPI?
>>>>
>>>
>>> Open MPI is not a requirement but it has been tuned for Singularity and 
>>> well tested. I can not speak definitively for IMPI, but people have told me 
>>> that it is working fine out of the box!
>>>  
>>>
>>  
>> I was mainly asking because of this statement:
>>
>> Notes: 
>>
>> Supported Open MPI Version(s): To achieve proper container'ized Open MPI 
>> support, you must use Open MPI version 2.1 which at the time of this 
>> writing has not been released yet. The above example builds from the 
>> current master development branch of Open MPI.  
>>
>> given at http://singularity.lbl.gov/#hpc
>>
>> We run a software stack with >10 different versions of MPI and thus it is 
>> important to us that containerized MPI apps are properly managed by Torque 
>> and can make use of the IB transport which is something docker was giving 
>> troubles when running unprivileged. Also, to me it is not fully clear 
>> whether MPI needs to be installed both in the container and on the host.
>>
>> I will soon commit further experiments with MPI, thanks and bye
>> Steven
>>
>> -- 
>> You received this message because you are subscribed to the Google Groups 
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an 
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
>
> -- 
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>

------=_Part_2716_1137509792.1470475555797
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Gregory,<br><br>=C2=A0sorry that I still won&#39;t let =
go off this topic :) ...to me this is the most exciting thing about singula=
rity.<br><br>But what are the advantages of that hybrid MPI usage in your o=
pinion?<br>To me it is not fully clear...<br><br>As I get it I can run MPI =
inside the container fine with just any version of MPI:<br>singularity mpir=
un &lt;OPTS&gt; myMPIapp<br><br>Is it like this is just going to work with =
OpenMPI 2.1+?<br><br>mpirun &lt;OPTS&gt; singularity exec myMPIapp<br><br>A=
ctually we are interested in that case since I can&#39;t see how multi-node=
 usage and torque/moab integration can be achieved otherwise...<br>Or can t=
his be done with arbitrary MPI versions even?<br><br>thanks for clarifying<=
br>Steven<br><br>Am Freitag, 5. August 2016 21:55:56 UTC+2 schrieb Gregory =
M. Kurtzer:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left=
: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">Hi=
 Steven,<div><br></div><div>Thank you for pointing the confusing wording ou=
t! I will fix that.</div><div><br></div><div>It is intended to say that to =
use Open MPI in a hybrid manner (like this) you must be using at least 2.1 =
(or maybe 2.0.1) as previous versions of OMPI do not work.</div><div><br></=
div><div>Hope that helps and sorry for the confusion!</div><div><br></div><=
div>Greg</div><div><br></div><div><br></div><div><div><br><div class=3D"gma=
il_quote">On Fri, Aug 5, 2016 at 8:55 AM, &#39;Stefan Kombrink&#39; via sin=
gularity <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_blank" gd=
f-obfuscated-mailto=3D"-CV1x4T7BwAJ" rel=3D"nofollow" onmousedown=3D"this.h=
ref=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D&#39;javas=
cript:&#39;;return true;">si...@lbl.gov</a>&gt;</span> wrote:<br><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div dir=3D"ltr">Hi Gregory,<br><br>=C2=A0thanks for y=
our responses!<span><br><br>Am Freitag, 5. August 2016 16:27:01 UTC+2 schri=
eb Gregory M. Kurtzer:<blockquote class=3D"gmail_quote" style=3D"margin:0;m=
argin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"l=
tr"><br><div><div><div class=3D"gmail_quote"><blockquote class=3D"gmail_quo=
te" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"=
><div dir=3D"ltr">-Is trunk OpenMPI really a requirement or can I use older=
 versions, too? What about IMPI?<br></div></blockquote><div><br></div><div>=
Open MPI is not a requirement but it has been tuned for Singularity and wel=
l tested. I can not speak definitively for IMPI, but people have told me th=
at it is working fine out of the box!</div><div>=C2=A0</div></div></div></d=
iv></div></blockquote><div>=C2=A0</div></span><div>I was mainly asking beca=
use of this statement:<br><br><h3>Notes:</h3>
<p>
</p><h4>Supported Open MPI Version(s):</h4>
To achieve proper container&#39;ized Open MPI support, you must use Open MP=
I
version 2.1 which at the time of this writing has not been released yet.
The above example builds from the current master development branch of Open
MPI.=C2=A0
<br><br>given at <a href=3D"http://singularity.lbl.gov/#hpc" target=3D"_bla=
nk" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;http://www.google.com/=
url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\=
x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;" onclick=3D"=
this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lb=
l.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-=
SUwsYEjw&#39;;return true;">http://singularity.lbl.gov/#<wbr>hpc</a><br><br=
>We run a software stack with &gt;10 different versions of MPI and thus it =
is important to us that containerized MPI apps are properly managed by Torq=
ue and can make use of the IB transport which is something docker was givin=
g troubles when running unprivileged. Also, to me it is not fully clear whe=
ther MPI needs to be installed both in the container and on the host.<br><b=
r>I will soon commit further experiments with MPI, thanks and bye<br>Steven=
<br></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
-CV1x4T7BwAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div></div>
</blockquote></div>
------=_Part_2716_1137509792.1470475555797--

------=_Part_2715_1272376246.1470475555796--
