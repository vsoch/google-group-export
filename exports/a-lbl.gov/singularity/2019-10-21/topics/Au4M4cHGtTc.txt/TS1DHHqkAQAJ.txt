X-Received: by 10.98.67.153 with SMTP id l25mr793637pfi.19.1507834084596;
        Thu, 12 Oct 2017 11:48:04 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.99.56.12 with SMTP id f12ls1185971pga.11.gmail; Thu, 12 Oct
 2017 11:48:03 -0700 (PDT)
X-Received: by 10.101.81.6 with SMTP id f6mr976357pgq.166.1507834083650;
        Thu, 12 Oct 2017 11:48:03 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1507834083; cv=none;
        d=google.com; s=arc-20160816;
        b=A3+2J124spRfsWByHNSuHFJzmdQ4obTIHe3XFNGuos/m+15Ljeoq50chbV+U5pYoNR
         tSeX6jSQmt8HlMcTxNS4YAYA6cKrnXXY69Zj+FhAWlq+uVCVj8Cs7pv5B35g3JBsba6j
         U5KjsxPHMnxud/1weZLKNwJqjCynA8hA3aBCRiTT+8thWRuZ1ehj7nFwRyrFHINUEE3g
         sTi/CVKMQ0Jpa10/srAHXSK602Fv5XnA21oJN0I9cJGsR88tOJMCfhJyfJ7KJf7RD2OM
         LOpQf080fMBGK0p0pWTzoUtonmlU+GvJhURtNWFTwocZ4FJGC2hmQHiGJXbA2JFHFmsi
         HJrg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=hpqfrZZR5S5w+TJhP2JCN1tlLZqoRzA8iv7+7r1aPSQ=;
        b=h79GbCTPRFgnaBusgwLswI1q2ya77Hxqsr8eYeHPVYki7C2tD6k9G1du+1gZxCYGbl
         5nMgJDSer+u0tU5ID8SUp8+lwMMhXyWATmQPEi5uYbFjahHysMgdHWLl/ICtl2/kYFTO
         HTjaHK1vim5sKV2NIQgPC9NLmeePWo2sZW/xohBnYOxMrStDGC9oly8ngXaXLZTz1lbS
         2Rqeg5ijbVgTxrY8VsH0DXHHPCy/vIg+J8hlXRdrQUZhIfLM1SawBx+wvs8IzoUYcQeT
         USXWlUHp13Fc0LV9xGXKo8y2k+f1fGqTquHn7Ap4HnoYZPfkFDRV4vugdSbeiNNztCwV
         BOSA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=B/DoAHkB;
       spf=pass (google.com: domain of arok...@gmail.com designates 209.85.192.181 as permitted sender) smtp.mailfrom=arok...@gmail.com
Return-Path: <arok...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id r65si10159004pfg.390.2017.10.12.11.48.03
        for <singu...@lbl.gov>;
        Thu, 12 Oct 2017 11:48:03 -0700 (PDT)
Received-SPF: pass (google.com: domain of arok...@gmail.com designates 209.85.192.181 as permitted sender) client-ip=209.85.192.181;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=B/DoAHkB;
       spf=pass (google.com: domain of arok...@gmail.com designates 209.85.192.181 as permitted sender) smtp.mailfrom=arok...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HdBQCUt99ZhrXAVdFeHRgHDBgHgngCU?=
 =?us-ascii?q?QGBLCcHg2sIgTaYGYF2kHCFPw6BQUMiAYUiAoQ4B0EWAQEBAQEBAQEBAQECEAE?=
 =?us-ascii?q?BAQgLCwgoL4I4BQIDGgYIRikvAQEBAQEBAQEBAR8CKyUbAQQBIx0BDQ4eAwELB?=
 =?us-ascii?q?gUEBzcCAiEBAQ4DAQUBHA4HBAEcBIgtgTcBAw0IBZ8lQIwMggUFARyDCgWDaAo?=
 =?us-ascii?q?ZJw1YgxQBAQEBBgEBAQEBGwIGEoMbggeBUYUUgl6BdAESAYMygmEBBKEIPI9xh?=
 =?us-ascii?q?HmTEI0AiFcZH4EVJgR9NAsyIQgdFUkahRCCDyQ2CIkaSIFtAQEB?=
X-IronPort-AV: E=Sophos;i="5.43,367,1503385200"; 
   d="scan'208,217";a="92599783"
Received: from mail-pf0-f181.google.com ([209.85.192.181])
  by fe3.lbl.gov with ESMTP; 12 Oct 2017 11:48:02 -0700
Received: by mail-pf0-f181.google.com with SMTP id n14so6126414pfh.8
        for <singu...@lbl.gov>; Thu, 12 Oct 2017 11:48:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=hpqfrZZR5S5w+TJhP2JCN1tlLZqoRzA8iv7+7r1aPSQ=;
        b=B/DoAHkBkFvCCY9RjRh3chAosCFhVgALWmS8BVQy0KCp9HftxGaVgZTcJUXTyua5y/
         6w/Um+wDm8MGvy3GjaJ/HajMoipn9k+bTGOq8EStg4kumxyVS58vx1bepbd6jNa7vkRD
         2iIuWJgoumZ481zQkgLzsJoVm8Yo//jXCOQ0oqn2nrds7v7tTFoN4E66ti9EHcY+STw7
         muLNe7E1CkbTqCXj8ZH4wXrhLN+yedjVlV7qNwPZPT+sdW917YYcMJku7lNYUpjEjE4c
         VURHdevSbfoS6Q38rxjX/1BUb+Ndb83hdnqH8yBLMj7WW+Rn75ZREYDAGpCnB3lXiegI
         MJqQ==
X-Gm-Message-State: AMCzsaUs8MkKQfrm8/rE+kSLzHV6cNMiDLA566ikE9XIyVe6rJ56Apt+
	6zReEEdaSxXQVQln2oI2LMn2tlOgSTH+JJLFa48=
X-Received: by 10.84.133.41 with SMTP id 38mr956805plf.203.1507834082248; Thu,
 12 Oct 2017 11:48:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.100.137.22 with HTTP; Thu, 12 Oct 2017 11:48:01 -0700 (PDT)
In-Reply-To: <43e70f0f-e4c3-4876-aff5-ed8255ebbae4@lbl.gov>
References: <43e70f0f-e4c3-4876-aff5-ed8255ebbae4@lbl.gov>
From: Andrew Rokitka <arok...@gmail.com>
Date: Thu, 12 Oct 2017 14:48:01 -0400
Message-ID: <CAHGBjRwXGr50wCTgknTD6TAhxoFc-hOvLr07qPw7YHeW3eqwdA@mail.gmail.com>
Subject: Re: [Singularity] Beginner Singularity question (StarCCM++)
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="94eb2c130c32da4ab6055b5dfa96"

--94eb2c130c32da4ab6055b5dfa96
Content-Type: text/plain; charset="UTF-8"

Just to clarify - you want mpi to run within the container itself?

On Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski <matthew...@gmail.com>
wrote:

> Hello all, we're just starting to use Singularity on our HPC system in
> conjunction with Slurm and I am having some difficulties understanding how
> to set up one of our containers (but I might be setting things up wrong).
>
> So I understand why we would use mpirun outside of a container, but what
> is a good configuration for a program (StarCCM++) that wraps its own MPI
> and remote commands into its execution?
>
> Example.  I have a 64 node cluster and starccm++ installed into a
> container.  If I want to run my job inside the container I execute:
> *srun singularity exec starccm.img starccm+ options simfile*where srun
> requests a compute node from slurm and executes the rest of the command on
> that compute node.  The problem I have is with parallel jobs.  starccm+
> wants to start its processes on the compute nodes itself (I can specify
> what sort of remote shell to use and which compute nodes I have requested
> via a machine list file in the options) but starccm+ is not aware that it
> needs to launch a singularity container on the compute nodes first.
>
> My first thought is I need to launch the containers on the compute nodes
> before executing the starccm++ parallel job so that when it uses SSH to log
> in to the compute nodes the container is already running but I'm not sure
> on how to accomplish this.  Any thoughts?
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--94eb2c130c32da4ab6055b5dfa96
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Just to clarify - you want mpi to run within the container=
 itself?</div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On =
Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski <span dir=3D"ltr">&lt;<a href=
=3D"mailto:matthew...@gmail.com" target=3D"_blank">matthew...@gmail.com</a>=
&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0=
 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hello=
 all, we&#39;re just starting to use Singularity on our HPC system in conju=
nction with Slurm and I am having some difficulties understanding how to se=
t up one of our containers (but I might be setting things up wrong).<br><br=
>So I understand why we would use mpirun outside of a container, but what i=
s a good configuration for a program (StarCCM++) that wraps its own MPI and=
 remote commands into its execution?<br><br>Example.=C2=A0 I have a 64 node=
 cluster and starccm++ installed into a container.=C2=A0 If I want to run m=
y job inside the container I execute: <i>srun singularity exec starccm.img =
starccm+ options simfile<br></i>where srun requests a compute node from slu=
rm and executes the rest of the command on that compute node.=C2=A0 The pro=
blem I have is with parallel jobs.=C2=A0 starccm+ wants to start its proces=
ses on the compute nodes itself (I can specify what sort of remote shell to=
 use and which compute nodes I have requested via a machine list file in th=
e options) but starccm+ is not aware that it needs to launch a singularity =
container on the compute nodes first.<br><br>My first thought is I need to =
launch the containers on the compute nodes before executing the starccm++ p=
arallel job so that when it uses SSH to log in to the compute nodes the con=
tainer is already running but I&#39;m not sure on how to accomplish this.=
=C2=A0 Any thoughts?<span class=3D"HOEnZb"><font color=3D"#888888"><br><i><=
/i></font></span></div><span class=3D"HOEnZb"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br></div>

--94eb2c130c32da4ab6055b5dfa96--
