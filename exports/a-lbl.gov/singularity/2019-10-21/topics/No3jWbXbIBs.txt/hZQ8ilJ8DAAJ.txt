X-Received: by 10.129.1.197 with SMTP id 188mr4944815ywb.3.1487884850307;
        Thu, 23 Feb 2017 13:20:50 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.170.68 with SMTP id y4ls100646iti.0.canary-gmail; Thu, 23
 Feb 2017 13:20:49 -0800 (PST)
X-Received: by 10.98.204.25 with SMTP id a25mr25417789pfg.6.1487884849435;
        Thu, 23 Feb 2017 13:20:49 -0800 (PST)
Return-Path: <chihs...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id o15si5295723pfg.288.2017.02.23.13.20.49
        for <singu...@lbl.gov>;
        Thu, 23 Feb 2017 13:20:49 -0800 (PST)
Received-SPF: pass (google.com: domain of chihs...@gmail.com designates 209.85.214.45 as permitted sender) client-ip=209.85.214.45;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of chihs...@gmail.com designates 209.85.214.45 as permitted sender) smtp.mailfrom=chihs...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FUBABnUa9Yhi3WVdFcARsBAQEDAQEBCQEBARcBAQQBAQoBAYJDgUOBCQeDTAibZIJkj3GCX4FKGyUDHwEMgW1DEIFcgVoCgx4HQBcBAQEBAQEBAQEBAQIQAQEBCAsLCh0vgioJBAIDAR0EBD0KAQIDAQEBAQEBAQEBAgEBAQEBAQEBAQEBAQIBAQECAQEBAgEBAQEBAwEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBARICDAEeBA8DKQEBAQMBGgEIHQENDh4DAQsGAwILDRkCBQEJAgIhAQEOAwEFAQsRDgcEARMHAgSHZwOBCEkBAw0IBQmQQZEWP4wDggQFARyDCQWDYQoZJw1VgykBCgEBAQEBAQEYAgYSh2qDP4JRgVUQAgFIMAGCKIJfBYkZYYVThhyFcTqCW4QZfIYUhCCBe1OBAgWDQol6ikQxhmwUHoEVDxEBcw0vCDUfUxeDeyoMAxEMgWI/NQEGiViBZwEBAQ
X-IronPort-AV: E=Sophos;i="5.35,198,1484035200"; 
   d="scan'208,217";a="65692970"
Received: from mail-it0-f45.google.com ([209.85.214.45])
  by fe3.lbl.gov with ESMTP; 23 Feb 2017 13:20:46 -0800
Received: by mail-it0-f45.google.com with SMTP id y135so348176itc.1
        for <singu...@lbl.gov>; Thu, 23 Feb 2017 13:20:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=GlV+QzHmPucGavjLDNhABi3ISUOTD6SIGLkf9SiwzK8=;
        b=LedUwlEC9TaEY5Z9hnnXaZ9umQcg6me6NRJWh36yaszsIP/1nYTYzurEoDwF1LFPWx
         ElaO27ftuH2/59N+PjK3S/XUGWvvmusuJqNU2cLoGbyzyvoKo8zLKtx9qFNrDiwny80Z
         qLriWa/ts9mJAzADKosQjry4q47Ssa635rnw3Gc22hkueQ6um0cL9xJ7G+awQMnAHuCw
         kCB3L9urgGZuo/wp+p3gGJ32IyuSFQxM6K+W27qcD5mocIq34SVeYuw/Rcld5SGBuSqm
         YuNXVAFS7i5Sll7OC7qAO2o3rh0Qc5m/7JxnKHuhr13q1tilahuc84aZKr6vsHn7zGsW
         r2Uw==
X-Gm-Message-State: AMke39kLSWLXemy2+m+Ma08IStG7758UOYMZOSFmqTaghcVqMUKRi5VCljDNkAZrdIDyRLP90KjDrDcBTrQsFA==
X-Received: by 10.107.189.197 with SMTP id n188mr34017849iof.88.1487884846072;
 Thu, 23 Feb 2017 13:20:46 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.4.18 with HTTP; Thu, 23 Feb 2017 13:20:45 -0800 (PST)
In-Reply-To: <CAAfrVp0UHD+Adh40KeTSr6x1Evo7bKdtyqTN_ULdQSNwO+ATCg@mail.gmail.com>
References: <d0a10fdc-f912-4e9c-8681-a54f5d53fd72@lbl.gov> <CAN7etTxxeYYxY7aB93H7E686y-8Qru-c_H3t1ANNQ_4oE1C-aA@mail.gmail.com>
 <887cfc8c-48ed-4720-8040-989e407f4203@lbl.gov> <CAN7etTwgGhbKjQw3EpWXMQN7jcQURt2tTHhWZn3FgZjij_=GDA@mail.gmail.com>
 <CABWwhHr9Z_h+nQM2SFq8Mhq=S1x+K9zCB74iPAn9tGdZKBUHNQ@mail.gmail.com>
 <CAA6Bz=fgxCcGtxQz7wVTTqgq_nyuLjNeuu2fOKMhJ3HTR6k5_g@mail.gmail.com>
 <edc73d0a-acb1-4cef-a23c-c4ab5f1b7289@lbl.gov> <a3f8cd92-cb6f-422a-8656-a127e48853f5@lbl.gov>
 <CAAfrVp0UHD+Adh40KeTSr6x1Evo7bKdtyqTN_ULdQSNwO+ATCg@mail.gmail.com>
From: Chihsong <chihs...@gmail.com>
Date: Thu, 23 Feb 2017 22:20:45 +0100
Message-ID: <CABWwhHrj6MKLWt8+4Pq9=Xti9cKtQjR53_aS0p-Ldhetx87S0w@mail.gmail.com>
Subject: Re: [Singularity] Performance impact: My experience
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=94eb2c05d26eb78e6b0549392f2c

--94eb2c05d26eb78e6b0549392f2c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

What did you mean by hybrid mode? MPI + OpenMP or something else?

Chih-Song

On Thu, Feb 23, 2017 at 10:17 PM, Tyler Trafford <ttra...@gmail.com>
wrote:

> I also thought that when running in hybrid mode we wouldn't have to worry
> about the interconnect inside the container....
>
> --
> Tyler Trafford
>
> On Thu, Feb 23, 2017 at 4:05 PM, Chih-Song Kuo <chihs...@gmail.com>
> wrote:
>
>> OK...Now after installing InfiniBand libraries things finally work
>> decently. From this lesson, I would suggest adding a remark about IB
>> support to http://singularity.lbl.gov/docs-hpc, because otherwise new
>> users can easily get confused about Singularity's support for InfiniBand=
.
>> And HPC is obviously defined by such high-speed interconnects (IB, OPA,
>> Cray, ...)
>>
>> To me, this does seem to be a serious issue which would potentially
>> decrease users' feeling about the portability of Singularity containers.
>> Think about a scientist developing codes on a workstation without
>> InfiniBand, and later making a test run on a small departmental cluster
>> with InfiniBand, and eventually a production run on some white elephant
>> with Intel Omni-Path. Just missing one interconnect library in the
>> container and not compiling the MPI against that library would immediate=
ly
>> break portability.
>>
>> Also I agree with the comment made in https://groups.google.com/a/lb
>> l.gov/forum/#!topic/singularity/fsCO1_StjjA that Singularity should
>> consider to include OFED drivers by default when creating a new containe=
r.
>> Not sure if Greg you would change your mind here. The problem was really
>> not about orted. Yes, Open MPI worked without the OFED driver, but only =
on
>> Ethernet. Since Singularity is claimed to be the container for HPC,
>> including a well defined set of interconnect drivers and libraries truly
>> makes sense.
>>
>> For those who try to make your containers work with InfiniBand, I can
>> provide a few general hints. These were not rigorously tested though and=
 I
>> doubt I have the time to do so.
>> 1. Issue ibv_devices in the container. You must be able able to see the
>> name of your InfiniBand adpter. If not, then possibly some IB libraries =
and
>> drivers are missing
>> 1.1. For Centos 7, the following rpm packages seem to be the minimum
>> requirement
>> libibverbs*
>> librdmacm*
>> 1.2. For Centos 6, the following rpm packages seem to be the minimum
>> requirement
>> libibverbs*
>> librdmacm*
>> libmlx*
>> 1.3. Nevertheless, in the end I installed more packages
>> yum -y install libmthca
>> yum -y install libibcm*
>> yum -y install libibmad*
>> yum -y install libibumad*
>> 1.4. Maybe the following will be useful, but I did not install them yet
>> yum -y install dapl*
>> yum -y install ibacm*
>> yum -y install ibutils
>>
>> 2. After ./configure Open MPI, make sure you see the following.
>> OpenFabrics Verbs: yes
>> If you have OmniPath, then it should be
>> Intel Omnipath (PSM2): yes
>>
>> There are also other components like MXM  which can potentially be "yes"
>> if you installed Mellanox MXM.
>>
>> 3. When make Open MPI, use serial compilation, i.e. do not add the -j
>> flag. It sometimes causes explainable behaviors.
>>
>> 4. After make install Open MPI, make sure that the OpenIB component is
>> indeed installed.
>> ompi_info |grep openib
>> #Expect to see non-empty response
>>  MCA btl: openib (MCA v2.1.0, API v3.0.0, Component v3.0.0)
>>
>> 5. Make sure that the MPI version is the same on the host and in the
>> container
>>
>> My updated test result is provided below. If anybody would like to put
>> these figures into any other documents, please inform me.
>>
>> Host configuration:
>> 2x Intel E5-2680v2 (Ivybridge)
>> 64GB memory
>> Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)
>> RHEL 6.7
>> OpenMPI development master branch (dated 8.2.17)
>> Intel MKL 2017.0 community edition
>> gcc 4.4
>>
>> Container:
>> Centos 6 and 7 both tested without noticeable performance difference
>> OpenMPI development master branch (dated 8.2.17)
>> Intel MKL 2017.0 community edition
>> gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)
>>
>> Benchmarks:
>> 1. LINPACK 2.2
>> 2. OSU 5.3.2
>>
>> <LINPACK>
>> Single node. N=3D40000, P=3D5, Q=3D4
>> Container: 368 GFlops
>> Host: 368 GFLOPS
>> #A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x 8 *
>> 2.8 =3D 448 GFlops
>> Efficiency =3D 368 / 448 =3D 82%. Not bad (given the small N (matrix siz=
e)
>> and that gcc instead of icc was used, and the executable was dynamically
>> linked -- by purpose)
>>
>> Dual-node, N=3D60000, P=3D8, Q=3D5
>> Container: 736 GFLOPS
>> Host:737 GFLOPS
>>
>>
>> <OSU-P2P-Bandwidth>
>> Container:
>> Msg size(bytes) BW (MB/s)
>> 65536                3738.89
>> 131072               3756.34
>> 262144               3767.35
>> 524288               3780.46
>> 1048576              3781.94
>> 2097152              3775.07
>> 4194304              3775.97
>>
>> Host:
>> Msg size(bytes) BW (MB/s)
>> 65536                3722.32
>> 131072               3751.33
>> 262144               3771.13
>> 524288               3774.33
>> 1048576              3781.43
>> 2097152              3775.00
>> 4194304              3773.68
>>
>> <OSU-P2P-Latency>
>> Container:
>> Msg size(bytes)  Latency (us)
>> 0                       1.61
>> 1                       1.67
>> 2                       1.66
>> 4                       1.66
>> 8                       1.70
>>
>> Host:
>> Msg size(bytes)  Latency (us)
>> 0                       1.57
>> 1                       1.63
>> 2                       1.63
>> 4                       1.63
>> 8                       1.65
>>
>> How were the benchmarks executed?
>> mpirun -n 20 -hostfile hostfile singularity exec /home/chih/containers/
>> container-centos6-demo.img xhpl
>> mpirun -n 2 -hostfile hostfile singularity exec
>> /home/chih/containers/container-centos6-demo.img osu_bw
>> mpirun -n 2 -hostfile hostfile singularity exec
>> /home/chih/containers/container-centos6-demo.img osu_latency
>>
>> My conclusion: No difference for HPL (1 & 2 nodes) and IB bandwidth.
>> There is a consistent 2% overhead in IB latency though, but this should =
be
>> affordable by most users.
>>
>>
>> On Thursday, February 23, 2017 at 12:44:47 PM UTC+1, Chih-Song Kuo wrote=
:
>>>
>>> I am not really sure if I can share the containers publicly as it
>>> contains an Intel MKL installation which is bound to a personalized ser=
ial
>>> number (although that MKL is from the community edition, so it is
>>> essentially free).
>>>
>>> Still, I am wondering if installing OFED into the container is the righ=
t
>>> approach. There was a thread on a similar topic and Greg said no. I hop=
e I
>>> understood it correctly.
>>> https://groups.google.com/a/lbl.gov/forum/#!topic/singularit
>>> y/fsCO1_StjjA
>>>
>>> After all, although requiring the user to install OFED into the
>>> container might be technically feasible, it would be very awkward for u=
sers
>>> who develop their applications on their own workstations without
>>> InfiniBand, not to mention that I am even not sure if one can install O=
FED
>>> into a machine without IB adapters. And what if the OFED library in the
>>> container differ from that on the host? I don't think that will work, w=
ill
>>> it?
>>>
>>> Sorry but somehow OpenMPI (or any MPI) is intent to reach OFED librarie=
s
>>> still remains a mystery for me. Please enlighten me.
>>>
>>> On Thursday, February 23, 2017 at 9:51:34 AM UTC+1, R=C3=A9my Dernat wr=
ote:
>>>>
>>>> Hi,
>>>>
>>>> You can try to copy your libraries directly from the host to the
>>>> container instead of a classical install ? I do not know what is the b=
est
>>>> way... But if you check woth solutions we will have an answer.
>>>> BTW, I am very interesting by your containers. Could you share it
>>>> (through singularity-hub ?) with your command line to run the benchmar=
k in
>>>> the "singularity" file inside the container ? Indeed, I will do some
>>>> performance tests also...
>>>>
>>>> Best regards,
>>>> R=C3=A9my
>>>>
>>>> 2017-02-23 0:58 GMT+01:00 Chihsong :
>>>>
>>>>> Hi Greg,
>>>>>
>>>>> The problem is that I don't feel I have IB libraries inside my
>>>>> container. How can I check that? Or did you simply install the ofed i=
nto
>>>>> the container?
>>>>>
>>>>> Chiu-Song
>>>>>
>>>>> On Thursday, February 23, 2017, Gregory M. Kurtzer <gm...@lbl.gov>
>>>>> wrote:
>>>>>
>>>>>> Hi Chih-Song,
>>>>>>
>>>>>> Haha, every now and then I get lucky when a new mail comes in and is
>>>>>> at the top of my mbox and I have a moment.
>>>>>>
>>>>>> In summary, yes. The MPI inside the container must also link against
>>>>>> the IB libraries (also within the container).
>>>>>>
>>>>>> Hopefully that helps!
>>>>>>
>>>>>> On Wed, Feb 22, 2017 at 10:58 AM, Chih-Song Kuo  wrote:
>>>>>>
>>>>>>> Hi Greg,
>>>>>>>
>>>>>>> That reply was very prompt! Anyways, my answer follows.
>>>>>>>
>>>>>>> > * Make sure the MPI inside the container is properly linking
>>>>>>> against the IB libraries
>>>>>>> Did that mean that I need to install IB libraries (like Mellanox
>>>>>>> OFED) into the container? I guess not?
>>>>>>> But apparently openib is missing in the container.
>>>>>>> [me@cn03 ompi-rhel6-host]$ ompi_info | grep openib
>>>>>>>                  MCA btl: openib (MCA v2.1.0, API v3.0.0, Component
>>>>>>> v3.0.0)
>>>>>>>
>>>>>>> Singularity.container-centos6-demo.img> ompi_info | grep openib
>>>>>>> Singularity.container-centos6-demo.img>
>>>>>>> Singularity.container-centos6-demo.img> ls
>>>>>>> /etc/infiniband/openib.conf
>>>>>>> ls: cannot access /etc/infiniband/openib.conf: No such file or
>>>>>>> directory
>>>>>>>
>>>>>>> > * Make sure that the MPI configuration is configured to use MPI
>>>>>>> I think you meant "to use IB" instead.
>>>>>>> But still, did you mean that OpenMPI should be "configured"
>>>>>>> "--with-verbs"? (Did you do so or you never had my problem?)
>>>>>>> I did not use this flag when compiling Open MPI either on the host
>>>>>>> or in the container.
>>>>>>> > * Make sure you have Singularity configured properly to share the
>>>>>>> devices properly, tmp, and you are *NOT* using the IPC or PID names=
paces
>>>>>>> Can you provide more hint on how that can be done?
>>>>>>>
>>>>>>> Thank you once again for your time.
>>>>>>> Chih-Song
>>>>>>>
>>>>>>> On Wednesday, February 22, 2017 at 7:33:22 PM UTC+1, Gregory M.
>>>>>>> Kurtzer wrote:
>>>>>>>>
>>>>>>>> There are various things that *could* go wrong, and usage of
>>>>>>>> containers (any of the technologies) actually introduce complexiti=
es in
>>>>>>>> kernel and user space alignment that normally we don't consider. F=
or that
>>>>>>>> reason, my first suspicion is that your IB fabric is not being pro=
perly
>>>>>>>> utilized by the MPI within the container. That could be due to any=
thing
>>>>>>>> from build errors within the container to IB library/kmod API misa=
lignment.
>>>>>>>>
>>>>>>>> Things I would look at and check:
>>>>>>>>
>>>>>>>> * Make sure the MPI inside the container is properly linking
>>>>>>>> against the IB libraries
>>>>>>>> * Make sure that the IB libraries inside the container are
>>>>>>>> compatible with the host kernel
>>>>>>>> * Make sure that the MPI configuration is configured to use MPI
>>>>>>>> * Make sure you have Singularity configured properly to share the
>>>>>>>> devices properly, tmp, and you are *NOT* using the IPC or PID name=
spaces
>>>>>>>>
>>>>>>>> Hope that helps.
>>>>>>>>
>>>>>>>> Greg
>>>>>>>>
>>>>>>>> On Wed, Feb 22, 2017 at 10:19 AM, Chih-Song Kuo wrote:
>>>>>>>>
>>>>>>>>> Hello,
>>>>>>>>>
>>>>>>>>> This is again Chih-Song from Fujitsu. I decided to make another
>>>>>>>>> post to share my experience of performance impact with two kernel
>>>>>>>>> benchmarks: High Performance Linpack (HPL) and the OSU MPI benchm=
ark suit.
>>>>>>>>>
>>>>>>>>> Overall, there was no noticeable performance difference for
>>>>>>>>> benchmarks running on a single node. But for benchmarks running a=
cross
>>>>>>>>> nodes, I did observe some difference, which was against to the cl=
aim of
>>>>>>>>> Singularity. Have anybody done any similar exercise? What are you=
r
>>>>>>>>> findings? Can you suspect whether I was doing anything wrong?
>>>>>>>>>
>>>>>>>>> Host configuration:
>>>>>>>>> 2x Intel E5-2680v2 (Ivybridge)
>>>>>>>>> 64GB memory
>>>>>>>>> Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR
>>>>>>>>> switch)
>>>>>>>>> RHEL 6.7
>>>>>>>>> OpenMPI development master branch (8.2.17)
>>>>>>>>> Intel MKL 2017.0 community edition
>>>>>>>>> gcc 4.4
>>>>>>>>>
>>>>>>>>> Container:
>>>>>>>>> Centos 6 and 7 both tested without noticeable performance
>>>>>>>>> difference
>>>>>>>>> OpenMPI development master branch (8.2.17)
>>>>>>>>> Intel MKL 2017.0 community edition
>>>>>>>>> gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)
>>>>>>>>>
>>>>>>>>> Benchmarks:
>>>>>>>>> 1. LINPACK 2.2
>>>>>>>>> 2. OSU 5.3.2
>>>>>>>>>
>>>>>>>>> <LINPACK>
>>>>>>>>> Single node. N=3D40000, P=3D5, Q=3D4
>>>>>>>>> Container: 368 GFlops
>>>>>>>>> Host: 368 GFLOPS
>>>>>>>>> #A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10
>>>>>>>>> x 8 * 2.8 =3D 448 GFlops
>>>>>>>>> Efficiency =3D 368 / 448 =3D 82%. Not bad (given the small N (mat=
rix
>>>>>>>>> size) and that gcc instead of icc was used, and the executable wa=
s
>>>>>>>>> dynamically linked -- by purpose)
>>>>>>>>>
>>>>>>>>> Dual-node, N=3D60000, P=3D8, Q=3D5
>>>>>>>>> Container: 702 GFLOPS
>>>>>>>>> Host:737 GFLOPS
>>>>>>>>> There is roughly 5% of performance degradation with the container=
.
>>>>>>>>>
>>>>>>>>> <OSU-P2P-Bandwidth>
>>>>>>>>> The container only saw 50-65% of the total bandwidth.
>>>>>>>>>
>>>>>>>>> Container:
>>>>>>>>> Msg size(bytes) BW (MB/s)
>>>>>>>>> 65536                2142.28
>>>>>>>>> 131072               2363.45
>>>>>>>>> 262144               1705.79
>>>>>>>>> 524288               1592.56
>>>>>>>>> 1048576              1721.88
>>>>>>>>> 2097152              1557.42
>>>>>>>>> 4194304              1655.90
>>>>>>>>>
>>>>>>>>> Host:
>>>>>>>>> Msg size(bytes) BW (MB/s)
>>>>>>>>> 65536                3722.32
>>>>>>>>> 131072               3751.33
>>>>>>>>> 262144               3771.13
>>>>>>>>> 524288               3774.33
>>>>>>>>> 1048576              3781.43
>>>>>>>>> 2097152              3775.00
>>>>>>>>> 4194304              3773.68
>>>>>>>>>
>>>>>>>>> <OSU-P2P-Latency>
>>>>>>>>> Here the container was significantly slower.
>>>>>>>>>
>>>>>>>>> Container:
>>>>>>>>> Msg size(bytes)  Latency (us)
>>>>>>>>> 0                      31.59
>>>>>>>>> 1                      31.86
>>>>>>>>> 2                      31.83
>>>>>>>>>
>>>>>>>>> Host:
>>>>>>>>> Msg size(bytes)  Latency (us)
>>>>>>>>> 0                       1.55
>>>>>>>>> 1                       1.63
>>>>>>>>> 2                       1.63
>>>>>>>>>
>>>>>>>>> Note 1: Run-to-run variation of performance was much smaller than
>>>>>>>>> the difference on the host and in the container.
>>>>>>>>> Note 2: When Singularity was used, I could not instruct mpirun to
>>>>>>>>> use the ofed by specifying "--mca btl openib,self,vader" in the m=
pirun
>>>>>>>>> parameter list. Doing so would give me an error message stating t=
hat the
>>>>>>>>> openib component is missing. However, from the bandwidth measured=
 above,
>>>>>>>>> the container did seem to be able to use InfiniBand, otherwise th=
e
>>>>>>>>> bandwidth would not be so high (the nodes only had InfiniBand and=
 1G
>>>>>>>>> Ethernet). Maybe container was using IPoIB? I did not check that =
yet.
>>>>>>>>>
>>>>>>>>> Reference: How the benchmarks were executed:
>>>>>>>>> mpirun -n 20 -hostfile hostfile singularity exec
>>>>>>>>> /home/chih/containers/container-centos6-demo.img xhpl
>>>>>>>>> mpirun -n 2 -hostfile hostfile singularity exec
>>>>>>>>> /home/chih/containers/container-centos6-demo.img osu_bw
>>>>>>>>> mpirun -n 2 -hostfile hostfile singularity exec
>>>>>>>>> /home/chih/containers/container-centos6-demo.img osu_latency
>>>>>>>>>
>>>>>>>>> Chih-Song
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> You received this message because you are subscribed to the Googl=
e
>>>>>>>>> Groups "singularity" group.
>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> Gregory M. Kurtzer
>>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>>> University of California Berkeley Research IT
>>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>>> er.com/gmkurtzer
>>>>>>>>
>>>>>>> --
>>>>>>> You received this message because you are subscribed to the Google
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Gregory M. Kurtzer
>>>>>> HPC Systems Architect and Technology Developer
>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>> University of California Berkeley Research IT
>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>> er.com/gmkurtzer
>>>>>>
>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Chih-Song Kuo =E9=83=AD=E7=9F=A5=E9=A0=8C
>>>>> Senior Sales Consultant - HPC Benchmark Specialist at Fujitsu
>>>>> M.Sc. RWTH with distinction in Software Systems Engineering with HPC
>>>>> focus
>>>>> B.Sc. NTHU in Computer Science, B.S.M. NTHU in Quantitative Finance
>>>>> Tel:  +49-177-88949-28; +49-241-88949-155; +886-2-26629518
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> Tyler Trafford
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--94eb2c05d26eb78e6b0549392f2c
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">What did you mean by hybrid mode? MPI + OpenMP or somethin=
g else?<br><div class=3D"gmail_extra"><br clear=3D"all"><div><div class=3D"=
gmail_signature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><span>=
<div><div dir=3D"ltr"><div>Chih-Song<br></div></div></div></span></div></di=
v></div>
<br><div class=3D"gmail_quote">On Thu, Feb 23, 2017 at 10:17 PM, Tyler Traf=
ford <span dir=3D"ltr">&lt;<a href=3D"mailto:ttra...@gmail.com" target=3D"_=
blank">ttra...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmai=
l_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left=
:1ex"><div dir=3D"ltr"><div style=3D"font-family:arial,helvetica,sans-serif=
">I also thought that when running in hybrid mode we wouldn&#39;t have to w=
orry about the interconnect inside the container....</div><div style=3D"fon=
t-family:arial,helvetica,sans-serif"><br></div><div style=3D"font-family:ar=
ial,helvetica,sans-serif">--=C2=A0</div><div style=3D"font-family:arial,hel=
vetica,sans-serif">Tyler Trafford</div></div><div class=3D"gmail_extra"><di=
v><div class=3D"h5"><br><div class=3D"gmail_quote">On Thu, Feb 23, 2017 at =
4:05 PM, Chih-Song Kuo <span dir=3D"ltr">&lt;<a href=3D"mailto:chihs...@gma=
il.com" target=3D"_blank">chihs...@gmail.com</a>&gt;</span> wrote:<br><bloc=
kquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #cc=
c solid;padding-left:1ex"><div dir=3D"ltr">OK...Now after installing Infini=
Band libraries things finally work decently. From this lesson, I would sugg=
est adding a remark about IB support to <a href=3D"http://singularity.lbl.g=
ov/docs-hpc" target=3D"_blank">http://singularity.lbl.gov/doc<wbr>s-hpc</a>=
, because otherwise new users can easily get confused about Singularity&#39=
;s support for InfiniBand. And HPC is obviously defined by such high-speed =
interconnects (IB, OPA, Cray, ...)<br><br>To me, this does seem to be a ser=
ious issue which would potentially decrease users&#39; feeling about the po=
rtability of Singularity containers. Think about a scientist developing cod=
es on a workstation without InfiniBand, and later making a test run on a sm=
all departmental cluster with InfiniBand, and eventually a production run o=
n some white elephant with Intel Omni-Path. Just missing one interconnect l=
ibrary in the container and not compiling the MPI against that library woul=
d immediately break portability.<br><br>Also I agree with the comment made =
in <a href=3D"https://groups.google.com/a/lbl.gov/forum/#!topic/singularity=
/fsCO1_StjjA" target=3D"_blank">https://groups.google.com/a/lb<wbr>l.gov/fo=
rum/#!topic/singularit<wbr>y/fsCO1_StjjA</a> that Singularity should consid=
er to include OFED drivers by default when creating a new container. Not su=
re if Greg you would change your mind here. The problem was really not abou=
t orted. Yes, Open MPI worked without the OFED driver, but only on Ethernet=
. Since Singularity is claimed to be the container for HPC, including a wel=
l defined set of interconnect drivers and libraries truly makes sense.<br><=
br>For those who try to make your containers work with InfiniBand, I can pr=
ovide a few general hints. These were not rigorously tested though and I do=
ubt I have the time to do so.<br>1. Issue ibv_devices in the container. You=
 must be able able to see the name of your InfiniBand adpter. If not, then =
possibly some IB libraries and drivers are missing<br>1.1. For Centos 7, th=
e following rpm packages seem to be the minimum requirement<br>libibverbs* =
<br>librdmacm*=C2=A0=C2=A0=C2=A0 <br>1.2. For Centos 6, the following rpm p=
ackages seem to be the minimum requirement<br>libibverbs* <br>librdmacm*=C2=
=A0=C2=A0=C2=A0 <br>libmlx*<br>1.3. Nevertheless, in the end I installed mo=
re packages<br>yum -y install libmthca <br>yum -y install libibcm*<br>yum -=
y install libibmad*<br>yum -y install libibumad*<br>1.4. Maybe the followin=
g will be useful, but I did not install them yet<br>yum -y install dapl*<br=
>yum -y install ibacm*<br>yum -y install ibutils<br><br>2. After ./configur=
e Open MPI, make sure you see the following.<br>OpenFabrics Verbs: yes<br>I=
f you have OmniPath, then it should be<br>Intel Omnipath (PSM2): yes<br><br=
>There are also other components like MXM=C2=A0 which can potentially be &q=
uot;yes&quot; if you installed Mellanox MXM.<br><br>3. When make Open MPI, =
use serial compilation, i.e. do not add the -j flag. It sometimes causes ex=
plainable behaviors.<br><br>4. After make install Open MPI, make sure that =
the OpenIB component is indeed installed.<br>ompi_info |grep openib <br>#Ex=
pect to see non-empty response<span><br>=C2=A0MCA btl: openib (MCA v2.1.0, =
API v3.0.0, Component v3.0.0)<br><br></span>5. Make sure that the MPI versi=
on is the same on the host and in the container<br><br>My updated test resu=
lt is provided below. If anybody would like to put these figures into any o=
ther documents, please inform me.<span><br><br>Host configuration:<br>2x In=
tel E5-2680v2 (Ivybridge)<br>64GB memory<br>Mellanox ConnectX-3 FDR adapter=
 (but connects to a Mellanox QDR switch)<br>RHEL 6.7<br></span>OpenMPI deve=
lopment master branch (dated 8.2.17)<span><br>Intel MKL 2017.0 community ed=
ition<br>gcc 4.4<br><br>Container:<br>Centos 6 and 7 both tested without no=
ticeable performance difference<br></span>OpenMPI development master branch=
 (dated 8.2.17)<span><br>Intel MKL 2017.0 community edition<br>gcc 4.4.7 (C=
entos-6), gcc 4.8.5 (Centos-7)<br><br>Benchmarks:<br>1. LINPACK 2.2<br>2. O=
SU 5.3.2<br><br>&lt;LINPACK&gt;<br>Single node. N=3D40000, P=3D5, Q=3D4<br>=
Container: 368 GFlops<br>Host: 368 GFLOPS<br>#A single node has 2x Intel E5=
-2680v2. So we are expecting 2 x 10 x 8 * 2.8 =3D 448 GFlops<br>Efficiency
 =3D 368 / 448 =3D 82%. Not bad (given the small N (matrix size) and that=
=20
gcc instead of icc was used, and the executable was dynamically linked=20
-- by purpose)<br><br>Dual-node, N=3D60000, P=3D8, Q=3D5<br></span>Containe=
r: 736 GFLOPS<br>Host:737 GFLOPS<br><br><br>&lt;OSU-P2P-Bandwidth&gt;<span>=
<br>Container: <br>Msg size(bytes) BW (MB/s)<br></span>65536=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 3738.89<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 3756.34<br>262144=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3767.35<br>524288=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 3780.46<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3781.94<br>2097152=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3775.07<br>4194304=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 3775.97<span><br><br>Host:<br>Msg size(bytes) BW (MB/s)<br>65536=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 3722.32<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3771.13<br>=
524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 3774.33<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3775.00<br>=
4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 3773.68<br><br>&lt;OSU-P2P-Latency&gt;<br></span><span>Contain=
er: <br>Msg size(bytes)=C2=A0 Latency (us)<br></span>0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.61<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.67<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 1.66<br>4=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1.66<br>8=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 1.70<span><br><br>Host:<br>Msg size(bytes)=C2=A0 Latency (us)<=
br></span>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.57<=
br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.63<br>2=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.63<br>4=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.63<br>8=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.65<br><br>How were the benc=
hmarks executed?<span><br>mpirun -n 20 -hostfile hostfile singularity exec =
/home/chih/containers/<div dir=3D"ltr">container-centos6-demo.img xhpl<br>m=
pirun -n 2 -hostfile hostfile singularity exec /home/chih/containers/contai=
ne<wbr>r-centos6-demo.img osu_bw<br>mpirun -n 2 -hostfile hostfile singular=
ity exec /home/chih/containers/containe<wbr>r-centos6-demo.img osu_latency<=
br></div><br></span>My conclusion: No difference for HPL (1 &amp; 2 nodes) =
and IB bandwidth. There is a consistent 2% overhead in IB latency though, b=
ut this should be affordable by most users.<div><div class=3D"m_60269707094=
99655369h5"><br><br>On Thursday, February 23, 2017 at 12:44:47 PM UTC+1, Ch=
ih-Song Kuo wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0;margi=
n-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">=
I am not really sure if I can share the containers publicly as it contains =
an Intel MKL installation which is bound to a personalized serial number (a=
lthough that MKL is from the community edition, so it is essentially free).=
<br><br>Still, I am wondering if installing OFED into the container is the =
right approach. There was a thread on a similar topic and Greg said no. I h=
ope I understood it correctly.<br><a href=3D"https://groups.google.com/a/lb=
l.gov/forum/#!topic/singularity/fsCO1_StjjA" rel=3D"nofollow" target=3D"_bl=
ank">https://groups.google.com/a/lb<wbr>l.gov/forum/#!topic/singularit<wbr>=
y/fsCO1_StjjA</a><br><br>After all, although requiring the user to install =
OFED  into the container might be technically feasible, it would be very aw=
kward for users who develop their applications on their own workstations wi=
thout InfiniBand, not to mention that I am even not sure if one can install=
 OFED into a machine without IB adapters. And what if the OFED library in t=
he container differ from that on the host? I don&#39;t think that will work=
, will it?<br><br>Sorry but somehow OpenMPI (or any MPI) is intent to reach=
 OFED libraries still remains a mystery for me. Please enlighten me.<br><br=
>On Thursday, February 23, 2017 at 9:51:34 AM UTC+1, R=C3=A9my Dernat wrote=
:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;bord=
er-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi,<div><br></div=
><div>You can try to copy your libraries=C2=A0directly=C2=A0from the host t=
o the container instead of a classical install ? I do not know what is the =
best way... But if you check woth solutions we will have an answer.</div><d=
iv>BTW, I am very interesting by your containers. Could you share it (throu=
gh singularity-hub ?) with your command line to run the benchmark in the &q=
uot;singularity&quot; file inside the container ? Indeed, I will do some pe=
rformance tests also...</div><div><br></div><div>Best regards,</div><div>R=
=C3=A9my</div></div><div><br><div class=3D"gmail_quote">2017-02-23 0:58 GMT=
+01:00 Chihsong <span dir=3D"ltr"></span>:<br><blockquote class=3D"gmail_qu=
ote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex=
">Hi Greg,<div><br></div><div>The problem is that I=C2=A0don&#39;t feel I=
=C2=A0have IB libraries inside my container. How can I=C2=A0check that? Or =
did you simply=C2=A0install the ofed into the container?=C2=A0</div><div><b=
r>Chiu-Song</div><div><div><div><br>On Thursday, February 23, 2017, Gregory=
 M. Kurtzer &lt;<a rel=3D"nofollow">gm...@lbl.gov</a>&gt; wrote:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc =
solid;padding-left:1ex"><div dir=3D"ltr">Hi Chih-Song,<div><br></div><div>H=
aha, every now and then I get lucky when a new mail comes in and is at the =
top of my mbox and I have a moment.</div><div><br></div><div>In summary, ye=
s. The MPI inside the container must also link against the IB libraries (al=
so within the container).</div><div><br></div><div>Hopefully that helps!</d=
iv></div><div><br><div class=3D"gmail_quote">On Wed, Feb 22, 2017 at 10:58 =
AM, Chih-Song Kuo=C2=A0 wrote:<br><blockquote class=3D"gmail_quote" style=
=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr">Hi Greg,<br><br>That reply was very prompt! Anyways, my answer fol=
lows.<span><br><br>&gt; * Make sure the MPI inside the container is properl=
y linking against the IB libraries<br></span>Did that mean that I need to i=
nstall IB libraries (like Mellanox OFED) into the container? I guess not?<b=
r>But apparently openib is missing in the container.<br>[me@cn03 ompi-rhel6=
-host]$ ompi_info | grep openib<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 MCA btl: openib (=
MCA v2.1.0, API v3.0.0, Component v3.0.0)<br><br>Singularity.container-cent=
os6-<wbr>demo.img&gt; ompi_info | grep openib<br>Singularity.container-cent=
os6-<wbr>demo.img&gt;<br>Singularity.container-centos6-<wbr>demo.img&gt; ls=
 /etc/infiniband/openib.conf<br>ls: cannot access /etc/infiniband/openib.co=
nf: No such file or directory<span><br><br>&gt; * Make sure that the MPI co=
nfiguration is configured to use MPI<br></span>I think you meant &quot;to u=
se IB&quot; instead. <br>But still, did you mean that OpenMPI should be &qu=
ot;configured&quot; &quot;--with-verbs&quot;? (Did you do so or you never h=
ad my problem?)<br>I did not use this flag when compiling Open MPI either o=
n the host or in the container.<span><br>&gt; * Make sure you have Singular=
ity configured properly to share the=20
devices properly, tmp, and you are *NOT* using the IPC or PID namespaces<br=
></span>Can you provide more hint on how that can be done?<br><br>Thank you=
 once again for your time.<br>Chih-Song<span><br><br>On Wednesday, February=
 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer wrote:</span><blockquote =
class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #=
ccc solid;padding-left:1ex"><span><div dir=3D"ltr">There are various things=
 that *could* go wrong, and usage of containers (any of the technologies) a=
ctually introduce complexities in kernel and user space alignment that norm=
ally we don&#39;t consider. For that reason, my first suspicion is that you=
r IB fabric is not being properly utilized by the MPI within the container.=
 That could be due to anything from build errors within the container to IB=
 library/kmod API misalignment.<div><br></div><div>Things I would look at a=
nd check:</div><div><br></div><div>* Make sure the MPI inside the container=
 is properly linking against the IB libraries</div><div>* Make sure that th=
e IB libraries inside the container are compatible with the host kernel</di=
v><div>* Make sure that the MPI configuration is configured to use MPI</div=
><div>* Make sure you have Singularity configured properly to share the dev=
ices properly, tmp, and you are *NOT* using the IPC or PID namespaces</div>=
<div><br></div><div>Hope that helps.</div><div><br></div><div>Greg</div></d=
iv></span><div><br><div class=3D"gmail_quote"><div><div>On Wed, Feb 22, 201=
7 at 10:19 AM, Chih-Song Kuo wrote:<br></div></div><blockquote class=3D"gma=
il_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-lef=
t:1ex"><div><div><div dir=3D"ltr">Hello,<br><br>This is again Chih-Song fro=
m Fujitsu. I decided to make another post to share my experience of perform=
ance impact with two kernel benchmarks: High Performance Linpack (HPL) and =
the OSU MPI benchmark suit.<br><br>Overall, there was no noticeable perform=
ance difference for benchmarks running on a single node. But for benchmarks=
 running across nodes, I did observe some difference, which was against to =
the claim of Singularity. Have anybody done any similar exercise? What are =
your findings? Can you suspect whether I was doing anything wrong?<br><br>H=
ost configuration:<br>2x Intel E5-2680v2 (Ivybridge)<br>64GB memory<br>Mell=
anox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)<br>RHEL=
 6.7<br>OpenMPI development master branch (8.2.17)<br>Intel MKL 2017.0 comm=
unity edition<br>gcc 4.4<br><br>Container:<br>Centos 6 and 7 both tested wi=
thout noticeable performance difference<br>OpenMPI development master branc=
h (8.2.17)<br>Intel MKL 2017.0 community edition<br>gcc 4.4.7 (Centos-6), g=
cc 4.8.5 (Centos-7)<br><br>Benchmarks:<br>1. LINPACK 2.2<br>2. OSU 5.3.2<br=
><br>&lt;LINPACK&gt;<br>Single node. N=3D40000, P=3D5, Q=3D4<br>Container: =
368 GFlops<br>Host: 368 GFLOPS<br>#A single node has 2x Intel E5-2680v2. So=
 we are expecting 2 x 10 x 8 * 2.8 =3D 448 GFlops<br>Efficiency =3D 368 / 4=
48 =3D 82%. Not bad (given the small N (matrix size) and that gcc instead o=
f icc was used, and the executable was dynamically linked -- by purpose)<br=
><br>Dual-node, N=3D60000, P=3D8, Q=3D5<br>Container: 702 GFLOPS<br>Host:73=
7 GFLOPS<br>There is roughly 5% of performance degradation with the contain=
er.<br><br>&lt;OSU-P2P-Bandwidth&gt;<br>The container only saw 50-65% of th=
e total bandwidth.<br><br>Container: <br>Msg size(bytes) BW (MB/s)<br>65536=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 2142.28<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2363.45<br>262144=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1705.=
79<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 1592.56<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1721.88<br>2097152=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1557.=
42<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1655.90<br><br>Host:<br>Msg size(bytes) BW (MB/s)<br>655=
36=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 3722.32<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
71.13<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 3774.33<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
75.00<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 3773.68<br><br>&lt;OSU-P2P-Latency&gt;<br>Here the co=
ntainer was significantly slower.<br><br>Container: <br>Msg size(bytes)=C2=
=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31=
.59<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.86<br>2=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.83<br><br>Host:<br>Msg =
size(bytes)=C2=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1.55<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 1.63<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 1.63<br><br>Note 1: Run-to-run variation of performance was much sma=
ller than the difference on the host and in the container.<br>Note 2: When =
Singularity was used, I could not instruct mpirun to use the ofed by specif=
ying &quot;--mca btl openib,self,vader&quot; in the mpirun parameter list. =
Doing so would give me an error message stating that the openib component i=
s missing. However, from the bandwidth measured above, the container did se=
em to be able to use InfiniBand, otherwise the bandwidth would not be so hi=
gh (the nodes only had InfiniBand and 1G Ethernet). Maybe container was usi=
ng IPoIB? I did not check that yet. <br><br>Reference: How the benchmarks w=
ere executed:<br>mpirun -n 20 -hostfile hostfile singularity exec /home/chi=
h/containers/containe<wbr>r-centos6-demo.img xhpl<br>mpirun -n 2 -hostfile =
hostfile singularity exec /home/chih/containers/containe<wbr>r-centos6-demo=
.img osu_bw<br>mpirun -n 2 -hostfile hostfile singularity exec /home/chih/c=
ontainers/containe<wbr>r-centos6-demo.img osu_latency<span><font color=3D"#=
888888"><br><br>Chih-Song<br><br></font></span></div></div></div><span><fon=
t color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><=
div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurt=
zer</div><div>HPC Systems Architect and Technology Developer</div><div>Lawr=
ence Berkeley National Laboratory HPCS<br>University of California Berkeley=
 Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singul=
arity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.=
lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http:/=
/warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<w=
br>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtz=
er" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a=
>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"ht=
tps://twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" ta=
rget=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></d=
iv></div></div></div></div></div></div></div>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.gov/=
</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewul=
f.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.gov=
/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=
=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
</blockquote></div><br><br></div></div><span><font color=3D"#888888">-- <br=
><div dir=3D"ltr"><span><div><div dir=3D"ltr"><div>Chih-Song Kuo =E9=83=AD=
=E7=9F=A5=E9=A0=8C<br>Senior Sales Consultant - HPC Benchmark Specialist at=
 Fujitsu<br>M.Sc. RWTH with distinction in Software Systems Engineering wit=
h HPC focus<br>B.Sc. NTHU in Computer Science, B.S.M. NTHU in Quantitative =
Finance<br>Tel: =C2=A0<a value=3D"+491778894928">+49-177-88949-28</a>; <a v=
alue=3D"+4924188949155">+49-241-88949-155</a>; <a value=3D"+886226629518">+=
886-2-26629518</a></div></div></div></span></div></font></span><div><div><b=
r>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br></div>
</blockquote></div></blockquote></div></div></div><div class=3D"m_602697070=
9499655369HOEnZb"><div class=3D"m_6026970709499655369h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div></div><=
/div><span class=3D"HOEnZb"><font color=3D"#888888">-- <br><div class=3D"m_=
6026970709499655369gmail_signature" data-smartmail=3D"gmail_signature">Tyle=
r Trafford</div>
</font></span></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div></div>

--94eb2c05d26eb78e6b0549392f2c--
