X-Received: by 2002:a63:a509:: with SMTP id n9-v6mr952963pgf.94.1526907515892;
        Mon, 21 May 2018 05:58:35 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a65:65c2:: with SMTP id y2-v6ls2998933pgv.28.gmail; Mon, 21
 May 2018 05:58:34 -0700 (PDT)
X-Received: by 2002:a63:7747:: with SMTP id s68-v6mr7443770pgc.338.1526907514253;
        Mon, 21 May 2018 05:58:34 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1526907514; cv=none;
        d=google.com; s=arc-20160816;
        b=aqpRizONheXAqAac8mFFa+80Cka113lJeOIzO98ZXtHMzE9TU6PTClvzYtMUn6KNGj
         jTNNL5chq5VbYpDvGZ9nyNFdsSTpsU51RzTpKY+02vsfO0aBl7JkhQ0QgCK3yTrfLF44
         uRAJ7dn/woO25aAWMwjmf6RIVrg3UvbdmwUvFjb1OsKqQR79yrZmrFzswYhpABwPnK75
         2lGh+37GyXTOEpg1+Jl7i86wLAKp4uymyqHMtaifBsKw4PGi4SDtLrOSaOF9ti2leM1m
         AhJfOd+zsC1HoHIOrkvlcqUq5Pz3WNtuTNVWq5hMRDO67yy0VJXqIoy1JLzhXFp0VpXn
         Lqkw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature:arc-authentication-results;
        bh=rB8nGUx3r/iX2ayeY9GZWk5SNqyUhVHCRMhOWdpGTjs=;
        b=RIMgh6lE3g2uVyCi7rkIOGgwS0hB4oRUQrFaiMxzAKyyZi8FCwxCTQgJ5ZpY/2yTTK
         USlDACmC7Qrp5lIIGOaus/cHrukA95+DDldTN4OokqawxFWHHiR57WeC6fUv0qjyOnJ5
         iBjQ8ga5bWCMUxAMI+OAPIzs86svjRQug2DqNnjb7qBV2481hhb5hgLUz47bJX2ZKy8X
         2zaILRjJ86Vzdof2scCU/NwZZuyeOB9ehkV7tehLCq72SM3SaOF5GO2vu7SHDqkZjba3
         6r7JS6h6YLNigXhWWAQrz0zR8GQnAJXkiP8LHeaHGnjwbo/wLH76WBF2Q1efmhVnN6S3
         wnCQ==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=P0CLfu2x;
       spf=pass (google.com: domain of georg...@gmail.com designates 209.85.216.176 as permitted sender) smtp.mailfrom=georg...@gmail.com
Return-Path: <georg...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id t71-v6si11244212pgd.7.2018.05.21.05.58.33
        for <singu...@lbl.gov>;
        Mon, 21 May 2018 05:58:34 -0700 (PDT)
Received-SPF: pass (google.com: domain of georg...@gmail.com designates 209.85.216.176 as permitted sender) client-ip=209.85.216.176;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=P0CLfu2x;
       spf=pass (google.com: domain of georg...@gmail.com designates 209.85.216.176 as permitted sender) smtp.mailfrom=georg...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HgAgD3wQJbhrDYVdFcGwEBAQEDAQEBC?=
 =?us-ascii?q?QEBAYMYgQx9KINvBoEdglCRCoF5gQ+TNhSBDgMYOAMjAQwJgUmCL0YCghghNhY?=
 =?us-ascii?q?BAgEBAQEBAQIBAQIQAQEBCAsLCCgjDII1BQIDAh4FBEsrATABAQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEUAgwhQwEBAQMBGgkdAQ0OBBoDAQsGAwILBgMBAQEBFQsBCQI?=
 =?us-ascii?q?CIQEBDgMBBQELCQgOBwQBGgIEgwECKIE+AQMNCAUKizuQAjyLBYF/BQEXgnAFg?=
 =?us-ascii?q?0kKGSYNVFeCBwIBBRKII4FUP4EPgl4ugk9CAQECgRkKCAESAT8NEoJBglQCh2C?=
 =?us-ascii?q?JNIZ1FywJhWqFboJ/jH2JX0qEB4I4MIEEIwFkLnEzGiNQMYISghQag06KbiMwA?=
 =?us-ascii?q?Q+NbUeBcAEB?=
X-IronPort-AV: E=Sophos;i="5.49,426,1520924400"; 
   d="scan'208,217";a="116281202"
Received: from mail-qt0-f176.google.com ([209.85.216.176])
  by fe3.lbl.gov with ESMTP; 21 May 2018 05:58:31 -0700
Received: by mail-qt0-f176.google.com with SMTP id q6-v6so18742765qtn.3
        for <singu...@lbl.gov>; Mon, 21 May 2018 05:58:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=rB8nGUx3r/iX2ayeY9GZWk5SNqyUhVHCRMhOWdpGTjs=;
        b=P0CLfu2xP14N0IwtG1/D/v+yLx2J1nzFuAnj8xHXwTgM/gQGiFB8yAvcRdKMCzalM8
         l6TcmK6vY9641p+j8nGorCoQlcXZw13ZkLsissjAwIOGg04Rs+6/MoTLTep6X3VXrS3R
         MD0f4e3RwSP3SX5BockKBGTM0217/rAb1QVbgGfiLtY5XDVNg9vFvASEWJU76gAQ4BVA
         zuC9n7HMD1P8d8A3KeEFNF62BYgZlLVEQ37pYh2nQniO+GuVRDz2f+MMRYbg6zjqXkMv
         usmjDc5wR4Q6B21Aon2VEPcOsjJwxDpwMGF3t0h3VJUMBuEYj9nOQ1CTJCPLSMms0juQ
         PxyQ==
X-Gm-Message-State: ALKqPwc4uvgaBPDStL0QazwXA/RWF741pYDtw+UIqLkRC54vGYIMcKVm
	0EgysrY+A28o6+L/pivnhrY17XmOa5PMbrqkjr8=
X-Received: by 2002:ac8:73d5:: with SMTP id v21-v6mr12470597qtp.329.1526907510726;
 Mon, 21 May 2018 05:58:30 -0700 (PDT)
MIME-Version: 1.0
References: <167671ff-657d-4560-a35f-87091223fe29@lbl.gov> <B58197C146EC324AA00A2A07DC082602C2CBA0C3@XMAIL-MBX-BT1.AD.UCSD.EDU>
 <CAB6eJZ+6PDjs3POSQPNoLdaMys5d7iYDYcUeQFpMyzJ6DEP31w@mail.gmail.com>
 <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com> <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com>
In-Reply-To: <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com>
From: George Zaki <georg...@gmail.com>
Date: Mon, 21 May 2018 08:58:15 -0400
Message-ID: <CAB6eJZL+F7DwMcebA6DC+QWHqqsnnqEfJ2uqR2D-6bas6DmnYw@mail.gmail.com>
Subject: Re: [Singularity] Running an mpi program with mvapitch
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000c8257b056cb6db11"

--000000000000c8257b056cb6db11
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Jason,

I was not able to run the program even without singularity when I use
mvapitch. I am in contact with our system admin

Here is the  output I got when I run mpiexec within singularity:

mpiexec -n 1 mpi-pi.o

[mpiexec@cn3137] HYDU_create_process (utils/launch/launch.c:75): execvp
error on file srun (No such file or directory)

Best regards,

George.

On Mon, May 21, 2018 at 3:04 AM victor sv <vict...@gmail.com> wrote:

> Hi George,
>
> not any experience with mvapitch. I think the compiler version has no
> effect here.
>
> Jason solution and is a good starting point to check if the container MPI
> works in a single node (not in several nodes).
>
> To run the hybrid MPI approach you should take into account that both
> version and vendor of MPI and PMI must match. Can you check if PMI
> libraries match?
>
> BR,
> V=C3=ADctor.
>
> 2018-05-18 20:14 GMT+02:00 Jason Stover <jason...@gmail.com>:
>
>> Hi George,
>>
>>   Can you run it from inside the container? For example:
>>
>>     singularity exec
>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1
>> mpi-pi.o
>>
>> -J
>>
>>
>> On Fri, May 18, 2018 at 12:56 PM, George Zaki <georg...@gmail.com>
>> wrote:
>> > Thanks Marty
>> >
>> > Below are the values I got, any obvious mismatch? I git this working
>> fine
>> > with OpenMPI.
>> >
>> > Here is also what I try to run:
>> >
>> > singularity exec
>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg
>> > mpicc mpi-pi.c -o  mpi-pi.o
>> >
>> > [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec
>> > /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o
>> >
>> > Then I kill after no response:
>> >
>> > ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested
>> >
>> > [mpiexec@cn2360] Press Ctrl-C again to force abort
>> >
>> > [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error
>> (Bad
>> > file descriptor)
>> >
>> > [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal
>> (pm/pmiserv/pmiserv_cb.c:169):
>> > unable to write data to proxy
>> >
>> > [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to
>> send
>> > signal downstream
>> >
>> > [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event
>> > (tools/demux/demux_poll.c:76): callback returned error status
>> >
>> > [mpiexec@cn2360] HYD_pmci_wait_for_completion
>> > (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event
>> >
>> > [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error
>> > waiting for completion
>> >
>> >
>> > Now about the versions:
>> >
>> > Singularity: Invoking an interactive shell within container...
>> >
>> >
>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> mpiexec --version
>> >
>> > HYDRA build details:
>> >
>> >     Version:                                 3.1.4
>> >
>> >     Release Date:                            Wed Sep  7 14:33:43 EDT
>> 2016
>> >
>> >     CC:                              gcc
>> >
>> >     CXX:                             g++
>> >
>> >     F77:                             gfortran
>> >
>> >     F90:                             gfortran
>> >
>> >     Configure options:                       '--disable-option-checkin=
g'
>> > '--prefix=3DNONE' '--cache-file=3D/dev/null' '--srcdir=3D.' 'CC=3Dgcc'=
 'CFLAGS=3D
>> > -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,-rpath,/=
lib
>> > -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad -libumad -libver=
bs
>> -ldl
>> > -lrt -lm -lpthread ' 'CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/include
>> > -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/src
>> > -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT
>> > -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include
>> -I/include
>> > -I/include'
>> >
>> >     Process Manager:                         pmi
>> >
>> >     Launchers available:                     ssh rsh fork slurm ll lsf
>> sge
>> > manual persist
>> >
>> >     Topology libraries available:            hwloc
>> >
>> >     Resource management kernels available:   user slurm ll lsf sge pbs
>> > cobalt
>> >
>> >     Checkpointing libraries available:
>> >
>> >     Demux engines available:                 poll select
>> >
>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> gcc --version
>> >
>> > gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>> >
>> > Copyright (C) 2015 Free Software Foundation, Inc.
>> >
>> > This is free software; see the source for copying conditions.  There i=
s
>> NO
>> >
>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>> PURPOSE.
>> >
>> >
>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> exit
>> >
>> > exit
>> >
>> >
>> > [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0
>> >
>> > [+] Loading mvapich2 2.2 for GCC 5.3.0
>> >
>> > [zakigf@cn2360 ~]$ gcc --version
>> >
>> > gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
>> >
>> > Copyright (C) 2010 Free Software Foundation, Inc.
>> >
>> > This is free software; see the source for copying conditions.  There i=
s
>> NO
>> >
>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>> PURPOSE.
>> >
>> >
>> > [zakigf@cn2360 ~]$ mpiexec --version
>> >
>> > HYDRA build details:
>> >
>> >     Version:                                 3.1.4
>> >
>> >     Release Date:                            Wed Sep  7 14:33:43 EDT
>> 2016
>> >
>> >     CC:                              /usr/local/GCC/5.3.0/bin/gcc
>> >
>> >     CXX:                             /usr/local/GCC/5.3.0/bin/g++
>> >
>> >     F77:                             /usr/local/GCC/5.3.0/bin/gfortran
>> >
>> >     F90:                             /usr/local/GCC/5.3.0/bin/gfortran
>> >
>> >     Configure options:                       '--disable-option-checkin=
g'
>> > '--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0'
>> > '--with-slurm-lib=3D/usr/local/slurm/lib'
>> > '--with-slurm-include=3D/usr/local/slurm/include' '--enable-debug=3Dno=
ne'
>> > '--enable-timing=3Druntime' 'CC=3D/usr/local/GCC/5.3.0/bin/gcc'
>> > 'CXX=3D/usr/local/GCC/5.3.0/bin/g++'
>> 'FC=3D/usr/local/GCC/5.3.0/bin/gfortran'
>> > 'F77=3D/usr/local/GCC/5.3.0/bin/gfortran' '--cache-file=3D/dev/null'
>> > '--srcdir=3D.' 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib =
-L/lib
>> > -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
>> 'LIBS=3D-libmad
>> > -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src
>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT
>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include -I/includ=
e
>> > -I/include -I/include -I/include'
>> >
>> >     Process Manager:                         pmi
>> >
>> >     Launchers available:                     ssh rsh fork slurm ll lsf
>> sge
>> > manual persist
>> >
>> >     Topology libraries available:            hwloc
>> >
>> >     Resource management kernels available:   user slurm ll lsf sge pbs
>> > cobalt
>> >
>> >     Checkpointing libraries available:
>> >
>> >     Demux engines available:                 poll select
>> >
>> >
>> > On Fri, May 18, 2018 at 1:31 PM Kandes, Martin <mka...@sdsc.edu>
>> wrote:
>> >>
>> >> Hi George,
>> >>
>> >> I run with different gcc compiler versions inside and outside my MPI
>> >> containers. So I would be surprised if that is the issue here. I'm no=
t
>> sure
>> >> I have a good recommendation of where to start debugging your problem=
.
>> But I
>> >> might start by double checking the MPI versions match inside and
>> outside the
>> >> container. e.g. see [1].
>> >>
>> >> Marty
>> >>
>> >> [1]
>> >>
>> >> [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1
>> >> --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/bas=
h
>> >> srun: job 16364303 queued and waiting for resources
>> >> srun: job 16364303 has been allocated resources
>> >> [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/
>> >> [mkandes@comet-14-06 16364303]$ cp
>> >>
>> /oasis/scratch/comet/mkandes/temp_project/singularity/images/ubuntu-mvap=
ich2.img
>> >> ./
>> >> [mkandes@comet-14-06 16364303]$ module purge
>> >> [mkandes@comet-14-06 16364303]$ module load gnu
>> >> [mkandes@comet-14-06 16364303]$ module load mvapich2_ib
>> >> [mkandes@comet-14-06 16364303]$ module list
>> >> Currently Loaded Modulefiles:
>> >>   1) gnu/4.9.2         2) mvapich2_ib/2.1
>> >> [mkandes@comet-14-06 16364303]$ module load singularity
>> >> [mkandes@comet-14-06 16364303]$ gcc --version
>> >> gcc (GCC) 4.9.2
>> >> Copyright (C) 2014 Free Software Foundation, Inc.
>> >> This is free software; see the source for copying conditions.  There
>> is NO
>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>> >> PURPOSE.
>> >>
>> >> [mkandes@comet-14-06 16364303]$ mpirun --version
>> >> HYDRA build details:
>> >>     Version:                                 3.1.4
>> >>     Release Date:                            Thu Apr  2 17:15:15 EDT
>> 2015
>> >>     CC:                              gcc  -fPIC -O3
>> >>     CXX:                             g++  -fPIC -O3
>> >>     F77:                             gfortran -fPIC -O3
>> >>     F90:                             gfortran -fPIC -O3
>> >>     Configure options:
>>  '--disable-option-checking'
>> >> '--prefix=3D/opt/mvapich2/gnu/ib' '--enable-shared'
>> '--enable-sharedlibs=3Dgcc'
>> >> '--with-hwloc' '--enable-f77' '--enable-fc' '--enable-hybrid'
>> >> '--with-ib-include=3D/usr/include/infiniband'
>> '--with-ib-libpath=3D/usr/lib64'
>> >> '--enable-fast=3DO3'
>> >>
>> '--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_=
ib-2.1/../..//cache/build-limic'
>> >> '--with-slurm=3D/usr/lib64/slurm' '--with-file-system=3Dlustre' 'CC=
=3Dgcc'
>> >> 'CFLAGS=3D-fPIC -O3 -O3' 'CXX=3Dg++' 'CXXFLAGS=3D-fPIC -O3 -O3' 'FC=
=3Dgfortran'
>> >> 'FCFLAGS=3D-fPIC -O3 -O3' 'F77=3Dgfortran' 'FFLAGS=3D-L/usr/lib64 -L/=
lib
>> -L/lib
>> >> -fPIC -O3 -O3' '--cache-file=3D/dev/null' '--srcdir=3D.'
>> 'LDFLAGS=3D-L/usr/lib64
>> >> -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib
>> -L/usr/lib64
>> >>
>> -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/../..//c=
ache/build-limic/lib
>> >> -L/lib -L/lib' 'LIBS=3D-libmad -lrdmacm -libumad -libverbs -ldl -lrt
>> -llimic2
>> >> -lm -lpthread ' 'CPPFLAGS=3D-I/usr/include/infiniband
>> >>
>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapich2=
-2.1/src/mpl/include
>> >>
>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapich2=
-2.1/src/mpl/include
>> >>
>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapich2=
-2.1/src/openpa/src
>> >>
>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapich2=
-2.1/src/openpa/src
>> >> -D_REENTRANT
>> >>
>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapich2=
-2.1/src/mpi/romio/include
>> >> -I/include -I/include -I/usr/include/infiniband
>> >>
>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/../..//c=
ache/build-limic/include
>> >> -I/include -I/include'
>> >>     Process Manager:                         pmi
>> >>     Launchers available:                     ssh rsh fork slurm ll ls=
f
>> sge
>> >> manual persist
>> >>     Topology libraries available:            hwloc
>> >>     Resource management kernels available:   user slurm ll lsf sge pb=
s
>> >> cobalt
>> >>     Checkpointing libraries available:
>> >>     Demux engines available:                 poll select
>> >> [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.img
>> >> Singularity: Invoking an interactive shell within container...
>> >>
>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> gcc
>> --version
>> >> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>> >> Copyright (C) 2015 Free Software Foundation, Inc.
>> >> This is free software; see the source for copying conditions.  There
>> is NO
>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>> >> PURPOSE.
>> >>
>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> mpirun
>> >> --version
>> >> HYDRA build details:
>> >>     Version:                                 3.1.4
>> >>     Release Date:                            Thu Apr  2 17:15:15 EDT
>> 2015
>> >>     CC:                              gcc
>> >>     CXX:                             g++
>> >>     F77:                             gfortran
>> >>     F90:                             gfortran
>> >>     Configure options:
>>  '--disable-option-checking'
>> >> '--prefix=3D/opt/mvapich2' '--cache-file=3D/dev/null' '--srcdir=3D.' =
'CC=3Dgcc'
>> >> 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib
>> >> -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad
>> -lrdmacm
>> >> -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>> >> -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/inclu=
de
>> >> -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/src
>> >> -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include
>> -I/include
>> >> -I/include -I/include'
>> >>     Process Manager:                         pmi
>> >>     Launchers available:                     ssh rsh fork slurm ll ls=
f
>> sge
>> >> manual persist
>> >>     Topology libraries available:            hwloc
>> >>     Resource management kernels available:   user slurm ll lsf sge pb=
s
>> >> cobalt
>> >>     Checkpointing libraries available:
>> >>     Demux engines available:                 poll select
>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303>
>> >>
>> >>
>> >> ________________________________
>> >> From: George Zaki [georg...@gmail.com]
>> >> Sent: Friday, May 18, 2018 6:48 AM
>> >> To: singularity
>> >> Subject: [Singularity] Running an mpi program with mvapitch
>> >>
>> >> Hi singularity team,
>> >>
>> >>
>> >> I would like to run an MPI program in a singularity container. The
>> program
>> >> is compiled using mvapicth2.2 using a gcc version 5.4.
>> >>
>> >>
>> >> I can see that my cluster has a compiled version of mvapitch2.2 with
>> gcc
>> >> 5.3
>> >>
>> >>
>> >> When I run:
>> >>
>> >>
>> >> mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o
>> >>
>> >> the call does not return.
>> >>
>> >>
>> >>
>> >> Does the gcc version has to be exactly the same? I tried the switch t=
he
>> >> compiler in this image:
>> >>
>> >>
>> >> BootStrap: docker
>> >> From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04
>> >>
>> >>
>> >> However when gcc 5.3 is used the mvapitch does not build correctly.
>> >>
>> >>
>> >> If that's the problem, Is there a preferred method of switching gcc
>> >> version in this container singularity container?
>> >>
>> >>
>> >> Thanks,
>> >> George
>> >>
>> >> --
>> >> You received this message because you are subscribed to the Google
>> Groups
>> >> "singularity" group.
>> >> To unsubscribe from this group and stop receiving emails from it, sen=
d
>> an
>> >> email to singu...@lbl.gov.
>> >>
>> >> --
>> >> You received this message because you are subscribed to a topic in th=
e
>> >> Google Groups "singularity" group.
>> >> To unsubscribe from this topic, visit
>> >>
>> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsu=
bscribe
>> .
>> >> To unsubscribe from this group and all its topics, send an email to
>> >> singu...@lbl.gov.
>> >
>> > --
>> > You received this message because you are subscribed to the Google
>> Groups
>> > "singularity" group.
>> > To unsubscribe from this group and stop receiving emails from it, send
>> an
>> > email to singu...@lbl.gov.
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to a topic in the
> Google Groups "singularity" group.
> To unsubscribe from this topic, visit
> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsub=
scribe
> .
> To unsubscribe from this group and all its topics, send an email to
> singu...@lbl.gov.
>

--000000000000c8257b056cb6db11
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Jason,=C2=A0<div><br></div><div>I was not able to run t=
he program even without singularity when I use mvapitch. I am in contact wi=
th our system admin</div><div><br></div><div>Here is the=C2=A0 output I got=
 when I run mpiexec within singularity:=C2=A0</div><div><br></div><div>mpie=
xec -n 1 mpi-pi.o<span class=3D"inbox-inbox-Apple-converted-space">=C2=A0</=
span><br></div><div>
<p class=3D"inbox-inbox-p1"><span class=3D"inbox-inbox-s1">[mpiexec@cn3137]=
 HYDU_create_process (utils/launch/launch.c:75): execvp error on file srun =
(No such file or directory)</span></p><p class=3D"inbox-inbox-p1">Best rega=
rds,</p><p class=3D"inbox-inbox-p1">George.</p></div></div><br><div class=
=3D"gmail_quote"><div dir=3D"ltr">On Mon, May 21, 2018 at 3:04 AM victor sv=
 &lt;<a href=3D"mailto:vict...@gmail.com">vict...@gmail.com</a>&gt; wrote:<=
br></div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><div><div><di=
v>Hi George,<br><br></div>not any experience with mvapitch. I think the com=
piler version has no effect here. <br><br>Jason solution and is a good star=
ting point to check if the container MPI works in a single node (not in sev=
eral nodes).<br><br></div>To run the hybrid MPI approach you should take in=
to account that both version and vendor of MPI and PMI must match. Can you =
check if PMI libraries match?<br><br></div>BR,<br></div>V=C3=ADctor.<br></d=
iv><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">2018-05-18 20:=
14 GMT+02:00 Jason Stover <span dir=3D"ltr">&lt;<a href=3D"mailto:jason...@=
gmail.com" target=3D"_blank">jason...@gmail.com</a>&gt;</span>:<br><blockqu=
ote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc s=
olid;padding-left:1ex">Hi George,<br>
<br>
=C2=A0 Can you run it from inside the container? For example:<br>
<br>
=C2=A0 =C2=A0 singularity exec<br>
/data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1<br>
mpi-pi.o<br>
<span class=3D"m_-6591493537311087044HOEnZb"><font color=3D"#888888"><br>
-J<br>
</font></span><div class=3D"m_-6591493537311087044HOEnZb"><div class=3D"m_-=
6591493537311087044h5"><br>
<br>
On Fri, May 18, 2018 at 12:56 PM, George Zaki &lt;<a href=3D"mailto:georg..=
.@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt; wrote:<br>
&gt; Thanks Marty<br>
&gt;<br>
&gt; Below are the values I got, any obvious mismatch? I git this working f=
ine<br>
&gt; with OpenMPI.<br>
&gt;<br>
&gt; Here is also what I try to run:<br>
&gt;<br>
&gt; singularity exec /data/zakigf/candle/swift-hypervisor-horovod-mvapich.=
simg<br>
&gt; mpicc mpi-pi.c -o=C2=A0 mpi-pi.o<br>
&gt;<br>
&gt; [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec<br>
&gt; /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o<br>
&gt;<br>
&gt; Then I kill after no response:<br>
&gt;<br>
&gt; ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested<br>
&gt;<br>
&gt; [mpiexec@cn2360] Press Ctrl-C again to force abort<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error =
(Bad<br>
&gt; file descriptor)<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal (pm/pmiserv/pmiserv_cb.c=
:169):<br>
&gt; unable to write data to proxy<br>
&gt;<br>
&gt; [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to s=
end<br>
&gt; signal downstream<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event<br>
&gt; (tools/demux/demux_poll.c:76): callback returned error status<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmci_wait_for_completion<br>
&gt; (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event<br>
&gt;<br>
&gt; [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error<=
br>
&gt; waiting for completion<br>
&gt;<br>
&gt;<br>
&gt; Now about the versions:<br>
&gt;<br>
&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; mpiexec --vers=
ion<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3DNONE&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--sr=
cdir=3D.&#39; &#39;CC=3Dgcc&#39; &#39;CFLAGS=3D<br>
&gt; -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,=
-rpath,/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -libumad=
 -libverbs -ldl<br>
&gt; -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/=
include<br>
&gt; -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/src=
<br>
&gt; -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT<br>
&gt; -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include -I/inc=
lude<br>
&gt; -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; gcc --version<=
br>
&gt;<br>
&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;<br>
&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; exit<br>
&gt;<br>
&gt; exit<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0<br>
&gt;<br>
&gt; [+] Loading mvapich2 2.2 for GCC 5.3.0<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ gcc --version<br>
&gt;<br>
&gt; gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)<br>
&gt;<br>
&gt; Copyright (C) 2010 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ mpiexec --version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 /usr/local/GCC/5.3=
.0/bin/gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0&#39;<br>
&gt; &#39;--with-slurm-lib=3D/usr/local/slurm/lib&#39;<br>
&gt; &#39;--with-slurm-include=3D/usr/local/slurm/include&#39; &#39;--enabl=
e-debug=3Dnone&#39;<br>
&gt; &#39;--enable-timing=3Druntime&#39; &#39;CC=3D/usr/local/GCC/5.3.0/bin=
/gcc&#39;<br>
&gt; &#39;CXX=3D/usr/local/GCC/5.3.0/bin/g++&#39; &#39;FC=3D/usr/local/GCC/=
5.3.0/bin/gfortran&#39;<br>
&gt; &#39;F77=3D/usr/local/GCC/5.3.0/bin/gfortran&#39; &#39;--cache-file=3D=
/dev/null&#39;<br>
&gt; &#39;--srcdir=3D.&#39; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#=
39;LDFLAGS=3D-L/lib -L/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;=
LIBS=3D-libmad<br>
&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include -I/includ=
e<br>
&gt; -I/include -I/include -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt;<br>
&gt; On Fri, May 18, 2018 at 1:31 PM Kandes, Martin &lt;<a href=3D"mailto:m=
ka...@sdsc.edu" target=3D"_blank">mka...@sdsc.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi George,<br>
&gt;&gt;<br>
&gt;&gt; I run with different gcc compiler versions inside and outside my M=
PI<br>
&gt;&gt; containers. So I would be surprised if that is the issue here. I&#=
39;m not sure<br>
&gt;&gt; I have a good recommendation of where to start debugging your prob=
lem. But I<br>
&gt;&gt; might start by double checking the MPI versions match inside and o=
utside the<br>
&gt;&gt; container. e.g. see [1].<br>
&gt;&gt;<br>
&gt;&gt; Marty<br>
&gt;&gt;<br>
&gt;&gt; [1]<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1<=
br>
&gt;&gt; --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash<br>
&gt;&gt; srun: job 16364303 queued and waiting for resources<br>
&gt;&gt; srun: job 16364303 has been allocated resources<br>
&gt;&gt; [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ cp<br>
&gt;&gt; /oasis/scratch/comet/mkandes/temp_project/singularity/images/ubunt=
u-mvapich2.img<br>
&gt;&gt; ./<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module purge<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load gnu<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load mvapich2_ib<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module list<br>
&gt;&gt; Currently Loaded Modulefiles:<br>
&gt;&gt;=C2=A0 =C2=A01) gnu/4.9.2=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A02) mvapi=
ch2_ib/2.1<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load singularity<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ gcc --version<br>
&gt;&gt; gcc (GCC) 4.9.2<br>
&gt;&gt; Copyright (C) 2014 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ mpirun --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc=C2=A0 -f=
PIC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++=C2=A0 -fP=
IC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2/gnu/ib&#39; &#39;--enable-shared&#39=
; &#39;--enable-sharedlibs=3Dgcc&#39;<br>
&gt;&gt; &#39;--with-hwloc&#39; &#39;--enable-f77&#39; &#39;--enable-fc&#39=
; &#39;--enable-hybrid&#39;<br>
&gt;&gt; &#39;--with-ib-include=3D/usr/include/infiniband&#39; &#39;--with-=
ib-libpath=3D/usr/lib64&#39;<br>
&gt;&gt; &#39;--enable-fast=3DO3&#39;<br>
&gt;&gt; &#39;--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/sdsc-mva=
pich2_gnu_ib-2.1/../..//cache/build-limic&#39;<br>
&gt;&gt; &#39;--with-slurm=3D/usr/lib64/slurm&#39; &#39;--with-file-system=
=3Dlustre&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D-fPIC -O3 -O3&#39; &#39;CXX=3Dg++&#39; &#39;CXXFLAGS=
=3D-fPIC -O3 -O3&#39; &#39;FC=3Dgfortran&#39;<br>
&gt;&gt; &#39;FCFLAGS=3D-fPIC -O3 -O3&#39; &#39;F77=3Dgfortran&#39; &#39;FF=
LAGS=3D-L/usr/lib64 -L/lib -L/lib<br>
&gt;&gt; -fPIC -O3 -O3&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--srcdi=
r=3D.&#39; &#39;LDFLAGS=3D-L/usr/lib64<br>
&gt;&gt; -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr=
/lib64<br>
&gt;&gt; -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/..=
/..//cache/build-limic/lib<br>
&gt;&gt; -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -lrdmacm -libumad -libverbs=
 -ldl -lrt -llimic2<br>
&gt;&gt; -lm -lpthread &#39; &#39;CPPFLAGS=3D-I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpl/include<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpl/include<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/openpa/src<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/openpa/src<br>
&gt;&gt; -D_REENTRANT<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpi/romio/include<br>
&gt;&gt; -I/include -I/include -I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/..=
/..//cache/build-limic/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.=
img<br>
&gt;&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt; gcc =
--version<br>
&gt;&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt; mpir=
un<br>
&gt;&gt; --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2&#39; &#39;--cache-file=3D/dev/null&#=
39; &#39;--srcdir=3D.&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib=
 -L/lib -L/lib<br>
&gt;&gt; -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIB=
S=3D-libmad -lrdmacm<br>
&gt;&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<=
br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/in=
clude<br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/=
src<br>
&gt;&gt; -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include =
-I/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ________________________________<br>
&gt;&gt; From: George Zaki [<a href=3D"mailto:georg...@gmail.com" target=3D=
"_blank">georg...@gmail.com</a>]<br>
&gt;&gt; Sent: Friday, May 18, 2018 6:48 AM<br>
&gt;&gt; To: singularity<br>
&gt;&gt; Subject: [Singularity] Running an mpi program with mvapitch<br>
&gt;&gt;<br>
&gt;&gt; Hi singularity team,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I would like to run an MPI program in a singularity container. The=
 program<br>
&gt;&gt; is compiled using mvapicth2.2 using a gcc version 5.4.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I can see that my cluster has a compiled version of mvapitch2.2 wi=
th gcc<br>
&gt;&gt; 5.3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; When I run:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o<br>
&gt;&gt;<br>
&gt;&gt; the call does not return.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Does the gcc version has to be exactly the same? I tried the switc=
h the<br>
&gt;&gt; compiler in this image:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; BootStrap: docker<br>
&gt;&gt; From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However when gcc 5.3 is used the mvapitch does not build correctly=
.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; If that&#39;s the problem, Is there a preferred method of switchin=
g gcc<br>
&gt;&gt; version in this container singularity container?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; George<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singu...@lbl.gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to a topic in=
 the<br>
&gt;&gt; Google Groups &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this topic, visit<br>
&gt;&gt; <a href=3D"https://groups.google.com/a/lbl.gov/d/topic/singularity=
/A6I5mZxnmFU/unsubscribe" rel=3D"noreferrer" target=3D"_blank">https://grou=
ps.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe</a>.<br=
>
&gt;&gt; To unsubscribe from this group and all its topics, send an email t=
o<br>
&gt;&gt; <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu=
...@lbl.gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singu...@lbl.gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu.=
..@lbl.gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscri=
be</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div>

--000000000000c8257b056cb6db11--
