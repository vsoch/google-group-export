X-Received: by 10.176.2.1 with SMTP id 1mr9329980uas.3.1470490324342;
        Sat, 06 Aug 2016 06:32:04 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.189.196 with SMTP id x187ls2629884ite.14.gmail; Sat, 06 Aug
 2016 06:32:03 -0700 (PDT)
X-Received: by 10.98.30.133 with SMTP id e127mr145738397pfe.104.1470490323337;
        Sat, 06 Aug 2016 06:32:03 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id zz7si14887067pac.246.2016.08.06.06.32.03
        for <singu...@lbl.gov>;
        Sat, 06 Aug 2016 06:32:03 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) client-ip=209.85.215.70;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.3
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EoAQC55aVXe0bXVdFdhBt8B4M4pCuMHYUHgT1AIIJGgzcCgTEHOBQBAQEBAQEBAw8BAQkNEhkvglM5CjIBAQEBAQEBAQEBAQEBAQEaAj4SGwEBBAESESswCwsLDSoCAiIPAwEFAQsRBggHBAEHFQQBiAcIBaJqgTI+MYs7kBkBCgEBAQEiEIpnhBIKBwGDHYJaBYkFhRN1hGiFRAGGHII4hjWBa06EDYh9jDSCOBMegQ8PD4RQHDIHhhcPF4EfAQEB
X-IPAS-Result: A2EoAQC55aVXe0bXVdFdhBt8B4M4pCuMHYUHgT1AIIJGgzcCgTEHOBQBAQEBAQEBAw8BAQkNEhkvglM5CjIBAQEBAQEBAQEBAQEBAQEaAj4SGwEBBAESESswCwsLDSoCAiIPAwEFAQsRBggHBAEHFQQBiAcIBaJqgTI+MYs7kBkBCgEBAQEiEIpnhBIKBwGDHYJaBYkFhRN1hGiFRAGGHII4hjWBa06EDYh9jDSCOBMegQ8PD4RQHDIHhhcPF4EfAQEB
X-IronPort-AV: E=Sophos;i="5.28,478,1464678000"; 
   d="scan'208,217";a="32492980"
Received: from mail-lf0-f70.google.com ([209.85.215.70])
  by fe3.lbl.gov with ESMTP; 06 Aug 2016 06:32:01 -0700
Received: by mail-lf0-f70.google.com with SMTP id e7so168147914lfe.0
        for <singu...@lbl.gov>; Sat, 06 Aug 2016 06:32:01 -0700 (PDT)
X-Gm-Message-State: AEkooutTrW93ys8ngjTPlGDSM5WAwp4MhXfc61WcwzvXY8h6l87kTVt4YpsualQg57i+3LwHfqdy6gLC24msaEhMdsNJkCkJ8OaCaeJSVcYucnZA9X+8Iqmp/kQ58saB3IT/YtpmC6NP8dxyKKEvhn6FRqU=
X-Received: by 10.25.156.77 with SMTP id f74mr29037694lfe.51.1470490320622;
        Sat, 06 Aug 2016 06:32:00 -0700 (PDT)
X-Received: by 10.25.156.77 with SMTP id f74mr29037688lfe.51.1470490320199;
 Sat, 06 Aug 2016 06:32:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.141 with HTTP; Sat, 6 Aug 2016 06:31:59 -0700 (PDT)
In-Reply-To: <94debf38-e0a1-40a8-92b3-26000a8ea6d6@lbl.gov>
References: <5397fc9b-cc20-4813-8eaf-c806f8e27771@lbl.gov> <CAN7etTxYNne=gysZyGC535ygutWn_WBX_xqHf5RVE9kYN=mSNw@mail.gmail.com>
 <54ad52db-a96b-42af-abc1-380b704edfca@lbl.gov> <CAN7etTyOparWNu7=2vLk4=Adzyt5=3DkU4aDO2HgtEvML_j22g@mail.gmail.com>
 <94debf38-e0a1-40a8-92b3-26000a8ea6d6@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Sat, 6 Aug 2016 06:31:59 -0700
Message-ID: <CAN7etTzgoM99cU9xnS9GVaTCwYiwmyfxavnYHvc8YAUPrK4SOw@mail.gmail.com>
Subject: Re: [Singularity] Archiving research experiments using singularity
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a114114a02e7f1a05396735fb

--001a114114a02e7f1a05396735fb
Content-Type: text/plain; charset=UTF-8

On Sat, Aug 6, 2016 at 2:25 AM, 'Stefan Kombrink' via singularity <
singu...@lbl.gov> wrote:

> Hi Gregory,
>
>  sorry that I still won't let go off this topic :) ...to me this is the
> most exciting thing about singularity.
>

No worries!


>
> But what are the advantages of that hybrid MPI usage in your opinion?
> To me it is not fully clear...
>
> As I get it I can run MPI inside the container fine with just any version
> of MPI:
> singularity mpirun <OPTS> myMPIapp
>

You are right, this will always work limited to a single node.


> Is it like this is just going to work with OpenMPI 2.1+?
>
> mpirun <OPTS> singularity exec myMPIapp
>

Almost...  You missed the image name:

mpirun <OPTS> singularity exec container.img myMPIapp

This also makes (a somewhat obvious assumption but I'll mention it) that
myMPIapp exists within the container image's PATH.


>
> Actually we are interested in that case since I can't see how multi-node
> usage and torque/moab integration can be achieved otherwise...
> Or can this be done with arbitrary MPI versions even?
>

While I have not personally tested it, I have been told it works with Intel
MPI and MPICH... If you find anything definitive, please let us know!


> thanks for clarifying
>

My pleasure, I hope that helps and don't hesitate to ask if you have more
questions.

Greg




> Steven
>
> Am Freitag, 5. August 2016 21:55:56 UTC+2 schrieb Gregory M. Kurtzer:
>>
>> Hi Steven,
>>
>> Thank you for pointing the confusing wording out! I will fix that.
>>
>> It is intended to say that to use Open MPI in a hybrid manner (like this)
>> you must be using at least 2.1 (or maybe 2.0.1) as previous versions of
>> OMPI do not work.
>>
>> Hope that helps and sorry for the confusion!
>>
>> Greg
>>
>>
>>
>> On Fri, Aug 5, 2016 at 8:55 AM, 'Stefan Kombrink' via singularity <
>> si...@lbl.gov> wrote:
>>
>>> Hi Gregory,
>>>
>>>  thanks for your responses!
>>>
>>> Am Freitag, 5. August 2016 16:27:01 UTC+2 schrieb Gregory M. Kurtzer:
>>>>
>>>>
>>>> -Is trunk OpenMPI really a requirement or can I use older versions,
>>>>> too? What about IMPI?
>>>>>
>>>>
>>>> Open MPI is not a requirement but it has been tuned for Singularity and
>>>> well tested. I can not speak definitively for IMPI, but people have told me
>>>> that it is working fine out of the box!
>>>>
>>>>
>>>
>>> I was mainly asking because of this statement:
>>>
>>> Notes:
>>>
>>> Supported Open MPI Version(s): To achieve proper container'ized Open
>>> MPI support, you must use Open MPI version 2.1 which at the time of this
>>> writing has not been released yet. The above example builds from the
>>> current master development branch of Open MPI.
>>>
>>> given at http://singularity.lbl.gov/#hpc
>>>
>>> We run a software stack with >10 different versions of MPI and thus it
>>> is important to us that containerized MPI apps are properly managed by
>>> Torque and can make use of the IB transport which is something docker was
>>> giving troubles when running unprivileged. Also, to me it is not fully
>>> clear whether MPI needs to be installed both in the container and on the
>>> host.
>>>
>>> I will soon commit further experiments with MPI, thanks and bye
>>> Steven
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a114114a02e7f1a05396735fb
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Sat, Aug 6, 2016 at 2:25 AM, &#39;Stefan Kombrink&#39; via singulari=
ty <span dir=3D"ltr">&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_bla=
nk">singu...@lbl.gov</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_qu=
ote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex=
"><div dir=3D"ltr">Hi Gregory,<br><br>=C2=A0sorry that I still won&#39;t le=
t go off this topic :) ...to me this is the most exciting thing about singu=
larity.<br></div></blockquote><div><br></div><div>No worries!</div><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>But what are t=
he advantages of that hybrid MPI usage in your opinion?<br>To me it is not =
fully clear...<br><br>As I get it I can run MPI inside the container fine w=
ith just any version of MPI:<br>singularity mpirun &lt;OPTS&gt; myMPIapp<br=
></div></blockquote><div><br></div><div>You are right, this will always wor=
k limited to a single node.</div><div><br></div><blockquote class=3D"gmail_=
quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1=
ex"><div dir=3D"ltr"><br>Is it like this is just going to work with OpenMPI=
 2.1+?<br><br>mpirun &lt;OPTS&gt; singularity exec myMPIapp<br></div></bloc=
kquote><div><br></div><div>Almost...=C2=A0 You missed the image name:</div>=
<div><br></div><div>mpirun &lt;OPTS&gt; singularity exec container.img myMP=
Iapp</div><div><br></div><div>This also makes (a somewhat obvious assumptio=
n but I&#39;ll mention it) that myMPIapp exists within the container image&=
#39;s PATH.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=
=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr"><br>Actually we are interested in that case since I can&#39;t see =
how multi-node usage and torque/moab integration can be achieved otherwise.=
..<br>Or can this be done with arbitrary MPI versions even?<br></div></bloc=
kquote><div><br></div><div>While I have not personally tested it, I have be=
en told it works with Intel MPI and MPICH... If you find anything definitiv=
e, please let us know!</div><div>=C2=A0</div><blockquote class=3D"gmail_quo=
te" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"=
><div dir=3D"ltr">thanks for clarifying<br></div></blockquote><div><br></di=
v><div>My pleasure, I hope that helps and don&#39;t hesitate to ask if you =
have more questions.</div><div><br></div><div>Greg</div><div><br></div><div=
><br></div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"marg=
in:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
>Steven<span class=3D""><br><br>Am Freitag, 5. August 2016 21:55:56 UTC+2 s=
chrieb Gregory M. Kurtzer:</span><blockquote class=3D"gmail_quote" style=3D=
"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><d=
iv dir=3D"ltr"><span class=3D"">Hi Steven,<div><br></div><div>Thank you for=
 pointing the confusing wording out! I will fix that.</div><div><br></div><=
div>It is intended to say that to use Open MPI in a hybrid manner (like thi=
s) you must be using at least 2.1 (or maybe 2.0.1) as previous versions of =
OMPI do not work.</div><div><br></div><div>Hope that helps and sorry for th=
e confusion!</div><div><br></div><div>Greg</div><div><br></div><div><br></d=
iv></span><div><div><br><div class=3D"gmail_quote"><span class=3D"">On Fri,=
 Aug 5, 2016 at 8:55 AM, &#39;Stefan Kombrink&#39; via singularity <span di=
r=3D"ltr">&lt;<a rel=3D"nofollow">si...@lbl.gov</a>&gt;</span> wrote:<br></=
span><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><span class=3D""><div dir=3D"ltr">Hi Gr=
egory,<br><br>=C2=A0thanks for your responses!<span><br><br>Am Freitag, 5. =
August 2016 16:27:01 UTC+2 schrieb Gregory M. Kurtzer:<blockquote class=3D"=
gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid=
;padding-left:1ex"><div dir=3D"ltr"><br><div><div><div class=3D"gmail_quote=
"><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:=
1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">-Is trunk OpenMPI really =
a requirement or can I use older versions, too? What about IMPI?<br></div><=
/blockquote><div><br></div><div>Open MPI is not a requirement but it has be=
en tuned for Singularity and well tested. I can not speak definitively for =
IMPI, but people have told me that it is working fine out of the box!</div>=
<div>=C2=A0</div></div></div></div></div></blockquote><div>=C2=A0</div></sp=
an><div>I was mainly asking because of this statement:<br><br><h3>Notes:</h=
3>
<p>
</p><h4>Supported Open MPI Version(s):</h4>
To achieve proper container&#39;ized Open MPI support, you must use Open MP=
I
version 2.1 which at the time of this writing has not been released yet.
The above example builds from the current master development branch of Open
MPI.=C2=A0
<br><br>given at <a href=3D"http://singularity.lbl.gov/#hpc" rel=3D"nofollo=
w" target=3D"_blank">http://singularity.lbl.gov/#hp<wbr>c</a><br><br>We run=
 a software stack with &gt;10 different versions of MPI and thus it is impo=
rtant to us that containerized MPI apps are properly managed by Torque and =
can make use of the IB transport which is something docker was giving troub=
les when running unprivileged. Also, to me it is not fully clear whether MP=
I needs to be installed both in the container and on the host.<br><br>I wil=
l soon commit further experiments with MPI, thanks and bye<br>Steven<br></d=
iv></div></span><div><div><span class=3D"">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></span>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><span class=3D""><br><br clear=3D"all"><div>=
<br></div>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Perf=
ormance Computing Services (HPCS)<br>University of California<br>Lawrence B=
erkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div>=
</div></div>
</span></div></div></div>
</blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>

--001a114114a02e7f1a05396735fb--
