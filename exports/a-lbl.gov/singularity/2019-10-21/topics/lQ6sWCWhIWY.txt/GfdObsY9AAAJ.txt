X-Received: by 10.31.174.20 with SMTP id x20mr2661469vke.12.1499860225464;
        Wed, 12 Jul 2017 04:50:25 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.16.148 with SMTP id 142ls1328790ity.18.gmail; Wed, 12 Jul
 2017 04:50:23 -0700 (PDT)
X-Received: by 10.99.2.17 with SMTP id 17mr3420156pgc.264.1499860223610;
        Wed, 12 Jul 2017 04:50:23 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1499860223; cv=none;
        d=google.com; s=arc-20160816;
        b=G0wqYfifqlYkd7fIqUhflZNPfF/ZSmGCD8UI0xtKu/05oPjhok8FZH1m2fXTKy2jDk
         5tv1i5PXVawTyk58UhiUYrLH0EkRpaxM8ddfR7S6nHyOWf+5CykuvWigWxoRFs0oZMNc
         biU9OAySDTfQ1On15F0D65uVzB7Lwi6XTbhMlElUgMDuorWUktbzSeQPTIIKpvf9FEmw
         jDULj59guBc0Wd84UgWdW910v3ddWFLrdC/YmN8tK7TczXfxXkadYgrdAFYXb/e5n3u6
         Sl2j62xkQQD1HOsZkU7DBb2dqR2IogQwDFNC679RspKXT4SGqpVQfTUH5c368NTNctk/
         mI9g==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:references:in-reply-to
         :mime-version:dkim-signature:arc-authentication-results;
        bh=jlgSMEh4doeD7cL0PFp8Tl19vYnGE8w6xmklFRYasAw=;
        b=x0o5Cc9CwcQ4Yhe201JyQSK61XTRpPooB/bSfWE6vVbp0ZWtKJi3Oi3kCYErlc705q
         2xCAm1XBjLIxCLjLf7pHUOVuJ0yYeHHplmdak2L+DfwYNp7/ls+Yq2HFh1JRPudxAEez
         Qe4AhKrUvGuOxNGzYVZMDdaCbM70YyCBHwvCQCn8cey3kElF8repCLFoi+ns4e/aOQyI
         X1/F0pyAlsuWMteNhEUmEozXDL0xlb6Jay7ZfQCrbBpcV4xgFu/qfwwxWGLOy9J2yEhq
         Rw7MG7adppYDplzJNx5D4K2+4J2ezyOluTREJ2cXv8BEGZAYl4ffWff2R5FWJcwdFZch
         sxTA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.b=nltPfOQ2;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.192.176 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id h2si1876497pln.170.2017.07.12.04.50.22
        for <singu...@lbl.gov>;
        Wed, 12 Jul 2017 04:50:23 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 209.85.192.176 as permitted sender) client-ip=209.85.192.176;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.b=nltPfOQ2;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.192.176 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2FNAAB3DGZZhrDAVdFdHQEFAQsBGAEFA?=
 =?us-ascii?q?QsBgkQ+AoEPgRQHg2GBPohjkXCCbJMXgU4bJQMhAQyCDYFcgRBPAoNFBz8YAQE?=
 =?us-ascii?q?BAQEBAQEBAQECEAEBAQgLCwgoL4IzBQIDGgEFBARGJgMDAQEBAQEBAQEBIwEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEBARoCCAUeEgESAQEYAQEBAQIBGgkdAQ0OEgsBAwELBgMCCwo?=
 =?us-ascii?q?DIAoCAiEBAQ4DAQUBHA4HBAEaAgICiDyBOQEDDQgFC45skRo/jAqCBAUBHIMGB?=
 =?us-ascii?q?YNaChknDVaDDgEBAQEBAQEDAQEBAQEBAQEBAQEVAgYSgxaDTYFhgySCV4FkEgF?=
 =?us-ascii?q?JgmaCYQEEhyKQGocxO4dIg0eDR0uEboIMV49CiUCCQ4gCFB+BFQ8QgQUzC3UVS?=
 =?us-ascii?q?RqEQCoPEAyBaT42AQEBAQSFdkeBaQEBAQ?=
X-IronPort-AV: E=Sophos;i="5.40,349,1496127600"; 
   d="scan'208,217";a="81264167"
Received: from mail-pf0-f176.google.com ([209.85.192.176])
  by fe4.lbl.gov with ESMTP; 12 Jul 2017 04:50:19 -0700
Received: by mail-pf0-f176.google.com with SMTP id q86so11785412pfl.3
        for <singu...@lbl.gov>; Wed, 12 Jul 2017 04:50:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc;
        bh=jlgSMEh4doeD7cL0PFp8Tl19vYnGE8w6xmklFRYasAw=;
        b=nltPfOQ22vcuSjBRNajHWOVKDLusjbjnH6ljQRatFIITetSFFNNs/PchI0YaImzu2X
         DoejbiLpWOMOL/UdO0YelzNqKtyXxS+iTcuwWUpQxFf5dmIulmNqF/R6/lFS/B1ntd0f
         mQ+sL5Z4Z/joOm8lSxKFsR7j1FiwwF4vgQgcAPaB3H7sGNrMLhb+gwSwsXFgqAkoe98j
         uS0O5rpl9Nri80XruYpA429ycjIpb9173iyaVi63HGRo0qJAKlyTeo1jZiD1iEnciVH3
         m09h4IbZgwSJHk+4GNNbOK3TMfsqcPI1YM1JrKA43HX6lGcRT7BTI5+/RREaFAlTH4Uz
         68uA==
X-Gm-Message-State: AIVw111D0OCyG2Excm01ot6I4PHtVxvS0zIjA9Jz4gQGZVkgT7HKCHTJ
	fPieQpdHX3e0Sjw+7+DTBleG5Fk42NWR
X-Received: by 10.99.218.81 with SMTP id l17mr3379133pgj.59.1499860218662;
 Wed, 12 Jul 2017 04:50:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.100.185.14 with HTTP; Wed, 12 Jul 2017 04:50:18 -0700 (PDT)
In-Reply-To: <CAApQTTj4FsbybqNWFxj=JCvNN2i8m0+AZvTNPHd25v3GV3FYSA@mail.gmail.com>
References: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
 <CAApQTTgQwZ3pu7Xa7RsbuHM7ndP05Kv5ciEBk_q5G-8P3xaUGw@mail.gmail.com>
 <CAApQTTg2vQY9wJg0g_Qp5n+558jYqb_ZX8-iezzwJf-6nM63QQ@mail.gmail.com>
 <CA+Wz_Fw=cXLngnXwv=m1DnK6-cYN+496MeHhuHyu9YmsV=CG0g@mail.gmail.com> <CAApQTTj4FsbybqNWFxj=JCvNN2i8m0+AZvTNPHd25v3GV3FYSA@mail.gmail.com>
From: victor sv <vict...@gmail.com>
Date: Wed, 12 Jul 2017 13:50:18 +0200
Message-ID: <CA+Wz_Fwj2KbXCpv4WQqv=SttRJwzXRBZw2KOfOnQs0cwfM3onQ@mail.gmail.com>
Subject: Re: [Singularity] OpenMPI, Slurm & portability
To: singularity@lbl.gov
Cc: Ralph Castain <rcas...@gmail.com>
Content-Type: multipart/alternative; boundary="001a114c2e788bd32505541d6b3c"

--001a114c2e788bd32505541d6b3c
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Greg and Ralph,

yes Greg, I agree with you that the mentioned strategy could be dangerous
and goes against the principals of containment.

sorry for the basic question ... but what do you mean with ABI compatible
containers? which components of the container environment are involved with
this ABI compatibility?

If we talk about libc or the kernel itself, as you say in your web page,
"If you require kernel dependent features, a container platform is probably
not the right solution for you."

If we focus on OpenMPI ABI compatibility, I figure out that the variables
involved in this compatibility could be (1) the compiler (vendor) and (2)
the OpenMPI library itself.

I'm right or I'm missing any other variables?

An interesting project called ABI-tracker has performed an OpenMPI ABI
compatibility study that you can watch in the following link:

https://abi-laboratory.pro/tracker/timeline/openmpi/

I think that, at least for OpenMPI 2.X, altough been a dangerous approach,
the ABI compatibility seems reasonable.

What do you think?

BR,
V=C3=ADctor.

2017-07-11 16:55 GMT+02:00 Gregory M. Kurtzer <gmku...@gmail.com>:

> Hi Victor,
>
> I will let Ralph comment on the OMPI versions and compatibilities, but
> regarding using the MPI host libraries within a container is dangerous fo=
r
> the reason that you are mentioning. If you are running ABI compatible
> containers with the host, then things *might* work as expected. But this
> breaks container portability, and goes against the principals of
> containment.
>
> We do however do exactly this for the Nvidia driver libraries, but...
> Nvidia builds these libraries with careful attention on ABI compatibility
> such that these binary libraries are indeed reasonably portable across
> containers.
>
> The only way to do this portably is with using a launcher on the host,
> outside the container, to spin up the container and launch the MPI within=
.
> PMIx is a fantastic approach to solving this.
>
> Hope that helps!
>
> Greg
>
>
>
> On Tue, Jul 11, 2017 at 6:03 AM, victor sv <vict...@gmail.com> wrote:
>
>> Hi Greg and Ralph,
>>
>> Thank you for your precise and elaborated answers.
>>
>> Only for confirmation and to sum up some conclussions (if I understood
>> correctly):
>>
>>  - OpenMPI process management compatibility depends on PMIx.
>>  - OpenMPI (and also Slurm) complete  backward/forward compatibility wil=
l
>> come (hopefully) in the future by means of PMIx 2.1.
>>  - Nowadays, there exists compatibility with OpenMPI 2.X if we compile i=
t
>> with default PMIx (1.X) support.
>>  - OpenMPI 2.1 must be compiled with --disable-pmix-dstore due to a
>> compatibility break.
>>  - OpenMPI 1.X does not suppot PMIx and we can ignore it from this threa=
d.
>>
>> I'm right?
>>
>> I'm interested in performing the tests you purpose. I will try to build
>> all three OMPI versions (2.0, 2.1 and 3.0) against the same PMIx externa=
l
>> library to check the compatibility. Which PMIx version (1.2.0, 1.2.1 or
>> 1.2.2 ) do you recommend as a start point?
>>
>> I will report this results ASAP to this thread.
>>
>> On the other hand, although we are planning to add support to PMIx,
>> unfortunately, our Slurm version (14.11.10-Bull.1.0) does not support it
>> yet.
>>
>> The second strategy we are testing to get compatibility between OpenMPI
>> inside and outside a Singularity container relies on replacing the OpenM=
PI
>> libraries inside the container by the host libraries hierarchy.
>>
>> This approach rest upon the assumption that OpenMPI symbols and data
>> structures are compatible through several versions of OpenMPI. At least
>> combining several releases that share the same major version.
>>
>> Although the empirical tests of this approach seem to work properly with
>> some tests, benchmarks and real apps, I'm afraid of getting unexepected
>> errors/warnings (segfaults, data errors, etc.) in the future.
>>
>> What do you think about this approach?
>>
>> Can you confirm that OpenMPI is compatible in this way?
>>
>> Finally, I think this thread could be very interesting for other users
>> too and I would like to keep it alive with your help.
>>
>> Thank you again for your support!
>>
>> BR,
>> V=C3=ADctor
>>
>> 2017-07-09 23:45 GMT+02:00 Gregory M. Kurtzer <gmku...@gmail.com>:
>>
>>> Hiya Victor, et al.,
>>>
>>> I didn't realize this but Ralph had to drop off of the Singularity list=
.
>>> Hopefully we will get him back again, as he is a fantastic resource for=
 all
>>> of the OMPI questions and always a great source of information and idea=
s
>>> (poke, poke Ralph!). Ralph did send me this in response to the previous
>>> email hoping it helps to explain things:
>>>
>>>
>>> On Sun, Jul 9, 2017 at 2:22 PM, r...@open-mpi.org <r...@open-mpi.org>
>>>  wrote:
>>>
>>>> ...
>>> You are welcome to forward the following to the list:
>>>
>>> As Greg said, we have been concerned about this since we started lookin=
g
>>> at Singularity support. Just for clarity, the version of PMI OMPI uses =
is
>>> PMIx (https://pmix.github.io/pmix/). While our plan from the beginning
>>> was to support cross-versions specifically to address this problem, we =
fell
>>> behind on its implementation due to priorities. We just committed the c=
ode
>>> to the PMIx repo in the last week, and it won=E2=80=99t be released int=
o production
>>> for a few months while we shake it down.
>>>
>>> I fear it will be impossible to get the OMPI 1.10 series to work with
>>> anything other than itself as it pre-dates PMIx.
>>>
>>> The OMPI 2.0 and 2.1 series should work across each other as they both
>>> include PMIx 1.x. However, you probably will need to configure the 2.1
>>> series with --disable-pmix-dstore as there was an unintended compatibil=
ity
>>> break there (the shared memory store was added during the PMIx 1.x seri=
es
>>> and we didn=E2=80=99t catch the compatibility break it introduced).
>>>
>>> Looking into the future, OMPI 3.0 is about to be released. It includes
>>> PMIx 2.0, which isn=E2=80=99t backwards compatible at this time, and so=
 it won=E2=80=99t
>>> cross-version with OMPI 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We haven=
=E2=80=99t tested this, but
>>> one thing you could try is to build all three OMPI versions against the
>>> same PMIx external library (you would probably have to experiment a bit
>>> with PMIx versions to see which works across the different OMPI version=
s as
>>> the glue between the two also changed a bit). This will ensure that the
>>> shared memory store in PMIx is compatible across the versions, and thin=
gs
>>> should work since OMPI doesn=E2=80=99t care how the data is moved acros=
s the
>>> host-container boundary.
>>>
>>> As I said, we will be adding cross-version support to the PMIx release
>>> series soon, without changing the API, that will ensure support across =
all
>>> PMIx versions starting with v1.2. Thus, you could (once that happens) b=
uild
>>> OMPI 2.0, 2.1, and 3.0 against the new PMIx release (probably PMIx v2.1=
.0)
>>> and the resulting containers would be future-proof as OMPI moves ahead.=
 The
>>> RMs plan to follow that path as well, so you should be in good shape on=
ce
>>> this is done if you prefer to =E2=80=9Cdirect launch=E2=80=9D your cont=
ainers (e.g., =E2=80=9Csrun
>>> ./mycontainer=E2=80=9D under SLURM).
>>>
>>> Sorry if that is all confusing - we sometimes get lost in the numbering
>>> schemes between OMPI and PMIx ourselves. Feel free to contact me direct=
ly,
>>> or on the OMPI or PMIx mailing lists, if you have more questions or
>>> encounter problems. We definitely want to make this work.
>>>
>>> Ralph
>>>
>>> On Sun, Jul 9, 2017 at 12:19 PM, Gregory M. Kurtzer <gmku...@gmail.com
>>> > wrote:
>>>
>>>> Hi Victor,
>>>>
>>>> Sorry for the latency, I'm on email overload.
>>>>
>>>> Open MPI uses PMI to communicate both inside and outside of the
>>>> container. Ralph Castain (on this list, but possibly not monitoring
>>>> actively) is leading the PMI effort and he is an active Open MPI devel=
oper.
>>>> We have had several talks about how to achieve "hetero-versionistic"
>>>> compatibility through the PMI handshake. I was under the impression th=
at
>>>> PMI now supports that, as long as you are running equal or newer versi=
on on
>>>> the host (outside the container). Also, I don't know what version of P=
MI
>>>> this feature was introduced in, nor do I know what version of Open MPI
>>>> includes that compatibility.
>>>>
>>>> I have CC'ed Ralph, and hopefully he will be able to offer some
>>>> suggestions.
>>>>
>>>> Regarding your question about supporting the MPI libraries in the same
>>>> manner that we are doing the Nvidia libraries, that would be hard. Nvi=
dia
>>>> specifically builds their libraries to be as generally compatible as
>>>> possible (e.g. the same libraries/binaries work on a large array of Li=
nux
>>>> distributions). Most people do not build host libraries in a manner th=
at
>>>> would be generally compatible as Nvidia does.
>>>>
>>>> Hope that helps!
>>>>
>>>> Greg
>>>>
>>>>
>>>>
>>>> On Mon, Jul 3, 2017 at 2:07 AM, victor sv <vict...@gmail.com> wrote:
>>>>
>>>>> Dear Singularity team,
>>>>>
>>>>> first of all, thanks for the great work with Singularity. It looks
>>>>> amazing!
>>>>>
>>>>> Sorry if this topic is duplicated and for the length of the email, bu=
t
>>>>> I want to share my experience about Singularity and OpenMPI compatibi=
lity,
>>>>> and also ask some questions.
>>>>>
>>>>> I've being reading a lot about OpenMPI and Singularity compatibility
>>>>> because we are trying to find the generic way to run OpenMPI applicat=
ions
>>>>> within Singularity containers. It was not so clear (for me) in the
>>>>> documentation, forums and mailing lists, and this is why we've perfor=
med an
>>>>> OpenMPI empiric compatibility study.
>>>>>
>>>>> We ran these comparisons in CESGA FinisTerrae II cluster (
>>>>> https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2).
>>>>>
>>>>> We used several versions of OpenMPI. The chosen versions of OpenMPI
>>>>> were the versions already installed in the cluster:
>>>>>
>>>>> - openmpi/1.10.2
>>>>> - openmpi/2.0.0
>>>>> - openmpi/2.0.1
>>>>> - openmpi/2.0.2
>>>>> - openmpi/2.1.1
>>>>>
>>>>> We have created Singularity images containing the same versions of
>>>>> OpenMPI and with the basic OpenMPI ring example. I share the bootstra=
p
>>>>> definition file template used below:
>>>>>
>>>>> ```
>>>>> BootStrap: docker
>>>>> From: ubuntu:16.04
>>>>> IncludeCmd: yes
>>>>>
>>>>> %post
>>>>>         sed -i 's/main/main restricted universe/g'
>>>>> /etc/apt/sources.list
>>>>>         apt-get update
>>>>>         apt-get install -y bash git wget build-essential gcc time
>>>>> libc6-dev libgcc-5-dev
>>>>>         apt-get install -y dapl2-utils libdapl-dev libdapl2
>>>>> libibverbs1 librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1
>>>>> libmthca1 libnes1 libpmi0 libpmi0-dev libslurm29 libslurm-dev
>>>>>
>>>>>         ##Install OpenMPI
>>>>>         cd /tmp
>>>>>         wget 'https://www.open-mpi.org/soft
>>>>> ware/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz' -O openmpi-X.X.X.tar.g=
z
>>>>>         tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X
>>>>>         mkdir -p /tmp/openmpi-X.X.X/build
>>>>>         cd /tmp/openmpi-X.X.X/build
>>>>>          ../configure --enable-shared --enable-mpi-thread-multiple
>>>>> --with-verbs --enable-mpirun-prefix-by-default --with-hwloc
>>>>> --disable-dlopen --with-pmi --prefix=3D/usr
>>>>>         make all install
>>>>>
>>>>>         # Install ring
>>>>>         cd /tmp
>>>>>         wget https://raw.githubusercontent.
>>>>> com/open-mpi/ompi/master/examples/ring_c.c
>>>>>         mpicc ring_c.c -o /usr/bin/ring
>>>>> ```
>>>>>
>>>>> Once the containers were created, we ran the ring app with mpirun
>>>>> using 2 cores of 2 different nodes mixing all possible combinations o=
f
>>>>> those OpenMPI versions inside and outside the container.
>>>>>
>>>>> The obtained results shown that we need the same versions of OpenMPI
>>>>> inside and outside the container to succesfully run the contained
>>>>> application in parallel with mpirun.
>>>>>
>>>>> Is this the expected behaviour or am I missing something?
>>>>>
>>>>> Will be this the expected behaviour in the future (with future
>>>>> versions of OpenMPI)?
>>>>>
>>>>> Currently, we have slurm 14.11.10-Bull.1.0 installed as job scheduler
>>>>> at FinisTerrae II. We found the following tip/trick to use srun as pr=
ocess
>>>>> manager:
>>>>>
>>>>> http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls
>>>>>
>>>>> In order to run whatever Singularity image containing OpenMPI
>>>>> applications using Slurm, we've adapted it to our infrastructure and
>>>>> checked the same test cases running them with srun. It seems that it'=
s
>>>>> working properly (no real world applications were tested yet).
>>>>>
>>>>> What do you think about this strategy?
>>>>> Can you confirm that it provides portability of singularity images
>>>>> containing OpenMPI applications?
>>>>>
>>>>> I think this strategy is similar to the one you are following with
>>>>> "--nv" option  for NVidia drivers.
>>>>>
>>>>> Why not to do the same strategy with MPI, PMI, libibverbs, etc.?
>>>>>
>>>>> Thanks in advance and congrats again for your great work!
>>>>>
>>>>> V=C3=ADctor.
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> CEO, SingularityWare, LLC.
>>>> Senior Architect, RStor
>>>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>>>
>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> CEO, SingularityWare, LLC.
>>> Senior Architect, RStor
>>> Computational Science Advisor, Lawrence Berkeley National Laboratory
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> Gregory M. Kurtzer
> CEO, SingularityWare, LLC.
> Senior Architect, RStor
> Computational Science Advisor, Lawrence Berkeley National Laboratory
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a114c2e788bd32505541d6b3c
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Greg and Ralph,<br><br>yes Greg, I agree with you that =
the mentioned strategy could be dangerous and goes against the principals o=
f containment.<br><br>sorry
 for the basic question ... but what do you mean with ABI compatible=20
containers? which components of the container environment are involved=20
with this ABI compatibility?<br><br>If we talk about libc or the kernel=20
itself, as you say in your web page, &quot;If you require kernel dependent=
=20
features, a container platform is probably not the right solution for=20
you.&quot;<br><br>If we focus on OpenMPI ABI compatibility, I figure out th=
at
 the variables involved in this compatibility could be (1) the compiler=20
(vendor) and (2) the OpenMPI library itself.<br><br>I&#39;m right or I&#39;=
m missing any other variables?<br><br>An
 interesting project called ABI-tracker has performed an OpenMPI ABI=20
compatibility study that you can watch in the following link:<br><br><span =
class=3D"gmail-Object" id=3D"gmail-OBJ_PREFIX_DWT1429_com_zimbra_url"><a ta=
rget=3D"_blank" href=3D"https://abi-laboratory.pro/tracker/timeline/openmpi=
/">https://abi-laboratory.pro/tracker/timeline/openmpi/</a></span><br><br>I=
 think that, at least for OpenMPI 2.X, altough been a dangerous approach, t=
he ABI compatibility seems reasonable.<br><br>What do you think?<br><br>BR,=
<br>V=C3=ADctor.</div><div class=3D"gmail_extra"><br><div class=3D"gmail_qu=
ote">2017-07-11 16:55 GMT+02:00 Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a=
 href=3D"mailto:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a>&=
gt;</span>:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex=
;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Victor,<d=
iv><br></div><div>I will let Ralph comment on the OMPI versions and compati=
bilities, but regarding using the MPI host libraries within a container is =
dangerous for the reason that you are mentioning. If you are running ABI co=
mpatible containers with the host, then things *might* work as expected. Bu=
t this breaks container portability, and goes against the principals of con=
tainment.</div><div><br></div><div>We do however do exactly this for the Nv=
idia driver libraries, but... Nvidia builds these libraries with careful at=
tention on ABI compatibility such that these binary libraries are indeed re=
asonably portable across containers.</div><div><br></div><div>The only way =
to do this portably is with using a launcher on the host, outside the conta=
iner, to spin up the container and launch the MPI within. PMIx is a fantast=
ic approach to solving this.</div><div><br></div><div>Hope that helps!</div=
><div><br></div><div>Greg</div><div><div class=3D"h5"><div><br></div><div><=
br><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Jul 11=
, 2017 at 6:03 AM, victor sv <span dir=3D"ltr">&lt;<a href=3D"mailto:vict..=
.@gmail.com" target=3D"_blank">vict...@gmail.com</a>&gt;</span> wrote:<br><=
blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px=
 #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Greg and Ralph,<br><br>Th=
ank you for your precise and elaborated answers.<br><br>Only for confirmati=
on and to sum up some conclussions (if I understood correctly):<br><br>=C2=
=A0- OpenMPI process management compatibility depends on PMIx.<br>=C2=A0- O=
penMPI (and also Slurm) complete=C2=A0 backward/forward compatibility will =
come (hopefully) in the future by means of PMIx 2.1.<br>=C2=A0- Nowadays, t=
here exists compatibility with OpenMPI 2.X if we compile it with default PM=
Ix (1.X) support.<br>=C2=A0- OpenMPI 2.1 must be compiled with --disable-pm=
ix-dstore due to a compatibility break.<br>=C2=A0- OpenMPI 1.X does not sup=
pot PMIx and we can ignore it from this thread.<br><br>I&#39;m right?<br><b=
r>I&#39;m interested in performing the tests you purpose. I will try to bui=
ld all three OMPI versions (2.0, 2.1 and 3.0) against the same PMIx externa=
l library to check the compatibility. Which PMIx version (1.2.0, 1.2.1 or 1=
.2.2 ) do you recommend as a start point? <br><br>I will report this result=
s ASAP to this thread.<br><br>On the other hand, although we are planning t=
o add support to PMIx, unfortunately, our Slurm version (14.11.10-Bull.1.0)=
 does not support it yet.<br><br>The second strategy we are testing to get =
compatibility between OpenMPI inside and outside a Singularity container re=
lies on replacing the OpenMPI libraries inside the container by the host li=
braries hierarchy.<br><br>This approach rest upon the assumption that OpenM=
PI symbols and data structures are compatible through several versions of O=
penMPI. At least combining several releases that share the same major versi=
on.<br><br>Although the empirical tests of this approach seem to work prope=
rly with some tests, benchmarks and real apps, I&#39;m afraid of getting un=
exepected errors/warnings (segfaults, data errors, etc.) in the future.<br>=
<br>What do you think about this approach?<br><br>Can you confirm that Open=
MPI is compatible in this way?<br><br>Finally, I think this thread could be=
 very interesting for other users too and I would like to keep it alive wit=
h your help.<br><br>Thank you again for your support!<br><br>BR,<br>V=C3=AD=
ctor<br></div><div class=3D"m_-4592851147206327630HOEnZb"><div class=3D"m_-=
4592851147206327630h5"><div class=3D"gmail_extra"><br><div class=3D"gmail_q=
uote">2017-07-09 23:45 GMT+02:00 Gregory M. Kurtzer <span dir=3D"ltr">&lt;<=
a href=3D"mailto:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a>=
&gt;</span>:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hiya Victor=
, et al.,=C2=A0<div><br></div><div>I didn&#39;t realize this but Ralph had =
to drop off of the Singularity list. Hopefully we will get him back again, =
as he is a fantastic resource for all of the OMPI questions and always a gr=
eat source of information and ideas (poke, poke Ralph!). Ralph did send me =
this in response to the previous email hoping it helps to explain things:</=
div><div><br></div><div><br></div><div>On Sun, Jul 9, 2017 at 2:22 PM, <a h=
ref=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>=C2=
=A0<span dir=3D"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org" target=3D"_bl=
ank">rhc@open-mpi<wbr>.org</a>&gt;</span>=C2=A0wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rg=
b(204,204,204);padding-left:1ex"></blockquote></div><div><div style=3D"font=
-size:12.8px">...</div><div style=3D"font-size:12.8px">You are welcome to f=
orward the following to the list:</div><div style=3D"font-size:12.8px"><br>=
</div><div style=3D"font-size:12.8px">As Greg said, we have been concerned =
about this since we started looking at Singularity support. Just for clarit=
y, the version of PMI OMPI uses is PMIx (<a href=3D"https://pmix.github.io/=
pmix/" target=3D"_blank">https://pmix.github.io/pmix/</a>)<wbr>. While our =
plan from the beginning was to support cross-versions specifically to addre=
ss this problem, we fell behind on its implementation due to priorities. We=
 just committed the code to the PMIx repo in the last week, and it won=E2=
=80=99t be released into production for a few months while we shake it down=
.</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-size:12=
.8px">I fear it will be impossible to get the OMPI 1.10 series to work with=
 anything other than itself as it pre-dates PMIx.</div><div style=3D"font-s=
ize:12.8px"><br></div><div style=3D"font-size:12.8px">The OMPI 2.0 and 2.1 =
series should work across each other as they both include PMIx 1.x. However=
, you probably will need to configure the 2.1 series with --disable-pmix-ds=
tore as there was an unintended compatibility break there (the shared memor=
y store was added during the PMIx 1.x series and we didn=E2=80=99t catch th=
e compatibility break it introduced).</div><div style=3D"font-size:12.8px">=
<br></div><div style=3D"font-size:12.8px">Looking into the future, OMPI 3.0=
 is about to be released. It includes PMIx 2.0, which isn=E2=80=99t backwar=
ds compatible at this time, and so it won=E2=80=99t cross-version with OMPI=
 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We haven=E2=80=99t tested this, but =
one thing you could try is to build all three OMPI versions against the sam=
e PMIx external library (you would probably have to experiment a bit with P=
MIx versions to see which works across the different OMPI versions as the g=
lue between the two also changed a bit). This will ensure that the shared m=
emory store in PMIx is compatible across the versions, and things should wo=
rk since OMPI doesn=E2=80=99t care how the data is moved across the host-co=
ntainer boundary.</div><div style=3D"font-size:12.8px"><br></div><div style=
=3D"font-size:12.8px">As I said, we will be adding cross-version support to=
 the PMIx release series soon, without changing the API, that will ensure s=
upport across all PMIx versions starting with v1.2. Thus, you could (once t=
hat happens) build OMPI 2.0, 2.1, and 3.0 against the new PMIx release (pro=
bably PMIx v2.1.0) and the resulting containers would be future-proof as OM=
PI moves ahead. The RMs plan to follow that path as well, so you should be =
in good shape once this is done if you prefer to =E2=80=9Cdirect launch=E2=
=80=9D your containers (e.g., =E2=80=9Csrun ./mycontainer=E2=80=9D under SL=
URM).</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-siz=
e:12.8px">Sorry if that is all confusing - we sometimes get lost in the num=
bering schemes between OMPI and PMIx ourselves. Feel free to contact me dir=
ectly, or on the OMPI or PMIx mailing lists, if you have more questions or =
encounter problems. We definitely want to make this work.</div><div style=
=3D"font-size:12.8px"><br></div><div style=3D"font-size:12.8px">Ralph</div>=
</div></div><div class=3D"m_-4592851147206327630m_-1840755824492384681HOEnZ=
b"><div class=3D"m_-4592851147206327630m_-1840755824492384681h5"><div class=
=3D"gmail_extra"><br><div class=3D"gmail_quote">On Sun, Jul 9, 2017 at 12:1=
9 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailto:gmku...@gm=
ail.com" target=3D"_blank">gmku...@gmail.com</a>&gt;</span> wrote:<br><bloc=
kquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #cc=
c solid;padding-left:1ex"><div dir=3D"ltr">Hi Victor,<div><br></div><div>So=
rry for the latency, I&#39;m on email overload.</div><div><br></div><div>Op=
en MPI uses PMI to communicate both inside and outside of the container. Ra=
lph Castain (on this list, but possibly not monitoring actively) is leading=
 the PMI effort and he is an active Open MPI developer. We have had several=
 talks about how to achieve &quot;hetero-versionistic&quot; compatibility t=
hrough the PMI handshake. I was under the impression that PMI now supports =
that, as long as you are running equal or newer version on the host (outsid=
e the container). Also, I don&#39;t know what version of PMI this feature w=
as introduced in, nor do I know what version of Open MPI includes that comp=
atibility.</div><div><br></div><div>I have CC&#39;ed Ralph, and hopefully h=
e will be able to offer some suggestions.</div><div><br></div><div>Regardin=
g your question about supporting the MPI libraries in the same manner that =
we are doing the Nvidia libraries, that would be hard. Nvidia specifically =
builds their libraries to be as generally compatible as possible (e.g. the =
same libraries/binaries work on a large array of Linux distributions). Most=
 people do not build host libraries in a manner that would be generally com=
patible as Nvidia does.</div><div><br></div><div>Hope that helps!</div><div=
><br></div><div>Greg</div><div><br></div><div><br></div></div><div class=3D=
"gmail_extra"><div><div class=3D"m_-4592851147206327630m_-18407558244923846=
81m_6215373137033584522h5"><br><div class=3D"gmail_quote">On Mon, Jul 3, 20=
17 at 2:07 AM, victor sv <span dir=3D"ltr">&lt;<a href=3D"mailto:vict...@gm=
ail.com" target=3D"_blank">vict...@gmail.com</a>&gt;</span> wrote:<br><bloc=
kquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #cc=
c solid;padding-left:1ex"><div dir=3D"ltr">Dear Singularity team,<br><br>fi=
rst of all, thanks for the great work with Singularity. It looks amazing!<b=
r><br>Sorry if this topic is duplicated and for the length of the email, bu=
t I want to share my experience about Singularity and OpenMPI compatibility=
, and also ask some questions.<br><br>I&#39;ve being reading a lot about Op=
enMPI and Singularity compatibility because we are trying to find the gener=
ic way to run OpenMPI applications within Singularity containers. It was no=
t so clear (for me) in the documentation, forums and mailing lists, and thi=
s is why we&#39;ve performed an OpenMPI empiric compatibility study.<br><br=
>We ran these comparisons in CESGA FinisTerrae II cluster (<a href=3D"https=
://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2" target=3D"_bl=
ank">https://www.cesga.es/en/infra<wbr>estructuras/computacion/FinisT<wbr>e=
rrae2</a>).<br><br>We used several versions of OpenMPI. The chosen versions=
 of OpenMPI were the versions already installed in the cluster:<br><br>- op=
enmpi/1.10.2<br>- openmpi/2.0.0<br>- openmpi/2.0.1<br>- openmpi/2.0.2<br>- =
openmpi/2.1.1<br><br>We have created Singularity images containing the same=
 versions of OpenMPI and with the basic OpenMPI ring example. I share the b=
ootstrap definition file template used below:<br><br>```<br>BootStrap: dock=
er<br>From: ubuntu:16.04<br>IncludeCmd: yes<br><br>%post<br>=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sed -i &#39;s/main/main restricted universe/g&#=
39; /etc/apt/sources.list<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt=
-get update<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get install -=
y bash git wget build-essential gcc time libc6-dev libgcc-5-dev<br>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get install -y dapl2-utils libdapl=
-dev libdapl2 libibverbs1 librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 li=
bmlx5-1 libmthca1 libnes1 libpmi0 libpmi0-dev libslurm29 libslurm-dev<br><b=
r>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 ##Install OpenMPI<br>=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 wget &#39;<a href=3D"https://www.open-mpi.org/software/ompi/vX=
.X/downloads/openmpi-X.X.X.tar.gz" target=3D"_blank">https://www.open-mpi.o=
rg/soft<wbr>ware/ompi/vX.X/downloads/openm<wbr>pi-X.X.X.tar.gz</a>&#39; -O =
openmpi-X.X.X.tar.gz<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 tar -xzf=
 openmpi-X.X.X.tar.gz -C openmpi-X.X.X<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 mkdir -p /tmp/openmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 cd /tmp/openmpi-X.X.X/build<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 ../configure --enable-shared --enable-mpi-thread-mult=
iple --with-verbs --enable-mpirun-prefix-by-defa<wbr>ult --with-hwloc --dis=
able-dlopen --with-pmi --prefix=3D/usr<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 make all install<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 # Install ring<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br=
>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget <a href=3D"https://raw.git=
hubusercontent.com/open-mpi/ompi/master/examples/ring_c.c" target=3D"_blank=
">https://raw.githubusercontent.<wbr>com/open-mpi/ompi/master/examp<wbr>les=
/ring_c.c</a><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mpicc ring_c.c =
-o /usr/bin/ring<br>```<br><br>Once the containers were created, we ran the=
 ring app with mpirun using 2 cores of 2 different nodes mixing all possibl=
e combinations of those OpenMPI versions inside and outside the container.<=
br><br>The obtained results shown that we need the same versions of OpenMPI=
 inside and outside the container to succesfully run the contained applicat=
ion in parallel with mpirun.<br><br>Is this the expected behaviour or am I =
missing something?<br><br>Will be this the expected behaviour in the future=
 (with future versions of OpenMPI)?<br><br>Currently, we have slurm 14.11.1=
0-Bull.1.0 installed as job scheduler at FinisTerrae II. We found the follo=
wing tip/trick to use srun as process manager:<br><br><a href=3D"http://sin=
gularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls" target=3D"_blank">http=
://singularity.lbl.gov/tut<wbr>orial-gpu-drivers-open-mpi-mtl<wbr>s</a><br>=
<br>In order to run whatever Singularity image containing OpenMPI applicati=
ons using Slurm, we&#39;ve adapted it to our infrastructure and checked the=
 same test cases running them with srun. It seems that it&#39;s working pro=
perly (no real world applications were tested yet).<br><br>What do you thin=
k about this strategy?<br>Can you confirm that it provides portability of s=
ingularity images containing OpenMPI applications?<br><br>I think this stra=
tegy is similar to the one you are following with &quot;--nv&quot; option=
=C2=A0 for NVidia drivers.<br><br>Why not to do the same strategy with MPI,=
 PMI, libibverbs, etc.?<br><br>Thanks in advance and congrats again for you=
r great work!<br><br>V=C3=ADctor.<span class=3D"m_-4592851147206327630m_-18=
40755824492384681m_6215373137033584522m_8312868002830329868HOEnZb"><font co=
lor=3D"#888888"><br></font></span></div><span class=3D"m_-45928511472063276=
30m_-1840755824492384681m_6215373137033584522m_8312868002830329868HOEnZb"><=
font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div></div=
></div><span class=3D"m_-4592851147206327630m_-1840755824492384681m_6215373=
137033584522HOEnZb"><font color=3D"#888888">-- <br><div class=3D"m_-4592851=
147206327630m_-1840755824492384681m_6215373137033584522m_831286800283032986=
8gmail_signature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div>=
<div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>CE=
O, SingularityWare, LLC.</div><div>Senior Architect, RStor</div><div><span =
style=3D"font-size:12.8px">Computational Science Advisor, Lawrence Berkeley=
 National Laboratory</span><br></div></div></div></div></div></div></div>
</font></span></div>
</blockquote></div><br><br clear=3D"all"><div><br></div>-- <br><div class=
=3D"m_-4592851147206327630m_-1840755824492384681m_6215373137033584522gmail_=
signature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div><div di=
r=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>CEO, Sing=
ularityWare, LLC.</div><div>Senior Architect, RStor</div><div><span style=
=3D"font-size:12.8px">Computational Science Advisor, Lawrence Berkeley Nati=
onal Laboratory</span><br></div></div></div></div></div></div></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"m_-4592851147206327630gmail_signature" data-smartmail=3D"gmai=
l_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><=
div>Gregory M. Kurtzer</div><div>CEO, SingularityWare, LLC.</div><div>Senio=
r Architect, RStor</div><div><span style=3D"font-size:12.8px">Computational=
 Science Advisor, Lawrence Berkeley National Laboratory</span><br></div></d=
iv></div></div></div></div></div>
</div></div></div></div></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--001a114c2e788bd32505541d6b3c--
