Date: Tue, 19 Mar 2019 07:50:50 -0700 (PDT)
From: Adrian Jackson <adrianj...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <3bc7c413-38a5-4354-8105-894588905ae6@lbl.gov>
Subject: Calling parallel python from singularity
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_1736_1762516060.1553007050428"

------=_Part_1736_1762516060.1553007050428
Content-Type: multipart/alternative; 
	boundary="----=_Part_1737_1469088670.1553007050428"

------=_Part_1737_1469088670.1553007050428
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

I'm trying to run a parallel python program from within singularity, but 
it's running multiple serial copies of my program instead of an MPI run. My 
container works fine for a C/Fortran MPI program, but not for an Python one.

For instance, if I run this:

mpirun -n 2 singularity exec -B /home/adrianj:/home parallel.img 
/home/mpi_hello_world

I get:

MPI: 1 of 2
MPI: 0 of 2

i.e. I get an MPI communicator of 2 with ranks 0 and 1. Whereas if I run 
this:

mpirun -n 2  singularity exec -B /home/adrianj:/home parallel.img  python3 
/home/volume_test.py

I get:

rank: 0
size: 1
rank: 0
size: 1

i.e. I get two MPI communicators of size 1 with ranks 0 and 0.

Should I be doing something different to call a python MPI program from 
singularity?

thanks

------=_Part_1737_1469088670.1553007050428
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>I&#39;m trying to run a parallel python program from =
within singularity, but it&#39;s running multiple serial copies of my progr=
am instead of an MPI run. My container works fine for a C/Fortran MPI progr=
am, but not for an Python one.</div><div><br></div><div>For instance, if I =
run this:</div><div><br></div><div>mpirun -n 2 singularity exec -B /home/ad=
rianj:/home parallel.img /home/mpi_hello_world</div><div><br></div><div>I g=
et:<br></div><div><br></div><div>MPI: 1 of 2<br>MPI: 0 of 2</div><div><br><=
/div><div>i.e. I get an MPI communicator of 2 with ranks 0 and 1. Whereas i=
f I run this:</div><div><br></div><div>mpirun -n 2=C2=A0 singularity exec -=
B /home/adrianj:/home parallel.img=C2=A0 python3 /home/volume_test.py</div>=
<div><br></div><div>I get:</div><div><br></div><div>rank: 0<br>size: 1<br>r=
ank: 0<br>size: 1</div><div><br></div><div>i.e. I get two MPI communicators=
 of size 1 with ranks 0 and 0.</div><div><br></div><div>Should I be doing s=
omething different to call a python MPI program from singularity?</div><div=
><br></div><div>thanks<br></div></div>
------=_Part_1737_1469088670.1553007050428--

------=_Part_1736_1762516060.1553007050428--
