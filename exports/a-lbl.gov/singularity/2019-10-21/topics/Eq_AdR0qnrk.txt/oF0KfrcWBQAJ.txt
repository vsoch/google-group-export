X-Received: by 10.99.7.9 with SMTP id 9mr1467112pgh.21.1518802307516;
        Fri, 16 Feb 2018 09:31:47 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:903:: with SMTP id 3-v6ls1043417plm.7.gmail; Fri, 16
 Feb 2018 09:31:46 -0800 (PST)
X-Received: by 2002:a17:902:718e:: with SMTP id b14-v6mr6686345pll.38.1518802306000;
        Fri, 16 Feb 2018 09:31:46 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1518802305; cv=none;
        d=google.com; s=arc-20160816;
        b=wz0oAtDpCDxF30hlxZ4DKxab9eWy+/9qAxgb0Hw9J/yxDodl8RbVQa+KKa0f0p4gsp
         TNzW+ilXybWgLOHWRqE1Y3AjEJb0d5F6NHdjvMB7ayU+9pLD6HrtgHRvlTm0EXK6l8c+
         XXR38zIh/J6RZ/aal8EaLir1bFZQo6O3POiIisYUi6MNSzlB3w/bdlZPwb+juQpXVP3Q
         o17MKEr8WdNtpY5xgRVLrNDl3wbPN+E0E3H5jX0zdQSO3w1tDpdJIXsVYOGMZ/dLA2iu
         kW/+ndxdFD8MDdZQ1bDvQ3VuTBYZ2VDh3jjVwxMUEStEcvA6KA+BK0ixeLma8rSE2PmP
         eINA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=A5aWEAtYjuzqj6fP9+FNRGpTNPsgOuhfs+S+Lqj2Kh4=;
        b=wcwB0H8xpnvDiBqa8kDN5X2XRmrtlIkBRWXGNzUVycKsp9AsTYglTUI6FVwUD8cVtL
         Nll+jsGtooVdcilcoEqi6NvsPUB4Knj0nWOSf0e/QTlAw3BFfuftnZobw0AdB/AuW6Ck
         wABJCpRItO8EGi/R9q5IJZGa238wO1ugx/9Q+vOdOFyixU2SsfJNpBkujFUbW0JaSiFF
         ctpcN55bzK9y/mVJBQQOkZFjsxO7p5lcws/xw4xXscKicFeIhRGqCk9riRjDiW/hSuT3
         ZDQLm3xbLxfhS4ONKaiIF4Np+s7Y6TttgSPzeHV1q56HCWA6XABIt2+wlueib19n6IME
         L5vw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=dW9R3nxY;
       spf=pass (google.com: domain of smahm...@gmail.com designates 209.85.223.171 as permitted sender) smtp.mailfrom=smahm...@gmail.com
Return-Path: <smahm...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id p13si3841528pgf.498.2018.02.16.09.31.45
        for <singu...@lbl.gov>;
        Fri, 16 Feb 2018 09:31:45 -0800 (PST)
Received-SPF: pass (google.com: domain of smahm...@gmail.com designates 209.85.223.171 as permitted sender) client-ip=209.85.223.171;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=dW9R3nxY;
       spf=pass (google.com: domain of smahm...@gmail.com designates 209.85.223.171 as permitted sender) smtp.mailfrom=smahm...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HHAQDtFIdahqvfVdFcGwEBAQEDAQEBC?=
 =?us-ascii?q?QEBAYJaSIETcCgKg0MHgTmIbI4FggKBF4JqhQ2FaohoFIE/QyIBBoM+gV4Cgj8?=
 =?us-ascii?q?HVBgBAgEBAQEBAQIBAhABAQEICwsIKC+COAUCAxoGCUsqLwEBAQEBAQEBAQEfA?=
 =?us-ascii?q?hMYJQIYAQEBAwEjHQENDgkVAwELBgULBgQBAQEqAgIhAQEOAwEFARQIDgcEAQc?=
 =?us-ascii?q?VBIgsgTsBAw0IBQugWkCMF4IFBQEcgwwFg2UKGScNWVmCEwEBAQEBAQEBAgEBA?=
 =?us-ascii?q?QEBAQEBAQEBFQIGEoR1giiDP4Mugmw5gUsBEgGDNoJlAQSBLQEBAYk9B4h0kBg?=
 =?us-ascii?q?0AQcBAYFzCo8ChQuEJZAjjk6GGYMkGSCBFx9sLnEzGiNPMWyBJgmCEiofgjAjN?=
 =?us-ascii?q?wEPi1hJgXUBAQE?=
X-IronPort-AV: E=Sophos;i="5.46,520,1511856000"; 
   d="scan'208,217";a="106255368"
Received: from mail-io0-f171.google.com ([209.85.223.171])
  by fe3.lbl.gov with ESMTP; 16 Feb 2018 09:31:41 -0800
Received: by mail-io0-f171.google.com with SMTP id e7so4891541ioj.1
        for <singu...@lbl.gov>; Fri, 16 Feb 2018 09:31:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=A5aWEAtYjuzqj6fP9+FNRGpTNPsgOuhfs+S+Lqj2Kh4=;
        b=dW9R3nxY77RgmUjW/3CLGfES0XaSGM1MPCwNu5j3VVho2ZPFBo7rTNvo/ewEQEpVEw
         xQP7yNYuZBpo52+a93cTIa6+yhdPdrhlAnDneWR+5xX3TOLNCRhw28M5Zs+QnDRT035c
         IF63TtT6y2eUB3tuHTydkl01ttQ20PG92nPYhoIElqcDewNPoqXoWB4JXCheHM7M22kt
         XaZgXwpZ1fYJ8y3fC7A9Cb66+NW3zjgNQpjLuMdlx6XVPI04C/G+7T8XECvMa1wCiDQW
         6Whz8HJJCK2ebvxqjr2SikE7lYtElsPACqUUXrovpTdy/AWt+iXqkI7egfd/YyDLVq2I
         BR+A==
X-Gm-Message-State: APf1xPDL+PK2IkyR+3W2dvh9NaVgVFIuqGBJx8ZdXNRfVXQWl8VZafJi
	jX5DfVuH3419VceZ9qaw5UmqhzHQSGg+Tfq7VSV5EQ==
X-Received: by 10.107.79.19 with SMTP id d19mr8932170iob.263.1518802300233;
 Fri, 16 Feb 2018 09:31:40 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.47.34 with HTTP; Fri, 16 Feb 2018 09:30:59 -0800 (PST)
In-Reply-To: <B58197C146EC324AA00A2A07DC082602B15A6C21@XMAIL-MBX-BT1.AD.UCSD.EDU>
References: <CAHqiYpfh5BO7h8MZM09+7wmWpz5jKQtzE7wAPMBf9Qbnk=ANoQ@mail.gmail.com>
 <CA+Wz_Fwcs-zqn5=G_8A2jAa4P7wGb=vvqSp6xCojYWFOdmSQhg@mail.gmail.com>
 <CAHqiYpc4mrffsA3kQ+q=1jttq-OKuHDPcZtAv+02mUqUNYbSbg@mail.gmail.com> <B58197C146EC324AA00A2A07DC082602B15A6C21@XMAIL-MBX-BT1.AD.UCSD.EDU>
From: Haseeb Mahmud <smahm...@gmail.com>
Date: Fri, 16 Feb 2018 12:30:59 -0500
Message-ID: <CAHqiYpeD7bTsY7pbo=Lhu3rRqD0_U4iSkt3NKDfbPau8rZ2XAw@mail.gmail.com>
Subject: Re: [Singularity] singularity container on HPC
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="f4f5e80328d096eaf5056557b7ce"

--f4f5e80328d096eaf5056557b7ce
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hello,

 I am able to build and run  MPI version of NAMD fine on my cluster outside
a container. I just would like to run it within a container as well just as
a test with singularity. Is there any other dependencies.libraries  I
should be adding to my container other than Open MPI 2.1.0.  My HPC'
network is  Ethernet (TCP), and not infiniband.

Regards,

Haseeb

On Thu, Feb 15, 2018 at 10:31 PM, Kandes, Martin <mka...@sdsc.edu> wrote:

> Hi Haseed,
>
> I guess my first question would be: why do you need to build NAMD within =
a
> Singularity container? I've built it on a few systems before with differe=
nt
> build options. It usually plays nicely with most systems. If you're doing
> an 'mpi' comm  build, here is a build script I've used before [1]. Maybe
> it'll help.
>
> Marty
>
> [1]
>
> #!/bin/bash
> #
> # A build script for NAMD
>
> declare -r COMPILER_MODULE=3D'gnu/4.9.2'
> declare -r MPI_MODULE=3D'mvapich2_ib/2.1'
>
> declare -r OS=3D'linux'
> declare -r ARCH=3D'x86_64'
>
> declare -r NAMD_NAME=3D'NAMD'
> declare -r NAMD_VERSION=3D'2.12'
> declare -r NAMD_TARBALL=3D"${NAMD_NAME}_${NAMD_VERSION}_Source.tar.gz"
> declare -r NAMD_DIR=3D"${NAMD_NAME}_${NAMD_VERSION}_Source"
> declare -r NAMD_URL=3D'http://www.ks.uiuc.edu/Research/namd'
> declare -r NAMD_COMPILER=3D'g++'
> declare -r NAMD_ARCH=3D"Linux-${ARCH}-${NAMD_COMPILER}"
>
> declare -r CHARM_NAME=3D'charm'
> declare -r CHARM_VERSION=3D'6.7.1'
> declare -r CHARM_TARFILE=3D"${CHARM_NAME}-${CHARM_VERSION}.tar"
> declare -r CHARM_DIR=3D"${CHARM_NAME}-${CHARM_VERSION}"
> declare -r CHARM_COMM=3D'mpi'
> declare -r CHARM_ARCH=3D"${CHARM_COMM}-${OS}-${ARCH}"
> declare -r CHARM_OPTIONS=3D'mpicxx'
>
> declare -r FFTW_NAME=3D'fftw'
> declare -r FFTW_TARBALL=3D"${FFTW_NAME}-${OS}-${ARCH}.tar.gz"
>
> declare -r TCL_NAME=3D'tcl'
> declare -r TCL_VERSION=3D'8.5.9'
> declare -r TCL_TARBALL=3D"${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}.tar.gz"
> declare -r TCL_THREADED_TARBALL=3D"${TCL_NAME}${TCL_VERSION}-${OS}-${
> ARCH}-threaded.tar.gz"
>
> module purge
> module load "${COMPILER_MODULE}"
> module load "${MPI_MODULE}"
>
> tar -xzvf "${PWD}/tarballs/${NAMD_TARBALL}"
> cd "${NAMD_DIR}"
>
> wget "${NAMD_URL}/libraries/${FFTW_TARBALL}"
> wget "${NAMD_URL}/libraries/${TCL_TARBALL}"
> wget "${NAMD_URL}/libraries/${TCL_THREADED_TARBALL}"
>
> tar -xvf "${CHARM_TARFILE}"
> tar -xzvf "${FFTW_TARBALL}"
> tar -xzvf "${TCL_TARBALL}"
> tar -xzvf "${TCL_THREADED_TARBALL}"
>
> mv "${OS}-${ARCH}" fftw
> mv "${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}" tcl
> mv "${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}-threaded" tcl-threaded
>
> cd "${CHARM_DIR}"
> ./build charm++ "${CHARM_ARCH}" "${CHARM_OPTIONS}" --with-production
>
> cd ../
> ./config "${NAMD_ARCH}" --charm-arch "${CHARM_ARCH}-${CHARM_OPTIONS}"
> cd "${NAMD_ARCH}"
> make
>
>
>
> ------------------------------
> *From:* Haseeb Mahmud [smahm...@gmail.com]
> *Sent:* Thursday, February 15, 2018 6:48 PM
> *To:* singu...@lbl.gov
> *Cc:* vict...@gmail.com
> *Subject:* Re: [Singularity] singularity container on HPC
>
> Hello,
>
> Since my email I rebuilt my container with NAMD linked against OpenMPI
> version 2.10. I then ran my container on my host which also uses OpenMPI
> version 2.10.. And I get new errors. Looks like now now it is using both
> nodes but I get the below errors.
>
>
> [compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received: 2=
55
> [compute-0.cluster:64549] unknown address family for tcp: 0
> [compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received: 2=
55
> [compute-0.cluster:64549] unknown address family for tcp: 0
> *** Error in `/NAMD_2.12_Source/Linux-x86_64-g++/namd2': munmap_chunk():
> invalid pointer: 0x00000000098c5cb0 ***
> *** Error in `/NAMD_2.12_Source/Linux-x86_64-g++/namd2': corrupted
> double-linked list: 0x0000000008b4b540 ***
>
> Charm++> Running on MPI version: 3.1
> Charm++> level of thread support used: MPI_THREAD_SINGLE (desired:
> MPI_THREAD_SINGLE)
> Charm++> Running in non-SMP mode: numPes 88
> Charm++> Using recursive bisection (scheme 3) for topology aware partitio=
ns
> Converse/Charm++ Commit ID: v6.7.1-0-gbdf6a1b-namd-charm-
> 6.7.1-build-2016-Nov-07-136676
> Warning> Randomization of stack pointer is turned on in kernel, thread
> migration may not work! Run 'echo 0 > /proc/sys/kernel/randomize_va_space=
'
> as root to disable it, or try run with '+isomalloc_sync'.
> CharmLB> Load balancer assumes all CPUs are same.
> Charm++> Running on 2 unique compute nodes (88-way SMP).
> Charm++> cpu topology info is gathered in 0.019 seconds.
>
> Info: Startup phase 9 took 0.00478208 s, 1237.92 MB of memory in use
> Info: CREATING 44375 COMPUTE OBJECTS
> Info: Startup phase 10 took 0.0075435 s, 1237.92 MB of memory in use
> Info: Startup phase 11 took 0.000689846 s, 1237.92 MB of memory in use
> Info: Startup phase 12 took 4.81852e-05 s, 1237.92 MB of memory in use
> Info: Finished startup at 13.4259 s, 1237.92 MB of memory in use
>
> Info: useSync: 0 useProxySync: 0
> [compute-0:64562] *** Process received signal ***
> [compute-0:64562] Signal: Segmentation fault (11)
> [compute-0:64562] Signal code: Address not mapped (1)
> [compute-0:64562] Failing at address: 0x7
> [compute-0:64699] *** Process received signal ***
> [compute-0:64699] Signal: Segmentation fault (11)
> [compute-0:64699] Signal code: Address not mapped (1)
> [compute-0:64699] Failing at address: 0xffffffffffffffff
> [compute-0:64737] *** Process received signal ***
> [compute-0:64737] Signal: Segmentation fault (11)
>
>
>
> Haseeb
>
>
>
> On Thu, Feb 15, 2018 at 10:07 AM, victor sv <vict...@gmail.com> wrote:
>
>> HI Haseeb,
>>
>> first of all I would like to understand with MPI family and version is
>> running in and out the containers.
>>
>> NAMD is linked agains OpenMPI or MPICH?
>>
>> Which MPI family and version is running in the host? it should be enough
>> if you show the output of "mpirun --version".
>>
>> BR,
>> V=C3=ADctor.
>>
>> 2018-02-15 14:33 GMT+01:00 Haseeb Mahmud <smahm...@gmail.com>:
>>
>>> Hello,
>>>
>>> I have built a namd container that uses the MPI build of NAMD 2.12 that
>>> I built from source. I am trying to execute  this container  on my HPC
>>> using mpirun on my host slurm script using multiple nodes , however,
>>> although the job runs , it looks to be only using 1 processer and 1 nod=
e.
>>>
>>> I built my container from a script  with an Ubuntu operating syytem and
>>> In my %post section, I first configure and make openmpi-2.1.0  then i
>>> install mpich using "apt install mpich"  because my MPI build of NAMD w=
ont
>>> compile without it.  Then in my %post section I also build the namd 2.1=
2
>>> MPI build itself as all the NAMD source files are in my container as we=
ll.
>>>
>>> In my slurm script when I run " mpirun -np #P singularity exec
>>> namdimage.simg /path_to namd_executable_in_container/namd2 inputfile"
>>> ,I get the problem  of no scaling.
>>>
>>>
>>>
>>>
>>> My sample out looks like :
>>> Charm++> Running on MPI version: 3.0
>>> Charm++> level of thread support used: MPI_THREAD_SINGLE (desired:
>>> MPI_THREAD_SINGLE)
>>> Charm++> Running in non-SMP mode: numPes 1
>>> Charm++> Using recursive bisection (scheme 3) for topology aware
>>> partitions
>>> ..........
>>>
>>> Info: Running on 1 processors, 1 nodes, 1 physical nodes.
>>>
>>>
>>> Any ideas what I may be doing wrong.
>>>
>>> Thanks,
>>>
>>> Haseeb
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--f4f5e80328d096eaf5056557b7ce
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hello,<div><br></div><div>=C2=A0I am able to build and run=
=C2=A0 MPI version of NAMD fine on my cluster outside a container. I just w=
ould like to run it within a container as well just as a test with singular=
ity. Is there any other dependencies.libraries=C2=A0 I should be adding to =
my container other than Open MPI 2.1.0.=C2=A0 My HPC&#39;=C2=A0 network is=
=C2=A0















<span style=3D"font-size:12pt;font-family:Calibri,sans-serif;color:black">E=
thernet (TCP), and not infiniband.</span></div><div><span style=3D"font-siz=
e:12pt;font-family:Calibri,sans-serif;color:black"><br></span></div><div><s=
pan style=3D"font-size:12pt;font-family:Calibri,sans-serif;color:black">Reg=
ards,</span></div><div><span style=3D"font-size:12pt;font-family:Calibri,sa=
ns-serif;color:black"><br></span></div><div><span style=3D"font-size:12pt;f=
ont-family:Calibri,sans-serif;color:black">Haseeb</span></div></div><div cl=
ass=3D"gmail_extra"><br><div class=3D"gmail_quote">On Thu, Feb 15, 2018 at =
10:31 PM, Kandes, Martin <span dir=3D"ltr">&lt;<a href=3D"mailto:mka...@sds=
c.edu" target=3D"_blank">mka...@sdsc.edu</a>&gt;</span> wrote:<br><blockquo=
te class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc so=
lid;padding-left:1ex">




<div>
<div style=3D"direction:ltr;font-family:Tahoma;color:#000000;font-size:10pt=
">Hi Haseed,<br>
<br>
I guess my first question would be: why do you need to build NAMD within a =
Singularity container? I&#39;ve built it on a few systems before with diffe=
rent build options. It usually plays nicely with most systems. If you&#39;r=
e doing an &#39;mpi&#39; comm=C2=A0 build, here is a
 build script I&#39;ve used before [1]. Maybe it&#39;ll help. <br>
<br>
Marty<br>
<br>
[1]<br>
<br>
#!/bin/bash<br>
#<br>
# A build script for NAMD<br>
<br>
declare -r COMPILER_MODULE=3D&#39;gnu/4.9.2&#39;<br>
declare -r MPI_MODULE=3D&#39;mvapich2_ib/2.1&#39;<br>
<br>
declare -r OS=3D&#39;linux&#39;<br>
declare -r ARCH=3D&#39;x86_64&#39;<br>
<br>
declare -r NAMD_NAME=3D&#39;NAMD&#39;<br>
declare -r NAMD_VERSION=3D&#39;2.12&#39;<br>
declare -r NAMD_TARBALL=3D&quot;${NAMD_NAME}_${<wbr>NAMD_VERSION}_Source.ta=
r.gz&quot;<br>
declare -r NAMD_DIR=3D&quot;${NAMD_NAME}_${NAMD_<wbr>VERSION}_Source&quot;<=
br>
declare -r NAMD_URL=3D&#39;<a href=3D"http://www.ks.uiuc.edu/Research/namd"=
 target=3D"_blank">http://www.ks.uiuc.<wbr>edu/Research/namd</a>&#39;<br>
declare -r NAMD_COMPILER=3D&#39;g++&#39;<br>
declare -r NAMD_ARCH=3D&quot;Linux-${ARCH}-${<wbr>NAMD_COMPILER}&quot;<br>
<br>
declare -r CHARM_NAME=3D&#39;charm&#39;<br>
declare -r CHARM_VERSION=3D&#39;6.7.1&#39;<br>
declare -r CHARM_TARFILE=3D&quot;${CHARM_NAME}-$<wbr>{CHARM_VERSION}.tar&qu=
ot;<br>
declare -r CHARM_DIR=3D&quot;${CHARM_NAME}-${<wbr>CHARM_VERSION}&quot;<br>
declare -r CHARM_COMM=3D&#39;mpi&#39;<br>
declare -r CHARM_ARCH=3D&quot;${CHARM_COMM}-${<wbr>OS}-${ARCH}&quot;<br>
declare -r CHARM_OPTIONS=3D&#39;mpicxx&#39;<br>
<br>
declare -r FFTW_NAME=3D&#39;fftw&#39;<br>
declare -r FFTW_TARBALL=3D&quot;${FFTW_NAME}-${<wbr>OS}-${ARCH}.tar.gz&quot=
;<br>
<br>
declare -r TCL_NAME=3D&#39;tcl&#39;<br>
declare -r TCL_VERSION=3D&#39;8.5.9&#39;<br>
declare -r TCL_TARBALL=3D&quot;${TCL_NAME}${TCL_<wbr>VERSION}-${OS}-${ARCH}=
.tar.gz&quot;<br>
declare -r TCL_THREADED_TARBALL=3D&quot;${TCL_<wbr>NAME}${TCL_VERSION}-${OS=
}-${<wbr>ARCH}-threaded.tar.gz&quot;<br>
<br>
module purge<br>
module load &quot;${COMPILER_MODULE}&quot;<br>
module load &quot;${MPI_MODULE}&quot;<br>
<br>
tar -xzvf &quot;${PWD}/tarballs/${NAMD_<wbr>TARBALL}&quot;<br>
cd &quot;${NAMD_DIR}&quot;<br>
<br>
wget &quot;${NAMD_URL}/libraries/${FFTW_<wbr>TARBALL}&quot;<br>
wget &quot;${NAMD_URL}/libraries/${TCL_<wbr>TARBALL}&quot;<br>
wget &quot;${NAMD_URL}/libraries/${TCL_<wbr>THREADED_TARBALL}&quot;<br>
<br>
tar -xvf &quot;${CHARM_TARFILE}&quot;<br>
tar -xzvf &quot;${FFTW_TARBALL}&quot;<br>
tar -xzvf &quot;${TCL_TARBALL}&quot;<br>
tar -xzvf &quot;${TCL_THREADED_TARBALL}&quot;<br>
<br>
mv &quot;${OS}-${ARCH}&quot; fftw<br>
mv &quot;${TCL_NAME}${TCL_VERSION}-${<wbr>OS}-${ARCH}&quot; tcl<br>
mv &quot;${TCL_NAME}${TCL_VERSION}-${<wbr>OS}-${ARCH}-threaded&quot; tcl-th=
readed<br>
<br>
cd &quot;${CHARM_DIR}&quot;<br>
./build charm++ &quot;${CHARM_ARCH}&quot; &quot;${CHARM_OPTIONS}&quot; --wi=
th-production<br>
<br>
cd ../<br>
./config &quot;${NAMD_ARCH}&quot; --charm-arch &quot;${CHARM_ARCH}-${CHARM_=
<wbr>OPTIONS}&quot;<br>
cd &quot;${NAMD_ARCH}&quot;<br>
make<br>
<br>
<br>
<br>
<div style=3D"font-family:Times New Roman;color:#000000;font-size:16px">
<hr>
<div id=3D"m_8833932117365074541divRpF234675" style=3D"direction:ltr"><font=
 size=3D"2" color=3D"#000000" face=3D"Tahoma"><b>From:</b> Haseeb Mahmud [<=
a href=3D"mailto:smahm...@gmail.com" target=3D"_blank">smahm...@gmail.com</=
a>]<br>
<b>Sent:</b> Thursday, February 15, 2018 6:48 PM<br>
<b>To:</b> <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@l=
bl.gov</a><br>
<b>Cc:</b> <a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@g=
mail.com</a><br>
<b>Subject:</b> Re: [Singularity] singularity container on HPC<br>
</font><br>
</div><div><div class=3D"h5">
<div></div>
<div>
<div dir=3D"ltr">Hello,
<div><br>
</div>
<div>Since my email I rebuilt my container with NAMD linked against OpenMPI=
 version 2.10. I then ran my container on my host which also uses OpenMPI v=
ersion 2.10.. And I get new errors. Looks like now now it is using both nod=
es but I get the below errors.</div>
<div><br>
</div>
<div><br>
</div>
<div>
<div>[compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received=
: 255</div>
<div>[compute-0.cluster:64549] unknown address family for tcp: 0</div>
<div>[compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received=
: 255</div>
<div>[compute-0.cluster:64549] unknown address family for tcp: 0</div>
<div>*** Error in `/NAMD_2.12_Source/Linux-x86_<wbr>64-g++/namd2&#39;: munm=
ap_chunk(): invalid pointer: 0x00000000098c5cb0 ***</div>
<div>*** Error in `/NAMD_2.12_Source/Linux-x86_<wbr>64-g++/namd2&#39;: corr=
upted double-linked list: 0x0000000008b4b540 ***</div>
</div>
<div><br>
</div>
<div>
<div>Charm++&gt; Running on MPI version: 3.1</div>
<div>Charm++&gt; level of thread support used: MPI_THREAD_SINGLE (desired: =
MPI_THREAD_SINGLE)</div>
<div>Charm++&gt; Running in non-SMP mode: numPes 88</div>
<div>Charm++&gt; Using recursive bisection (scheme 3) for topology aware pa=
rtitions</div>
<div>Converse/Charm++ Commit ID: v6.7.1-0-gbdf6a1b-namd-charm-<wbr>6.7.1-bu=
ild-2016-Nov-07-136676</div>
<div>Warning&gt; Randomization of stack pointer is turned on in kernel, thr=
ead migration may not work! Run &#39;echo 0 &gt; /proc/sys/kernel/randomize=
_va_<wbr>space&#39; as root to disable it, or try run with &#39;+isomalloc_=
sync&#39;.</div>
<div>CharmLB&gt; Load balancer assumes all CPUs are same.</div>
<div>Charm++&gt; Running on 2 unique compute nodes (88-way SMP).</div>
<div>Charm++&gt; cpu topology info is gathered in 0.019 seconds.</div>
</div>
<div><br>
</div>
<div>
<div>Info: Startup phase 9 took 0.00478208 s, 1237.92 MB of memory in use</=
div>
<div>Info: CREATING 44375 COMPUTE OBJECTS</div>
<div>Info: Startup phase 10 took 0.0075435 s, 1237.92 MB of memory in use</=
div>
<div>Info: Startup phase 11 took 0.000689846 s, 1237.92 MB of memory in use=
</div>
<div>Info: Startup phase 12 took 4.81852e-05 s, 1237.92 MB of memory in use=
</div>
<div>Info: Finished startup at 13.4259 s, 1237.92 MB of memory in use</div>
<div><br>
</div>
<div>Info: useSync: 0 useProxySync: 0</div>
<div>[compute-0:64562] *** Process received signal ***</div>
<div>[compute-0:64562] Signal: Segmentation fault (11)</div>
<div>[compute-0:64562] Signal code: Address not mapped (1)</div>
<div>[compute-0:64562] Failing at address: 0x7</div>
<div>[compute-0:64699] *** Process received signal ***</div>
<div>[compute-0:64699] Signal: Segmentation fault (11)</div>
<div>[compute-0:64699] Signal code: Address not mapped (1)</div>
<div>[compute-0:64699] Failing at address: 0xffffffffffffffff</div>
<div>[compute-0:64737] *** Process received signal ***</div>
<div>[compute-0:64737] Signal: Segmentation fault (11)</div>
</div>
<div><br>
</div>
<div><br>
</div>
<div><br>
</div>
<div>Haseeb=C2=A0</div>
<div><br>
</div>
<div><br>
</div>
</div>
<div class=3D"gmail_extra"><br>
<div class=3D"gmail_quote">On Thu, Feb 15, 2018 at 10:07 AM, victor sv <spa=
n dir=3D"ltr">
&lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@gmail.co=
m</a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">
<div dir=3D"ltr">
<div>
<div>
<div>
<div>
<div>HI Haseeb,<br>
<br>
</div>
first of all I would like to understand with MPI family and version is runn=
ing in and out the containers.<br>
<br>
</div>
NAMD is linked agains OpenMPI or MPICH?<br>
<br>
</div>
Which MPI family and version is running in the host? it should be enough if=
 you show the output of &quot;mpirun --version&quot;.<br>
<br>
</div>
BR,<br>
</div>
V=C3=ADctor.<br>
</div>
<div class=3D"gmail_extra"><br>
<div class=3D"gmail_quote">
<div>
<div class=3D"m_8833932117365074541h5">2018-02-15 14:33 GMT+01:00 Haseeb Ma=
hmud <span dir=3D"ltr">&lt;<a href=3D"mailto:smahm...@gmail.com" target=3D"=
_blank">smahm...@gmail.com</a>&gt;</span>:<br>
</div>
</div>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">
<div>
<div class=3D"m_8833932117365074541h5">
<div dir=3D"ltr">Hello,
<div><br>
</div>
<div>I have built a namd container that uses the MPI build of NAMD 2.12 tha=
t I built from source. I am trying to execute=C2=A0 this container=C2=A0 on=
 my HPC using mpirun on my host slurm script using multiple nodes , however=
, although the job runs , it looks to be only
 using 1 processer and 1 node.=C2=A0</div>
<div><br>
</div>
<div>I built my container from a script=C2=A0 with an Ubuntu operating syyt=
em and In my %post section, I first configure and make openmpi-2.1.0=C2=A0 =
then i install mpich using &quot;apt install mpich&quot;=C2=A0 because my M=
PI build of NAMD wont compile without it.=C2=A0 Then in my %post
 section I also build the namd 2.12 MPI build itself as all the NAMD source=
 files are in my container as well.</div>
<div><br>
</div>
<div>In my slurm script when I run &quot; mpirun -np #P singularity exec=C2=
=A0 namdimage.simg /path_to namd_executable_in_container/n<wbr>amd2 inputfi=
le&quot; ,I get the problem=C2=A0 of no scaling.</div>
<div><br>
</div>
<div><br>
</div>
<div><br>
</div>
<div><br>
</div>
<div>My sample out looks like :</div>
<div>
<div>Charm++&gt; Running on MPI version: 3.0</div>
<div>Charm++&gt; level of thread support used: MPI_THREAD_SINGLE (desired: =
MPI_THREAD_SINGLE)</div>
<div>Charm++&gt; Running in non-SMP mode: numPes 1</div>
<div>Charm++&gt; Using recursive bisection (scheme 3) for topology aware pa=
rtitions</div>
</div>
<div>..........</div>
<div><br>
</div>
<div>
<div>Info: Running on 1 processors, 1 nodes, 1 physical nodes.</div>
</div>
<div><br>
</div>
<div><br>
</div>
<div>Any ideas what I may be doing wrong.</div>
<div><br>
</div>
<div>Thanks,</div>
<div><br>
</div>
<div>Haseeb</div>
</div>
</div>
</div>
<span class=3D"m_8833932117365074541HOEnZb"><font color=3D"#888888"><span c=
lass=3D"m_8833932117365074541m_-6963216037349168429HOEnZb"><font color=3D"#=
888888">
<p></p>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to
<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscri=
be@lbl.go<wbr>v</a>.<br>
</font></span></font></span></blockquote>
</div>
<span class=3D"m_8833932117365074541HOEnZb"><font color=3D"#888888"><br>
</font></span></div>
<span class=3D"m_8833932117365074541HOEnZb"><font color=3D"#888888">
<p></p>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to
<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscri=
be@lbl.go<wbr>v</a>.<br>
</font></span></blockquote>
</div>
<br>
</div>
<p></p>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to
<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscri=
be@lbl.<wbr>gov</a>.<br>
</div>
</div></div></div>
</div>
</div><div class=3D"HOEnZb"><div class=3D"h5">


<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--f4f5e80328d096eaf5056557b7ce--
