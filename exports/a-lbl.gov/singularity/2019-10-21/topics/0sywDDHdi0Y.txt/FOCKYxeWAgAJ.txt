X-Received: by 10.200.42.242 with SMTP id c47mr31762111qta.22.1466599374618;
        Wed, 22 Jun 2016 05:42:54 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.15.223 with SMTP id 92ls1492727iop.99.gmail; Wed, 22 Jun
 2016 05:42:54 -0700 (PDT)
X-Received: by 10.98.157.135 with SMTP id a7mr34717308pfk.117.1466599374080;
        Wed, 22 Jun 2016 05:42:54 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id z62si46900616pfb.179.2016.06.22.05.42.53
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 22 Jun 2016 05:42:54 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.71 as permitted sender) client-ip=74.125.82.71;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.71 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2DpAgCih2pXhkdSfUpehBRuDwaDNqYDhHCHBIUBgXoXAQaFeQKBJQc5EwEBAQEBAQESAQEBCAsLCSEkC4RMAQEBAwESESsZIgsLDSoCAiEBDwMBBQELEQYIBwQBHAQBh3QDDwgFpHKBMT4xizuMQg2DdQEBAQcBAQEBAQEdBAsFimSCQ4FPEQEGgxeCWgWIcoV6hF6EfzQBhgeGK4F6gWlOhAWIZ4gNhjASHoEPDxEBgkyBdxwyB4k1gTUBAQE
X-IronPort-AV: E=Sophos;i="5.26,509,1459839600"; 
   d="scan'208,217";a="27093406"
Received: from mail-wm0-f71.google.com ([74.125.82.71])
  by fe4.lbl.gov with ESMTP; 22 Jun 2016 05:42:52 -0700
Received: by mail-wm0-f71.google.com with SMTP id r190so1766902wmr.0
        for <singu...@lbl.gov>; Wed, 22 Jun 2016 05:42:51 -0700 (PDT)
X-Gm-Message-State: ALyK8tLiSc7LUywIVyXFemNW7YF/eH7opakNngVe9D/ZOvALTnHUpyq6cnhlL41pUZcNHk4xLY6cXM7pZFVJxveRYlwk435mEA3IfNgKxOaFRYyh3vzP3oPZFAypayX5ThVy7r8H5uFwdo7K3Hej7DgsOq4=
X-Received: by 10.25.24.80 with SMTP id o77mr7117330lfi.23.1466599370470;
        Wed, 22 Jun 2016 05:42:50 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.25.24.80 with SMTP id o77mr7117325lfi.23.1466599370070; Wed,
 22 Jun 2016 05:42:50 -0700 (PDT)
Received: by 10.25.214.158 with HTTP; Wed, 22 Jun 2016 05:42:49 -0700 (PDT)
In-Reply-To: <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
	<CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
	<3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov>
Date: Wed, 22 Jun 2016 05:42:49 -0700
Message-ID: <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: "singu...@lbl.gov" <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a114076b27b34470535dd4699

--001a114076b27b34470535dd4699
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Raimon,

The quick answer is you have mpirun handle that as you would normally where
the container file lives on a shared file system:

$ mpirun singularity exec ~/container.img mpi_prog_in_container

Let the MPI outside the container launch the singularity container on each
host as it would normally launch any MPI program. Then it will call
Singulairty and Singularity will launch the MPI program inside the
container on each of your hosts/servers.

Hope that helps!



On Wednesday, June 22, 2016, Raimon Bosch <raimo...@gmail.com> wrote:

>
> Hi Gregory,
>
> Thank you for your answer. One of our experiments needs to run OpenMPI
> among several servers. This means that we should put one of our container=
s
> in host01, another in host02 and another in host03 and collect the result=
s.
>
> How can I do this execution in parallel if I need to communicate with mor=
e
> than one server?
>
> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer
> escribi=C3=B3:
>>
>> Hi Raimon,
>>
>> The communication model of a Singularity container is very different fro=
m
>> that of a Docker implementation. This is because Docker for all practica=
l
>> purposes emulates a virtual machine as each container has it's own IP
>> address and thus it's own ssh server. It also carries its own set of
>> complexities, for example networks need to be segregated/VLan'ed, DNS/ho=
st
>> resolution needs to be dynamic and passed down to the containers (so the=
y
>> can reach each other), ssh daemons and other process running inside the
>> containers, management via an existing scheduling system, and the list g=
oes
>> on and on.
>>
>> Think of it this way, Singularity does not do any of that... It runs a
>> program within the container as if it were running on the host itself, s=
o
>> to communicate between containers is as easy as communicating between
>> programs. So for MPI, it would happen with the MPI on the physical host
>> (outside the container) invoking the container subsystem which then invo=
kes
>> the MPI programs within the container and the MPI programs within the
>> container communicate back to the MPI (orted) outside the container on t=
he
>> host to get access to the host resources. In this model all available
>> resources and infrastructure can be leveraged at full bandwidth by the
>> contained processes and all of the aforementioned complexities akin to
>> running on a virtualized mini-cluster are circumvented.
>>
>> There is additional information I have written at:
>>
>> http://singularity.lbl.gov/#hpc
>>
>> That page is still coming along, and needs more information still but if
>> you have any questions, comments or change proposals please let us know!
>>
>> Thanks and hope that helps!
>>
>>
>>
>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>
>> wrote:
>>
>>>
>>>
>>> Hi,
>>>
>>> We are trying to run experiments using singularity containers. The idea
>>> is to run OpenMPI among several containers and check performance result=
s.
>>>
>>> How can I communicate with another container? In docker this is clear
>>> because every container gets an assigned IP and you can ping there, but
>>> what is the situation in the case of singularity? Is it possible to ass=
ign
>>> an IP to each container? Can I connect via ssh to them?
>>>
>>> Thanks in advance,
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>


--=20
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a114076b27b34470535dd4699
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Raimon,<div><br></div><div>The quick answer is you have mpirun handle th=
at as you would normally where the container file lives on a shared file sy=
stem:</div><div><br></div><div>$ mpirun singularity exec ~/container.img mp=
i_prog_in_container</div><div><br></div><div>Let the MPI outside the contai=
ner launch the singularity container on each host as it would normally laun=
ch any MPI program. Then it will call Singulairty and Singularity will laun=
ch the MPI program inside the container on each of your hosts/servers.=C2=
=A0</div><div><br></div><div>Hope that helps!</div><div><br></div><div><spa=
n></span><br><br>On Wednesday, June 22, 2016, Raimon Bosch &lt;<a href=3D"m=
ailto:raimo...@gmail.com">raimo...@gmail.com</a>&gt; wrote:<br><blockquote =
class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid=
;padding-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<br><br>Thank you for yo=
ur answer. One of our experiments needs to run OpenMPI among several server=
s. This means that we should put one of our containers in host01, another i=
n host02 and another in host03 and collect the results. <br><br>How can I d=
o this execution in parallel if I need to communicate with more than one se=
rver?<br><br>El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. K=
urtzer  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0;m=
argin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"l=
tr">Hi Raimon,<div><br></div><div>The communication model of a Singularity =
container is very different from that of a Docker implementation. This is b=
ecause Docker for all practical purposes emulates a virtual machine as each=
 container has it&#39;s own IP address and thus it&#39;s own ssh server. It=
 also carries its own set of complexities, for example networks need to be =
segregated/VLan&#39;ed, DNS/host resolution needs to be dynamic and passed =
down to the containers (so they can reach each other), ssh daemons and othe=
r process running inside the containers, management via an existing schedul=
ing system, and the list goes on and on.</div><div><br></div><div>Think of =
it this way, Singularity does not do any of that... It runs a program withi=
n the container as if it were running on the host itself, so to communicate=
 between containers is as easy as communicating between programs. So for MP=
I, it would happen with the MPI on the physical host (outside the container=
) invoking the container subsystem which then invokes the MPI programs with=
in the container and the MPI programs within the container communicate back=
 to the MPI (orted) outside the container on the host to get access to the =
host resources. In this model all available resources and infrastructure ca=
n be leveraged at full bandwidth by the contained processes and all of the =
aforementioned complexities akin to running on a virtualized mini-cluster a=
re circumvented.</div><div><br></div><div>There is additional information I=
 have written at:</div><div><br></div><div><a href=3D"http://singularity.lb=
l.gov/#hpc" rel=3D"nofollow" target=3D"_blank">http://singularity.lbl.gov/#=
hpc</a><br></div><div><br></div><div>That page is still coming along, and n=
eeds more information still but if you have any questions, comments or chan=
ge proposals please let us know!</div><div><br></div><div>Thanks and hope t=
hat helps!</div><div><br></div><div><br></div></div><div><br><div class=3D"=
gmail_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <span dir=3D"ltr=
">&lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt;</span> wrote:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc =
solid;padding-left:1ex"><div dir=3D"ltr"><br><br>Hi,<br><br>We are trying t=
o run experiments using singularity containers. The idea is to run OpenMPI =
among several containers and check performance results. <br><br>How can I c=
ommunicate with another container? In docker this is clear because every co=
ntainer gets an assigned IP and you can ping there, but what is the situati=
on in the case of singularity? Is it possible to assign an IP to each conta=
iner? Can I connect via ssh to them?<br><br>Thanks in advance,<span><font c=
olor=3D"#888888"><br></font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computi=
ng Services (HPCS)<br>University of California<br>Lawrence Berkeley Nationa=
l Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div>Gregory M. Kurtzer<=
br>High Performance Computing Services (HPCS)<br>University of California<b=
r>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA=
 94720</div></div><br>

--001a114076b27b34470535dd4699--
