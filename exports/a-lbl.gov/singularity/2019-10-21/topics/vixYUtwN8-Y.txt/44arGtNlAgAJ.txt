Date: Mon, 18 Jul 2016 08:16:30 -0700 (PDT)
From: Steve Mehlberg <sgmeh...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <9f8d98db-a276-4607-a699-59cde5ce0b94@lbl.gov>
In-Reply-To: <CAN7etTx3rzFp_SZ9w_XrgX=04JufSBhHm2G9vqMd9WNU9aGV9Q@mail.gmail.com>
References: <03a19fb0-27ce-43c4-9400-8e58cf726500@lbl.gov> <CAN7etTwRbSe1MMh9wdQAMYoKVJTb_SGJeHto+WrZ=aU7NoBmhQ@mail.gmail.com>
 <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov> <CAN7etTyqGkWy1P57-cVJgyru5BT_DvnhwDzLe1p38BV8z_PPww@mail.gmail.com>
 <66b82c74-2778-44f2-ae5c-87e01ec8885d@lbl.gov> <CAN7etTz09uzfP-NpZ3-+HnijfrC+u+=pOuBQDTShXG+unxgOVg@mail.gmail.com>
 <c4e5864c-cbaa-4269-9522-db61d63d7cff@lbl.gov> <CAN7etTztBSE1YXY3etq9ipMNnPSKh2Eatz5i-QQOH=ecdNDVCg@mail.gmail.com>
 <0686e644-e7d6-45d7-a371-bf17bead57a4@lbl.gov> <CAN7etTy7cLFaG_NK8-izwc4V-WiU6tsGqNwyVY79TPND9eZnrw@mail.gmail.com>
 <f942d15c-1e7f-401a-8f6b-01ecfec1c7e9@lbl.gov> <CAN7etTz8Sj3CvHsn6ywyLVxfk3g5+kPy021sBbvXCZTtjsyi4w@mail.gmail.com>
 <8d23e9ce-1ce5-4ddb-ab81-f3aafdec4577@lbl.gov> <CAN7etTxyzBC_VSa8Zv6RVEe1C5gETyaMJM=Dkzej94jWaMnbwA@mail.gmail.com>
 <CAN7etTx3rzFp_SZ9w_XrgX=04JufSBhHm2G9vqMd9WNU9aGV9Q@mail.gmail.com>
Subject: Re: [Singularity] Image is locked by another process
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_3972_1758367883.1468854990911"

------=_Part_3972_1758367883.1468854990911
Content-Type: multipart/alternative; 
	boundary="----=_Part_3973_1138197283.1468854990912"

------=_Part_3973_1138197283.1468854990912
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Gregory,

I have rebuilt with the latest from master this morning and repopulated 
everything on my small virtual cluster - mach.  Good news!  I am unable to 
recreate the problem.  One of my colleagues had a similar or perhaps the 
same problem on a larger bare metal system.  We will update that system 
with the latest master and test to see if the problem still occurs. 

Thanks for your help,

Steve

On Friday, July 15, 2016 at 5:07:27 PM UTC-6, Gregory M. Kurtzer wrote:
>
> Hi Steve,
>
> Sorry for the delay. The cause of the first error was an easy fix, but 
> debugging the second had me really stumped for a while! It just so happens 
> that I was on a conference call with several other developers a bit ago, 
> and when we were going over this Ben Allen (LANL) pointed me in a new 
> direction to look, and sure enough it was the loop driver in the kernel 
> holding the file descriptor that contained the lock open! So to fix that 
> and ensure that it won't happen again, I decided to use the 
> LO_FLAGS_AUTOCLEAR flag option, but that brings up a potential race 
> condition. Which luckily, after much stumpedness (and reviewing the loop 
> kernel source) I think that I came up with a very simple workaround of 
> simply opening the loop device itself so it doesn't auto clear itself until 
> each process has completed! Anyway, long story short... 
>
> Can you test the latest master and let me know if it is indeed fixed!
>
> Thanks!
>
>
>
> On Fri, Jul 15, 2016 at 11:28 AM, Gregory M. Kurtzer <gm...@lbl.gov 
> <javascript:>> wrote:
>
>> Hi Steve,
>>
>> I think I have fixed the initial error you got regarding being unable to 
>> create the sessiondir. The second issue regarding file locks, I am 
>> debugging now. I'll let you know shortly when to retest.
>>
>> Thanks!
>>
>>
>>
>> On Fri, Jul 15, 2016 at 11:26 AM, Steve Mehlberg <sg...@gmail.com 
>> <javascript:>> wrote:
>>
>>> The problem only arises when there is an error.
>>> The tmp directory is different on each machine, it is not shared or an 
>>> NFS mount.  Is that a problem?
>>>
>>> Steve
>>>
>>> On Friday, July 15, 2016 at 9:51:55 AM UTC-6, Gregory M. Kurtzer wrote:
>>>
>>>>
>>>> On Fri, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <sg...@gmail.com> 
>>>> wrote:
>>>>
>>>>> Hello,
>>>>>
>>>>> I went back and recreated everything from scratch using v2.1.  When I 
>>>>> moved the image file to the mach1 and mach2 nodes I used tar with the -S 
>>>>> option.  I used /tmp for the image files.  After everything was set I ran 
>>>>> mpirun on each individual system (only starting jobs on that system) with 
>>>>> no errors.  For some reason when I ran mpirun from mach0 and asked for jobs 
>>>>> to be run on mach0 and another system I get the error.  The mpirun 
>>>>> complains about not being able to create a file (already exists) and 
>>>>> hangs.  The singularity image file on mach0 and the other system selected 
>>>>> are locked with pids that are no longer running.
>>>>>
>>>>> Saying all that, I decided to go back and put --debug on the 
>>>>> singularity command in the mpirun to get you better diagnostics.  I 
>>>>> rebooted mach0 and mach2, then reran the command and it worked correctly.  
>>>>> I then removed the --debug command and it worked.  I added mach1 to the mix 
>>>>> and everything worked fine.  What? I guess the bottom line is that if the 
>>>>> mpirun aborts (and it seems to hang) then you can get this condition.
>>>>>
>>>>
>>>> So does the problem seem to arise only when there is a failure on one 
>>>> of the processes, and MPI kills the remaining processes?
>>>>
>>>> Ralph, do you know how OMPI kills the leftover processes in a job abort 
>>>> and what signal it uses?
>>>>
>>>> (additional comments inline)
>>>>
>>>>
>>>>> Steve
>>>>>
>>>>>
>>>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 2 -H mach2 
>>>>> singularity exec /tmp/c7new.img /usr/bin/ring
>>>>> Process 1 exiting
>>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>>> Process 0 sent to 1
>>>>> Process 0 decremented value: 9
>>>>> Process 0 decremented value: 8
>>>>> Process 0 decremented value: 7
>>>>> Process 0 decremented value: 6
>>>>> Process 0 decremented value: 5
>>>>> Process 0 decremented value: 4
>>>>> Process 0 decremented value: 3
>>>>> Process 0 decremented value: 2
>>>>> Process 0 decremented value: 1
>>>>> Process 0 decremented value: 0
>>>>> Process 0 exiting
>>>>>
>>>>
>>>> Is /tmp NFS mounted or shared?
>>>>  
>>>>
>>>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 4 -H mach0,mach2 
>>>>> singularity exec /tmp/c7new.img /usr/bin/ring
>>>>> ERROR  : Could not create directory 
>>>>> /tmp/.singularity-session-0.64768.158221: File exists
>>>>> ERROR  : Failed creating session directory: 
>>>>> /tmp/.singularity-session-0.64768.158221
>>>>> -------------------------------------------------------
>>>>> Primary job  terminated normally, but 1 process returned
>>>>> a non-zero exit code. Per user-direction, the job has been aborted.
>>>>> -------------------------------------------------------
>>>>> ^CKilled by signal 2.
>>>>> mpirun: abort is already in progress...hit ctrl-c again to forcibly 
>>>>> terminate
>>>>>
>>>>
>>>> Ahhhhh, I think you may have found a race condition in Singularity 
>>>> which killed one of the processes. Then the MPI killed the rest.
>>>>  
>>>>
>>>>> [root@mach0 ompi]# singularity shell -w /tmp/c7new.img
>>>>> ERROR  : Image is locked by another process
>>>>> [root@mach0 ompi]# lslocks
>>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>>> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>>>> crond             606 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>>> master            820 FLOCK  33B WRITE 0     0   0 
>>>>> /var/spool/postfix/pid/master.pid
>>>>> master            820 FLOCK  33B WRITE 0     0   0 
>>>>> /var/lib/postfix/master.lock
>>>>> slurmctld         924 POSIX   4B WRITE 0     0   0 /run/slurmctld.pid
>>>>> slurmd           1103 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
>>>>> (unknown)        2477 FLOCK   0B READ  0     0   0 /
>>>>> [root@mach0 ompi]# ls /proc/2477
>>>>> ls: cannot access /proc/2477: No such file or directory
>>>>>
>>>>
>>>> OK, that is weird. If process 2477 is dead, there should be no active 
>>>> lock. I would guess this might be a kernel bug? What distribution and 
>>>> kernel are you running?
>>>>  
>>>>
>>>>> [root@mach0 ompi]# ssh mach2
>>>>> Last login: Fri Jul 15 07:52:56 2016 from mach0
>>>>> [root@mach2 ~]# singularity shell -w /tmp/c7new.img
>>>>> ERROR  : Image is locked by another process
>>>>> [root@mach2 ~]# lslocks
>>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>>> lvmetad           473 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>>>> master            851 FLOCK  33B WRITE 0     0   0 
>>>>> /var/spool/postfix/pid/master.pid
>>>>> master            851 FLOCK  33B WRITE 0     0   0 
>>>>> /var/lib/postfix/master.lock
>>>>> (unknown)       10110 FLOCK   0B READ  0     0   0 /
>>>>> crond             668 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>>> slurmd           1610 POSIX   5B WRITE 0     0   0 /run/slurmd.pid
>>>>> [root@mach2 ~]# ls /proc/10110
>>>>> ls: cannot access /proc/10110: No such file or directory
>>>>>
>>>>
>>>> Very weird. Seems both nodes have the same issue..?
>>>>
>>>> I wonder if I can replicate this by kill -9 the outer parent of a 
>>>> Singularity run. I will test when I get to my development box.
>>>>  
>>>>
>>>>>
>>>>> Good Run:
>>>>>
>>>>> [root@mach0 ompi]# mpirun --allow-run-as-root -np 6 -H 
>>>>> mach0,mach1,mach2 singularity exec /tmp/c7new.img /usr/bin/ring
>>>>> Process 0 sending 10 to 1, tag 201 (6 processes in ring)
>>>>> Process 0 sent to 1
>>>>> Process 1 exiting
>>>>> Process 0 decremented value: 9
>>>>> Process 0 decremented value: 8
>>>>> Process 0 decremented value: 7
>>>>> Process 0 decremented value: 6
>>>>> Process 0 decremented value: 5
>>>>> Process 0 decremented value: 4
>>>>> Process 0 decremented value: 3
>>>>> Process 0 decremented value: 2
>>>>> Process 0 decremented value: 1
>>>>> Process 0 decremented value: 0
>>>>> Process 0 exiting
>>>>> Process 2 exiting
>>>>> Process 5 exiting
>>>>> Process 3 exiting
>>>>> Process 4 exiting
>>>>>
>>>>
>>>> So as long as we don't crash (or get killed), all seems fine... 
>>>> Interesting.
>>>>
>>>> I will try and replicate the error condition.
>>>>
>>>> Greg
>>>>  
>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Thursday, July 14, 2016 at 3:56:48 PM UTC-6, Gregory M. Kurtzer 
>>>>> wrote:
>>>>>
>>>>>> I am wondering if some aspect of the sparse file has caused this. 
>>>>>> Sparse files are weird, and do require some attention when copying around. 
>>>>>> With that said, I do not think it should ever corrupt an image, but it 
>>>>>> might take along garbage data with it. One way to retest this is to go back 
>>>>>> to the original image, and tar it before sending using the -S/--sparse tar 
>>>>>> option when packaging it up. Then bring it to the other test system and see 
>>>>>> if it still does that.
>>>>>>
>>>>>> Aside from that, I'm a bit confused as to what is happening. There 
>>>>>> should be no issue with 2.0 vs. 2.1, but it might be worth checking with 
>>>>>> some images also created with 2.1 on the same target system where you are 
>>>>>> seeing the problem.
>>>>>>
>>>>>> On Thu, Jul 14, 2016 at 2:46 PM, Steve Mehlberg <sg...@gmail.com> 
>>>>>> wrote:
>>>>>>
>>>>>>> It is very possible that I did copy it from another system - maybe 
>>>>>>> created it with singularity 2.0 and now running it with 2.1??  I will start 
>>>>>>> over and recreate from scratch and see if I still have the problem.
>>>>>>>
>>>>>>> Sorry, I checked there is no /proc/2299
>>>>>>>
>>>>>>> The only way I've found to get the file unlocked is to reboot.  Not 
>>>>>>> an issue with the little VM.
>>>>>>>
>>>>>>> Steve
>>>>>>>
>>>>>>> On Thursday, July 14, 2016 at 3:30:12 PM UTC-6, Gregory M. Kurtzer 
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Very odd...
>>>>>>>>
>>>>>>>> I'm concerned about the Buffer I/O errors. It is almost like the 
>>>>>>>> image itself has a problem with it. Did you copy this image from another 
>>>>>>>> system? I wonder if it is the sparseness...
>>>>>>>>
>>>>>>>> Does the directory exist for /proc/2299/ ?
>>>>>>>>
>>>>>>>> Can you kill it with a -9?
>>>>>>>>
>>>>>>>> On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlberg <sg...@gmail.com
>>>>>>>> > wrote:
>>>>>>>>
>>>>>>>>> Seems that process isn't running any more.  I did find this in the 
>>>>>>>>> /var/log/messages file:
>>>>>>>>>
>>>>>>>>> [root@mach0 ~]# cat /var/log/messages | grep 2299
>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> 
>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1, 
>>>>>>>>> logical block 22299
>>>>>>>>> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1, 
>>>>>>>>> logical block 22990
>>>>>>>>> ...
>>>>>>>>>
>>>>>>>>> [root@mach0 ~]# ps -ef |grep 2299
>>>>>>>>> root     10545  2002  0 14:15 pts/0    00:00:00 grep --color=auto 
>>>>>>>>> 2299
>>>>>>>>> [root@mach0 ~]# ps |grep 2299
>>>>>>>>>
>>>>>>>>> On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer 
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>> What is running at PID 2299?
>>>>>>>>>>
>>>>>>>>>> On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <
>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>
>>>>>>>>>>> Ok, I see my error with shell vs exec.  I'm running on a VM with 
>>>>>>>>>>> my own space, no NFS involved.  Here is the debug command run and lsofl
>>>>>>>>>>>
>>>>>>>>>>> [root@mach0 ~]# df -h
>>>>>>>>>>> Filesystem             Size  Used Avail Use% Mounted on
>>>>>>>>>>> /dev/mapper/vg1-lv001  5.7G  4.3G  1.1G  80% /
>>>>>>>>>>> devtmpfs               911M     0  911M   0% /dev
>>>>>>>>>>> tmpfs                  920M     0  920M   0% /dev/shm
>>>>>>>>>>> tmpfs                  920M   41M  880M   5% /run
>>>>>>>>>>> tmpfs                  920M     0  920M   0% /sys/fs/cgroup
>>>>>>>>>>> /dev/vda1              190M  110M   67M  63% /boot
>>>>>>>>>>> /dev/mapper/vg1-lv002   12G  203M   11G   2% /var
>>>>>>>>>>> tmpfs                  184M     0  184M   0% /run/user/0
>>>>>>>>>>> [root@mach0 ~]# singularity --debug shell -w c7
>>>>>>>>>>> enabling debugging
>>>>>>>>>>> ending argument loop
>>>>>>>>>>> Exec'ing: /usr/local/libexec/singularity/cli/shell.exec -w+ '[' 
>>>>>>>>>>> -f /usr/local/etc/singularity/init ']'
>>>>>>>>>>> + . /usr/local/etc/singularity/init
>>>>>>>>>>> ++ unset module
>>>>>>>>>>> ++ 
>>>>>>>>>>> PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin
>>>>>>>>>>> ++ HISTFILE=/dev/null
>>>>>>>>>>> ++ export PATH HISTFILE
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> ++ '[' -n '' ']'
>>>>>>>>>>> + true
>>>>>>>>>>> + case ${1:-} in
>>>>>>>>>>> + shift
>>>>>>>>>>> + SINGULARITY_WRITABLE=1
>>>>>>>>>>> + export SINGULARITY_WRITABLE
>>>>>>>>>>> + true
>>>>>>>>>>> + case ${1:-} in
>>>>>>>>>>> + break
>>>>>>>>>>> + '[' -z c7 ']'
>>>>>>>>>>> + SINGULARITY_IMAGE=c7
>>>>>>>>>>> + export SINGULARITY_IMAGE
>>>>>>>>>>> + shift
>>>>>>>>>>> + exec /usr/local/libexec/singularity/sexec
>>>>>>>>>>> VERBOSE [U=0,P=8045]       
>>>>>>>>>>> message.c:52:init()                        : Set messagelevel to: 5
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:127:main()                         : Gathering and caching user 
>>>>>>>>>>> info.
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:43:get_user_privs()            : Called get_user_privs(struct 
>>>>>>>>>>> s_privinfo *uinfo)
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:54:get_user_privs()            : Returning 
>>>>>>>>>>> get_user_privs(struct s_privinfo *uinfo) = 0
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:134:main()                         : Checking if we can escalate 
>>>>>>>>>>> privs properly.
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:61:escalate_privs()            : Called escalate_privs(void)
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:73:escalate_privs()            : Returning escalate_privs(void) 
>>>>>>>>>>> = 0
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:141:main()                         : Setting privs to calling user
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:79:drop_privs()                : Called drop_privs(struct 
>>>>>>>>>>> s_privinfo *uinfo)
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:87:drop_privs()                : Dropping privileges to GID = 
>>>>>>>>>>> '0'
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:93:drop_privs()                : Dropping privileges to UID = 
>>>>>>>>>>> '0'
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:103:drop_privs()               : Confirming we have correct GID
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:109:drop_privs()               : Confirming we have correct UID
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> privilege.c:115:drop_privs()               : Returning drop_privs(struct 
>>>>>>>>>>> s_privinfo *uinfo) = 0
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:146:main()                         : Obtaining user's homedir
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:150:main()                         : Obtaining file descriptor to 
>>>>>>>>>>> current directory
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:155:main()                         : Getting current working 
>>>>>>>>>>> directory path string
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:161:main()                         : Obtaining SINGULARITY_COMMAND 
>>>>>>>>>>> from environment
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:168:main()                         : Obtaining SINGULARITY_IMAGE 
>>>>>>>>>>> from environment
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:174:main()                         : Checking container image is a 
>>>>>>>>>>> file: c7
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:180:main()                         : Building configuration file 
>>>>>>>>>>> location
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:183:main()                         : Config location: 
>>>>>>>>>>> /usr/local/etc/singularity/singularity.conf
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:185:main()                         : Checking Singularity 
>>>>>>>>>>> configuration is a file: /usr/local/etc/singularity/singularity.conf
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:191:main()                         : Checking Singularity 
>>>>>>>>>>> configuration file is owned by root
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:197:main()                         : Opening Singularity 
>>>>>>>>>>> configuration file
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:210:main()                         : Checking Singularity 
>>>>>>>>>>> configuration for 'sessiondir prefix'
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> config_parser.c:47:config_get_key_value()  : Called 
>>>>>>>>>>> config_get_key_value(fp, sessiondir prefix)
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> config_parser.c:66:config_get_key_value()  : Return 
>>>>>>>>>>> config_get_key_value(fp, sessiondir prefix) = NULL
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> file.c:48:file_id()                        : Called file_id(c7)
>>>>>>>>>>> VERBOSE [U=0,P=8045]       
>>>>>>>>>>> file.c:58:file_id()                        : Generated file_id: 
>>>>>>>>>>> 0.64768.26052
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> file.c:60:file_id()                        : Returning file_id(c7) = 
>>>>>>>>>>> 0.64768.26052
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:217:main()                         : Set sessiondir to: 
>>>>>>>>>>> /tmp/.singularity-session-0.64768.26052
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:221:main()                         : Set containername to: c7
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:223:main()                         : Setting loop_dev_* paths
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> config_parser.c:47:config_get_key_value()  : Called 
>>>>>>>>>>> config_get_key_value(fp, container dir)
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> config_parser.c:58:config_get_key_value()  : Return 
>>>>>>>>>>> config_get_key_value(fp, container dir) = /var/singularity/mnt
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:232:main()                         : Set image mount path to: 
>>>>>>>>>>> /var/singularity/mnt
>>>>>>>>>>> LOG     [U=0,P=8045]       
>>>>>>>>>>> sexec.c:234:main()                         : Command=shell, Container=c7, 
>>>>>>>>>>> CWD=/root, Arg1=(null)
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:247:main()                         : Set prompt to: Singularity.c7>
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:249:main()                         : Checking if we are opening 
>>>>>>>>>>> image as read/write
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:264:main()                         : Opening image as read/write 
>>>>>>>>>>> only: c7
>>>>>>>>>>> DEBUG   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:271:main()                         : Setting exclusive lock on file 
>>>>>>>>>>> descriptor: 6
>>>>>>>>>>> ERROR   [U=0,P=8045]       
>>>>>>>>>>> sexec.c:273:main()                         : Image is locked by another 
>>>>>>>>>>> process
>>>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>>>> [root@mach0 ~]# lslocks
>>>>>>>>>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>>>>>>>>>> crond             601 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>>>>>>>>>> master            826 FLOCK  33B WRITE 0     0   0 
>>>>>>>>>>> /var/spool/postfix/pid/master.pid
>>>>>>>>>>> master            826 FLOCK  33B WRITE 0     0   0 
>>>>>>>>>>> /var/lib/postfix/master.lock
>>>>>>>>>>> lvmetad           478 POSIX   4B WRITE 0     0   0 
>>>>>>>>>>> /run/lvmetad.pid
>>>>>>>>>>> slurmctld         940 POSIX   4B WRITE 0     0   0 
>>>>>>>>>>> /run/slurmctld.pid
>>>>>>>>>>> slurmd            966 POSIX   4B WRITE 0     0   0 
>>>>>>>>>>> /run/slurmd.pid
>>>>>>>>>>> (unknown)        2299 FLOCK   0B READ  0     0   0 /
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>> On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gregory M. 
>>>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <
>>>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>>>
>>>>>>>>>>>>> Gregory,
>>>>>>>>>>>>>
>>>>>>>>>>>>> Thanks for the quick suggestions.  There don't seem to be any 
>>>>>>>>>>>>> processes attached to the container and I can't seem to run other commands 
>>>>>>>>>>>>> (in read mode). I'm not sure what's going on.
>>>>>>>>>>>>>
>>>>>>>>>>>>> Just lazy with root right now, I need to create a normal uid.
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> Ahh, ok. Been there, done that! lol
>>>>>>>>>>>>  
>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> Steve
>>>>>>>>>>>>>
>>>>>>>>>>>>> [root@mach0 ~]# ls -la c7
>>>>>>>>>>>>> -rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7
>>>>>>>>>>>>> [root@mach0 ~]# singularity shell -w c7
>>>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> Can you run this command again in --debug mode (singularity 
>>>>>>>>>>>> --debug ....)
>>>>>>>>>>>>  
>>>>>>>>>>>>
>>>>>>>>>>>>> [root@mach0 ~]# lsof c7
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> What about the command "lslocks"?
>>>>>>>>>>>>  
>>>>>>>>>>>>
>>>>>>>>>>>>> [root@mach0 ~]# singularity shell c7 whoami
>>>>>>>>>>>>> /usr/bin/whoami: /usr/bin/whoami: cannot execute binary file
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> Ahh, this is normal. You are asking the shell script to read in 
>>>>>>>>>>>> /usr/bin/whoami. If you want to use shell to run whoami, you must prefix it 
>>>>>>>>>>>> with the -c (e.g. -c "whoami [args]"), or use the 'exec' Singularity 
>>>>>>>>>>>> subcommand: "singularity exec c7 whoami"
>>>>>>>>>>>>  
>>>>>>>>>>>>
>>>>>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>>>>> [root@mach0 ~]# ps -ef |grep c7
>>>>>>>>>>>>> root      7479  2002  0 11:49 pts/0    00:00:00 grep 
>>>>>>>>>>>>> --color=auto c7
>>>>>>>>>>>>> [root@mach0 ~]#
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> What kind of file system does your image exist on? Is it NFS by 
>>>>>>>>>>>> chance? I wonder if there is a host issue with a locking daemon or 
>>>>>>>>>>>> something else weird going on where it is not giving the exclusive lock 
>>>>>>>>>>>> properly. If this is NFS or other non local file system, can you copy the 
>>>>>>>>>>>> image to /tmp, rerun the MPI command to get it to fail again, and try 
>>>>>>>>>>>> again? 
>>>>>>>>>>>>
>>>>>>>>>>>> Thanks!
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>  
>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>>
>>>>>>>>>>>>> On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M. 
>>>>>>>>>>>>> Kurtzer wrote:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Hi Steve,
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> That means there is an active file descriptor/process still 
>>>>>>>>>>>>>> running and attached to the container maintaining a shared lock. You can 
>>>>>>>>>>>>>> run other commands against the container as long as long as the container 
>>>>>>>>>>>>>> is not being requested as --writable(-w), because that will try and obtain 
>>>>>>>>>>>>>> an exclusive lock and it will fail if there are any active shared locks. 
>>>>>>>>>>>>>> Try an "lsof /path/to/c7" to see what processes are attached to it. You may 
>>>>>>>>>>>>>> see a list like:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> # lsof /tmp/Demo-2.img 
>>>>>>>>>>>>>> COMMAND    PID USER   FD   TYPE DEVICE   SIZE/OFF      NODE 
>>>>>>>>>>>>>> NAME
>>>>>>>>>>>>>> sexec   107975 root    6rR  REG  253,0 1073741856 202112247 
>>>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>>> sexec   107977 root    6r   REG  253,0 1073741856 202112247 
>>>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>>> bash    107982 root    6r   REG  253,0 1073741856 202112247 
>>>>>>>>>>>>>> /tmp/Demo-2.img
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Notice the two top ones are 'sexec' which are part of the 
>>>>>>>>>>>>>> Singularity process stack. Kill the bottom one, and those should go away 
>>>>>>>>>>>>>> naturally.
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> BTW, as long as you have installed Singularity as root, there 
>>>>>>>>>>>>>> is no need to run Singularity commands as root (unless you want to make 
>>>>>>>>>>>>>> system changes within the container).
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> Hope that helps!
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <
>>>>>>>>>>>>>> sg...@gmail.com> wrote:
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Running mpirun tests, when an abort occurs, my image ends up 
>>>>>>>>>>>>>>> locked.  Is there a way to clear the lock without rebooting?  I looked for 
>>>>>>>>>>>>>>> processes that I could kill, but didn't see anything worthy.
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> I'm using singularity v2.1 on Centos 7.2 (both host and 
>>>>>>>>>>>>>>> container).
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Regards,
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> Steve
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H 
>>>>>>>>>>>>>>> mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>>>>>>>>>>>>> Process 0 sent to 1
>>>>>>>>>>>>>>> Process 0 decremented value: 9
>>>>>>>>>>>>>>> Process 0 decremented value: 8
>>>>>>>>>>>>>>> Process 0 decremented value: 7
>>>>>>>>>>>>>>> Process 0 decremented value: 6
>>>>>>>>>>>>>>> Process 0 decremented value: 5
>>>>>>>>>>>>>>> Process 0 decremented value: 4
>>>>>>>>>>>>>>> Process 0 decremented value: 3
>>>>>>>>>>>>>>> Process 0 decremented value: 2
>>>>>>>>>>>>>>> Process 0 decremented value: 1
>>>>>>>>>>>>>>> Process 0 decremented value: 0
>>>>>>>>>>>>>>> Process 0 exiting
>>>>>>>>>>>>>>> Process 1 exiting
>>>>>>>>>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H 
>>>>>>>>>>>>>>> mach0,mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>>>>>> It appears as if there is not enough space for 
>>>>>>>>>>>>>>> /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory 
>>>>>>>>>>>>>>> backing
>>>>>>>>>>>>>>> file). It is likely that your MPI job will now either abort 
>>>>>>>>>>>>>>> or experience
>>>>>>>>>>>>>>> performance degradation.
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>   Local host:  mach0
>>>>>>>>>>>>>>>   Space Requested: 4194312 B
>>>>>>>>>>>>>>>   Space Available: 0 B
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> --------------------------------------------------------------------------
>>>>>>>>>>>>>>> [mach0:02308] create_and_attach: unable to create shared 
>>>>>>>>>>>>>>> memory BTL coordinating structure :: size 134217728
>>>>>>>>>>>>>>> [mach0:02291] 2 more processes have sent help message 
>>>>>>>>>>>>>>> help-opal-shmem-mmap.txt / target full
>>>>>>>>>>>>>>> [mach0:02291] Set MCA parameter "orte_base_help_aggregate" 
>>>>>>>>>>>>>>> to 0 to see all help / error messages
>>>>>>>>>>>>>>> ^CKilled by signal 2.
>>>>>>>>>>>>>>> Killed by signal 2.
>>>>>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2308
>>>>>>>>>>>>>>> Singularity is sending SIGKILL to child pid: 2309
>>>>>>>>>>>>>>> [warn] Epoll ADD(4) on fd 31 failed.  Old events were 0; 
>>>>>>>>>>>>>>> read change was 0 (none); write change was 1 (add): Bad file descriptor
>>>>>>>>>>>>>>> ^C[root@mach0 ~]singularity shell -w c7
>>>>>>>>>>>>>>> ERROR  : Image is locked by another process
>>>>>>>>>>>>>>> [root@mach0 ~]# tail -30 /var/log/messages
>>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon 
>>>>>>>>>>>>>>> management.
>>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Reached target Multi-User 
>>>>>>>>>>>>>>> System.
>>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.
>>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about 
>>>>>>>>>>>>>>> System Runlevel Changes...
>>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data 
>>>>>>>>>>>>>>> Collection 10s After Completed Startup.
>>>>>>>>>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Update UTMP about 
>>>>>>>>>>>>>>> System Runlevel Changes.
>>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel
>>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]
>>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel 
>>>>>>>>>>>>>>> arming.
>>>>>>>>>>>>>>> Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms 
>>>>>>>>>>>>>>> (kernel) + 1.100s (initrd) + 4.931s (userspace) = 6.446s.
>>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.
>>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting user-0.slice.
>>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user 
>>>>>>>>>>>>>>> root.
>>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Started Session 1 of user 
>>>>>>>>>>>>>>> root.
>>>>>>>>>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user 
>>>>>>>>>>>>>>> root.
>>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2023)> 
>>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> 
>>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Image 
>>>>>>>>>>>>>>> is locked by another process
>>>>>>>>>>>>>>> Jul 14 10:42:36 mach0 kernel: loop: module loaded
>>>>>>>>>>>>>>> Jul 14 10:43:38 mach0 Singularity: sexec (U=0,P=2050)> 
>>>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>>>> Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted 
>>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>>> Jul 14 10:49:17 mach0 Singularity: sexec (U=0,P=2203)> 
>>>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>>>> Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted 
>>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>>> Jul 14 10:50:39 mach0 Singularity: sexec (U=0,P=2244)> 
>>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>>> Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted 
>>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> 
>>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2300)> 
>>>>>>>>>>>>>>> Command=exec, Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>>>>>>>>>> Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted 
>>>>>>>>>>>>>>> filesystem with ordered data mode. Opts: discard
>>>>>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> 
>>>>>>>>>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>>>>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Image 
>>>>>>>>>>>>>>> is locked by another process
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>> -- 
>>>>>>>>>>>>>>> You received this message because you are subscribed to the 
>>>>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails 
>>>>>>>>>>>>>>> from it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>>
>>>>>>>>>>>>>> -- 
>>>>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>>>>>> University of California
>>>>>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>>>>>
>>>>>>>>>>>>> -- 
>>>>>>>>>>>>> You received this message because you are subscribed to the 
>>>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from 
>>>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>>> -- 
>>>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>>>> University of California
>>>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>>>
>>>>>>>>>>> -- 
>>>>>>>>>>> You received this message because you are subscribed to the 
>>>>>>>>>>> Google Groups "singularity" group.
>>>>>>>>>>> To unsubscribe from this group and stop receiving emails from 
>>>>>>>>>>> it, send an email to singu...@lbl.gov.
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> -- 
>>>>>>>>>> Gregory M. Kurtzer
>>>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>>>> University of California
>>>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>>>
>>>>>>>>> -- 
>>>>>>>>> You received this message because you are subscribed to the Google 
>>>>>>>>> Groups "singularity" group.
>>>>>>>>> To unsubscribe from this group and stop receiving emails from it, 
>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> -- 
>>>>>>>> Gregory M. Kurtzer
>>>>>>>> High Performance Computing Services (HPCS)
>>>>>>>> University of California
>>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>>
>>>>>>> -- 
>>>>>>> You received this message because you are subscribed to the Google 
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it, 
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> -- 
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>> -- 
>>>>> You received this message because you are subscribed to the Google 
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send 
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> -- 
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>> -- 
>>> You received this message because you are subscribed to the Google 
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send 
>>> an email to singu...@lbl.gov <javascript:>.
>>>
>>
>>
>>
>> -- 
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
>
>
>
> -- 
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>

------=_Part_3973_1138197283.1468854990912
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Gregory,<br><br>I have rebuilt with the latest from master=
 this morning and repopulated everything on my small virtual cluster - mach=
.=C2=A0 Good news!=C2=A0 I am unable to recreate the problem.=C2=A0 One of =
my colleagues had a similar or perhaps the same problem on a larger bare me=
tal system.=C2=A0 We will update that system with the latest master and tes=
t to see if the problem still occurs. <br><br>Thanks for your help,<br><br>=
Steve<br><br>On Friday, July 15, 2016 at 5:07:27 PM UTC-6, Gregory M. Kurtz=
er wrote:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left: =
0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">Hi S=
teve,<div><br></div><div>Sorry for the delay. The cause of the first error =
was an easy fix, but debugging the second had me really stumped for a while=
! It just so happens that I was on a conference call with several other dev=
elopers a bit ago, and when we were going over this Ben Allen (LANL) pointe=
d me in a new direction to look, and sure enough it was the loop driver in =
the kernel holding the file descriptor that contained the lock open! So to =
fix that and ensure that it won&#39;t happen again, I decided to use the LO=
_FLAGS_AUTOCLEAR flag option, but that brings up a potential race condition=
. Which luckily, after much stumpedness (and reviewing the loop kernel sour=
ce) I think that I came up with a very simple workaround of simply opening =
the loop device itself so it doesn&#39;t auto clear itself until each proce=
ss has completed! Anyway, long story short...=C2=A0</div><div><br></div><di=
v>Can you test the latest master and let me know if it is indeed fixed!</di=
v><div><br></div><div>Thanks!</div><div><br></div><div><br></div></div><div=
><br><div class=3D"gmail_quote">On Fri, Jul 15, 2016 at 11:28 AM, Gregory M=
. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_blank" g=
df-obfuscated-mailto=3D"o6EbX8iTAQAJ" rel=3D"nofollow" onmousedown=3D"this.=
href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D&#39;java=
script:&#39;;return true;">gm...@lbl.gov</a>&gt;</span> wrote:<br><blockquo=
te class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc so=
lid;padding-left:1ex"><div dir=3D"ltr">Hi Steve,<div><br></div><div>I think=
 I have fixed the initial error you got regarding being unable to create th=
e sessiondir. The second issue regarding file locks, I am debugging now. I&=
#39;ll let you know shortly when to retest.</div><div><br></div><div>Thanks=
!</div><div><div><div><br></div><div><br><div><div><br><div class=3D"gmail_=
quote">On Fri, Jul 15, 2016 at 11:26 AM, Steve Mehlberg <span dir=3D"ltr">&=
lt;<a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"o6EbX=
8iTAQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;=
return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">sg=
...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" st=
yle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div =
dir=3D"ltr"><div>The problem only arises when there is an error.</div><div>=
The tmp directory is different on each machine, it is not shared or an NFS =
mount.=C2=A0 Is that a problem?</div><div><br></div><div>Steve<br><br>On Fr=
iday, July 15, 2016 at 9:51:55 AM UTC-6, Gregory M. Kurtzer wrote:</div><di=
v><div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;=
padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;b=
order-left-style:solid"><div dir=3D"ltr"><div><br><div class=3D"gmail_quote=
">On Fri, Jul 15, 2016 at 8:26 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a =
rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-=
left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid">=
<div dir=3D"ltr"><div>Hello,</div><div><br></div><div>I went back and recre=
ated everything from scratch using v2.1.=C2=A0 When I moved the image file =
to the mach1 and mach2 nodes I used tar with the -S option.=C2=A0 I used /t=
mp for the image files.=C2=A0 After everything was set I ran mpirun on each=
 individual system (only starting jobs on that system)=C2=A0with no errors.=
=C2=A0 For some reason when I ran mpirun from mach0 and asked for jobs to b=
e run on mach0 and another system I get the error.=C2=A0 The mpirun complai=
ns about not being able to create a file (already exists) and hangs.=C2=A0 =
The singularity image file on mach0 and the other system selected are locke=
d with pids that are no longer running.</div><div><br></div><div>Saying all=
 that, I decided to go back and put --debug on the singularity command in t=
he mpirun to get you better diagnostics.=C2=A0 I rebooted mach0 and mach2, =
then reran the command and it worked correctly.=C2=A0 I then removed the --=
debug command and it worked.=C2=A0 I added mach1 to the mix and everything =
worked fine.=C2=A0 What? I guess the bottom line is that if the mpirun abor=
ts (and it seems to hang) then you can get this condition.</div></div></blo=
ckquote><div><br></div><div>So does the problem seem to arise only when the=
re is a failure on one of the processes, and MPI kills the remaining proces=
ses?<br></div><div><br></div><div>Ralph, do you know how OMPI kills the lef=
tover processes in a job abort and what signal it uses?</div><div><br></div=
><div>(additional comments inline)</div><div><br></div><blockquote class=3D=
"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-lef=
t-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><di=
v dir=3D"ltr"><div><br>Steve</div><div><br></div><div><br></div><div><font =
face=3D"courier new,monospace">[root@mach0 ompi]# mpirun --allow-run-as-roo=
t -np 2 -H mach2 singularity exec /tmp/c7new.img /usr/bin/ring<br>Process 1=
 exiting<span><br>Process 0 sending 10 to 1, tag 201 (2 processes in ring)<=
br>Process 0 sent to 1<br>Process 0 decremented value: 9<br>Process 0 decre=
mented value: 8<br>Process 0 decremented value: 7<br>Process 0 decremented =
value: 6<br>Process 0 decremented value: 5<br>Process 0 decremented value: =
4<br>Process 0 decremented value: 3<br>Process 0 decremented value: 2<br>Pr=
ocess 0 decremented value: 1<br>Process 0 decremented value: 0<br>Process 0=
 exiting<br></span></font></div></div></blockquote><div><br></div><div>Is /=
tmp NFS mounted or shared?</div><div>=C2=A0</div><blockquote class=3D"gmail=
_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-colo=
r:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><div dir=
=3D"ltr"><div><font face=3D"courier new,monospace"><span></span>[root@mach0=
 ompi]# mpirun --allow-run-as-root -np 4 -H mach0,mach2 singularity exec /t=
mp/c7new.img /usr/bin/ring<br><font color=3D"#ff0000">ERROR=C2=A0 : Could n=
ot create directory /tmp/.singularity-session-0.<wbr>64768.158221: File exi=
sts<br>ERROR=C2=A0 : Failed creating session directory: /tmp/.singularity-s=
ession-0.<wbr>64768.158221</font><br>------------------------------<wbr>---=
----------------------<br>Primary job=C2=A0 terminated normally, but 1 proc=
ess returned<br>a non-zero exit code. Per user-direction, the job has been =
aborted.<br>------------------------------<wbr>-------------------------<br=
><font color=3D"#ff0000">^C</font>Killed by signal 2.<br>mpirun: abort is a=
lready in progress...hit ctrl-c again to forcibly terminate</font></div></d=
iv></blockquote><div><br></div><div>Ahhhhh, I think you may have found a ra=
ce condition in Singularity which killed one of the processes. Then the MPI=
 killed the rest.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204=
,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><=
div><font face=3D"courier new,monospace">[root@mach0 ompi]# singularity she=
ll -w /tmp/c7new.img<span><br>ERROR=C2=A0 : Image is locked by another proc=
ess<br></span>[root@mach0 ompi]# lslocks<span><br>COMMAND=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M=
 START END PATH<br></span>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 473 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0=
 0=C2=A0=C2=A0 0 /run/lvmetad.pid<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 606 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=
=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 820 FLOCK=C2=A0 33B =
WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/mast=
er.<wbr>pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 820 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=
=A0 0 /var/lib/postfix/master.lock<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 924 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0=C2=A0 0 /run/slurmctld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1103 POSIX=C2=A0=C2=A0 5B WRITE 0=C2=A0=
=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmd.pid<br>(unknown)=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <font color=3D"#ff0000">2477</font> FLOCK=C2=
=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>[roo=
t@mach0 ompi]# ls /proc/2477<br>ls: cannot access /proc/2477: No such file =
or directory<br></font></div></div></blockquote><div><br></div><div>OK, tha=
t is weird. If process 2477 is dead, there should be no active lock. I woul=
d guess this might be a kernel bug? What distribution and kernel are you ru=
nning?</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"mar=
gin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);b=
order-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><div><font f=
ace=3D"courier new,monospace">[root@mach0 ompi]# ssh mach2<br>Last login: F=
ri Jul 15 07:52:56 2016 from mach0<br>[root@mach2 ~]# singularity shell -w =
/tmp/c7new.img<span><br>ERROR=C2=A0 : Image is locked by another process<br=
></span>[root@mach2 ~]# lslocks<span><br>COMMAND=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MODE=C2=A0 M START EN=
D PATH<br></span>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 473 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=
=C2=A0 0 /run/lvmetad.pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 851 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.<wbr>pid<br>master=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 851 FLOCK=
=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/lib/postfix=
/master.lock<br>(unknown)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 <font color=
=3D"#ff0000">10110</font> FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=A0=C2=A0=C2=
=A0=C2=A0 0=C2=A0=C2=A0 0 /<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 668 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=
=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br>slurmd=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1610 POSIX=C2=A0=C2=A0 5B WRITE =
0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmd.pid<br>[root@mach2 ~]=
# ls /proc/10110<br>ls: cannot access /proc/10110: No such file or director=
y</font></div></div></blockquote><div><br></div><div>Very weird. Seems both=
 nodes have the same issue..?</div><div><br></div><div>I wonder if I can re=
plicate this by kill -9 the outer parent of a Singularity run. I will test =
when I get to my development box.</div><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-=
left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid">=
<div dir=3D"ltr"><div><br></div><div>Good Run:</div><div><br></div><div>[<f=
ont face=3D"courier new,monospace">root@mach0 ompi]# mpirun --allow-run-as-=
root -np 6 -H mach0,mach1,mach2 singularity exec /tmp/c7new.img /usr/bin/ri=
ng<br>Process 0 sending 10 to 1, tag 201 (6 processes in ring)<span><br>Pro=
cess 0 sent to 1<br></span>Process 1 exiting<span><br>Process 0 decremented=
 value: 9<br>Process 0 decremented value: 8<br>Process 0 decremented value:=
 7<br>Process 0 decremented value: 6<br>Process 0 decremented value: 5<br>P=
rocess 0 decremented value: 4<br>Process 0 decremented value: 3<br>Process =
0 decremented value: 2<br>Process 0 decremented value: 1<br>Process 0 decre=
mented value: 0<br>Process 0 exiting<br></span>Process 2 exiting<br>Process=
 5 exiting<br>Process 3 exiting<br>Process 4 exiting</font></div></div></bl=
ockquote><div><br></div><div>So as long as we don&#39;t crash (or get kille=
d), all seems fine... Interesting.</div><div><br></div><div>I will try and =
replicate the error condition.</div><div><br></div><div>Greg</div><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8e=
x;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px=
;border-left-style:solid"><div dir=3D"ltr"><div><br></div><span><div><br></=
div><div><br>On Thursday, July 14, 2016 at 3:56:48 PM UTC-6, Gregory M. Kur=
tzer wrote:</div></span><blockquote class=3D"gmail_quote" style=3D"margin:0=
px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border=
-left-width:1px;border-left-style:solid"><span><div dir=3D"ltr">I am wonder=
ing if some aspect of the sparse file has caused this. Sparse files are wei=
rd, and do require some attention when copying around. With that said, I do=
 not think it should ever corrupt an image, but it might take along garbage=
 data with it. One way to retest this is to go back to the original image, =
and tar it before sending using the -S/--sparse tar option when packaging i=
t up. Then bring it to the other test system and see if it still does that.=
<div><br></div><div>Aside from that, I&#39;m a bit confused as to what is h=
appening. There should be no issue with 2.0 vs. 2.1, but it might be worth =
checking with some images also created with 2.1 on the same target system w=
here you are seeing the problem.</div></div></span><div><div><div><br><div =
class=3D"gmail_quote">On Thu, Jul 14, 2016 at 2:46 PM, Steve Mehlberg <span=
 dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<=
br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padd=
ing-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;borde=
r-left-style:solid"><div dir=3D"ltr"><div>It is very possible that I did co=
py it from another system - maybe created it with singularity 2.0 and now r=
unning it with 2.1??=C2=A0 I will start over and recreate from scratch and =
see if I still have the problem.</div><div><br></div><div>Sorry, I checked =
there is no /proc/2299</div><div><br></div><div>The only way I&#39;ve found=
 to get the file unlocked is to reboot.=C2=A0 Not an issue with the little =
VM.</div><span><font color=3D"#888888"><div><br></div></font></span><div><s=
pan><font color=3D"#888888">Steve</font></span><span><br><br>On Thursday, J=
uly 14, 2016 at 3:30:12 PM UTC-6, Gregory M. Kurtzer wrote:</span></div><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-le=
ft:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left=
-style:solid"><span><div dir=3D"ltr">Very odd...<div><br></div><div>I&#39;m=
 concerned about the Buffer I/O errors. It is almost like the image itself =
has a problem with it. Did you copy this image from another system? I wonde=
r if it is the sparseness...</div><div><br></div><div>Does the directory ex=
ist for /proc/2299/ ?</div><div><br></div><div>Can you kill it with a -9?</=
div></div></span><div><div><div><br><div class=3D"gmail_quote">On Thu, Jul =
14, 2016 at 2:27 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollo=
w">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quot=
e" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb=
(204,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"lt=
r"><div>Seems that process isn&#39;t running any more.=C2=A0 I did find thi=
s in the /var/log/messages file:</div><div><br></div><div><font face=3D"cou=
rier new,monospace">[root@mach0 ~]# cat /var/log/messages | grep 2299<span>=
<br>Jul 14 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2299)&gt; Command=
=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br></span>Jul 14=
 11:37:17 mach0 kernel: Buffer I/O error on device loop1, logical block 222=
99<br>Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1, logic=
al block 22990<br>...</font></div><div><font face=3D"courier new,monospace"=
><br></font></div><div><font face=3D"courier new,monospace">[root@mach0 ~]#=
 ps -ef |grep 2299<br>root=C2=A0=C2=A0=C2=A0=C2=A0 10545=C2=A0 2002=C2=A0 0=
 14:15 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --color=3Dauto 2299<br>[root@m=
ach0 ~]# ps |grep 2299</font><br></div><span><div><br>On Thursday, July 14,=
 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer wrote:</div></span><blockquot=
e class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;=
border-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:=
solid"><span><div dir=3D"ltr">What is running at PID=C2=A0<span style=3D"fo=
nt-size:12.8px">2299?</span></div></span><div><div><div><br><div class=3D"g=
mail_quote">On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <span dir=3D"lt=
r">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1=
ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-sty=
le:solid"><div dir=3D"ltr">Ok, I see my error with shell vs exec.=C2=A0 I&#=
39;m running on a VM with my own space, no NFS involved.=C2=A0 Here is the =
debug command run and lsofl<br><br>[root@mach0 ~]# df -h<br>Filesystem=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 Size=
=C2=A0 Used Avail Use% Mounted on<br>/dev/mapper/vg1-lv001=C2=A0 5.7G=C2=A0=
 4.3G=C2=A0 1.1G=C2=A0 80% /<br>devtmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 911M=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0 911M=C2=A0=C2=A0 0% /dev<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=C2=A0 0% /dev/shm<br>tmpfs=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0 41M=C2=A0 880M=C2=A0=C2=A0 5% =
/run<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=
=C2=A0 920M=C2=A0=C2=A0 0% /sys/fs/cgroup<br>/dev/vda1=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 190M=C2=A0 110M=
=C2=A0=C2=A0 67M=C2=A0 63% /boot<br>/dev/mapper/vg1-lv002=C2=A0=C2=A0 12G=
=C2=A0 203M=C2=A0=C2=A0 11G=C2=A0=C2=A0 2% /var<br>tmpfs=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 184M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 184M=C2=A0=C2=A0 0% /run/use=
r/0<br>[root@mach0 ~]# singularity --debug shell -w c7<br>enabling debuggin=
g<br>ending argument loop<br>Exec&#39;ing: /usr/local/libexec/<wbr>singular=
ity/cli/shell.exec -w+ &#39;[&#39; -f /usr/local/etc/singularity/<wbr>init =
&#39;]&#39;<br>+ . /usr/local/etc/singularity/<wbr>init<br>++ unset module<=
br>++ PATH=3D/usr/local/sbin:/usr/<wbr>local/bin:/usr/sbin:/usr/bin:/<wbr>r=
oot/bin:/bin:/sbin:/usr/bin:/<wbr>usr/sbin<br>++ HISTFILE=3D/dev/null<br>++=
 export PATH HISTFILE<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#3=
9;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#=
39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&=
#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#=
39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<b=
r>+ true<br>+ case ${1:-} in<br>+ shift<br>+ SINGULARITY_WRITABLE=3D1<br>+ =
export SINGULARITY_WRITABLE<br>+ true<br>+ case ${1:-} in<br>+ break<br>+ &=
#39;[&#39; -z c7 &#39;]&#39;<br>+ SINGULARITY_IMAGE=3Dc7<br>+ export SINGUL=
ARITY_IMAGE<br>+ shift<br>+ exec /usr/local/libexec/<wbr>singularity/sexec<=
br>VERBOSE [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 message.c:5=
2:init()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<=
wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Set messagelevel to: 5<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:127:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Gathering and caching us=
er info.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 privilege.c:43:get_user_privs(<wbr>)=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called get_user_privs(struct s_pr=
ivinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 privilege.c:54:get_user_privs(<wbr>)=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning get_user_privs(st=
ruct s_privinfo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:134:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we can es=
calate privs properly.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 privilege.c:61:escalate_privs(<wbr>)=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called escalate_privs=
(void)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 privilege.c:73:escalate_privs(<wbr>)=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning escalate_privs(void) =3D 0=
<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
sexec.c:141:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Setting privs to calling user<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:79:drop_priv=
s()=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Called drop_privs(struct s_privinfo *uinfo)<br>D=
EBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privi=
lege.c:87:drop_privs()=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping privileges to GID =3D=
 &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 privilege.c:93:drop_privs()=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping pri=
vileges to UID =3D &#39;0&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:103:drop_privs()=C2=A0=C2=A0<wbr=
>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 :=
 Confirming we have correct GID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:109:drop_privs()=C2=A0=C2=A0<wbr=
>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 :=
 Confirming we have correct UID<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:115:drop_privs()=C2=A0=C2=A0<wbr=
>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 :=
 Returning drop_privs(struct s_privinfo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 =
[U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:146:main()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Obtaining user&#39;s homedir<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:150:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining file descripto=
r to current directory<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:155:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Getting current working direct=
ory path string<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:161:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_COMMAND from e=
nvironment<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 sexec.c:168:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_IMAGE from environme=
nt<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:174:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Checking container image is a file: c7<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:1=
80:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Building configuration file location<br>DEBUG=C2=A0=C2=A0 [U=3D=
0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:183:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Config=
 location: /usr/local/etc/singularity/<wbr>singularity.conf<br>DEBUG=C2=A0=
=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:185:mai=
n()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 : Checking Singularity configuration is a file: /usr/local/etc/singular=
ity/<wbr>singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:191:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity config=
uration file is owned by root<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:197:main()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening Singularity c=
onfiguration file<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 sexec.c:210:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singularity configuratio=
n for &#39;sessiondir prefix&#39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:47:config_get_<wbr>key_va=
lue()=C2=A0 : Called config_get_key_value(fp, sessiondir prefix)<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_pa=
rser.c:66:config_get_<wbr>key_value()=C2=A0 : Return config_get_key_value(f=
p, sessiondir prefix) =3D NULL<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:48:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called file_id(c7)<br>VERBO=
SE [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:58:file_id()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Gen=
erated file_id: 0.64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:60:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning file_id(c7) =3D 0=
.64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:217:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set sessiondir to: /tmp/.singularity-se=
ssion-0.<wbr>64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:221:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set containername to: c7<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sex=
ec.c:223:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 : Setting loop_dev_* paths<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D=
8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:47:config_get_<wb=
r>key_value()=C2=A0 : Called config_get_key_value(fp, container dir)<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_=
parser.c:58:config_get_<wbr>key_value()=C2=A0 : Return config_get_key_value=
(fp, container dir) =3D /var/singularity/mnt<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:232:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set image=
 mount path to: /var/singularity/mnt<br>LOG=C2=A0=C2=A0=C2=A0=C2=A0 [U=3D0,=
P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:234:main()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Comman=
d=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br>DEBUG=C2=A0=C2=A0 =
[U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:247:main()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : =
Set prompt to: Singularity.c7&gt;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:249:main()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we are=
 opening image as read/write<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:264:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening image as read/wr=
ite only: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:271:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting exclusive lock on file descript=
or: 6<br>ERROR=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:273:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Image is locked by another process<span><br>[=
root@mach0 ~]# lsof c7<br></span>[root@mach0 ~]# lslocks<br>COMMAND=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE =
MODE=C2=A0 M START END PATH<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 601 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=
=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 33B WRITE =
0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.<wb=
r>pid<br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 826 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 =
/var/lib/postfix/master.lock<br>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 478 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0=C2=A0=C2=A0 0 /run/lvmetad.pid<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 940 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=
=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmctld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 966 POSIX=C2=A0=C2=A0 4B WRIT=
E 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmd.pid<br>(unknown)=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2299 FLOCK=C2=A0=C2=A0 0B READ=C2=
=A0 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br><br><br>On Thursday, Jul=
y 14, 2016 at 1:03:50 PM UTC-6, Gregory M. Kurtzer wrote:<div><div><blockqu=
ote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1e=
x;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-styl=
e:solid"><div dir=3D"ltr"><br><div><br><div class=3D"gmail_quote">On Thu, J=
ul 14, 2016 at 11:50 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nof=
ollow">sg...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_=
quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color=
:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><div dir=
=3D"ltr">Gregory,<br><br>Thanks for the quick suggestions.=C2=A0 There don&=
#39;t seem to be any processes attached to the container and I can&#39;t se=
em to run other commands (in read mode). I&#39;m not sure what&#39;s going =
on.<br><br>Just lazy with root right now, I need to create a normal uid.<br=
></div></blockquote><div><br></div><div>Ahh, ok. Been there, done that! lol=
</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0p=
x 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-=
left-width:1px;border-left-style:solid"><div dir=3D"ltr"><br>Steve<br><br><=
span style=3D"font-family:&quot;courier new&quot;,monospace">[root@mach0 ~]=
# ls -la c7<br>-rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7<br>[root@m=
ach0 ~]# singularity shell -w c7<span><br>ERROR=C2=A0 : Image is locked by =
another process<br></span></span></div></blockquote><div><br></div><div>Can=
 you run this command again in --debug mode (singularity --debug ....)</div=
><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px=
 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-=
width:1px;border-left-style:solid"><div dir=3D"ltr"><span style=3D"font-fam=
ily:&quot;courier new&quot;,monospace"><span></span>[root@mach0 ~]# lsof c7=
<br></span></div></blockquote><div><br></div><div>What about the command &q=
uot;lslocks&quot;?</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(20=
4,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">=
<span style=3D"font-family:&quot;courier new&quot;,monospace">[root@mach0 ~=
]# singularity shell c7 whoami<br>/usr/bin/whoami: /usr/bin/whoami: cannot =
execute binary file<br></span></div></blockquote><div><br></div><div>Ahh, t=
his is normal. You are asking the shell script to read in /usr/bin/whoami. =
If you want to use shell to run whoami, you must prefix it with the -c (e.g=
. -c &quot;whoami [args]&quot;), or use the &#39;exec&#39; Singularity subc=
ommand: &quot;singularity exec c7 whoami&quot;</div><div>=C2=A0</div><block=
quote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:=
1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-st=
yle:solid"><div dir=3D"ltr"><span style=3D"font-family:&quot;courier new&qu=
ot;,monospace">[root@mach0 ~]#<br>[root@mach0 ~]# ps -ef |grep c7<br>root=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 7479=C2=A0 2002=C2=A0 0 11:49 pts/0=C2=A0=C2=
=A0=C2=A0 00:00:00 grep --color=3Dauto c7<br>[root@mach0 ~]#</span></div></=
blockquote><div><br></div><div>What kind of file system does your image exi=
st on? Is it NFS by chance? I wonder if there is a host issue with a lockin=
g daemon or something else weird going on where it is not giving the exclus=
ive lock properly. If this is NFS or other non local file system, can you c=
opy the image to /tmp, rerun the MPI command to get it to fail again, and t=
ry again?=C2=A0</div><div><br></div><div>Thanks!</div><div><br></div><div><=
br></div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin=
:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);bord=
er-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><span><br><br><=
br><br>On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M. Kurtzer =
wrote:</span><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width=
:1px;border-left-style:solid"><span><div dir=3D"ltr">Hi Steve,<div><br></di=
v><div>That means there is an active file descriptor/process still running =
and attached to the container maintaining a shared lock. You can run other =
commands against the container as long as long as the container is not bein=
g requested as --writable(-w), because that will try and obtain an exclusiv=
e lock and it will fail if there are any active shared locks. Try an &quot;=
lsof /path/to/c7&quot; to see what processes are attached to it. You may se=
e a list like:</div><div><br></div><div><div># lsof /tmp/Demo-2.img=C2=A0</=
div><div>COMMAND =C2=A0 =C2=A0PID USER =C2=A0 FD =C2=A0 TYPE DEVICE =C2=A0 =
SIZE/OFF =C2=A0 =C2=A0 =C2=A0NODE NAME</div><div>sexec =C2=A0 107975 root =
=C2=A0 =C2=A06rR =C2=A0REG =C2=A0253,0 1073741856 202112247 /tmp/Demo-2.img=
</div><div>sexec =C2=A0 107977 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 =
1073741856 202112247 /tmp/Demo-2.img</div><div>bash =C2=A0 =C2=A0107982 roo=
t =C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 1073741856 202112247 /tmp/Demo-2.i=
mg</div></div><div><br></div><div>Notice the two top ones are &#39;sexec&#3=
9; which are part of the Singularity process stack. Kill the bottom one, an=
d those should go away naturally.</div><div><br></div><div>BTW, as long as =
you have installed Singularity as root, there is no need to run Singularity=
 commands as root (unless you want to make system changes within the contai=
ner).</div><div><br></div><div>Hope that helps!</div><div><br></div></div><=
/span><div><br><div class=3D"gmail_quote"><div><div>On Thu, Jul 14, 2016 at=
 10:56 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@g=
mail.com</a>&gt;</span> wrote:<br></div></div><blockquote class=3D"gmail_qu=
ote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:r=
gb(204,204,204);border-left-width:1px;border-left-style:solid"><div><div><d=
iv dir=3D"ltr">Running mpirun tests, when an abort occurs, my image ends up=
 locked.=C2=A0 Is there a way to clear the lock without rebooting?=C2=A0 I =
looked for processes that I could kill, but didn&#39;t see anything worthy.=
<br><br>I&#39;m using singularity v2.1 on Centos 7.2 (both host and contain=
er).<br><br>Regards,<br><br>Steve<br><br><span style=3D"font-family:&quot;c=
ourier new&quot;,monospace">[root@mach0 ~]# mpirun --allow-run-as-root -n 2=
 -H mach1,mach2 singularity exec c7 /usr/bin/ring<br>Process 0 sending 10 t=
o 1, tag 201 (2 processes in ring)<br>Process 0 sent to 1<br>Process 0 decr=
emented value: 9<br>Process 0 decremented value: 8<br>Process 0 decremented=
 value: 7<br>Process 0 decremented value: 6<br>Process 0 decremented value:=
 5<br>Process 0 decremented value: 4<br>Process 0 decremented value: 3<br>P=
rocess 0 decremented value: 2<br>Process 0 decremented value: 1<br>Process =
0 decremented value: 0<br>Process 0 exiting<br>Process 1 exiting<br>[root@m=
ach0 ~]# mpirun --allow-run-as-root -n 3 -H mach0,mach1,mach2 singularity e=
xec c7 /usr/bin/ring<br>------------------------------<wbr>----------------=
--------------<wbr>--------------<br>It appears as if there is not enough s=
pace for /tmp/ompi.mach0.2291/54935/1/<wbr>0/vader_segment.mach0.0 (the sha=
red-memory backing<br>file). It is likely that your MPI job will now either=
 abort or experience<br>performance degradation.<br><br>=C2=A0 Local host:=
=C2=A0 mach0<br>=C2=A0 Space Requested: 4194312 B<br>=C2=A0 Space Available=
: 0 B<br>------------------------------<wbr>------------------------------<=
wbr>--------------<br>[mach0:02308] create_and_attach: unable to create sha=
red memory BTL coordinating structure :: size 134217728<br>[mach0:02291] 2 =
more processes have sent help message help-opal-shmem-mmap.txt / target ful=
l<br>[mach0:02291] Set MCA parameter &quot;orte_base_help_aggregate&quot; t=
o 0 to see all help / error messages<br>^CKilled by signal 2.<br>Killed by =
signal 2.<br>Singularity is sending SIGKILL to child pid: 2308<br>Singulari=
ty is sending SIGKILL to child pid: 2309<br>[warn] Epoll ADD(4) on fd 31 fa=
iled.=C2=A0 Old events were 0; read change was 0 (none); write change was 1=
 (add): Bad file descriptor<br>^C[root@mach0 ~]singularity shell -w c7<br>E=
RROR=C2=A0 : Image is locked by another process<br>[root@mach0 ~]# tail -30=
 /var/log/messages<br>Jul 14 10:42:17 mach0 systemd: Started LSB: slurm dae=
mon management.<br>Jul 14 10:42:17 mach0 systemd: Reached target Multi-User=
 System.<br>Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.<br>J=
ul 14 10:42:17 mach0 systemd: Starting Update UTMP about System Runlevel Ch=
anges...<br>Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data Col=
lection 10s After Completed Startup.<br>Jul 14 10:42:17 mach0 systemd: Star=
ted Update UTMP about System Runlevel Changes.<br>Jul 14 10:42:18 mach0 kdu=
mpctl: kexec: loaded kdump kernel<br>Jul 14 10:42:18 mach0 kdumpctl: Starti=
ng kdump: [OK]<br>Jul 14 10:42:18 mach0 systemd: Started Crash recovery ker=
nel arming.<br>Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms (ke=
rnel) + 1.100s (initrd) + 4.931s (userspace) =3D 6.446s.<br>Jul 14 10:42:34=
 mach0 systemd: Created slice user-0.slice.<br>Jul 14 10:42:34 mach0 system=
d: Starting user-0.slice.<br>Jul 14 10:42:34 mach0 systemd-logind: New sess=
ion 1 of user root.<br>Jul 14 10:42:34 mach0 systemd: Started Session 1 of =
user root.<br>Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user roo=
t.<br>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D2023)&gt; Command=
=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:=
36 mach0 Singularity: sexec (U=3D0,P=3D2024)&gt; Command=3Dexec, Container=
=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:36 mach0 Singulari=
ty: sexec (U=3D0,P=3D2024)&gt; Image is locked by another process<br>Jul 14=
 10:42:36 mach0 kernel: loop: module loaded<br>Jul 14 10:43:38 mach0 Singul=
arity: sexec (U=3D0,P=3D2050)&gt; Command=3Dshell, Container=3Dc7, CWD=3D/r=
oot, Arg1=3D(null)<br>Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounte=
d filesystem with ordered data mode. Opts: discard<br>Jul 14 10:49:17 mach0=
 Singularity: sexec (U=3D0,P=3D2203)&gt; Command=3Dshell, Container=3Dc7, C=
WD=3D/root, Arg1=3D(null)<br>Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0):=
 mounted filesystem with ordered data mode. Opts: discard<br>Jul 14 10:50:3=
9 mach0 Singularity: sexec (U=3D0,P=3D2244)&gt; Command=3Dexec, Container=
=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:50:39 mach0 kernel: E=
XT4-fs (loop0): mounted filesystem with ordered data mode. Opts: discard<br=
>Jul 14 10:51:34 mach0 Singularity: sexec (U=3D0,P=3D2299)&gt; Command=3Dex=
ec, Container=3Dc7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 ma=
ch0 Singularity: sexec (U=3D0,P=3D2300)&gt; Command=3Dexec, Container=3Dc7,=
 CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mach0 kernel: EXT4-fs=
 (loop0): mounted filesystem with ordered data mode. Opts: discard<br>Jul 1=
4 10:51:57 mach0 Singularity: sexec (U=3D0,P=3D2322)&gt; Command=3Dshell, C=
ontainer=3Dc7, CWD=3D/root, Arg1=3D(null)<br>Jul 14 10:51:57 mach0 Singular=
ity: sexec (U=3D0,P=3D2322)&gt; Image is locked by another process</span><s=
pan><font color=3D"#888888"><br><br><br></font></span></div></div></div><sp=
an><font color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance C=
omputing Services (HPCS)<br>University of California<br>Lawrence Berkeley N=
ational Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></d=
iv>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
o6EbX8iTAQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div></div></div></div></div>
</blockquote></div><br><br clear=3D"all"><div><br></div>-- <br><div><div di=
r=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (H=
PCS)<br>University of California<br>Lawrence Berkeley National Laboratory<b=
r>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>
------=_Part_3973_1138197283.1468854990912--

------=_Part_3972_1758367883.1468854990911--
