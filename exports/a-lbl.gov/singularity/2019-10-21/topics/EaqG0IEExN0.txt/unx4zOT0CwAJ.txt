X-Received: by 10.129.145.212 with SMTP id i203mr13511558ywg.7.1465412946953;
        Wed, 08 Jun 2016 12:09:06 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.166.206 with SMTP id p197ls205260ioe.39.gmail; Wed, 08 Jun
 2016 12:09:06 -0700 (PDT)
X-Received: by 10.36.50.132 with SMTP id j126mr14950623ita.70.1465412946435;
        Wed, 08 Jun 2016 12:09:06 -0700 (PDT)
Return-Path: <goh...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id r10si2736735pad.67.2016.06.08.12.09.06
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 08 Jun 2016 12:09:06 -0700 (PDT)
Received-SPF: pass (google.com: domain of goh...@gmail.com designates 209.85.214.50 as permitted sender) client-ip=209.85.214.50;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of goh...@gmail.com designates 209.85.214.50 as permitted sender) smtp.mailfrom=goh...@gmail.com
X-Ironport-SBRS: 1.9
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EQEQBzbFhXdzLWVdFeg1w4fQamIAEBBQwBBgEBAQEBBYEPgkyFRYYlhjZCHoczBzsRAQEBAQEBAQMPAQoLCwkfMYI6OQoGLAEBAQEBAQEBAQEBAQEBARoCKxMSNBEdARseAxIDDTcCJAERAQUBV4dyAQMXBY8ujTuCB4ExPjGLO4FqglgFh3kKGScNUoNWAgYQhU+JJxEBgx2CWQWHfAGHHok0gTCEU4gjgjeMaI4iEh6BDw8lgVYMRRyBTToyiEsECxeBHgEBAQ
X-IronPort-AV: E=Sophos;i="5.26,440,1459839600"; 
   d="scan'208,217";a="25836056"
Received: from mail-it0-f50.google.com ([209.85.214.50])
  by fe4.lbl.gov with ESMTP; 08 Jun 2016 12:09:04 -0700
Received: by mail-it0-f50.google.com with SMTP id z123so16952288itg.0
        for <singu...@lbl.gov>; Wed, 08 Jun 2016 12:09:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to;
        bh=kHWYkn6dXe5N5JVRrZ6RiKGc/bVw/NYEe3qbZzYGePY=;
        b=B3SLyD++HQjYE3fo4x2Bg12siMu/IzYi4aqDl8HfwmIOIrUT3ellmd5Wv6IcmHoS6/
         FPzkWEdW41SSO5JYPADBZiL4aidmHPoXWJTd4iCm9xbQUHD3/1awqFgWocvxlz52rwH6
         VNMaY0d6ZWa9wccsdhbKBScTyWgwSC9bkCsLYbUjDLBaUmj+e4CtTPpInll+t+kmmSR3
         dsP2uA8ng2dYabU+cC7LaKX62sD5PU9cOeCuvyNt95Vxr4Vm934/aPIlYZ54fclDfnnH
         Sco11aF45Dyon3L0tocuZ2wJ9PKzb3XL4MVRww5G1LkSIL0BYJHTVv8dwVaJ5k19G8cp
         ymIA==
X-Gm-Message-State: ALyK8tKGpIKVFZj1QwLNqNZ5oFPPkt6BCxIFPAG17CxctJu4LAAdgkCDLIqlFrRO7pr3pbXdix611AGENFQtzQ==
X-Received: by 10.36.37.73 with SMTP id g70mr14920483itg.51.1465412944535;
 Wed, 08 Jun 2016 12:09:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.50.227.142 with HTTP; Wed, 8 Jun 2016 12:09:04 -0700 (PDT)
From: yiannis georgiou <goh...@gmail.com>
Date: Wed, 8 Jun 2016 21:09:04 +0200
Message-ID: <CA+zw9q3gSaqZgo3tzOXyn5kwfeUS-LdCuyTnmWyPgfVjmxGBqA@mail.gmail.com>
Subject: Singularity with Slurm and PMIx
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a1145a850022be90534c90a75

--001a1145a850022be90534c90a75
Content-Type: text/plain; charset=UTF-8

Hello,

I'm trying to execute an MPI application from within a singularity image
through Slurm and pmix using the following command

srun --mpi=pmix -n 4 -N 2 singularity exec /tmp/ubuntu_v4.img
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello

The slurmstepd process is spawned on the compute node and it seems to have
launched correctly the application from what we see on one of the compute
nodes:

========================================
[root@rio12 ~]# ps -aux
root     35147  0.1  0.0 562284  4016 ?        Sl   20:06   0:00
slurmstepd: [83.0]
georgioy 35158  0.2  0.0  10428   896 ?        S    20:06   0:00
Singularity: namespace
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
root     35162  0.0  0.0      0     0 ?        S<   20:06   0:00 [loop0]
georgioy 35163  0.0  0.0  10428   400 ?        S    20:06   0:00
Singularity: exec
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
root     35164  0.0  0.0      0     0 ?        S    20:06   0:00
[jbd2/loop0-8]
root     35165  0.0  0.0      0     0 ?        S    20:06   0:00
[ext4-dio-unwrit]
georgioy 35166  102  0.0 285476 14828 ?        RLl  20:06   0:12
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
==========================================

and I see the pmix related functions starting correctly in the slurmd log
files :

========================================

[2016-06-08T20:12:08.004] [86.0] debug:  (null) [0] mpi_pmix.c:90
[p_mpi_hook_slurmstepd_prefork] mpi/pmix: start
[2016-06-08T20:12:08.004] [86.0] debug:  mpi/pmix: setup sockets
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:78
[errhandler_reg_callbk] mpi/pmix: Error handler registration callback is
called with status=0, ref=0
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:581
[pmixp_libpmix_job_set] mpi/pmix: task initialization
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:220
[_agent_thread] mpi/pmix: Start agent thread
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:313
[pmixp_agent_start] mpi/pmix: agent thread started: tid = 139672278615808
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84
[_conn_readable] mpi/pmix: fd = 9
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84
[_conn_readable] mpi/pmix: fd = 19
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:256
[_pmix_timer_thread] mpi/pmix: Start timer thread
[2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:335
[pmixp_agent_start] mpi/pmix: timer thread started: tid = 139672277563136

=======================================

However the application is never actually started and the srun fails
completely with the following output:

=========================================
[georgioy@rio11 ~]$ srun --mpi=pmix -n 4 -N 2 singularity exec
/tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
[rio12:00001] *** Process received signal ***
[rio12:00001] Signal: Segmentation fault (11)
[rio12:00001] Signal code: Address not mapped (1)
[rio12:00001] Failing at address: 0x7fbd937e6010
[rio12:00001] [ 0] [rio12:00001] *** Process received signal ***
/lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f26aeb233d0]
[rio12:00001] [ 1]
/usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f269f964035]
[rio12:00001] [ 2]
/usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f269f964583]
[rio12:00001] [ 3]
/usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7f269fb68771]
[rio12:00001] [ 4]
/usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f26a451fbe7]
[rio12:00001] [ 5]
/usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f269ef35662]
[rio12:00001] [ 6]
/usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f26aed75d31]
[rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26aed9bdb9]
[rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]
[rio12:00001] [ 9]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f26ae769830]
[rio12:00001] Signal: Segmentation fault (11)
[rio12:00001] Signal code: Address not mapped (1)
[rio12:00001] Failing at address: 0x7fbd937e6010
[rio12:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]
[rio12:00001] *** End of error message ***
[rio12:00001] [ 0]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f3f89c063d0]
[rio12:00001] [ 1]
/usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f3f7eb1d035]
[rio12:00001] [ 2]
/usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f3f7eb1d583]
[rio12:00001] [ 3]
/usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7f3f7ed21771]
[rio12:00001] [ 4]
/usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f3f7f5f3be7]
[rio12:00001] [ 5]
/usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f3f7e0ee662]
[rio12:00001] [ 6]
/usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f3f89e58d31]
[rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7edb9]
[rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]
[rio12:00001] [ 9]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f3f8984c830]
[rio12:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]
[rio12:00001] *** End of error message ***
slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix:
ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 86.0 ON rio12 CANCELLED AT 2016-06-08T20:12:08
***
slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix:
ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
slurmstepd: error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix:
ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix:
ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
srun: error: rio12: tasks 0-2: Killed
srun: error: rio13: task 3: Killed

============================================

the results I get when launching with mpirun instead of srun are more or
less the same.

From my understanding, the orted process, or in my case slurmstepd process
that launches the singularity container
and the MPI application, enables the communication of MPI libraries and
orted (or slurmstepd) through PMI (in my case here PMIx).
So I suppose the problem I see should be related with the mapping of PMI
from the container towards the orted or slurmstepd.

What do you think?

To give some more details, my singularity container has OpenMPI and PMIx
installed but not Slurm. I don't think that Slurm needs to reside within
the container in that context.
But I wasn't sure if OpenMPI and PMIx are needed both inside and outside of
the container.
So, I've tried using the exact same version of PMIx and OpenMPI in and out
of the container and the problem still persists.
The experiments have been done using RedHat hosts with CentOS or Ubuntu
singularity containers and the latest github versions of slurm, OpenMPI,
PMIx and singularity.

By the way, do you have any MPI example for singularity v2?
Because the MPI example that you show here : http://singularity.lbl.gov/#hpc

is actually done using singularity v1, no?
In my understanding with singularity v2 we actually build a complete image
with OS not just the application with its libraries. Is this correct?

Sorry for the long email and
thanks a lot for any extra info you can provide regarding singularity v2
and MPI.

Best Regards,
Yiannis

--001a1145a850022be90534c90a75
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div>Hello,<br><br></div>I&#39;m trying to execu=
te an MPI application from within a singularity image through Slurm and pmi=
x using the following command <br><br></div>srun --mpi=3Dpmix -n 4 -N 2 sin=
gularity exec
 /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br><br><=
/div>The
 slurmstepd process is spawned on the compute node and it seems to have=20
launched correctly the application from what we see on one of the=20
compute nodes:<br><div><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br=
>[root@rio12 ~]# ps -aux<br>root=C2=A0=C2=A0=C2=A0=C2=A0 35147=C2=A0 0.1=C2=
=A0 0.0 562284=C2=A0 4016 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 Sl=C2=
=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 slurmstepd: [83.0]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 <br>georgioy
 35158=C2=A0 0.2=C2=A0 0.0=C2=A0 10428=C2=A0=C2=A0 896 ?=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 Singula=
rity:=20
namespace=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=20
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>root=C2=A0=C2=A0=C2=A0=C2=
=A0 35162=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S&lt;=C2=A0=C2=
=A0 20:06=C2=A0=C2=A0 0:00 [loop0]<br>georgioy
 35163=C2=A0 0.0=C2=A0 0.0=C2=A0 10428=C2=A0=C2=A0 400 ?=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 Singula=
rity:=20
exec=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=20
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>root=C2=A0=C2=A0=C2=A0=C2=
=A0 35164=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=
=A0 20:06=C2=A0=C2=A0 0:00 [jbd2/loop0-8]<br>root=C2=A0=C2=A0=C2=A0=C2=A0 3=
5165=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20=
:06=C2=A0=C2=A0 0:00 [ext4-dio-unwrit]<br>georgioy 35166=C2=A0 102=C2=A0 0.=
0 285476 14828 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 RLl=C2=A0 20:06=
=C2=A0=C2=A0 0:12 /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br></div><div>and I see t=
he pmix related functions starting correctly in the slurmd log files :<br><=
br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br>[2016-06-08T20:12:0=
8.004] [86.0] debug:=C2=A0 (null) [0] mpi_pmix.c:90 [p_mpi_hook_slurmstepd_=
prefork] mpi/pmix: start<br>[2016-06-08T20:12:08.004] [86.0] debug:=C2=A0 m=
pi/pmix: setup sockets<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_client.c:78 [errhandler_reg_callbk]=20
mpi/pmix: Error handler registration callback is called with status=3D0,=20
ref=3D0<br>[2016-06-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_cl=
ient.c:581 [pmixp_libpmix_job_set] mpi/pmix: task initialization<br>[2016-0=
6-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:220 [_agent_=
thread] mpi/pmix: Start agent thread<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:313 [pmixp_agent_start]=20
mpi/pmix: agent thread started: tid =3D 139672278615808<br>[2016-06-08T20:1=
2:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:84 [_conn_readable] m=
pi/pmix: fd =3D 9<br>[2016-06-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0=
] pmixp_agent.c:84 [_conn_readable] mpi/pmix: fd =3D 19<br>[2016-06-08T20:1=
2:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:256 [_pmix_timer_thre=
ad] mpi/pmix: Start timer thread<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:335 [pmixp_agent_start]=20
mpi/pmix: timer thread started: tid =3D 139672277563136<br><br>=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br></div><div><div><br></div><div>However th=
e application is never actually started and the srun fails completely with =
the following output:<br></div><div><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D<br>[georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N 2 singular=
ity exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<=
br>[rio12:00001] *** Process received signal ***<br>[rio12:00001] Signal: S=
egmentation fault (11)<br>[rio12:00001] Signal code: Address not mapped (1)=
<br>[rio12:00001] Failing at address: 0x7fbd937e6010<br>[rio12:00001] [ 0] =
[rio12:00001] *** Process received signal ***<br>/lib/x86_64-linux-gnu/libp=
thread.so.0(+0x113d0)[0x7f26aeb233d0]<br>[rio12:00001] [ 1] /usr/local/lib/=
libmca_common_sm.so.0(+0x1035)[0x7f269f964035]<br>[rio12:00001] [ 2] /usr/l=
ocal/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f269f964583]=
<br>[rio12:00001] [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_=
procs+0x5f1)[0x7f269fb68771]<br>[rio12:00001] [ 4] /usr/local/lib/openmpi/m=
ca_bml_r2.so(+0x2be7)[0x7f26a451fbe7]<br>[rio12:00001] [ 5] /usr/local/lib/=
openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f269ef35662]<br>[rio1=
2:00001] [ 6] /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f26aed75d3=
1]<br>[rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26ae=
d9bdb9]<br>[rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hell=
o[0x4007ec]<br>[rio12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_st=
art_main+0xf0)[0x7f26ae769830]<br>[rio12:00001] Signal: Segmentation fault =
(11)<br>[rio12:00001] Signal code: Address not mapped (1)<br>[rio12:00001] =
Failing at address: 0x7fbd937e6010<br>[rio12:00001] [10] /home_nfs/georgioy=
/BENCHS/mpi-openmp/mpi_hello[0x400709]<br>[rio12:00001] *** End of error me=
ssage ***<br>[rio12:00001] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11=
3d0)[0x7f3f89c063d0]<br>[rio12:00001] [ 1] /usr/local/lib/libmca_common_sm.=
so.0(+0x1035)[0x7f3f7eb1d035]<br>[rio12:00001] [ 2] /usr/local/lib/libmca_c=
ommon_sm.so.0(common_sm_mpool_create+0xa3)[0x7f3f7eb1d583]<br>[rio12:00001]=
 [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7f=
3f7ed21771]<br>[rio12:00001] [ 4] /usr/local/lib/openmpi/mca_bml_r2.so(+0x2=
be7)[0x7f3f7f5f3be7]<br>[rio12:00001] [ 5] /usr/local/lib/openmpi/mca_pml_o=
b1.so(mca_pml_ob1_add_procs+0xd2)[0x7f3f7e0ee662]<br>[rio12:00001] [ 6] /us=
r/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f3f89e58d31]<br>[rio12:0000=
1] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7edb9]<br>[rio12=
:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]<br>[r=
io12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x=
7f3f8984c830]<br>[rio12:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mp=
i_hello[0x400709]<br>[rio12:00001] *** End of error message ***<br>slurmste=
pd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>srun: Job s=
tep aborted: Waiting up to 32 seconds for job step to finish.<br>slurmstepd=
: error: *** STEP 86.0 ON rio12 CANCELLED AT 2016-06-08T20:12:08 ***<br>slu=
rmstepd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>slurmstepd:=
=20
error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>slurmstepd:=
=20
error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>srun: error:=
 rio12: tasks 0-2: Killed<br>srun: error: rio13: task 3: Killed<br><br>=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br></div><div>th=
e results I get when launching with mpirun instead of srun are more or less=
 the same. <br><br></div><div>From my understanding, the orted process, or =
in my case slurmstepd process that launches the singularity container <br><=
/div><div>and
 the MPI application, enables the communication of MPI libraries and=20
orted (or slurmstepd) through PMI (in my case here PMIx).<br></div><div>So =
I suppose the problem I see should be related with the mapping of PMI from =
the container towards the orted or slurmstepd.<br><br></div><div>What do yo=
u think?<br></div><div><br></div><div>To
 give some more details, my singularity container has OpenMPI and PMIx=20
installed but not Slurm. I don&#39;t think that Slurm needs to reside withi=
n
 the container in that context.<br></div><div>But I wasn&#39;t sure if Open=
MPI and PMIx are needed both inside and outside of the container.<br></div>=
<div>So, I&#39;ve tried using the exact same version of PMIx and OpenMPI in=
 and out of the container and the problem still persists. <br></div><div>Th=
e
 experiments have been done using RedHat hosts with CentOS or Ubuntu=20
singularity containers and the latest github versions of slurm, OpenMPI,
 PMIx and singularity.<br></div><div><br></div><div>By the way, do you have=
 any MPI example for singularity v2? <br></div><div>Because the MPI example=
 that you show here : <a href=3D"http://singularity.lbl.gov/#hpc" target=3D=
"_blank">http://singularity.lbl.gov/#hpc</a><br><br></div><div>is actually =
done using singularity v1, no? <br>In
 my understanding with singularity v2 we actually build a complete image
 with OS not just the application with its libraries. Is this correct?<br><=
/div><div><br></div><div>Sorry for the long email and<br>thanks a lot for a=
ny extra info you can provide regarding singularity v2 and MPI.<br><br></di=
v><div>Best Regards,<br></div><div>Yiannis</div></div></div>

--001a1145a850022be90534c90a75--
