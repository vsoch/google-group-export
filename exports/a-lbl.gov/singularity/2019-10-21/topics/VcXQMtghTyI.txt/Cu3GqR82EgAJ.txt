X-Received: by 10.36.230.68 with SMTP id e65mr1150097ith.28.1484069347497;
        Tue, 10 Jan 2017 09:29:07 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.51.14 with SMTP id k14ls840950itk.15.canary-gmail; Tue, 10
 Jan 2017 09:29:05 -0800 (PST)
X-Received: by 10.98.149.93 with SMTP id p90mr5174807pfd.72.1484069345722;
        Tue, 10 Jan 2017 09:29:05 -0800 (PST)
Return-Path: <l...@penguincomputing.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id l7si2803502pfc.49.2017.01.10.09.29.05
        for <singu...@lbl.gov>;
        Tue, 10 Jan 2017 09:29:05 -0800 (PST)
Received-SPF: pass (google.com: domain of l...@penguincomputing.com designates 209.85.213.174 as permitted sender) client-ip=209.85.213.174;
Authentication-Results: mx.google.com;
       dkim=pass head...@penguincomputing.com;
       spf=pass (google.com: domain of l...@penguincomputing.com designates 209.85.213.174 as permitted sender) smtp.mailfrom=l...@penguincomputing.com
X-Ironport-SBRS: 0.0
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FCAQCJGHVYZK7VVdFdHAEBBAEBCgEBFwEBBAEBCgEBgw8BAQEBAR+BbAeDQAiKCJImh3+HfYUrggsfAYYCAoF8Bz8UAQEBAQEBAQEBAQECARwLCgcfMIIzBAIDEgUITQEBAQEBAQEBAUwCDV0BAQEDASMdAQEMLA8LCw0jBwICIQEPAwEFARwOBwQBHASINAMQCAWkEz+LG2iCJYMIAQEFRoNrDYJIAQEIAQEBAQEBGggShjOEYYJOgUoRAYJqOoJeiGYNB4YlfUiERIVIOI1Tg3+QYYoLhw0UHoEUH3wtUBVMBIQLDxwYgWUgNQeGMUeBZwEBAQ
X-IronPort-AV: E=Sophos;i="5.33,344,1477983600"; 
   d="scan'208,217";a="60430366"
Received: from mail-yb0-f174.google.com ([209.85.213.174])
  by fe3.lbl.gov with ESMTP; 10 Jan 2017 09:29:04 -0800
Received: by mail-yb0-f174.google.com with SMTP id v132so110040535yba.0
        for <singu...@lbl.gov>; Tue, 10 Jan 2017 09:29:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=penguincomputing.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=KbK9OYA5CBBegJlZ3ZmM0xKr9w93l7g/Xc9Qh9lJMu0=;
        b=aE41/fgqnlOHBPbSzSGb/ZMnLdHcL9an7rwx86BwlE/ywWtF+VaAnj7/xqYoIPuaUR
         J33XBCZmECXYih13XGMAbaUX6s7kWpOZx1OrND3e4UjLBJCqQtY/ot7IU1jX5RyRkIyv
         dJvosdi0t5khHQ3jiSnJ7ftTcSFAKPxI24CsQ=
X-Gm-Message-State: AIkVDXJM20Fyx/79UzFqWCM32Wo/SWRf70kAKDmPpLnIaK8l2VtjODGQzPA1O6P2gBO0SDX7ZbFjTIScGoOcEFBQ
X-Received: by 10.37.172.15 with SMTP id w15mr1010061ybi.93.1484069343662;
 Tue, 10 Jan 2017 09:29:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.129.165.129 with HTTP; Tue, 10 Jan 2017 09:28:43 -0800 (PST)
In-Reply-To: <91549d58-7ec6-43d2-978a-9c357e227f04@lbl.gov>
References: <1a497544-c3e1-45c2-9e60-9de178912467@lbl.gov> <91549d58-7ec6-43d2-978a-9c357e227f04@lbl.gov>
From: Limin Gu <l...@penguincomputing.com>
Date: Tue, 10 Jan 2017 12:28:43 -0500
Message-ID: <CAAU_HgbuOi_9OhLCMmm=a3jvzeK9gX34ZPEpLsgcPs9Guca0Fw@mail.gmail.com>
Subject: Re: [Singularity] Re: Failed to run openmpi program within the
 container by calling mpirun on the host
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=f403045db0fe0d6a890545c0d2cb

--f403045db0fe0d6a890545c0d2cb
Content-Type: text/plain; charset=UTF-8

Thanks Matt!

I tried again from scratch, it is working now.

Limin

On Tue, Jan 10, 2017 at 10:04 AM, Matthew Dwyer <
mdwyer.pr...@gmail.com> wrote:

>
> I received the same error when the versions of openmpi differed on the
> host and within the container. Particularly, I received the null
> communicator error when the host's version of ompi was less than the ompi
> version within the container. When the host's version is greater than that
> within the container you will get a not found error. Run ompi_info from the
> host and container and make sure they are the same. Make sure you are
> installing ompi 2.0.1, installing 1.10.X will not work regardless of
> whether or not the versions match up.
>
> ompi_info | grep MPI:
> singularity exec Centos7-ompi.img ompi_info | grep MPI:
>
> I do not know if the errors are general or just for ompi versions 1.10.3/4
> and 2.0.1. I was using an ubuntu 16.04 container and singularity 2.2.
>
> --Matt
>
>
> On Monday, January 9, 2017 at 9:47:47 PM UTC-5, Limin Gu wrote:
>>
>> Hi All,
>>
>> I have a CentOS 7.2 host, I installed singularity 2.2 on the host and
>> used  centos7 bootstrap definition file to successfully created a
>> Centos7-ompi.img, also on the host I installed openmpi the same way as it
>> was done inside the container.
>>
>> I am able to run openmpi on the host:
>>
>> [root@uranus tetuser]# /usr/local/bin/mpirun --allow-run-as-root
>> /usr/bin/mpi_ring
>>
>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>
>> Process 0 sent to 1
>>
>> Process 0 decremented value: 9
>>
>> Process 0 decremented value: 8
>>
>> Process 0 decremented value: 7
>>
>> Process 0 decremented value: 6
>>
>> Process 0 decremented value: 5
>>
>> Process 0 decremented value: 4
>>
>> Process 0 decremented value: 3
>>
>> Process 0 decremented value: 2
>>
>> Process 0 decremented value: 1
>>
>> Process 0 decremented value: 0
>>
>> Process 0 exiting
>>
>> Process 1 exiting
>>
>> [root@uranus tetuser]#
>>
>>
>> I am able to run openmpi inside the container also:
>>
>>
>> [root@uranus tetuser]# /opt/singularity/bin/singularity exec
>> ./Centos7-ompi.img /usr/local/bin/mpirun --allow-run-as-root
>> /usr/bin/mpi_ring
>>
>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>
>> Process 0 sent to 1
>>
>> Process 0 decremented value: 9
>>
>> Process 0 decremented value: 8
>>
>> Process 0 decremented value: 7
>>
>> Process 0 decremented value: 6
>>
>> Process 0 decremented value: 5
>>
>> Process 0 decremented value: 4
>>
>> Process 0 decremented value: 3
>>
>> Process 0 decremented value: 2
>>
>> Process 0 decremented value: 1
>>
>> Process 0 decremented value: 0
>>
>> Process 0 exiting
>>
>> Process 1 exiting
>>
>> [root@uranus tetuser]#
>>
>>
>>
> But when I try to run openmpi within the container by calling mpirun on
>> the host, it fails with those messages:
>>
>>
>> [root@uranus tetuser]# /usr/local/bin/mpirun --allow-run-as-root
>> /opt/singularity/bin/singularity exec ./Centos7-ompi.img
>> /usr/bin/mpi_ring
>>
>> *** An error occurred in MPI_Init
>>
>> *** on a NULL communicator
>>
>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
>>
>> ***    and potentially your MPI job)
>>
>> [uranus:47063] Local abort before MPI_INIT completed completed
>> successfully, but am not able to aggregate error messages, and not able to
>> guarantee that all other processes were killed!
>>
>> *** An error occurred in MPI_Init
>>
>> *** on a NULL communicator
>>
>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
>>
>> ***    and potentially your MPI job)
>>
>> [uranus:47061] Local abort before MPI_INIT completed completed
>> successfully, but am not able to aggregate error messages, and not able to
>> guarantee that all other processes were killed!
>>
>> -------------------------------------------------------
>>
>> Primary job  terminated normally, but 1 process returned
>>
>> a non-zero exit code. Per user-direction, the job has been aborted.
>>
>> -------------------------------------------------------
>>
>> ------------------------------------------------------------
>> --------------
>>
>> mpirun detected that one or more processes exited with non-zero status,
>> thus causing
>>
>> the job to be terminated. The first process to do so was:
>>
>>
>>   Process name: [[6415,1],1]
>>
>>   Exit code:    1
>>
>> ------------------------------------------------------------
>> --------------
>>
>> [root@uranus tetuser]#
>>
>>
>> What did I do wrong? How to fix this?
>>
>>
>> Thank you!
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--f403045db0fe0d6a890545c0d2cb
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Thanks Matt!<div><br></div><div>I tried again from scratch=
, it is working now.</div><div><br></div><div>Limin</div></div><div class=
=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Jan 10, 2017 at 10:=
04 AM, Matthew Dwyer <span dir=3D"ltr">&lt;<a href=3D"mailto:mdwyer.pr...@g=
mail.com" target=3D"_blank">mdwyer.pr...@gmail.com</a>&gt;</span> wrote:<br=
><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1=
px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><br></div>I received =
the same error when the versions of openmpi differed on the host and within=
 the container. Particularly, I received the null communicator error when t=
he host&#39;s version of ompi was less than the ompi version within the con=
tainer. When the host&#39;s version is greater than that within the contain=
er you will get a not found error. Run ompi_info from the host and containe=
r and make sure they are the same. Make sure you are installing ompi 2.0.1,=
 installing 1.10.X will not work regardless of whether or not the versions =
match up.=C2=A0<div><br></div><div>ompi_info | grep MPI:</div><div>singular=
ity exec Centos7-ompi.img ompi_info | grep MPI:</div><div><br></div><div>I =
do not know if the errors are general or just for ompi versions 1.10.3/4 an=
d 2.0.1. I was using an ubuntu 16.04 container and singularity 2.2.<div><di=
v><div><br></div><div>--Matt<div><div class=3D"h5"><br><br>On Monday, Janua=
ry 9, 2017 at 9:47:47 PM UTC-5, Limin Gu wrote:<blockquote class=3D"gmail_q=
uote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;paddin=
g-left:1ex"><div dir=3D"ltr">Hi All,<div><br></div><div>I have a CentOS 7.2=
 host, I installed singularity 2.2 on the host and used =C2=A0centos7 boots=
trap definition file to successfully created a Centos7-ompi.img, also on th=
e host I installed openmpi the same way as it was done inside the container=
.</div><div><br></div><div>I am able to run openmpi on the host:</div><div>=
<br></div><div><p style=3D"line-height:normal;font-family:Menlo"><span>[roo=
t@uranus tetuser]# /usr/local/bin/mpirun --allow-run-as-root /usr/bin/mpi_r=
ing</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 sending 1=
0 to 1, tag 201 (2 processes in ring)</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 sent to 1=
</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 9</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 8</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 7</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 6</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 5</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 4</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 3</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 2</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 1</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrement=
ed value: 0</span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 exiting</=
span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>Process 1 exiting</=
span></p>
<p style=3D"line-height:normal;font-family:Menlo"><span>[root@uranus tetuse=
r]#=C2=A0</span></p><p style=3D"line-height:normal;font-family:Menlo"><span=
><br></span></p><p style=3D"line-height:normal;font-family:Menlo"><span><sp=
an style=3D"font-family:Arial,Helvetica,sans-serif">I am able to run openmp=
i inside the container also:</span><br></span></p><p style=3D"line-height:n=
ormal;font-family:Menlo"><span><span style=3D"font-family:Arial,Helvetica,s=
ans-serif"><br></span></span></p><p style=3D"line-height:normal;font-family=
:Menlo"><span>[root@uranus tetuser]# /opt/singularity/bin/singulari<wbr>ty =
exec ./Centos7-ompi.img /usr/local/bin/mpirun --allow-run-as-root /usr/bin/=
mpi_ring</span></p><p style=3D"line-height:normal;font-family:Menlo"><span>=
Process 0 sending 10 to 1, tag 201 (2 processes in ring)</span></p><p style=
=3D"line-height:normal;font-family:Menlo"><span>Process 0 sent to 1</span><=
/p><p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 decrem=
ented value: 9</span></p><p style=3D"line-height:normal;font-family:Menlo">=
<span>Process 0 decremented value: 8</span></p><p style=3D"line-height:norm=
al;font-family:Menlo"><span>Process 0 decremented value: 7</span></p><p sty=
le=3D"line-height:normal;font-family:Menlo"><span>Process 0 decremented val=
ue: 6</span></p><p style=3D"line-height:normal;font-family:Menlo"><span>Pro=
cess 0 decremented value: 5</span></p><p style=3D"line-height:normal;font-f=
amily:Menlo"><span>Process 0 decremented value: 4</span></p><p style=3D"lin=
e-height:normal;font-family:Menlo"><span>Process 0 decremented value: 3</sp=
an></p><p style=3D"line-height:normal;font-family:Menlo"><span>Process 0 de=
cremented value: 2</span></p><p style=3D"line-height:normal;font-family:Men=
lo"><span>Process 0 decremented value: 1</span></p><p style=3D"line-height:=
normal;font-family:Menlo"><span>Process 0 decremented value: 0</span></p><p=
 style=3D"line-height:normal;font-family:Menlo"><span>Process 0 exiting</sp=
an></p><p style=3D"line-height:normal;font-family:Menlo"><span>Process 1 ex=
iting</span></p><p style=3D"line-height:normal;font-family:Menlo"><span>














</span></p><p style=3D"line-height:normal;font-family:Menlo"><span>[root@ur=
anus tetuser]#</span></p><p style=3D"line-height:normal;font-family:Menlo">=
<span style=3D"font-family:Arial,Helvetica,sans-serif">=C2=A0</span><br></p=
></div></div></blockquote><blockquote class=3D"gmail_quote" style=3D"margin=
:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr"><div><p style=3D"line-height:normal;font-family:Menlo"><span><span=
 style=3D"font-family:Arial,Helvetica,sans-serif">But when I try to run ope=
nmpi within the container by calling mpirun on the host, it fails with thos=
e messages:</span><br></span></p><p style=3D"line-height:normal;font-family=
:Menlo"><span><span style=3D"font-family:Arial,Helvetica,sans-serif"><br></=
span></span></p><p style=3D"line-height:normal;font-family:Menlo"><span>[ro=
ot@uranus tetuser]# /usr/local/bin/mpirun --allow-run-as-root /opt/singular=
ity/bin/singulari<wbr>ty exec ./Centos7-ompi.img /usr/bin/mpi_ring</span></=
p><p style=3D"line-height:normal;font-family:Menlo"><span>*** An error occu=
rred in MPI_Init</span></p><p style=3D"line-height:normal;font-family:Menlo=
"><span>*** on a NULL communicator</span></p><p style=3D"line-height:normal=
;font-family:Menlo"><span>*** MPI_ERRORS_ARE_FATAL (processes in this commu=
nicator will now abort,</span></p><p style=3D"line-height:normal;font-famil=
y:Menlo"><span>***=C2=A0 =C2=A0 and potentially your MPI job)</span></p><p =
style=3D"line-height:normal;font-family:Menlo"><span>[uranus:47063] Local a=
bort before MPI_INIT completed completed successfully, but am not able to a=
ggregate error messages, and not able to guarantee that all other processes=
 were killed!</span></p><p style=3D"line-height:normal;font-family:Menlo"><=
span>*** An error occurred in MPI_Init</span></p><p style=3D"line-height:no=
rmal;font-family:Menlo"><span>*** on a NULL communicator</span></p><p style=
=3D"line-height:normal;font-family:Menlo"><span>*** MPI_ERRORS_ARE_FATAL (p=
rocesses in this communicator will now abort,</span></p><p style=3D"line-he=
ight:normal;font-family:Menlo"><span>***=C2=A0 =C2=A0 and potentially your =
MPI job)</span></p><p style=3D"line-height:normal;font-family:Menlo"><span>=
[uranus:47061] Local abort before MPI_INIT completed completed successfully=
, but am not able to aggregate error messages, and not able to guarantee th=
at all other processes were killed!</span></p><p style=3D"line-height:norma=
l;font-family:Menlo"><span>------------------------------<wbr>-------------=
------------</span></p><p style=3D"line-height:normal;font-family:Menlo"><s=
pan>Primary job=C2=A0 terminated normally, but 1 process returned</span></p=
><p style=3D"line-height:normal;font-family:Menlo"><span>a non-zero exit co=
de. Per user-direction, the job has been aborted.</span></p><p style=3D"lin=
e-height:normal;font-family:Menlo"><span>------------------------------<wbr=
>-------------------------</span></p><p style=3D"line-height:normal;font-fa=
mily:Menlo"><span>------------------------------<wbr>----------------------=
--------<wbr>--------------</span></p><p style=3D"line-height:normal;font-f=
amily:Menlo"><span>mpirun detected that one or more processes exited with n=
on-zero status, thus causing</span></p><p style=3D"line-height:normal;font-=
family:Menlo"><span>the job to be terminated. The first process to do so wa=
s:</span></p><p style=3D"line-height:normal;font-family:Menlo;min-height:15=
px"><span></span><br></p><p style=3D"line-height:normal;font-family:Menlo">=
<span>=C2=A0 Process name: [[6415,1],1]</span></p><p style=3D"line-height:n=
ormal;font-family:Menlo"><span>=C2=A0 Exit code:=C2=A0 =C2=A0 1</span></p><=
p style=3D"line-height:normal;font-family:Menlo"><span>--------------------=
----------<wbr>------------------------------<wbr>--------------</span></p>=
<p style=3D"line-height:normal;font-family:Menlo"><span>





















</span></p><p style=3D"line-height:normal;font-family:Menlo"><span>[root@ur=
anus tetuser]#=C2=A0</span></p><p style=3D"line-height:normal;font-family:M=
enlo"><span><br></span></p><p style=3D"line-height:normal;font-family:Menlo=
"><span><span style=3D"font-family:Arial,Helvetica,sans-serif">What did I d=
o wrong? How to fix this?</span><br></span></p><p style=3D"line-height:norm=
al;font-family:Menlo"><span><span style=3D"font-family:Arial,Helvetica,sans=
-serif"><br></span></span></p><p style=3D"line-height:normal;font-family:Me=
nlo"><span><span style=3D"font-family:Arial,Helvetica,sans-serif">Thank you=
!</span></span></p></div></div></blockquote></div></div></div></div></div><=
/div></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--f403045db0fe0d6a890545c0d2cb--
