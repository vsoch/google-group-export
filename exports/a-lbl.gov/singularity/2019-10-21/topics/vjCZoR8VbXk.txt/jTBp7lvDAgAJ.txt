X-Received: by 10.36.120.16 with SMTP id p16mr209152itc.0.1476238787254;
        Tue, 11 Oct 2016 19:19:47 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.105.201 with SMTP id e192ls4610592itc.11.canary; Tue, 11
 Oct 2016 19:19:46 -0700 (PDT)
X-Received: by 10.98.19.208 with SMTP id 77mr1608036pft.102.1476238786635;
        Tue, 11 Oct 2016 19:19:46 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id v143si3238815pgb.176.2016.10.11.19.19.46
        for <singu...@lbl.gov>;
        Tue, 11 Oct 2016 19:19:46 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.71 as permitted sender) client-ip=209.85.215.71;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.71 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
IronPort-PHdr: 9a23:kfRM9BTPpvW811IdNM2xQITExdpsv+yvbD5Q0YIujvd0So/mwa65ZB2N2/xhgRfzUJnB7Loc0qyN4vqmBTVLvcbJmUtBWaQEbwUCh8QSkl5oK+++Imq/EsTXaTcnFt9JTl5v8iLzG0FUHMHjew+a+SXqvnYsExnyfTB4Ov7yUtaLyZ/mjabuotaPM01hv3mUWftKNhK4rAHc5IE9oLBJDeIP8CbPuWZCYO9MxGlldhq5lhf44dqsrtY4q3wD888784Z8dYmyP+FmDO8QMTI9Lmpg4cTqsQXEHxCO4HQBVmwMkwZZQBXD9wzwRZzrsyH3nu533G+VOtOlcbdhQjWk4LpvQQXplDZPYyU49m7KjsVqjb5KiBaro1pwxJCCM6+PM/8rNIzHcNwdX3sJe4AZdDFbHpGxdcFHW+UEPvtCs5vwvXMKpx/4Cg6yUrC8ggRUj2P7iPVpm98qFhvLiUl+Eg==
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2F+AAC1nP1XhkfXVdFcHgYMgwc1AQEBAQE7OW0PB4M4oHiCVoUFh0WFFIFJGycbAQaBdIQKAoF+BzgUAQEBAQEBAQEBAQECEAEBAQgLCwkZL4IyBAIBAhEFBAE5CjEBAQEBAQEBAQEBAQEBAQEBGgINIg8DKwEBAwESESswCwsLNwICIQEPAwEFARwGCAcEARwEAYgUAw8IBahXgTI+MotCiQ8Ng2wBAQEBAQUBAQEBAQEBASAQiwKCR4FSEQGDIIJbBY81ihg1AYYmhkuDC4I8jTmIZYQUgj0THoERDw9bgxkRC4FzHjQHhj1HMYEoAQEB
X-IronPort-AV: E=Sophos;i="5.31,332,1473145200"; 
   d="scan'208,217";a="42850321"
Received: from mail-lf0-f71.google.com ([209.85.215.71])
  by fe4.lbl.gov with ESMTP; 11 Oct 2016 19:19:43 -0700
Received: by mail-lf0-f71.google.com with SMTP id i187so21554665lfe.4
        for <singu...@lbl.gov>; Tue, 11 Oct 2016 19:19:43 -0700 (PDT)
X-Gm-Message-State: AA6/9Rme0EGN4OW4nvki8/AkPHfJzO8vhvT7R6Sz2Nc42jtiyzjkuQDGGi4sMP12TjRrst9ZtgXzJsCbT1AnvdYneVzoGWAAaD6Svwff3hg1ykkn4cnunIDq9iuJhTCv/B8NJGR/NZDVry/yEzpQbSFRhI8=
X-Received: by 10.25.217.209 with SMTP id s78mr341060lfi.8.1476238782292;
        Tue, 11 Oct 2016 19:19:42 -0700 (PDT)
X-Received: by 10.25.217.209 with SMTP id s78mr341058lfi.8.1476238782030; Tue,
 11 Oct 2016 19:19:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.167.67 with HTTP; Tue, 11 Oct 2016 19:19:41 -0700 (PDT)
In-Reply-To: <04cccf38-72fb-49ef-b010-15fb80e71e8e@lbl.gov>
References: <04cccf38-72fb-49ef-b010-15fb80e71e8e@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Tue, 11 Oct 2016 19:19:41 -0700
Message-ID: <CAN7etTyY_+ytGXpY4Te-=xED4zHNzz4Pg7xkB4ULj2sZ=gk2WA@mail.gmail.com>
Subject: Re: [Singularity] Using singularity with MPI jobs
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=94eb2c0655c434f06c053ea1a087

--94eb2c0655c434f06c053ea1a087
Content-Type: text/plain; charset=UTF-8

Hi Steve,

I'm not sure at first glance, but just to touch on the basics... Is
/opt/intel available from within the container? Do all tasks exit code 6,
or just some of them?

What version of OMPI are you using?

I wonder if the PID namespace is causing a problem here... I'm not sure it
gets effectively disabled when running via srun and pmi. Can you export the
environment variable "SINGULARITY_NO_NAMESPACE_PID=1" in a place where
Singularity will pick it up definitively by all ranks? That will ensure
that the PID namespace is not being exported.

Additionally, you could try version 2.2. I just released it, and by default
it does not unshare() out the PID namespace. But... It is the first release
in the 2.2 series so it may bring with it other issues that still need
resolving.... But we should debug those too! :)

Greg



On Tue, Oct 11, 2016 at 2:40 PM, Steve Mehlberg <sgmeh...@gmail.com>
wrote:

> Does singularity support MPI PMI-2 jobs?  I've had mixed success testing
> benchmark applications using a singularity container.
>
> Currently I'm struggling to get the NEMO benchmark to run using slurm
> 16.05 and pmi2.  I can run the exact same executable on bare metal with the
> same slurm, but I get Rank errors when I run using "srun --mpi=pmi2
> singularity...".  The application aborts with an exit code 6.
>
> I tried pmix too, but that gets mpi aborts for both bare metal and
> singularity.
>
> The only way I could get the NEMO application to compile was to use the
> intel compilers and mpi:
>
> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/compilervars.sh
> intel64
> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/ifortvars.sh
> intel64
> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/iccvars.sh
> intel64
> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>
> It runs fine when I use mpirun with or without singularity.
>
> Example run/error:
>
> sbatch ...
> srun --mpi=pmi2 -n16 singularity exec c7.img run.it > out_now
>
> .......
> srun: error: node11: tasks 0-7: Exited with exit code 6
> srun: error: node12: tasks 8-15: Exited with exit code 6
>
> $ cat run.it
> #!/bin/sh
> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/compilervars.sh
> intel64
> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/ifortvars.sh
> intel64
> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/iccvars.sh
> intel64
> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
> source env_bench
> export PATH=/opt/mpi/openmpi-icc/2.0.0/bin:/opt/pmix/1.1.5/bin:$PATH
> export LD_LIBRARY_PATH=/opt/openmpi-icc/2.0.0/lib:/opt/pmix/1.1.5/
> lib:$LD_LIBRARY_PATH
> export OMPI_MCA_btl=self,sm,openib
>
> ./opa_8_2 namelist >out_now
>
> $ cat out_now
> [node12:29725] *** An error occurred in MPI_Isend
> [node12:29725] *** reported by process [3865116673,0]
> [node12:29725] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
> [node12:29725] *** MPI_ERR_RANK: invalid rank
> [node12:29725] *** MPI_ERRORS_ARE_FATAL (processes in this communicator
> will now abort,
> [node12:29725] ***    and potentially your MPI job)
>
> I am running singularity 2.1 - any ideas?
>
> -Steve
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--94eb2c0655c434f06c053ea1a087
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Steve,<div><br></div><div>I&#39;m not sure at first gla=
nce, but just to touch on the basics... Is /opt/intel available from within=
 the container? Do all tasks exit code 6, or just some of them?</div><div><=
br></div><div>What version of OMPI are you using?</div><div><br></div><div>=
I wonder if the PID namespace is causing a problem here... I&#39;m not sure=
 it gets effectively disabled when running via srun and pmi. Can you export=
 the environment variable &quot;<span class=3D"gmail-s1">SINGULARITY_NO_NAM=
ESPACE_</span><span class=3D"gmail-s2">PID=3D1&quot; in a place where Singu=
larity will pick it up definitively by all ranks? That will ensure that the=
 PID namespace is not being exported.</span></div><div><span class=3D"gmail=
-s2"><br></span></div><div><span class=3D"gmail-s2">Additionally, you could=
 try version 2.2. I just released it, and by default it does not unshare() =
out the PID namespace. But... It is the first release in the 2.2 series so =
it may bring with it other issues that still need resolving.... But we shou=
ld debug those too! :)</span></div><div><span class=3D"gmail-s2"><br></span=
></div><div>Greg</div><div><br></div><div><br></div>







</div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Oct=
 11, 2016 at 2:40 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a href=3D"mailt=
o:sgmeh...@gmail.com" target=3D"_blank">sgmeh...@gmail.com</a>&gt;</span> w=
rote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Does singularity s=
upport MPI PMI-2 jobs?=C2=A0 I&#39;ve had mixed success testing benchmark a=
pplications using a singularity container.=C2=A0 <br><br>Currently I&#39;m =
struggling to get the NEMO benchmark to run using slurm 16.05 and pmi2.=C2=
=A0 I can run the exact same executable on bare metal with the same slurm, =
but I get Rank errors when I run using &quot;srun --mpi=3Dpmi2 singularity.=
..&quot;.=C2=A0 The application aborts with an exit code 6.<br><br>I tried =
pmix too, but that gets mpi aborts for both bare metal and singularity.<br>=
<br>The only way I could get the NEMO application to compile was to use the=
 intel compilers and mpi:<br><br><span style=3D"font-size:12.0pt;font-famil=
y:&quot;Times New Roman&quot;,&quot;serif&quot;;color:black">source
/opt/intel/compilers_and_<wbr>libraries_2016.3.210/linux/<wbr>bin/compilerv=
ars.sh intel64<br>
source /opt/intel/compilers_and_<wbr>libraries_2016.3.210/linux/<wbr>bin/if=
ortvars.sh
intel64<br>
source /opt/intel/compilers_and_<wbr>libraries_2016.3.210/linux/<wbr>bin/ic=
cvars.sh
intel64<br>
source /opt/mpi/openmpi-icc/2.0.0/<wbr>bin/mpivars.sh<br><br></span>It runs=
 fine when I use mpirun with or without singularity.<br><br>Example run/err=
or:<br><br>sbatch ...<br>srun --mpi=3Dpmi2 -n16 singularity exec c7.img <a =
href=3D"http://run.it" target=3D"_blank">run.it</a> &gt; out_now<br><br>...=
....<br>srun: error: node11: tasks 0-7: Exited with exit code 6<br>srun: er=
ror: node12: tasks 8-15: Exited with exit code 6<br><br>$ cat <a href=3D"ht=
tp://run.it" target=3D"_blank">run.it</a><br>#!/bin/sh<br>source /opt/intel=
/compilers_and_<wbr>libraries_2016.3.210/linux/<wbr>bin/compilervars.sh int=
el64<br>source /opt/intel/compilers_and_<wbr>libraries_2016.3.210/linux/<wb=
r>bin/ifortvars.sh intel64<br>source /opt/intel/compilers_and_<wbr>librarie=
s_2016.3.210/linux/<wbr>bin/iccvars.sh intel64<br>source /opt/mpi/openmpi-i=
cc/2.0.0/<wbr>bin/mpivars.sh<br>source env_bench<br>export PATH=3D/opt/mpi/=
openmpi-icc/2.0.<wbr>0/bin:/opt/pmix/1.1.5/bin:$<wbr>PATH<br>export LD_LIBR=
ARY_PATH=3D/opt/openmpi-<wbr>icc/2.0.0/lib:/opt/pmix/1.1.5/<wbr>lib:$LD_LIB=
RARY_PATH<br>export OMPI_MCA_btl=3Dself,sm,openib<br><br>./opa_8_2 namelist=
 &gt;out_now<br><br>$ cat out_now<br>[node12:29725] *** An error occurred i=
n MPI_Isend<br>[node12:29725] *** reported by process <a href=3D"tel:%5B386=
5116673" value=3D"+13865116673" target=3D"_blank">[3865116673</a>,0]<br>[no=
de12:29725] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0<br>[node12:29=
725] *** MPI_ERR_RANK: invalid rank<br>[node12:29725] *** MPI_ERRORS_ARE_FA=
TAL (processes in this communicator will now abort,<br>[node12:29725] ***=
=C2=A0=C2=A0=C2=A0 and potentially your MPI job)<br><br>I am running singul=
arity 2.1 - any ideas?<span class=3D"HOEnZb"><font color=3D"#888888"><br><b=
r>-Steve<br><br></font></span></div><span class=3D"HOEnZb"><font color=3D"#=
888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div di=
r=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"=
><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sy=
stems Architect and Technology Developer</div><div>Lawrence Berkeley Nation=
al Laboratory HPCS<br>University of California Berkeley Research IT<br>Sing=
ularity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targ=
et=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster M=
anagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http=
://warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.=
com/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<sp=
an style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitt=
er.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twit=
ter.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div>=
</div></div>
</div>

--94eb2c0655c434f06c053ea1a087--
