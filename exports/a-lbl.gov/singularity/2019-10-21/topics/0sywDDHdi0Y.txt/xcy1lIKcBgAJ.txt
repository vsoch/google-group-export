Date: Tue, 5 Jul 2016 08:25:31 -0700 (PDT)
From: Raimon Bosch <raimo...@gmail.com>
To: singularity <singu...@lbl.gov>
Cc: r...@open-mpi.org
Message-Id: <007b47f4-0aea-42dc-b871-d653bb7a67a1@lbl.gov>
In-Reply-To: <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov> <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com> <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov> <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com> <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov> <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com> <054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov>
 <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
Subject: Re: [Singularity] Communication between singularity containers
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_12526_529625697.1467732331618"

------=_Part_12526_529625697.1467732331618
Content-Type: multipart/alternative; 
	boundary="----=_Part_12527_1450457286.1467732331618"

------=_Part_12527_1450457286.1467732331618
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable


That solution does not work with nas/mpi benchmark. That's because bt.C.16=
=20
expects 16 processes. When you split processes it throws an exception=20
because number of processes is lower than 16.=20

I am still trying to figure out how to do this. Let me know if you have any=
=20
suggestion.

Cheers,

El jueves, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Castain escribi=C3=
=B3:
>
> I think you are misunderstanding the basic nature of the Singularity=20
> =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overlay. So =
=E2=80=9Csharing=E2=80=9D a container is=20
> no different than running on a node where the procs all see the same file=
=20
> system. Thus, having multiple containers that are identical makes no sens=
e=20
> - it=E2=80=99s all the same file system.
>
> Now if you want to run different containers (e.g., with different=20
> libraries or OS in them), then you would use mpirun=E2=80=99s MPMD syntax=
 - for=20
> example:
>
> mpirun -n 1 <container1> : -n 1 <container2>
>
> HTH
> Ralph
>
> On Jun 23, 2016, at 1:53 AM, Raimon Bosch <rai...@gmail.com=20
> <javascript:>> wrote:
>
>
> One last question: What if I want to execute more than one container in=
=20
> the same host? With this technique I am bounded always to the same=20
> container. One of our experiments was based in measuring performance of=
=20
> several containers working in parallel in the same node. Also we had=20
> experiments with N containers per host in a multihost environment.
>
> El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Kurt=
zer=20
> escribi=C3=B3:
>>
>> Hi Raimon,
>>
>> Sorry I wasn't clear. I am not yet at my computer and thinking while=20
>> typing on an iPhone hinders my mental processes. Lol
>>
>> If I understand your example properly, you have a docker or VM=20
>> infrastructure already set up and you are invoking the mpirun commands f=
rom=20
>> within the virtual environment. Singularity works on a very different=20
>> premis because integrating a virtual cluster into an existing cluster an=
d=20
>> scheduling system is a mess.
>>
>> So starting from the physical nodes, which already have access to all=20
>> other nodes in the cluster, and already scheduled properly, and in direc=
t=20
>> access to optimized hardware and file systems.... You call mpirun.=20
>>
>> The mpirun command will take the standard format as you illustrated with=
=20
>> the following change to call Singularity inline:
>>
>> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img=20
>> trace.sh bt.C.4
>>
>> This assumes the following:
>>
>> 1. The container image which contains the program's you want to run is a=
t=20
>> ~/container.img and accessible at this path on all nodes referenced in=
=20
>> hosts.txt
>> 2. The hosts.txt references other physical nodes you want to run on
>> 3. The executables trace.sh and bt.C.4 are both inside the container and=
=20
>> in a standard path
>>
>> In this case we are performing one execution and MPI + singularity is=20
>> managing all of the communication between processes, nodes and container=
s.=20
>> Also it is now using any optimized hardware (eg. Infiniband) and existin=
g=20
>> high performance file systems (which should not be accessible via a=20
>> virtualized or Docker'ized cluster for security reasons).
>>
>> This way is actually MUCH simpler then what you are proposing because=20
>> there is no need to manage any virtual nodes, virtual networks, or resou=
rce=20
>> manager hacks. It really is as easy as just running any other MPI proces=
s=20
>> on an existing cluster.=20
>>
>> Hope that helps better!
>>
>>
>>
>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>
>>>
>>>
>>> Hi Gregory,
>>>
>>> I'm not sure if I would achieve the same with your commands. In an=20
>>> environment based on dockers or virtual machines we would do something =
like=20
>>> this [non applicable to Singularity]:
>>>
>>> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh=20
>>> ./bt.C.4
>>>
>>> where hosts.txt* is:
>>>
>>> >vm-ip-01-on-*host01* slots=3D2
>>> >vm-ip-01-on-*host02* slots=3D2
>>>
>>> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>>>
>>> and trace.sh is:
>>>
>>> >#!/bin/bash
>>> >
>>> >export EXTRAE_HOME=3D/opt/extrae/
>>> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
>>> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>>> >
>>> >## Run the desired program
>>> >$*
>>>
>>> As you see we only perform one execution and OpenMPI transparently=20
>>> manages communication between containers or virtual machines. This comm=
and=20
>>> would work well rather VMs are on the same host or not.
>>>
>>> What I understand from your response is that now we should execute=20
>>> OpenMPI on each host and then merge results manually. I don't know yet =
how=20
>>> to do this merge step or if it is any way to centralize everything like=
 I=20
>>> would do with VMs.
>>>
>>> Thanks in advance,
>>>
>>> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Ku=
rtzer=20
>>> escribi=C3=B3:
>>>>
>>>> Hi Raimon,
>>>>
>>>> The quick answer is you have mpirun handle that as you would normally=
=20
>>>> where the container file lives on a shared file system:
>>>>
>>>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>>>
>>>> Let the MPI outside the container launch the singularity container on=
=20
>>>> each host as it would normally launch any MPI program. Then it will ca=
ll=20
>>>> Singulairty and Singularity will launch the MPI program inside the=20
>>>> container on each of your hosts/servers.=20
>>>>
>>>> Hope that helps!
>>>>
>>>>
>>>>
>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>>>
>>>>>
>>>>> Hi Gregory,
>>>>>
>>>>> Thank you for your answer. One of our experiments needs to run OpenMP=
I=20
>>>>> among several servers. This means that we should put one of our conta=
iners=20
>>>>> in host01, another in host02 and another in host03 and collect the re=
sults.
>>>>> =20
>>>>>
>>>>> How can I do this execution in parallel if I need to communicate with=
=20
>>>>> more than one server?
>>>>>
>>>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer=
=20
>>>>> escribi=C3=B3:
>>>>>>
>>>>>> Hi Raimon,
>>>>>>
>>>>>> The communication model of a Singularity container is very different=
=20
>>>>>> from that of a Docker implementation. This is because Docker for all=
=20
>>>>>> practical purposes emulates a virtual machine as each container has =
it's=20
>>>>>> own IP address and thus it's own ssh server. It also carries its own=
 set of=20
>>>>>> complexities, for example networks need to be segregated/VLan'ed, DN=
S/host=20
>>>>>> resolution needs to be dynamic and passed down to the containers (so=
 they=20
>>>>>> can reach each other), ssh daemons and other process running inside =
the=20
>>>>>> containers, management via an existing scheduling system, and the li=
st goes=20
>>>>>> on and on.
>>>>>>
>>>>>> Think of it this way, Singularity does not do any of that... It runs=
=20
>>>>>> a program within the container as if it were running on the host its=
elf, so=20
>>>>>> to communicate between containers is as easy as communicating betwee=
n=20
>>>>>> programs. So for MPI, it would happen with the MPI on the physical h=
ost=20
>>>>>> (outside the container) invoking the container subsystem which then =
invokes=20
>>>>>> the MPI programs within the container and the MPI programs within th=
e=20
>>>>>> container communicate back to the MPI (orted) outside the container =
on the=20
>>>>>> host to get access to the host resources. In this model all availabl=
e=20
>>>>>> resources and infrastructure can be leveraged at full bandwidth by t=
he=20
>>>>>> contained processes and all of the aforementioned complexities akin =
to=20
>>>>>> running on a virtualized mini-cluster are circumvented.
>>>>>>
>>>>>> There is additional information I have written at:
>>>>>>
>>>>>> http://singularity.lbl.gov/#hpc
>>>>>>
>>>>>> That page is still coming along, and needs more information still bu=
t=20
>>>>>> if you have any questions, comments or change proposals please let u=
s know!
>>>>>>
>>>>>> Thanks and hope that helps!
>>>>>>
>>>>>>
>>>>>>
>>>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>=20
>>>>>> wrote:
>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> We are trying to run experiments using singularity containers. The=
=20
>>>>>>> idea is to run OpenMPI among several containers and check performan=
ce=20
>>>>>>> results.=20
>>>>>>>
>>>>>>> How can I communicate with another container? In docker this is=20
>>>>>>> clear because every container gets an assigned IP and you can ping =
there,=20
>>>>>>> but what is the situation in the case of singularity? Is it possibl=
e to=20
>>>>>>> assign an IP to each container? Can I connect via ssh to them?
>>>>>>>
>>>>>>> Thanks in advance,
>>>>>>>
>>>>>>> --=20
>>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,=
=20
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --=20
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>>
>>>>> --=20
>>>>> You received this message because you are subscribed to the Google=20
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d=20
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>> --=20
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>>>
>>> --=20
>>> You received this message because you are subscribed to the Google=20
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>> --=20
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
>>
> --=20
> You received this message because you are subscribed to the Google Groups=
=20
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
=20
> email to singu...@lbl.gov <javascript:>.
>
>
>
------=_Part_12527_1450457286.1467732331618
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br>That solution does not work with nas/mpi benchmark. Th=
at&#39;s because bt.C.16 expects 16 processes. When you split processes it =
throws an exception because number of processes is lower than 16. <br><br>I=
 am still trying to figure out how to do this. Let me know if you have any =
suggestion.<br><br>Cheers,<br><br>El jueves, 23 de junio de 2016, 15:09:13 =
(UTC+2), Ralph Castain  escribi=C3=B3:<blockquote class=3D"gmail_quote" sty=
le=3D"margin: 0;margin-left: 0.8ex;border-left: 1px #ccc solid;padding-left=
: 1ex;"><div style=3D"word-wrap:break-word"><div>I think you are misunderst=
anding the basic nature of the Singularity =E2=80=9Ccontainer=E2=80=9D. It=
=E2=80=99s just a file system overlay. So =E2=80=9Csharing=E2=80=9D a conta=
iner is no different than running on a node where the procs all see the sam=
e file system. Thus, having multiple containers that are identical makes no=
 sense - it=E2=80=99s all the same file system.</div><div><br></div><div>No=
w if you want to run different containers (e.g., with different libraries o=
r OS in them), then you would use mpirun=E2=80=99s MPMD syntax - for exampl=
e:</div><div><br></div><div>mpirun -n 1 &lt;container1&gt; : -n 1 &lt;conta=
iner2&gt;</div><div><br></div><div>HTH</div><div>Ralph</div><div><br><block=
quote type=3D"cite"><div>On Jun 23, 2016, at 1:53 AM, Raimon Bosch &lt;<a h=
ref=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"sxrdchvmAgAJ=
" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;return =
true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">rai...@gm=
ail.com</a>&gt; wrote:</div><br><div><div dir=3D"ltr" style=3D"font-family:=
Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-spacin=
g:normal;text-align:start;text-indent:0px;text-transform:none;white-space:n=
ormal;word-spacing:0px"><br>One last question: What if I want to execute mo=
re than one container in the same host? With this technique I am bounded al=
ways to the same container. One of our experiments was based in measuring p=
erformance of several containers working in parallel in the same node. Also=
 we had experiments with N containers per host in a multihost environment.<=
br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M.=
 Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0p=
x 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);bo=
rder-left-style:solid;padding-left:1ex">Hi Raimon,<div><br></div><div>Sorry=
 I wasn&#39;t clear. I am not yet at my computer and thinking while typing =
on an iPhone hinders my mental processes. Lol</div><div><br></div><div>If I=
 understand your example properly, you have a docker or VM infrastructure a=
lready set up and you are invoking the mpirun commands from within the virt=
ual environment. Singularity works on a very different premis because integ=
rating a virtual cluster into an existing cluster and scheduling system is =
a mess.</div><div><br></div><div>So starting from the physical nodes, which=
 already have access to all other nodes in the cluster, and already schedul=
ed properly, and in direct access to optimized hardware and file systems...=
. You call mpirun.=C2=A0</div><div><br></div><div>The mpirun command will t=
ake the standard format as you illustrated with the following change to cal=
l Singularity inline:</div><div><br></div><div>$ mpirun -np 4 --hostfile ho=
sts.txt singularity exec ~/container.img trace.sh bt.C.4</div><div><br></di=
v><div>This assumes the following:</div><div><br></div><div>1. The containe=
r image which contains the program&#39;s you want to run is at ~/container.=
img and accessible at this path=C2=A0on all nodes referenced in hosts.txt</=
div><div>2. The hosts.txt references other physical nodes you want to run o=
n</div><div>3. The executables trace.sh and bt.C.4 are both inside the cont=
ainer and in a standard path</div><br><div>In this case we are performing o=
ne execution and MPI + singularity is managing all of the communication bet=
ween processes, nodes and containers. Also it is now using any optimized ha=
rdware (eg. Infiniband)=C2=A0and existing high performance=C2=A0file system=
s (which should not be accessible via a virtualized or Docker&#39;ized=C2=
=A0cluster for security reasons).</div><div><br></div><div>This way is actu=
ally MUCH simpler then what you are proposing because there is no need to m=
anage any virtual nodes, virtual networks, or resource manager hacks. It re=
ally is as easy as just running any other MPI=C2=A0process on an existing c=
luster.=C2=A0</div><div><br></div><div>Hope that helps better!</div><div><b=
r></div><div><span></span><br><br>On Wednesday, June 22, 2016, Raimon Bosch=
 &lt;<a rel=3D"nofollow">raimon...@</a><a href=3D"http://gmail.com/" target=
=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;http://gmail.c=
om/&#39;;return true;" onclick=3D"this.href=3D&#39;http://gmail.com/&#39;;r=
eturn true;">gmail.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:r=
gb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir=3D"ltr">=
<br><br>Hi Gregory,<br><br>I&#39;m not sure if I would achieve the same wit=
h your commands. In an environment based on dockers or virtual machines we =
would do something like this [non applicable to Singularity]:<br><br>&gt; c=
d $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile hosts.txt ./trace.sh ./b=
t.C.4<br><br>where hosts.txt* is:<br><br>&gt;vm-ip-01-on-<b>host01</b><span=
>=C2=A0</span>slots=3D2<br>&gt;vm-ip-01-on-<b>host02</b><span>=C2=A0</span>=
slots=3D2<br><br>* vm-ip-XX-on-hostXX<b><span>=C2=A0</span></b>are IPs i.e.=
 172.100.60.XX<br><br>and trace.sh is:<br><br>&gt;#!/bin/bash<br>&gt;<br>&g=
t;export EXTRAE_HOME=3D/opt/extrae/<br>&gt;export EXTRAE_CONFIG_FILE=3D/ext=
rae.xml<br>&gt;export LD_PRELOAD=3D${EXTRAE_HOME}/lib/<wbr>libmpitrace.so<b=
r>&gt;<br>&gt;## Run the desired program<br>&gt;$*<br><br>As you see we onl=
y perform one execution and OpenMPI transparently manages communication bet=
ween containers or virtual machines. This command would work well rather VM=
s are on the same host or not.<br><br>What I understand from your response =
is that now we should execute OpenMPI on each host and then merge results m=
anually. I don&#39;t know yet how to do this merge step or if it is any way=
 to centralize everything like I would do with VMs.<br><br>Thanks in advanc=
e,<br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory=
 M. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin=
:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204)=
;border-left-style:solid;padding-left:1ex">Hi Raimon,<div><br></div><div>Th=
e quick answer is you have mpirun handle that as you would normally where t=
he container file lives on a shared file system:</div><div><br></div><div>$=
 mpirun singularity exec ~/container.img mpi_prog_in_container</div><div><b=
r></div><div>Let the MPI outside the container launch the singularity conta=
iner on each host as it would normally launch any MPI program. Then it will=
 call Singulairty and Singularity will launch the MPI program inside the co=
ntainer on each of your hosts/servers.=C2=A0</div><div><br></div><div>Hope =
that helps!</div><div><br></div><div><span></span><br><br>On Wednesday, Jun=
e 22, 2016, Raimon Bosch &lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt; w=
rote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8e=
x;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-styl=
e:solid;padding-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<br><br>Thank you=
 for your answer. One of our experiments needs to run OpenMPI among several=
 servers. This means that we should put one of our containers in host01, an=
other in host02 and another in host03 and collect the results.<span>=C2=A0<=
/span><br><br>How can I do this execution in parallel if I need to communic=
ate with more than one server?<br><br>El martes, 21 de junio de 2016, 16:51=
:03 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_qu=
ote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-co=
lor:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir=3D"=
ltr">Hi Raimon,<div><br></div><div>The communication model of a Singularity=
 container is very different from that of a Docker implementation. This is =
because Docker for all practical purposes emulates a virtual machine as eac=
h container has it&#39;s own IP address and thus it&#39;s own ssh server. I=
t also carries its own set of complexities, for example networks need to be=
 segregated/VLan&#39;ed, DNS/host resolution needs to be dynamic and passed=
 down to the containers (so they can reach each other), ssh daemons and oth=
er process running inside the containers, management via an existing schedu=
ling system, and the list goes on and on.</div><div><br></div><div>Think of=
 it this way, Singularity does not do any of that... It runs a program with=
in the container as if it were running on the host itself, so to communicat=
e between containers is as easy as communicating between programs. So for M=
PI, it would happen with the MPI on the physical host (outside the containe=
r) invoking the container subsystem which then invokes the MPI programs wit=
hin the container and the MPI programs within the container communicate bac=
k to the MPI (orted) outside the container on the host to get access to the=
 host resources. In this model all available resources and infrastructure c=
an be leveraged at full bandwidth by the contained processes and all of the=
 aforementioned complexities akin to running on a virtualized mini-cluster =
are circumvented.</div><div><br></div><div>There is additional information =
I have written at:</div><div><br></div><div><a href=3D"http://singularity.l=
bl.gov/#hpc" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D=
&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23h=
pc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39=
;;return true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3d=
http%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3=
dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;">http://singularity.l=
bl.gov/#<wbr>hpc</a><br></div><div><br></div><div>That page is still coming=
 along, and needs more information still but if you have any questions, com=
ments or change proposals please let us know!</div><div><br></div><div>Than=
ks and hope that helps!</div><div><br></div><div><br></div></div><div><br><=
div class=3D"gmail_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch<spa=
n>=C2=A0</span><span dir=3D"ltr">&lt;<a rel=3D"nofollow">rai...@gmail.com</=
a>&gt;</span><span>=C2=A0</span>wr<wbr>ote:<br><blockquote class=3D"gmail_q=
uote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-c=
olor:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir=3D=
"ltr"><br><br>Hi,<br><br>We are trying to run experiments using singularity=
 containers. The idea is to run OpenMPI among several containers and check =
performance results.<span>=C2=A0</span><br><br>How can I communicate with a=
nother container? In docker this is clear because every container gets an a=
ssigned IP and you can ping there, but what is the situation in the case of=
 singularity? Is it possible to assign an IP to each container? Can I conne=
ct via ssh to them?<br><br>Thanks in advance,<span><font color=3D"#888888">=
<br></font></span></div><span><font color=3D"#888888"><div><br></div>--<spa=
n>=C2=A0</span><br>You received this message because you are subscribed to =
the Google Groups &quot;singularity&quot; group.<br>To unsubscribe from thi=
s group and stop receiving emails from it, send an email to<span>=C2=A0</sp=
an><a rel=3D"nofollow">singu...@lbl.gov</a>.<br></font></span></blockquote>=
</div><br><br clear=3D"all"><div><br></div>--<span>=C2=A0</span><br><div><d=
iv dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Servic=
es (HPCS)<br>University of California<br>Lawrence Berkeley National Laborat=
ory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div></div></bloc=
kquote></div><div><br></div>--<span>=C2=A0</span><br>You received this mess=
age because you are subscribed to the Google Groups &quot;singularity&quot;=
 group.<br>To unsubscribe from this group and stop receiving emails from it=
, send an email to<span>=C2=A0</span><a>singularity+unsubscribe@<wbr>lbl.go=
v</a>.<br></blockquote></div><br><br>--<span>=C2=A0</span><br><div dir=3D"l=
tr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HPCS)<b=
r>University of California<br>Lawrence Berkeley National Laboratory<br>One =
Cyclotron Road, Berkeley, CA 94720</div></div><br></blockquote></div><div><=
br></div>--<span>=C2=A0</span><br>You received this message because you are=
 subscribed to the Google Groups &quot;singularity&quot; group.<br>To unsub=
scribe from this group and stop receiving emails from it, send an email to<=
span>=C2=A0</span><a>singularity+unsubscribe@<wbr>lbl.gov</a>.<br></blockqu=
ote></div><br><br>--<span>=C2=A0</span><br><div dir=3D"ltr"><div>Gregory M.=
 Kurtzer<br>High Performance Computing Services (HPCS)<br>University of Cal=
ifornia<br>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Ber=
keley, CA 94720</div></div><br></blockquote></div><div style=3D"font-family=
:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-spaci=
ng:normal;text-align:start;text-indent:0px;text-transform:none;white-space:=
normal;word-spacing:0px"><br></div><span style=3D"font-family:Helvetica;fon=
t-size:12px;font-style:normal;font-weight:normal;letter-spacing:normal;text=
-align:start;text-indent:0px;text-transform:none;white-space:normal;word-sp=
acing:0px;float:none;display:inline!important">--<span>=C2=A0</span></span>=
<br style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-we=
ight:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-tra=
nsform:none;white-space:normal;word-spacing:0px"><span style=3D"font-family=
:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-spaci=
ng:normal;text-align:start;text-indent:0px;text-transform:none;white-space:=
normal;word-spacing:0px;float:none;display:inline!important">You received t=
his message because you are subscribed to the Google Groups &quot;singulari=
ty&quot; group.</span><br style=3D"font-family:Helvetica;font-size:12px;fon=
t-style:normal;font-weight:normal;letter-spacing:normal;text-align:start;te=
xt-indent:0px;text-transform:none;white-space:normal;word-spacing:0px"><spa=
n style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weig=
ht:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-trans=
form:none;white-space:normal;word-spacing:0px;float:none;display:inline!imp=
ortant">To unsubscribe from this group and stop receiving emails from it, s=
end an email to<span>=C2=A0</span></span><a href=3D"javascript:" style=3D"f=
ont-family:Helvetica;font-size:12px;font-style:normal;font-weight:normal;le=
tter-spacing:normal;text-align:start;text-indent:0px;text-transform:none;wh=
ite-space:normal;word-spacing:0px" target=3D"_blank" gdf-obfuscated-mailto=
=3D"sxrdchvmAgAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascri=
pt:&#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return =
true;">singularity...@<wbr>lbl.gov</a><span style=3D"font-family:Helvetica;=
font-size:12px;font-style:normal;font-weight:normal;letter-spacing:normal;t=
ext-align:start;text-indent:0px;text-transform:none;white-space:normal;word=
-spacing:0px;float:none;display:inline!important">.</span></div></blockquot=
e></div><br></div></blockquote></div>
------=_Part_12527_1450457286.1467732331618--

------=_Part_12526_529625697.1467732331618--
