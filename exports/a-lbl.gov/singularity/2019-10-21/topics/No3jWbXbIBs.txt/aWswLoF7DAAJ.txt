Date: Thu, 23 Feb 2017 13:05:50 -0800 (PST)
From: Chih-Song Kuo <chihs...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <a3f8cd92-cb6f-422a-8656-a127e48853f5@lbl.gov>
In-Reply-To: <edc73d0a-acb1-4cef-a23c-c4ab5f1b7289@lbl.gov>
References: <d0a10fdc-f912-4e9c-8681-a54f5d53fd72@lbl.gov> <CAN7etTxxeYYxY7aB93H7E686y-8Qru-c_H3t1ANNQ_4oE1C-aA@mail.gmail.com>
 <887cfc8c-48ed-4720-8040-989e407f4203@lbl.gov> <CAN7etTwgGhbKjQw3EpWXMQN7jcQURt2tTHhWZn3FgZjij_=GDA@mail.gmail.com>
 <CABWwhHr9Z_h+nQM2SFq8Mhq=S1x+K9zCB74iPAn9tGdZKBUHNQ@mail.gmail.com>
 <CAA6Bz=fgxCcGtxQz7wVTTqgq_nyuLjNeuu2fOKMhJ3HTR6k5_g@mail.gmail.com>
 <edc73d0a-acb1-4cef-a23c-c4ab5f1b7289@lbl.gov>
Subject: Re: [Singularity] Performance impact: My experience
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_2374_1087618444.1487883950805"

------=_Part_2374_1087618444.1487883950805
Content-Type: multipart/alternative; 
	boundary="----=_Part_2375_840773803.1487883950806"

------=_Part_2375_840773803.1487883950806
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

OK...Now after installing InfiniBand libraries things finally work=20
decently. From this lesson, I would suggest adding a remark about IB=20
support to http://singularity.lbl.gov/docs-hpc, because otherwise new users=
=20
can easily get confused about Singularity's support for InfiniBand. And HPC=
=20
is obviously defined by such high-speed interconnects (IB, OPA, Cray, ...)

To me, this does seem to be a serious issue which would potentially=20
decrease users' feeling about the portability of Singularity containers.=20
Think about a scientist developing codes on a workstation without=20
InfiniBand, and later making a test run on a small departmental cluster=20
with InfiniBand, and eventually a production run on some white elephant=20
with Intel Omni-Path. Just missing one interconnect library in the=20
container and not compiling the MPI against that library would immediately=
=20
break portability.

Also I agree with the comment made in=20
https://groups.google.com/a/lbl.gov/forum/#!topic/singularity/fsCO1_StjjA=
=20
that Singularity should consider to include OFED drivers by default when=20
creating a new container. Not sure if Greg you would change your mind here.=
=20
The problem was really not about orted. Yes, Open MPI worked without the=20
OFED driver, but only on Ethernet. Since Singularity is claimed to be the=
=20
container for HPC, including a well defined set of interconnect drivers and=
=20
libraries truly makes sense.

For those who try to make your containers work with InfiniBand, I can=20
provide a few general hints. These were not rigorously tested though and I=
=20
doubt I have the time to do so.
1. Issue ibv_devices in the container. You must be able able to see the=20
name of your InfiniBand adpter. If not, then possibly some IB libraries and=
=20
drivers are missing
1.1. For Centos 7, the following rpm packages seem to be the minimum=20
requirement
libibverbs*=20
librdmacm*   =20
1.2. For Centos 6, the following rpm packages seem to be the minimum=20
requirement
libibverbs*=20
librdmacm*   =20
libmlx*
1.3. Nevertheless, in the end I installed more packages
yum -y install libmthca=20
yum -y install libibcm*
yum -y install libibmad*
yum -y install libibumad*
1.4. Maybe the following will be useful, but I did not install them yet
yum -y install dapl*
yum -y install ibacm*
yum -y install ibutils

2. After ./configure Open MPI, make sure you see the following.
OpenFabrics Verbs: yes
If you have OmniPath, then it should be
Intel Omnipath (PSM2): yes

There are also other components like MXM  which can potentially be "yes" if=
=20
you installed Mellanox MXM.

3. When make Open MPI, use serial compilation, i.e. do not add the -j flag.=
=20
It sometimes causes explainable behaviors.

4. After make install Open MPI, make sure that the OpenIB component is=20
indeed installed.
ompi_info |grep openib=20
#Expect to see non-empty response
 MCA btl: openib (MCA v2.1.0, API v3.0.0, Component v3.0.0)

5. Make sure that the MPI version is the same on the host and in the=20
container

My updated test result is provided below. If anybody would like to put=20
these figures into any other documents, please inform me.

Host configuration:
2x Intel E5-2680v2 (Ivybridge)
64GB memory
Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)
RHEL 6.7
OpenMPI development master branch (dated 8.2.17)
Intel MKL 2017.0 community edition
gcc 4.4

Container:
Centos 6 and 7 both tested without noticeable performance difference
OpenMPI development master branch (dated 8.2.17)
Intel MKL 2017.0 community edition
gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)

Benchmarks:
1. LINPACK 2.2
2. OSU 5.3.2

<LINPACK>
Single node. N=3D40000, P=3D5, Q=3D4
Container: 368 GFlops
Host: 368 GFLOPS
#A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x 8 * 2.8=
=20
=3D 448 GFlops
Efficiency =3D 368 / 448 =3D 82%. Not bad (given the small N (matrix size) =
and=20
that gcc instead of icc was used, and the executable was dynamically linked=
=20
-- by purpose)

Dual-node, N=3D60000, P=3D8, Q=3D5
Container: 736 GFLOPS
Host:737 GFLOPS


<OSU-P2P-Bandwidth>
Container:=20
Msg size(bytes) BW (MB/s)
65536                3738.89
131072               3756.34
262144               3767.35
524288               3780.46
1048576              3781.94
2097152              3775.07
4194304              3775.97

Host:
Msg size(bytes) BW (MB/s)
65536                3722.32
131072               3751.33
262144               3771.13
524288               3774.33
1048576              3781.43
2097152              3775.00
4194304              3773.68

<OSU-P2P-Latency>
Container:=20
Msg size(bytes)  Latency (us)
0                       1.61
1                       1.67
2                       1.66
4                       1.66
8                       1.70

Host:
Msg size(bytes)  Latency (us)
0                       1.57
1                       1.63
2                       1.63
4                       1.63
8                       1.65

How were the benchmarks executed?
mpirun -n 20 -hostfile hostfile singularity exec /home/chih/containers/
container-centos6-demo.img xhpl
mpirun -n 2 -hostfile hostfile singularity exec=20
/home/chih/containers/container-centos6-demo.img osu_bw
mpirun -n 2 -hostfile hostfile singularity exec=20
/home/chih/containers/container-centos6-demo.img osu_latency

My conclusion: No difference for HPL (1 & 2 nodes) and IB bandwidth. There=
=20
is a consistent 2% overhead in IB latency though, but this should be=20
affordable by most users.

On Thursday, February 23, 2017 at 12:44:47 PM UTC+1, Chih-Song Kuo wrote:
>
> I am not really sure if I can share the containers publicly as it contain=
s=20
> an Intel MKL installation which is bound to a personalized serial number=
=20
> (although that MKL is from the community edition, so it is essentially=20
> free).
>
> Still, I am wondering if installing OFED into the container is the right=
=20
> approach. There was a thread on a similar topic and Greg said no. I hope =
I=20
> understood it correctly.
> https://groups.google.com/a/lbl.gov/forum/#!topic/singularity/fsCO1_StjjA
>
> After all, although requiring the user to install OFED into the container=
=20
> might be technically feasible, it would be very awkward for users who=20
> develop their applications on their own workstations without InfiniBand,=
=20
> not to mention that I am even not sure if one can install OFED into a=20
> machine without IB adapters. And what if the OFED library in the containe=
r=20
> differ from that on the host? I don't think that will work, will it?
>
> Sorry but somehow OpenMPI (or any MPI) is intent to reach OFED libraries=
=20
> still remains a mystery for me. Please enlighten me.
>
> On Thursday, February 23, 2017 at 9:51:34 AM UTC+1, R=C3=A9my Dernat wrot=
e:
>>
>> Hi,
>>
>> You can try to copy your libraries directly from the host to the=20
>> container instead of a classical install ? I do not know what is the bes=
t=20
>> way... But if you check woth solutions we will have an answer.
>> BTW, I am very interesting by your containers. Could you share it=20
>> (through singularity-hub ?) with your command line to run the benchmark =
in=20
>> the "singularity" file inside the container ? Indeed, I will do some=20
>> performance tests also...
>>
>> Best regards,
>> R=C3=A9my
>>
>> 2017-02-23 0:58 GMT+01:00 Chihsong :
>>
>>> Hi Greg,
>>>
>>> The problem is that I don't feel I have IB libraries inside my=20
>>> container. How can I check that? Or did you simply install the ofed int=
o=20
>>> the container?=20
>>>
>>> Chiu-Song
>>>
>>> On Thursday, February 23, 2017, Gregory M. Kurtzer <gm...@lbl.gov>=20
>>> wrote:
>>>
>>>> Hi Chih-Song,
>>>>
>>>> Haha, every now and then I get lucky when a new mail comes in and is a=
t=20
>>>> the top of my mbox and I have a moment.
>>>>
>>>> In summary, yes. The MPI inside the container must also link against=
=20
>>>> the IB libraries (also within the container).
>>>>
>>>> Hopefully that helps!
>>>>
>>>> On Wed, Feb 22, 2017 at 10:58 AM, Chih-Song Kuo  wrote:
>>>>
>>>>> Hi Greg,
>>>>>
>>>>> That reply was very prompt! Anyways, my answer follows.
>>>>>
>>>>> > * Make sure the MPI inside the container is properly linking agains=
t=20
>>>>> the IB libraries
>>>>> Did that mean that I need to install IB libraries (like Mellanox OFED=
)=20
>>>>> into the container? I guess not?
>>>>> But apparently openib is missing in the container.
>>>>> [me@cn03 ompi-rhel6-host]$ ompi_info | grep openib
>>>>>                  MCA btl: openib (MCA v2.1.0, API v3.0.0, Component=
=20
>>>>> v3.0.0)
>>>>>
>>>>> Singularity.container-centos6-demo.img> ompi_info | grep openib
>>>>> Singularity.container-centos6-demo.img>
>>>>> Singularity.container-centos6-demo.img> ls /etc/infiniband/openib.con=
f
>>>>> ls: cannot access /etc/infiniband/openib.conf: No such file or=20
>>>>> directory
>>>>>
>>>>> > * Make sure that the MPI configuration is configured to use MPI
>>>>> I think you meant "to use IB" instead.=20
>>>>> But still, did you mean that OpenMPI should be "configured"=20
>>>>> "--with-verbs"? (Did you do so or you never had my problem?)
>>>>> I did not use this flag when compiling Open MPI either on the host or=
=20
>>>>> in the container.
>>>>> > * Make sure you have Singularity configured properly to share the=
=20
>>>>> devices properly, tmp, and you are *NOT* using the IPC or PID namespa=
ces
>>>>> Can you provide more hint on how that can be done?
>>>>>
>>>>> Thank you once again for your time.
>>>>> Chih-Song
>>>>>
>>>>> On Wednesday, February 22, 2017 at 7:33:22 PM UTC+1, Gregory M.=20
>>>>> Kurtzer wrote:
>>>>>>
>>>>>> There are various things that *could* go wrong, and usage of=20
>>>>>> containers (any of the technologies) actually introduce complexities=
 in=20
>>>>>> kernel and user space alignment that normally we don't consider. For=
 that=20
>>>>>> reason, my first suspicion is that your IB fabric is not being prope=
rly=20
>>>>>> utilized by the MPI within the container. That could be due to anyth=
ing=20
>>>>>> from build errors within the container to IB library/kmod API misali=
gnment.
>>>>>>
>>>>>> Things I would look at and check:
>>>>>>
>>>>>> * Make sure the MPI inside the container is properly linking against=
=20
>>>>>> the IB libraries
>>>>>> * Make sure that the IB libraries inside the container are compatibl=
e=20
>>>>>> with the host kernel
>>>>>> * Make sure that the MPI configuration is configured to use MPI
>>>>>> * Make sure you have Singularity configured properly to share the=20
>>>>>> devices properly, tmp, and you are *NOT* using the IPC or PID namesp=
aces
>>>>>>
>>>>>> Hope that helps.
>>>>>>
>>>>>> Greg
>>>>>>
>>>>>> On Wed, Feb 22, 2017 at 10:19 AM, Chih-Song Kuo wrote:
>>>>>>
>>>>>>> Hello,
>>>>>>>
>>>>>>> This is again Chih-Song from Fujitsu. I decided to make another pos=
t=20
>>>>>>> to share my experience of performance impact with two kernel benchm=
arks:=20
>>>>>>> High Performance Linpack (HPL) and the OSU MPI benchmark suit.
>>>>>>>
>>>>>>> Overall, there was no noticeable performance difference for=20
>>>>>>> benchmarks running on a single node. But for benchmarks running acr=
oss=20
>>>>>>> nodes, I did observe some difference, which was against to the clai=
m of=20
>>>>>>> Singularity. Have anybody done any similar exercise? What are your=
=20
>>>>>>> findings? Can you suspect whether I was doing anything wrong?
>>>>>>>
>>>>>>> Host configuration:
>>>>>>> 2x Intel E5-2680v2 (Ivybridge)
>>>>>>> 64GB memory
>>>>>>> Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR=20
>>>>>>> switch)
>>>>>>> RHEL 6.7
>>>>>>> OpenMPI development master branch (8.2.17)
>>>>>>> Intel MKL 2017.0 community edition
>>>>>>> gcc 4.4
>>>>>>>
>>>>>>> Container:
>>>>>>> Centos 6 and 7 both tested without noticeable performance differenc=
e
>>>>>>> OpenMPI development master branch (8.2.17)
>>>>>>> Intel MKL 2017.0 community edition
>>>>>>> gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)
>>>>>>>
>>>>>>> Benchmarks:
>>>>>>> 1. LINPACK 2.2
>>>>>>> 2. OSU 5.3.2
>>>>>>>
>>>>>>> <LINPACK>
>>>>>>> Single node. N=3D40000, P=3D5, Q=3D4
>>>>>>> Container: 368 GFlops
>>>>>>> Host: 368 GFLOPS
>>>>>>> #A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x=
=20
>>>>>>> 8 * 2.8 =3D 448 GFlops
>>>>>>> Efficiency =3D 368 / 448 =3D 82%. Not bad (given the small N (matri=
x=20
>>>>>>> size) and that gcc instead of icc was used, and the executable was=
=20
>>>>>>> dynamically linked -- by purpose)
>>>>>>>
>>>>>>> Dual-node, N=3D60000, P=3D8, Q=3D5
>>>>>>> Container: 702 GFLOPS
>>>>>>> Host:737 GFLOPS
>>>>>>> There is roughly 5% of performance degradation with the container.
>>>>>>>
>>>>>>> <OSU-P2P-Bandwidth>
>>>>>>> The container only saw 50-65% of the total bandwidth.
>>>>>>>
>>>>>>> Container:=20
>>>>>>> Msg size(bytes) BW (MB/s)
>>>>>>> 65536                2142.28
>>>>>>> 131072               2363.45
>>>>>>> 262144               1705.79
>>>>>>> 524288               1592.56
>>>>>>> 1048576              1721.88
>>>>>>> 2097152              1557.42
>>>>>>> 4194304              1655.90
>>>>>>>
>>>>>>> Host:
>>>>>>> Msg size(bytes) BW (MB/s)
>>>>>>> 65536                3722.32
>>>>>>> 131072               3751.33
>>>>>>> 262144               3771.13
>>>>>>> 524288               3774.33
>>>>>>> 1048576              3781.43
>>>>>>> 2097152              3775.00
>>>>>>> 4194304              3773.68
>>>>>>>
>>>>>>> <OSU-P2P-Latency>
>>>>>>> Here the container was significantly slower.
>>>>>>>
>>>>>>> Container:=20
>>>>>>> Msg size(bytes)  Latency (us)
>>>>>>> 0                      31.59
>>>>>>> 1                      31.86
>>>>>>> 2                      31.83
>>>>>>>
>>>>>>> Host:
>>>>>>> Msg size(bytes)  Latency (us)
>>>>>>> 0                       1.55
>>>>>>> 1                       1.63
>>>>>>> 2                       1.63
>>>>>>>
>>>>>>> Note 1: Run-to-run variation of performance was much smaller than=
=20
>>>>>>> the difference on the host and in the container.
>>>>>>> Note 2: When Singularity was used, I could not instruct mpirun to=
=20
>>>>>>> use the ofed by specifying "--mca btl openib,self,vader" in the mpi=
run=20
>>>>>>> parameter list. Doing so would give me an error message stating tha=
t the=20
>>>>>>> openib component is missing. However, from the bandwidth measured a=
bove,=20
>>>>>>> the container did seem to be able to use InfiniBand, otherwise the=
=20
>>>>>>> bandwidth would not be so high (the nodes only had InfiniBand and 1=
G=20
>>>>>>> Ethernet). Maybe container was using IPoIB? I did not check that ye=
t.=20
>>>>>>>
>>>>>>> Reference: How the benchmarks were executed:
>>>>>>> mpirun -n 20 -hostfile hostfile singularity exec=20
>>>>>>> /home/chih/containers/container-centos6-demo.img xhpl
>>>>>>> mpirun -n 2 -hostfile hostfile singularity exec=20
>>>>>>> /home/chih/containers/container-centos6-demo.img osu_bw
>>>>>>> mpirun -n 2 -hostfile hostfile singularity exec=20
>>>>>>> /home/chih/containers/container-centos6-demo.img osu_latency
>>>>>>>
>>>>>>> Chih-Song
>>>>>>>
>>>>>>> --=20
>>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,=
=20
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --=20
>>>>>> Gregory M. Kurtzer
>>>>>> HPC Systems Architect and Technology Developer
>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>> University of California Berkeley Research IT
>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>> GitHub: https://github.com/gmkurtzer, Twitter:=20
>>>>>> https://twitter.com/gmkurtzer
>>>>>>
>>>>> --=20
>>>>> You received this message because you are subscribed to the Google=20
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d=20
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --=20
>>>> Gregory M. Kurtzer
>>>> HPC Systems Architect and Technology Developer
>>>> Lawrence Berkeley National Laboratory HPCS
>>>> University of California Berkeley Research IT
>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>> GitHub: https://github.com/gmkurtzer, Twitter:=20
>>>> https://twitter.com/gmkurtzer
>>>>
>>>> --=20
>>>> You received this message because you are subscribed to the Google=20
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>> --=20
>>> Chih-Song Kuo =E9=83=AD=E7=9F=A5=E9=A0=8C
>>> Senior Sales Consultant - HPC Benchmark Specialist at Fujitsu
>>> M.Sc. RWTH with distinction in Software Systems Engineering with HPC=20
>>> focus
>>> B.Sc. NTHU in Computer Science, B.S.M. NTHU in Quantitative Finance
>>> Tel:  +49-177-88949-28; +49-241-88949-155; +886-2-26629518
>>>
>>> --=20
>>> You received this message because you are subscribed to the Google=20
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>> an email to singu...@lbl.gov.
>>>
>>
>>
------=_Part_2375_840773803.1487883950806
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">OK...Now after installing InfiniBand libraries things fina=
lly work decently. From this lesson, I would suggest adding a remark about =
IB support to http://singularity.lbl.gov/docs-hpc, because otherwise new us=
ers can easily get confused about Singularity&#39;s support for InfiniBand.=
 And HPC is obviously defined by such high-speed interconnects (IB, OPA, Cr=
ay, ...)<br><br>To me, this does seem to be a serious issue which would pot=
entially decrease users&#39; feeling about the portability of Singularity c=
ontainers. Think about a scientist developing codes on a workstation withou=
t InfiniBand, and later making a test run on a small departmental cluster w=
ith InfiniBand, and eventually a production run on some white elephant with=
 Intel Omni-Path. Just missing one interconnect library in the container an=
d not compiling the MPI against that library would immediately break portab=
ility.<br><br>Also I agree with the comment made in https://groups.google.c=
om/a/lbl.gov/forum/#!topic/singularity/fsCO1_StjjA that Singularity should =
consider to include OFED drivers by default when creating a new container. =
Not sure if Greg you would change your mind here. The problem was really no=
t about orted. Yes, Open MPI worked without the OFED driver, but only on Et=
hernet. Since Singularity is claimed to be the container for HPC, including=
 a well defined set of interconnect drivers and libraries truly makes sense=
.<br><br>For those who try to make your containers work with InfiniBand, I =
can provide a few general hints. These were not rigorously tested though an=
d I doubt I have the time to do so.<br>1. Issue ibv_devices in the containe=
r. You must be able able to see the name of your InfiniBand adpter. If not,=
 then possibly some IB libraries and drivers are missing<br>1.1. For Centos=
 7, the following rpm packages seem to be the minimum requirement<br>libibv=
erbs* <br>librdmacm*=C2=A0=C2=A0=C2=A0 <br>1.2. For Centos 6, the following=
 rpm packages seem to be the minimum requirement<br>libibverbs* <br>librdma=
cm*=C2=A0=C2=A0=C2=A0 <br>libmlx*<br>1.3. Nevertheless, in the end I instal=
led more packages<br>yum -y install libmthca <br>yum -y install libibcm*<br=
>yum -y install libibmad*<br>yum -y install libibumad*<br>1.4. Maybe the fo=
llowing will be useful, but I did not install them yet<br>yum -y install da=
pl*<br>yum -y install ibacm*<br>yum -y install ibutils<br><br>2. After ./co=
nfigure Open MPI, make sure you see the following.<br>OpenFabrics Verbs: ye=
s<br>If you have OmniPath, then it should be<br>Intel Omnipath (PSM2): yes<=
br><br>There are also other components like MXM=C2=A0 which can potentially=
 be &quot;yes&quot; if you installed Mellanox MXM.<br><br>3. When make Open=
 MPI, use serial compilation, i.e. do not add the -j flag. It sometimes cau=
ses explainable behaviors.<br><br>4. After make install Open MPI, make sure=
 that the OpenIB component is indeed installed.<br>ompi_info |grep openib <=
br>#Expect to see non-empty response<br>=C2=A0MCA btl: openib (MCA v2.1.0, =
API v3.0.0, Component v3.0.0)<br><br>5. Make sure that the MPI version is t=
he same on the host and in the container<br><br>My updated test result is p=
rovided below. If anybody would like to put these figures into any other do=
cuments, please inform me.<br><br>Host configuration:<br>2x Intel E5-2680v2=
 (Ivybridge)<br>64GB memory<br>Mellanox ConnectX-3 FDR adapter (but connect=
s to a Mellanox QDR switch)<br>RHEL 6.7<br>OpenMPI development master branc=
h (dated 8.2.17)<br>Intel MKL 2017.0 community edition<br>gcc 4.4<br><br>Co=
ntainer:<br>Centos 6 and 7 both tested without noticeable performance diffe=
rence<br>OpenMPI development master branch (dated 8.2.17)<br>Intel MKL 2017=
.0 community edition<br>gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)<br><br>B=
enchmarks:<br>1. LINPACK 2.2<br>2. OSU 5.3.2<br><br>&lt;LINPACK&gt;<br>Sing=
le node. N=3D40000, P=3D5, Q=3D4<br>Container: 368 GFlops<br>Host: 368 GFLO=
PS<br>#A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x 8=
 * 2.8 =3D 448 GFlops<br>Efficiency
 =3D 368 / 448 =3D 82%. Not bad (given the small N (matrix size) and that=
=20
gcc instead of icc was used, and the executable was dynamically linked=20
-- by purpose)<br><br>Dual-node, N=3D60000, P=3D8, Q=3D5<br>Container: 736 =
GFLOPS<br>Host:737 GFLOPS<br><br><br>&lt;OSU-P2P-Bandwidth&gt;<br>Container=
: <br>Msg size(bytes) BW (MB/s)<br>65536=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3738.89<br>131072=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 3756.34<br>262144=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3767.35<br>524288=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3780.46<br>=
1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 3781.94<br>2097152=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3775.07<br>4194304=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3775.97<br><br=
>Host:<br>Msg size(bytes) BW (MB/s)<br>65536=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3722.32<br>131=
072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3771.13<br>524288=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3774.33<=
br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3775.00<br>4194304=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3773.68<br>=
<br>&lt;OSU-P2P-Latency&gt;<br>Container: <br>Msg size(bytes)=C2=A0 Latency=
 (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.61<=
br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.67<br>2=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.66<br>4=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.66<br>8=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.70<br><br>Host:<br>Msg size=
(bytes)=C2=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 1.57<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 1.63<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 1.63<br>4=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.=
63<br>8=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.65<br>=
<br>How were the benchmarks executed?<br>mpirun -n 20 -hostfile hostfile si=
ngularity exec /home/chih/containers/<div dir=3D"ltr"><wbr>container-centos=
6-demo.img xhpl<br>mpirun -n 2 -hostfile hostfile singularity exec /home/ch=
ih/containers/<wbr>container-centos6-demo.img osu_bw<br>mpirun -n 2 -hostfi=
le hostfile singularity exec /home/chih/containers/<wbr>container-centos6-d=
emo.img osu_latency<br></div><br>My conclusion: No difference for HPL (1 &a=
mp; 2 nodes) and IB bandwidth. There is a consistent 2% overhead in IB late=
ncy though, but this should be affordable by most users.<br><br>On Thursday=
, February 23, 2017 at 12:44:47 PM UTC+1, Chih-Song Kuo wrote:<blockquote c=
lass=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;border-left: 1px=
 #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">I am not really sure if I =
can share the containers publicly as it contains an Intel MKL installation =
which is bound to a personalized serial number (although that MKL is from t=
he community edition, so it is essentially free).<br><br>Still, I am wonder=
ing if installing OFED into the container is the right approach. There was =
a thread on a similar topic and Greg said no. I hope I understood it correc=
tly.<br><a href=3D"https://groups.google.com/a/lbl.gov/forum/#!topic/singul=
arity/fsCO1_StjjA" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.h=
ref=3D&#39;https://groups.google.com/a/lbl.gov/forum/#!topic/singularity/fs=
CO1_StjjA&#39;;return true;" onclick=3D"this.href=3D&#39;https://groups.goo=
gle.com/a/lbl.gov/forum/#!topic/singularity/fsCO1_StjjA&#39;;return true;">=
https://groups.google.com/a/<wbr>lbl.gov/forum/#!topic/<wbr>singularity/fsC=
O1_StjjA</a><br><br>After all, although requiring the user to install OFED =
 into the container might be technically feasible, it would be very awkward=
 for users who develop their applications on their own workstations without=
 InfiniBand, not to mention that I am even not sure if one can install OFED=
 into a machine without IB adapters. And what if the OFED library in the co=
ntainer differ from that on the host? I don&#39;t think that will work, wil=
l it?<br><br>Sorry but somehow OpenMPI (or any MPI) is intent to reach OFED=
 libraries still remains a mystery for me. Please enlighten me.<br><br>On T=
hursday, February 23, 2017 at 9:51:34 AM UTC+1, R=C3=A9my Dernat wrote:<blo=
ckquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi,<div><br></div><div=
>You can try to copy your libraries=C2=A0directly=C2=A0from the host to the=
 container instead of a classical install ? I do not know what is the best =
way... But if you check woth solutions we will have an answer.</div><div>BT=
W, I am very interesting by your containers. Could you share it (through si=
ngularity-hub ?) with your command line to run the benchmark in the &quot;s=
ingularity&quot; file inside the container ? Indeed, I will do some perform=
ance tests also...</div><div><br></div><div>Best regards,</div><div>R=C3=A9=
my</div></div><div><br><div class=3D"gmail_quote">2017-02-23 0:58 GMT+01:00=
 Chihsong <span dir=3D"ltr"></span>:<br><blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi G=
reg,<div><br></div><div>The problem is that I=C2=A0don&#39;t feel I=C2=A0ha=
ve IB libraries inside my container. How can I=C2=A0check that? Or did you =
simply=C2=A0install the ofed into the container?=C2=A0</div><div><br>Chiu-S=
ong</div><div><div><div><br>On Thursday, February 23, 2017, Gregory M. Kurt=
zer &lt;<a rel=3D"nofollow">gm...@lbl.gov</a>&gt; wrote:<br><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pa=
dding-left:1ex"><div dir=3D"ltr">Hi Chih-Song,<div><br></div><div>Haha, eve=
ry now and then I get lucky when a new mail comes in and is at the top of m=
y mbox and I have a moment.</div><div><br></div><div>In summary, yes. The M=
PI inside the container must also link against the IB libraries (also withi=
n the container).</div><div><br></div><div>Hopefully that helps!</div></div=
><div><br><div class=3D"gmail_quote">On Wed, Feb 22, 2017 at 10:58 AM, Chih=
-Song Kuo=C2=A0 wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin=
:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">H=
i Greg,<br><br>That reply was very prompt! Anyways, my answer follows.<span=
><br><br>&gt; * Make sure the MPI inside the container is properly linking =
against the IB libraries<br></span>Did that mean that I need to install IB =
libraries (like Mellanox OFED) into the container? I guess not?<br>But appa=
rently openib is missing in the container.<br>[me@cn03 ompi-rhel6-host]$ om=
pi_info | grep openib<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 MCA btl: openib (MCA v2.1.=
0, API v3.0.0, Component v3.0.0)<br><br>Singularity.container-centos6-<wbr>=
demo.img&gt; ompi_info | grep openib<br>Singularity.container-centos6-<wbr>=
demo.img&gt;<br>Singularity.container-centos6-<wbr>demo.img&gt; ls /etc/inf=
iniband/openib.conf<br>ls: cannot access /etc/infiniband/openib.conf: No su=
ch file or directory<span><br><br>&gt; * Make sure that the MPI configurati=
on is configured to use MPI<br></span>I think you meant &quot;to use IB&quo=
t; instead. <br>But still, did you mean that OpenMPI should be &quot;config=
ured&quot; &quot;--with-verbs&quot;? (Did you do so or you never had my pro=
blem?)<br>I did not use this flag when compiling Open MPI either on the hos=
t or in the container.<span><br>&gt; * Make sure you have Singularity confi=
gured properly to share the=20
devices properly, tmp, and you are *NOT* using the IPC or PID namespaces<br=
></span>Can you provide more hint on how that can be done?<br><br>Thank you=
 once again for your time.<br>Chih-Song<span><br><br>On Wednesday, February=
 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer wrote:</span><blockquote =
class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #=
ccc solid;padding-left:1ex"><span><div dir=3D"ltr">There are various things=
 that *could* go wrong, and usage of containers (any of the technologies) a=
ctually introduce complexities in kernel and user space alignment that norm=
ally we don&#39;t consider. For that reason, my first suspicion is that you=
r IB fabric is not being properly utilized by the MPI within the container.=
 That could be due to anything from build errors within the container to IB=
 library/kmod API misalignment.<div><br></div><div>Things I would look at a=
nd check:</div><div><br></div><div>* Make sure the MPI inside the container=
 is properly linking against the IB libraries</div><div>* Make sure that th=
e IB libraries inside the container are compatible with the host kernel</di=
v><div>* Make sure that the MPI configuration is configured to use MPI</div=
><div>* Make sure you have Singularity configured properly to share the dev=
ices properly, tmp, and you are *NOT* using the IPC or PID namespaces</div>=
<div><br></div><div>Hope that helps.</div><div><br></div><div>Greg</div></d=
iv></span><div><br><div class=3D"gmail_quote"><div><div>On Wed, Feb 22, 201=
7 at 10:19 AM, Chih-Song Kuo wrote:<br></div></div><blockquote class=3D"gma=
il_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-lef=
t:1ex"><div><div><div dir=3D"ltr">Hello,<br><br>This is again Chih-Song fro=
m Fujitsu. I decided to make another post to share my experience of perform=
ance impact with two kernel benchmarks: High Performance Linpack (HPL) and =
the OSU MPI benchmark suit.<br><br>Overall, there was no noticeable perform=
ance difference for benchmarks running on a single node. But for benchmarks=
 running across nodes, I did observe some difference, which was against to =
the claim of Singularity. Have anybody done any similar exercise? What are =
your findings? Can you suspect whether I was doing anything wrong?<br><br>H=
ost configuration:<br>2x Intel E5-2680v2 (Ivybridge)<br>64GB memory<br>Mell=
anox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)<br>RHEL=
 6.7<br>OpenMPI development master branch (8.2.17)<br>Intel MKL 2017.0 comm=
unity edition<br>gcc 4.4<br><br>Container:<br>Centos 6 and 7 both tested wi=
thout noticeable performance difference<br>OpenMPI development master branc=
h (8.2.17)<br>Intel MKL 2017.0 community edition<br>gcc 4.4.7 (Centos-6), g=
cc 4.8.5 (Centos-7)<br><br>Benchmarks:<br>1. LINPACK 2.2<br>2. OSU 5.3.2<br=
><br>&lt;LINPACK&gt;<br>Single node. N=3D40000, P=3D5, Q=3D4<br>Container: =
368 GFlops<br>Host: 368 GFLOPS<br>#A single node has 2x Intel E5-2680v2. So=
 we are expecting 2 x 10 x 8 * 2.8 =3D 448 GFlops<br>Efficiency =3D 368 / 4=
48 =3D 82%. Not bad (given the small N (matrix size) and that gcc instead o=
f icc was used, and the executable was dynamically linked -- by purpose)<br=
><br>Dual-node, N=3D60000, P=3D8, Q=3D5<br>Container: 702 GFLOPS<br>Host:73=
7 GFLOPS<br>There is roughly 5% of performance degradation with the contain=
er.<br><br>&lt;OSU-P2P-Bandwidth&gt;<br>The container only saw 50-65% of th=
e total bandwidth.<br><br>Container: <br>Msg size(bytes) BW (MB/s)<br>65536=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 2142.28<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2363.45<br>262144=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1705.=
79<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 1592.56<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1721.88<br>2097152=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1557.=
42<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1655.90<br><br>Host:<br>Msg size(bytes) BW (MB/s)<br>655=
36=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 3722.32<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
71.13<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 3774.33<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
75.00<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 3773.68<br><br>&lt;OSU-P2P-Latency&gt;<br>Here the co=
ntainer was significantly slower.<br><br>Container: <br>Msg size(bytes)=C2=
=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31=
.59<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.86<br>2=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.83<br><br>Host:<br>Msg =
size(bytes)=C2=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1.55<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 1.63<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 1.63<br><br>Note 1: Run-to-run variation of performance was much sma=
ller than the difference on the host and in the container.<br>Note 2: When =
Singularity was used, I could not instruct mpirun to use the ofed by specif=
ying &quot;--mca btl openib,self,vader&quot; in the mpirun parameter list. =
Doing so would give me an error message stating that the openib component i=
s missing. However, from the bandwidth measured above, the container did se=
em to be able to use InfiniBand, otherwise the bandwidth would not be so hi=
gh (the nodes only had InfiniBand and 1G Ethernet). Maybe container was usi=
ng IPoIB? I did not check that yet. <br><br>Reference: How the benchmarks w=
ere executed:<br>mpirun -n 20 -hostfile hostfile singularity exec /home/chi=
h/containers/<wbr>container-centos6-demo.img xhpl<br>mpirun -n 2 -hostfile =
hostfile singularity exec /home/chih/containers/<wbr>container-centos6-demo=
.img osu_bw<br>mpirun -n 2 -hostfile hostfile singularity exec /home/chih/c=
ontainers/<wbr>container-centos6-demo.img osu_latency<span><font color=3D"#=
888888"><br><br>Chih-Song<br><br></font></span></div></div></div><span><fon=
t color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><=
div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurt=
zer</div><div>HPC Systems Architect and Technology Developer</div><div>Lawr=
ence Berkeley National Laboratory HPCS<br>University of California Berkeley=
 Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singul=
arity.lbl.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=
=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\=
x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;r=
eturn true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhtt=
p%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHI=
TKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.g=
ov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://ware=
wulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=
=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26=
sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;retu=
rn true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3=
A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wi=
Dx3C_BKcVgBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)<=
/div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"nofo=
llow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;https://www.google.=
com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x=
26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"t=
his.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2F=
gmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg=
2uw&#39;;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span st=
yle=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.co=
m/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=3D"_blank" =
onmousedown=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F=
%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-Y=
HVVhLsKsNsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https:/=
/www.google.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x=
26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;"=
>https://<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div>=
</div></div></div></div></div>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;=
http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\x3=
dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return tr=
ue;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%=
2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-=
iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a>)=
</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lbl=
.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;h=
ttp://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD\x=
26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true;"=
 onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwa=
rewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcV=
gBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><div=
>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"nofollow" tar=
get=3D"_blank" onmousedown=3D"this.href=3D&#39;https://www.google.com/url?q=
\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3d=
AFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.href=
=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtze=
r\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;=
;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"f=
ont-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurt=
zer" style=3D"font-size:12.8px" rel=3D"nofollow" target=3D"_blank" onmoused=
own=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwitt=
er.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKs=
NsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.goo=
gle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x=
3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:/=
/<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div></=
div></div></div></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
</blockquote></div><br><br></div></div><span><font color=3D"#888888">-- <br=
><div dir=3D"ltr"><span><div><div dir=3D"ltr"><div>Chih-Song Kuo =E9=83=AD=
=E7=9F=A5=E9=A0=8C<br>Senior Sales Consultant - HPC Benchmark Specialist at=
 Fujitsu<br>M.Sc. RWTH with distinction in Software Systems Engineering wit=
h HPC focus<br>B.Sc. NTHU in Computer Science, B.S.M. NTHU in Quantitative =
Finance<br>Tel: =C2=A0<a value=3D"+491778894928">+49-177-88949-28</a>; <a v=
alue=3D"+4924188949155">+49-241-88949-155</a>; <a value=3D"+886226629518">+=
886-2-26629518</a></div></div></div></span></div></font></span><div><div><b=
r>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br></div>
</blockquote></div></blockquote></div>
------=_Part_2375_840773803.1487883950806--

------=_Part_2374_1087618444.1487883950805--
