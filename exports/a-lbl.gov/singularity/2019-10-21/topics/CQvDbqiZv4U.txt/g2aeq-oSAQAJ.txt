Date: Mon, 26 Feb 2018 07:06:19 -0800 (PST)
From: Azat Khuziyakhmetov <a3a...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <8b813901-59c1-4689-abe1-40318b702f23@lbl.gov>
In-Reply-To: <74cbc46b-1eb9-4f43-a764-9de792ea86a6@lbl.gov>
References: <74cbc46b-1eb9-4f43-a764-9de792ea86a6@lbl.gov>
Subject: Re: singularity install on RHEL7 + GPU
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_15345_1380121539.1519657579617"

------=_Part_15345_1380121539.1519657579617
Content-Type: multipart/alternative; 
	boundary="----=_Part_15346_1275517597.1519657579617"

------=_Part_15346_1275517597.1519657579617
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Hi Valentin,

Try to use --nv flag of singularity. It will also bind nvidia drivers from 
the host machine. 

If anyone knows how to properly bind the drivers from the host manually to 
the container (without installing them via apt) please reply too :) thank 
you,

Best regards,
Azat

On Sunday, February 25, 2018 at 1:28:23 AM UTC+1, Valentin Kozlov wrote:
>
> Hi all,
>
> I am a bit experimenting with singularity and trying to install it on AWS, 
> RHEL7 AMI (ami-c90195b0). 
> I first install Nvidia stuff by downloading rpm from nvidia, 
> cuda-repo-rhel7-9-1-local-9.1.85-1.x86_64. I clone singularity from git, 
> compile it and install it (though no mksquashfs was installed).  nvidia-smi 
> outputs 387.26
>
> However, when I run under unprivileged user: singularity shell --nv 
> docker://tensorflow/tensorflow:latest-gpu
>
> I get following error messages:
> ~~~~~~~~~~
> failed call to cuInit: CUDA_ERROR_UNKNOWN
> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:158] retrieving CUDA 
> diagnostic information for host: ip-172-31-20-167.eu-west-1.compute.internal
> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: 
> ip-172-31-20-167.eu-west-1.compute.internal
> I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda 
> reported version is: Invalid argument: expected %d.%d, %d.%d.%d, or 
> %d.%d.%d.%d form for driver version; got "1"
> ~~~~~~~~~~
> I can still run nvidia-smi inside container and it produces right output.
>
> 'Funny' enough, if I install docker-ce and nvidia-docker, I can run same 
> container in nvidia-docker, BUT I then can also run my command above, i.e. 
> no error message. It seems to be related to the fact, that nvidia-docker 
> puts additional kernel drivers in memory.
>
> Any idea how to avoid the error without installing nvidia-docker?
>
> Best,
> Valentin
>

------=_Part_15346_1275517597.1519657579617
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Valentin,<div><br></div><div>Try to use --nv flag of si=
ngularity. It will also bind nvidia drivers from the host machine.=C2=A0</d=
iv><div><br></div><div>If anyone knows how to properly bind the drivers fro=
m the host manually to the container (without installing them via apt) plea=
se reply too :) thank you,</div><div><br></div><div>Best regards,</div><div=
>Azat<br><br>On Sunday, February 25, 2018 at 1:28:23 AM UTC+1, Valentin Koz=
lov wrote:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left:=
 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">Hi =
all,<div><br></div><div>I am a bit experimenting with singularity and tryin=
g to install it on AWS, RHEL7 AMI (ami-c90195b0).=C2=A0</div><div>I first i=
nstall Nvidia stuff by downloading rpm from nvidia, cuda-repo-rhel7-9-1-loc=
al-9.1.<wbr>85-1.x86_64. I clone singularity from git, compile it and insta=
ll it (though no mksquashfs was installed).=C2=A0 nvidia-smi outputs 387.26=
</div><div><br></div><div>However, when I run under unprivileged user:=C2=
=A0singularity shell --nv docker://tensorflow/<wbr>tensorflow:latest-gpu</d=
iv><div><br></div><div>I get following error messages:</div><div><div>~~~~~=
~~~~~</div><div>failed call to cuInit: CUDA_ERROR_UNKNOWN</div><div>I tenso=
rflow/stream_executor/<wbr>cuda/cuda_diagnostics.cc:158] retrieving CUDA di=
agnostic information for host: ip-172-31-20-167.eu-west-1.<wbr>compute.inte=
rnal</div><div>I tensorflow/stream_executor/<wbr>cuda/cuda_diagnostics.cc:1=
65] hostname: ip-172-31-20-167.eu-west-1.<wbr>compute.internal</div><div>I =
tensorflow/stream_executor/<wbr>cuda/cuda_diagnostics.cc:189] libcuda repor=
ted version is: Invalid argument: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d =
form for driver version; got &quot;1&quot;</div></div><div>~~~~~~~~~~</div>=
<div>I can still run nvidia-smi inside container and it produces right outp=
ut.</div><div><br></div><div>&#39;Funny&#39; enough, if I install docker-ce=
 and nvidia-docker, I can run same container in nvidia-docker, BUT I then c=
an also run my command above, i.e. no error message. It seems to be related=
 to the fact, that nvidia-docker puts additional kernel drivers in memory.<=
/div><div><br></div><div>Any idea how to avoid the error without installing=
 nvidia-docker?</div><div><br></div><div>Best,</div><div>Valentin</div></di=
v></blockquote></div></div>
------=_Part_15346_1275517597.1519657579617--

------=_Part_15345_1380121539.1519657579617--
