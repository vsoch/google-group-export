X-Received: by 10.98.130.136 with SMTP id w130mr1626253pfd.60.1518807398360;
        Fri, 16 Feb 2018 10:56:38 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.99.116.86 with SMTP id e22ls620122pgn.0.gmail; Fri, 16 Feb
 2018 10:56:37 -0800 (PST)
X-Received: by 10.99.127.86 with SMTP id p22mr5879261pgn.157.1518807396970;
        Fri, 16 Feb 2018 10:56:36 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1518807396; cv=none;
        d=google.com; s=arc-20160816;
        b=kI1NOIFfnwS/VpIGc05ns3+I1CY4neNVp6U1v9NCV0+3DvKQ2/5QM55ODBqROhyJcw
         kjb7L1fzzi6v3m/wUJSkNMi1URDK+FF4q64ozK/j17Umh0MVbDuvBIrqiu/sJF5ezt8+
         yCqzIo6uBUGUitt7DP+I2lBWYGPXLLlGXfxKCYASMyTO5g4PSmd4orhC7tlt/nDLvO8E
         uNUY40aw1tqRXW56p7o4ckhCc/FCzUxbIpwsHSWNZLS8NOCYovoH1s6eZSxNEenzesdN
         OahpvgXuOdgnFWmcV8wrVTvInXrZeJFiDtpu4qCEazG2k82YE4Gg6F5NJ5D6ZnU4/XcB
         yp0w==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=content-transfer-encoding:to:subject:message-id:date:from
         :references:in-reply-to:mime-version:dkim-signature
         :arc-authentication-results;
        bh=3hKPep80q0pVsL4Svv4cUbMLWhliOlKBsSgf4FHV9NM=;
        b=XiW1QhW8AO8o9/Qu5+odXkEGXAxv7cLvLKF7xGvDQY84BqDqVyen/TEpIBZZjxuLy+
         yVsxSK4SL7AxFOppN20kozIDSsr5fQXRrJuEflsMjBnZsknYH3fhVXGWkJMWNZSZb3cf
         9JX2evvWJsf86iLUudpi3B9GM6l1HKJxObDvRX1XESWAAmW0BcIoRsrfGPWUI+/u0zm7
         sXYEMGTELSLp+G7F76IIyNwdAkL3A0BNDZbuYehIpFNiS/h7bN+vxMiRFGfshS0PqQeA
         yR/4NCzKQulwFOKhlV0sDo7athxIXjUcXT8UyP8y71/nhxLsL4lX+iBYgiNBvRmHeCHq
         6xfg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=GnwLDzHL;
       spf=pass (google.com: domain of jason...@gmail.com designates 209.85.220.174 as permitted sender) smtp.mailfrom=jason...@gmail.com
Return-Path: <jason...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id k6-v6si1624289pla.333.2018.02.16.10.56.36
        for <singu...@lbl.gov>;
        Fri, 16 Feb 2018 10:56:36 -0800 (PST)
Received-SPF: pass (google.com: domain of jason...@gmail.com designates 209.85.220.174 as permitted sender) client-ip=209.85.220.174;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=GnwLDzHL;
       spf=pass (google.com: domain of jason...@gmail.com designates 209.85.220.174 as permitted sender) smtp.mailfrom=jason...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HtAAD2KIdahq7cVdFcGgEBAQEBAgEBA?=
 =?us-ascii?q?QEIAQEBAYMigRNwKAqDQweKJY4FggKBF4Jqk18UgT9DIgeFHAKCPwdUGAECAQE?=
 =?us-ascii?q?BAQEBAgECEAEBAQgLCwgoL4I4IhFLKi8BAQEBAQEBAQEBHwITGCUCGAEBAQQjH?=
 =?us-ascii?q?QENDgkVAwwGBQsGBAEBAQICJgICIQEBDgMBBQEUCA4HBAEHFQSILIE7AQMVBQu?=
 =?us-ascii?q?gVkCMF4IFBQEcgwwFg2YKGScNWVmCEwEBAQEGAQEBAQEBARkCBhJ9g3iCKIFXg?=
 =?us-ascii?q?WiDLoJsOYFLARIBH4MXgmUFk2iQGDUJgk2OMoULlEiOToYZgyQZIIEXH2wucTM?=
 =?us-ascii?q?aI08xghIJgjwfgjEiNwEPi1iCPgEBAQ?=
X-IronPort-AV: E=Sophos;i="5.46,520,1511856000"; 
   d="scan'208";a="106269590"
Received: from mail-qk0-f174.google.com ([209.85.220.174])
  by fe3.lbl.gov with ESMTP; 16 Feb 2018 10:56:33 -0800
Received: by mail-qk0-f174.google.com with SMTP id c4so5047612qkm.2
        for <singu...@lbl.gov>; Fri, 16 Feb 2018 10:56:33 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-transfer-encoding;
        bh=3hKPep80q0pVsL4Svv4cUbMLWhliOlKBsSgf4FHV9NM=;
        b=GnwLDzHL2qr9uaapjNmFBXK1aA+oVbLjEocM60y3S7sfsNisCZFUbu6E8Mt7OHWDOU
         AmvDWeiQPn0zjiXoy8jUzcQAm1FRONusWvJri5/k44h+pddYa4cFbM04MMJCyYIk6b4n
         tMf0t5727xCbtEAc3Q8rlQO1LCCNZFSIMpLqnlYR8Fntj2Er3rSttTMD92vSBv15iZPX
         AE1YY0BxhcDMguJyHAQBe713+RWdD6Jxnua3Pu143WFzBFYv0LHwk2fRCoYoh568/8nX
         VPBgq8csyvQRJnT+AgOECmrSsXQ7gtFHaYDTl0toaXgevGQ4BLn+ih8LCcFPhLNQPIFE
         312Q==
X-Gm-Message-State: APf1xPCCEjxjQ9ek00in3hek37ydYoJPkOs7+xz54CLn3UtIzB/ZlPw8
	GN9mRrgZ1Nh5U+fm1E1jGIFLZmh9GvnaTdh4A1YgD+Rt
X-Received: by 10.55.93.66 with SMTP id r63mr11224009qkb.163.1518807392944;
 Fri, 16 Feb 2018 10:56:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.200.3.88 with HTTP; Fri, 16 Feb 2018 10:56:32 -0800 (PST)
In-Reply-To: <CAHqiYpeD7bTsY7pbo=Lhu3rRqD0_U4iSkt3NKDfbPau8rZ2XAw@mail.gmail.com>
References: <CAHqiYpfh5BO7h8MZM09+7wmWpz5jKQtzE7wAPMBf9Qbnk=ANoQ@mail.gmail.com>
 <CA+Wz_Fwcs-zqn5=G_8A2jAa4P7wGb=vvqSp6xCojYWFOdmSQhg@mail.gmail.com>
 <CAHqiYpc4mrffsA3kQ+q=1jttq-OKuHDPcZtAv+02mUqUNYbSbg@mail.gmail.com>
 <B58197C146EC324AA00A2A07DC082602B15A6C21@XMAIL-MBX-BT1.AD.UCSD.EDU> <CAHqiYpeD7bTsY7pbo=Lhu3rRqD0_U4iSkt3NKDfbPau8rZ2XAw@mail.gmail.com>
From: Jason Stover <jason...@gmail.com>
Date: Fri, 16 Feb 2018 12:56:32 -0600
Message-ID: <CAGfAqt9i9oebv+SXphmVAJ6_tLCosZ9BrvKQd1HZ1bqgmiwspQ@mail.gmail.com>
Subject: Re: [Singularity] singularity container on HPC
To: singularity@lbl.gov
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Haseeb,

  I think what you will want to do is when you build the image have:

    %runscript
    /path_to namd_executable_in_container/namd2 ${@}

  Then change your mpi line to being:

    mpirun -np #P /path/to/namdimage.simg inputfile

  So you will be executing the image itself.

-J


On Fri, Feb 16, 2018 at 11:30 AM, Haseeb Mahmud <smahm...@gmail.com> wrote:
> Hello,
>
>  I am able to build and run  MPI version of NAMD fine on my cluster outsi=
de
> a container. I just would like to run it within a container as well just =
as
> a test with singularity. Is there any other dependencies.libraries  I sho=
uld
> be adding to my container other than Open MPI 2.1.0.  My HPC'  network is
> Ethernet (TCP), and not infiniband.
>
> Regards,
>
> Haseeb
>
> On Thu, Feb 15, 2018 at 10:31 PM, Kandes, Martin <mka...@sdsc.edu> wrote:
>>
>> Hi Haseed,
>>
>> I guess my first question would be: why do you need to build NAMD within=
 a
>> Singularity container? I've built it on a few systems before with differ=
ent
>> build options. It usually plays nicely with most systems. If you're doin=
g an
>> 'mpi' comm  build, here is a build script I've used before [1]. Maybe it=
'll
>> help.
>>
>> Marty
>>
>> [1]
>>
>> #!/bin/bash
>> #
>> # A build script for NAMD
>>
>> declare -r COMPILER_MODULE=3D'gnu/4.9.2'
>> declare -r MPI_MODULE=3D'mvapich2_ib/2.1'
>>
>> declare -r OS=3D'linux'
>> declare -r ARCH=3D'x86_64'
>>
>> declare -r NAMD_NAME=3D'NAMD'
>> declare -r NAMD_VERSION=3D'2.12'
>> declare -r NAMD_TARBALL=3D"${NAMD_NAME}_${NAMD_VERSION}_Source.tar.gz"
>> declare -r NAMD_DIR=3D"${NAMD_NAME}_${NAMD_VERSION}_Source"
>> declare -r NAMD_URL=3D'http://www.ks.uiuc.edu/Research/namd'
>> declare -r NAMD_COMPILER=3D'g++'
>> declare -r NAMD_ARCH=3D"Linux-${ARCH}-${NAMD_COMPILER}"
>>
>> declare -r CHARM_NAME=3D'charm'
>> declare -r CHARM_VERSION=3D'6.7.1'
>> declare -r CHARM_TARFILE=3D"${CHARM_NAME}-${CHARM_VERSION}.tar"
>> declare -r CHARM_DIR=3D"${CHARM_NAME}-${CHARM_VERSION}"
>> declare -r CHARM_COMM=3D'mpi'
>> declare -r CHARM_ARCH=3D"${CHARM_COMM}-${OS}-${ARCH}"
>> declare -r CHARM_OPTIONS=3D'mpicxx'
>>
>> declare -r FFTW_NAME=3D'fftw'
>> declare -r FFTW_TARBALL=3D"${FFTW_NAME}-${OS}-${ARCH}.tar.gz"
>>
>> declare -r TCL_NAME=3D'tcl'
>> declare -r TCL_VERSION=3D'8.5.9'
>> declare -r TCL_TARBALL=3D"${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}.tar.gz=
"
>> declare -r
>> TCL_THREADED_TARBALL=3D"${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}-threaded=
.tar.gz"
>>
>> module purge
>> module load "${COMPILER_MODULE}"
>> module load "${MPI_MODULE}"
>>
>> tar -xzvf "${PWD}/tarballs/${NAMD_TARBALL}"
>> cd "${NAMD_DIR}"
>>
>> wget "${NAMD_URL}/libraries/${FFTW_TARBALL}"
>> wget "${NAMD_URL}/libraries/${TCL_TARBALL}"
>> wget "${NAMD_URL}/libraries/${TCL_THREADED_TARBALL}"
>>
>> tar -xvf "${CHARM_TARFILE}"
>> tar -xzvf "${FFTW_TARBALL}"
>> tar -xzvf "${TCL_TARBALL}"
>> tar -xzvf "${TCL_THREADED_TARBALL}"
>>
>> mv "${OS}-${ARCH}" fftw
>> mv "${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}" tcl
>> mv "${TCL_NAME}${TCL_VERSION}-${OS}-${ARCH}-threaded" tcl-threaded
>>
>> cd "${CHARM_DIR}"
>> ./build charm++ "${CHARM_ARCH}" "${CHARM_OPTIONS}" --with-production
>>
>> cd ../
>> ./config "${NAMD_ARCH}" --charm-arch "${CHARM_ARCH}-${CHARM_OPTIONS}"
>> cd "${NAMD_ARCH}"
>> make
>>
>>
>>
>> ________________________________
>> From: Haseeb Mahmud [smahm...@gmail.com]
>> Sent: Thursday, February 15, 2018 6:48 PM
>> To: singu...@lbl.gov
>> Cc: vict...@gmail.com
>> Subject: Re: [Singularity] singularity container on HPC
>>
>> Hello,
>>
>> Since my email I rebuilt my container with NAMD linked against OpenMPI
>> version 2.10. I then ran my container on my host which also uses OpenMPI
>> version 2.10.. And I get new errors. Looks like now now it is using both
>> nodes but I get the below errors.
>>
>>
>> [compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received:
>> 255
>> [compute-0.cluster:64549] unknown address family for tcp: 0
>> [compute-0.cluster:64549] mca_btl_tcp_proc: unknown af_family received:
>> 255
>> [compute-0.cluster:64549] unknown address family for tcp: 0
>> *** Error in `/NAMD_2.12_Source/Linux-x86_64-g++/namd2': munmap_chunk():
>> invalid pointer: 0x00000000098c5cb0 ***
>> *** Error in `/NAMD_2.12_Source/Linux-x86_64-g++/namd2': corrupted
>> double-linked list: 0x0000000008b4b540 ***
>>
>> Charm++> Running on MPI version: 3.1
>> Charm++> level of thread support used: MPI_THREAD_SINGLE (desired:
>> MPI_THREAD_SINGLE)
>> Charm++> Running in non-SMP mode: numPes 88
>> Charm++> Using recursive bisection (scheme 3) for topology aware
>> partitions
>> Converse/Charm++ Commit ID:
>> v6.7.1-0-gbdf6a1b-namd-charm-6.7.1-build-2016-Nov-07-136676
>> Warning> Randomization of stack pointer is turned on in kernel, thread
>> migration may not work! Run 'echo 0 > /proc/sys/kernel/randomize_va_spac=
e'
>> as root to disable it, or try run with '+isomalloc_sync'.
>> CharmLB> Load balancer assumes all CPUs are same.
>> Charm++> Running on 2 unique compute nodes (88-way SMP).
>> Charm++> cpu topology info is gathered in 0.019 seconds.
>>
>> Info: Startup phase 9 took 0.00478208 s, 1237.92 MB of memory in use
>> Info: CREATING 44375 COMPUTE OBJECTS
>> Info: Startup phase 10 took 0.0075435 s, 1237.92 MB of memory in use
>> Info: Startup phase 11 took 0.000689846 s, 1237.92 MB of memory in use
>> Info: Startup phase 12 took 4.81852e-05 s, 1237.92 MB of memory in use
>> Info: Finished startup at 13.4259 s, 1237.92 MB of memory in use
>>
>> Info: useSync: 0 useProxySync: 0
>> [compute-0:64562] *** Process received signal ***
>> [compute-0:64562] Signal: Segmentation fault (11)
>> [compute-0:64562] Signal code: Address not mapped (1)
>> [compute-0:64562] Failing at address: 0x7
>> [compute-0:64699] *** Process received signal ***
>> [compute-0:64699] Signal: Segmentation fault (11)
>> [compute-0:64699] Signal code: Address not mapped (1)
>> [compute-0:64699] Failing at address: 0xffffffffffffffff
>> [compute-0:64737] *** Process received signal ***
>> [compute-0:64737] Signal: Segmentation fault (11)
>>
>>
>>
>> Haseeb
>>
>>
>>
>> On Thu, Feb 15, 2018 at 10:07 AM, victor sv <vict...@gmail.com> wrote:
>>>
>>> HI Haseeb,
>>>
>>> first of all I would like to understand with MPI family and version is
>>> running in and out the containers.
>>>
>>> NAMD is linked agains OpenMPI or MPICH?
>>>
>>> Which MPI family and version is running in the host? it should be enoug=
h
>>> if you show the output of "mpirun --version".
>>>
>>> BR,
>>> V=C3=ADctor.
>>>
>>> 2018-02-15 14:33 GMT+01:00 Haseeb Mahmud <smahm...@gmail.com>:
>>>>
>>>> Hello,
>>>>
>>>> I have built a namd container that uses the MPI build of NAMD 2.12 tha=
t
>>>> I built from source. I am trying to execute  this container  on my HPC=
 using
>>>> mpirun on my host slurm script using multiple nodes , however, althoug=
h the
>>>> job runs , it looks to be only using 1 processer and 1 node.
>>>>
>>>> I built my container from a script  with an Ubuntu operating syytem an=
d
>>>> In my %post section, I first configure and make openmpi-2.1.0  then i
>>>> install mpich using "apt install mpich"  because my MPI build of NAMD =
wont
>>>> compile without it.  Then in my %post section I also build the namd 2.=
12 MPI
>>>> build itself as all the NAMD source files are in my container as well.
>>>>
>>>> In my slurm script when I run " mpirun -np #P singularity exec
>>>> namdimage.simg /path_to namd_executable_in_container/namd2 inputfile" =
,I get
>>>> the problem  of no scaling.
>>>>
>>>>
>>>>
>>>>
>>>> My sample out looks like :
>>>> Charm++> Running on MPI version: 3.0
>>>> Charm++> level of thread support used: MPI_THREAD_SINGLE (desired:
>>>> MPI_THREAD_SINGLE)
>>>> Charm++> Running in non-SMP mode: numPes 1
>>>> Charm++> Using recursive bisection (scheme 3) for topology aware
>>>> partitions
>>>> ..........
>>>>
>>>> Info: Running on 1 processors, 1 nodes, 1 physical nodes.
>>>>
>>>>
>>>> Any ideas what I may be doing wrong.
>>>>
>>>> Thanks,
>>>>
>>>> Haseeb
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google Grou=
ps
>>> "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send =
an
>>> email to singu...@lbl.gov.
>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
