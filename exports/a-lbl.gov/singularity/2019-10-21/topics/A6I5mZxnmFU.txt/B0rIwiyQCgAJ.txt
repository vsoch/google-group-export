X-Received: by 2002:a62:824c:: with SMTP id w73-v6mr5694228pfd.33.1526916966167;
        Mon, 21 May 2018 08:36:06 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:7849:: with SMTP id e9-v6ls5444827pln.11.gmail; Mon,
 21 May 2018 08:36:05 -0700 (PDT)
X-Received: by 2002:a17:902:3303:: with SMTP id a3-v6mr20822493plc.209.1526916964824;
        Mon, 21 May 2018 08:36:04 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1526916964; cv=none;
        d=google.com; s=arc-20160816;
        b=DX4j8zNpby3Zvxw+51gQi8HF0EVUSBe1k/8pEQanAidYoacaVAcef5dIs1rph9OXWZ
         ojB69zWK7iKwmsKu33DRtr8qB8Wf418jyJVio8oX+qrCRGTfha6/c8KaERrPb4QNlg2q
         XLLu4FjjBN+vpngktDtpwjgTkEnvZAtYbxEvU/mQDb7WDmAYw9p3Gq3QyalHjuaDDpXV
         nA4A36ehpVx8xhUpUfmoG3T69tnNHzJbOd0UODSDSRV+U7saztYZwHV58OvECrPWfbjw
         VTGz/ztIM7+YWmDr5jvlYUWwwhQ/CSm2MjICgqlRsmKJvJyq5asagTRN/oStTg303c6q
         gs5Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature:arc-authentication-results;
        bh=uwtp0EkvILz1qKHph6iO/N55u4rv4UsyqKV6289jJ7w=;
        b=m0/r+XNuXKDH1jMizLGtXkpLAo0e893NlLyLFLjksvwxFc9PGMZG5EJu5c76MRTxme
         u2NzUpj2FXJ3sPVVn1wGTzCfhums1gC6EmzYTmF0pLvx1fIuf0GTQEbB4MhGH1rjFoLx
         H6rZNTiIrnDOsEg75KUI6ecwOD8j4xfHxEuqhKQ3G4+tmSvvEk1/2RQU8e8nEvAN+o5b
         l0zBHsZV4O3Qoj5VIHGdjbMVGGpxdC+oN2JEUPT9j1l0YhYL0HiW4cxQLuB5xyfzGb8v
         VIc+ym+ZeAnVRCvxU0xuQBU4D5uwiKDCwIOU7ygGADhF134VDGqDvW0jpMGyjnZLYyZI
         4rxA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Kpsai6KV;
       spf=pass (google.com: domain of georg...@gmail.com designates 209.85.220.177 as permitted sender) smtp.mailfrom=georg...@gmail.com
Return-Path: <georg...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id y5-v6si13670640pfe.134.2018.05.21.08.36.04
        for <singu...@lbl.gov>;
        Mon, 21 May 2018 08:36:04 -0700 (PDT)
Received-SPF: pass (google.com: domain of georg...@gmail.com designates 209.85.220.177 as permitted sender) client-ip=209.85.220.177;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Kpsai6KV;
       spf=pass (google.com: domain of georg...@gmail.com designates 209.85.220.177 as permitted sender) smtp.mailfrom=georg...@gmail.com
X-Ironport-SBRS: 3.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2EvAgBZ5gJbh7HcVdFcGgEBAQEBAgEBA?=
 =?us-ascii?q?QEIAQEBAYMYgQx9KINvBoEdglCRCoF5gQ+TSoEOAxgXIQMjAQwJgUmCL0YCgho?=
 =?us-ascii?q?hNxUBAgEBAQEBAQIBAQIQAQEBCA0JCCgjDII1BQIDAh4FBEsrATABAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEBAQEUAgwBIEMBAQEDARoBCB0BDQ4EGgMBCwYDAgsGAwEBAQE?=
 =?us-ascii?q?VCwEJAgIhAQEOAwEFAQsJCA4HBAEaAgSDAQIogT4BAw0IBQqLZJACPIsFgX8FA?=
 =?us-ascii?q?ReCcAWDSgoZJg1UV4IHAgEFEogjgVQ/gQ+CXi6CT0IBAQKBGQoIARIBPw0SgkG?=
 =?us-ascii?q?CVAKHYIk0hnUXLAmFaoVugn+BN4Nth1mJX0qEB4I4MIEEMlYucTMaI1AxghKCF?=
 =?us-ascii?q?BqDTopuIzABD41tR4FwAQE?=
X-IronPort-AV: E=Sophos;i="5.49,426,1520924400"; 
   d="scan'208,217";a="116298178"
Received: from mail-qk0-f177.google.com ([209.85.220.177])
  by fe3.lbl.gov with ESMTP; 21 May 2018 08:36:01 -0700
Received: by mail-qk0-f177.google.com with SMTP id s70-v6so12092270qks.13
        for <singu...@lbl.gov>; Mon, 21 May 2018 08:36:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=uwtp0EkvILz1qKHph6iO/N55u4rv4UsyqKV6289jJ7w=;
        b=Kpsai6KVg3plEoLwUlM9ssltEKfIouVcndUaS9EU3Jkued2VbYFyXfh48ZDXOqQy4o
         oCCv9kJFrnSNDfJD4wrOSymwHkau7x6KvwdgN5c3YIomA54OwKXb6ARyzjvNkv+T37Ar
         v+X9l8sTyWMuVnXyRhQYfATGLGEpJYdZAz9Dq35o8hkAcc7L+ck1hhDya2Xlqe8hagx9
         QXtLl8BWRMvOD+2/P17lTRRmSCktKkGfvIS4iVkZmIk+9xs33/GL0iMsISGmHxS6V9IV
         CA150qlGB3UH9ekqLQcbF9DWZvqn9jbHQpC8J2k0Ma7p0s7Gij88m5Olfga2pETil1Xb
         NuBw==
X-Gm-Message-State: ALKqPwdIBh4EP/KQZD9sYGPChcILPhti8qhSOavi9IdyL/dpwrNlb0bH
	5kP/LhY3s0wI5O+TmSXVrxm6VpghJgG10tLRhBs=
X-Received: by 2002:a37:d496:: with SMTP id s22-v6mr12036630qks.79.1526916960711;
 Mon, 21 May 2018 08:36:00 -0700 (PDT)
MIME-Version: 1.0
References: <167671ff-657d-4560-a35f-87091223fe29@lbl.gov> <B58197C146EC324AA00A2A07DC082602C2CBA0C3@XMAIL-MBX-BT1.AD.UCSD.EDU>
 <CAB6eJZ+6PDjs3POSQPNoLdaMys5d7iYDYcUeQFpMyzJ6DEP31w@mail.gmail.com>
 <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com>
 <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com>
 <CAB6eJZL+F7DwMcebA6DC+QWHqqsnnqEfJ2uqR2D-6bas6DmnYw@mail.gmail.com> <CA+Wz_FznUJ9kWSy7qneZsVEYqemqnm6eq0ASo+AoD49VNqU=zw@mail.gmail.com>
In-Reply-To: <CA+Wz_FznUJ9kWSy7qneZsVEYqemqnm6eq0ASo+AoD49VNqU=zw@mail.gmail.com>
From: George Zaki <georg...@gmail.com>
Date: Mon, 21 May 2018 11:35:48 -0400
Message-ID: <CAB6eJZLupe=P+5awRJjV0=V4uNWuPcwTevLELZpMsRR5uJVStA@mail.gmail.com>
Subject: Re: [Singularity] Running an mpi program with mvapitch
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="0000000000000b777b056cb90f95"

--0000000000000b777b056cb90f95
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Thanks Victor,

Here is the results with the path of the executable:

Singularity
swift-hypervisor-horovod-mvapich.simg:~/mpi-examples/mpi-example> mpiexec
-n 1 ./mpi-pi.o

[mpiexec@cn3112] HYDU_create_process (utils/launch/launch.c:75): execvp
error on file srun (No such file or directory)



On Mon, May 21, 2018 at 9:18 AM victor sv <vict...@gmail.com> wrote:

> Hi George,
>
> please check that you are calling the right program. Is the executable
> path in the PATH environment variable? If not you have the prepend the pa=
th
> to call the executable.
>
> Take a look to this:
>
>
> https://stackoverflow.com/questions/47472153/mpi-mpirun-execvp-error-no-s=
uch-file-or-directory
>
> BR,
> V=C3=ADctor.
>
> 2018-05-21 14:58 GMT+02:00 George Zaki <georg...@gmail.com>:
>
>> Hi Jason,
>>
>> I was not able to run the program even without singularity when I use
>> mvapitch. I am in contact with our system admin
>>
>> Here is the  output I got when I run mpiexec within singularity:
>>
>> mpiexec -n 1 mpi-pi.o
>>
>> [mpiexec@cn3137] HYDU_create_process (utils/launch/launch.c:75): execvp
>> error on file srun (No such file or directory)
>>
>> Best regards,
>>
>> George.
>>
>> On Mon, May 21, 2018 at 3:04 AM victor sv <vict...@gmail.com> wrote:
>>
>>> Hi George,
>>>
>>> not any experience with mvapitch. I think the compiler version has no
>>> effect here.
>>>
>>> Jason solution and is a good starting point to check if the container
>>> MPI works in a single node (not in several nodes).
>>>
>>> To run the hybrid MPI approach you should take into account that both
>>> version and vendor of MPI and PMI must match. Can you check if PMI
>>> libraries match?
>>>
>>> BR,
>>> V=C3=ADctor.
>>>
>>> 2018-05-18 20:14 GMT+02:00 Jason Stover <jason...@gmail.com>:
>>>
>>>> Hi George,
>>>>
>>>>   Can you run it from inside the container? For example:
>>>>
>>>>     singularity exec
>>>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1
>>>> mpi-pi.o
>>>>
>>>> -J
>>>>
>>>>
>>>> On Fri, May 18, 2018 at 12:56 PM, George Zaki <georg...@gmail.com>
>>>> wrote:
>>>> > Thanks Marty
>>>> >
>>>> > Below are the values I got, any obvious mismatch? I git this working
>>>> fine
>>>> > with OpenMPI.
>>>> >
>>>> > Here is also what I try to run:
>>>> >
>>>> > singularity exec
>>>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg
>>>> > mpicc mpi-pi.c -o  mpi-pi.o
>>>> >
>>>> > [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec
>>>> > /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o
>>>> >
>>>> > Then I kill after no response:
>>>> >
>>>> > ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested
>>>> >
>>>> > [mpiexec@cn2360] Press Ctrl-C again to force abort
>>>> >
>>>> > [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write
>>>> error (Bad
>>>> > file descriptor)
>>>> >
>>>> > [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal
>>>> (pm/pmiserv/pmiserv_cb.c:169):
>>>> > unable to write data to proxy
>>>> >
>>>> > [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to
>>>> send
>>>> > signal downstream
>>>> >
>>>> > [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event
>>>> > (tools/demux/demux_poll.c:76): callback returned error status
>>>> >
>>>> > [mpiexec@cn2360] HYD_pmci_wait_for_completion
>>>> > (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event
>>>> >
>>>> > [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager erro=
r
>>>> > waiting for completion
>>>> >
>>>> >
>>>> > Now about the versions:
>>>> >
>>>> > Singularity: Invoking an interactive shell within container...
>>>> >
>>>> >
>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> mpiexec --versi=
on
>>>> >
>>>> > HYDRA build details:
>>>> >
>>>> >     Version:                                 3.1.4
>>>> >
>>>> >     Release Date:                            Wed Sep  7 14:33:43 EDT
>>>> 2016
>>>> >
>>>> >     CC:                              gcc
>>>> >
>>>> >     CXX:                             g++
>>>> >
>>>> >     F77:                             gfortran
>>>> >
>>>> >     F90:                             gfortran
>>>> >
>>>> >     Configure options:
>>>>  '--disable-option-checking'
>>>> > '--prefix=3DNONE' '--cache-file=3D/dev/null' '--srcdir=3D.' 'CC=3Dgc=
c'
>>>> 'CFLAGS=3D
>>>> > -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib
>>>> -Wl,-rpath,/lib
>>>> > -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad -libumad
>>>> -libverbs -ldl
>>>> > -lrt -lm -lpthread ' 'CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/includ=
e
>>>> > -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/s=
rc
>>>> > -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>>> > -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include
>>>> -I/include
>>>> > -I/include'
>>>> >
>>>> >     Process Manager:                         pmi
>>>> >
>>>> >     Launchers available:                     ssh rsh fork slurm ll
>>>> lsf sge
>>>> > manual persist
>>>> >
>>>> >     Topology libraries available:            hwloc
>>>> >
>>>> >     Resource management kernels available:   user slurm ll lsf sge p=
bs
>>>> > cobalt
>>>> >
>>>> >     Checkpointing libraries available:
>>>> >
>>>> >     Demux engines available:                 poll select
>>>> >
>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> gcc --version
>>>> >
>>>> > gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>>> >
>>>> > Copyright (C) 2015 Free Software Foundation, Inc.
>>>> >
>>>> > This is free software; see the source for copying conditions.  There
>>>> is NO
>>>> >
>>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>> PURPOSE.
>>>> >
>>>> >
>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> exit
>>>> >
>>>> > exit
>>>> >
>>>> >
>>>> > [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0
>>>> >
>>>> > [+] Loading mvapich2 2.2 for GCC 5.3.0
>>>> >
>>>> > [zakigf@cn2360 ~]$ gcc --version
>>>> >
>>>> > gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
>>>> >
>>>> > Copyright (C) 2010 Free Software Foundation, Inc.
>>>> >
>>>> > This is free software; see the source for copying conditions.  There
>>>> is NO
>>>> >
>>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>> PURPOSE.
>>>> >
>>>> >
>>>> > [zakigf@cn2360 ~]$ mpiexec --version
>>>> >
>>>> > HYDRA build details:
>>>> >
>>>> >     Version:                                 3.1.4
>>>> >
>>>> >     Release Date:                            Wed Sep  7 14:33:43 EDT
>>>> 2016
>>>> >
>>>> >     CC:                              /usr/local/GCC/5.3.0/bin/gcc
>>>> >
>>>> >     CXX:                             /usr/local/GCC/5.3.0/bin/g++
>>>> >
>>>> >     F77:                             /usr/local/GCC/5.3.0/bin/gfortr=
an
>>>> >
>>>> >     F90:                             /usr/local/GCC/5.3.0/bin/gfortr=
an
>>>> >
>>>> >     Configure options:
>>>>  '--disable-option-checking'
>>>> > '--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0'
>>>> > '--with-slurm-lib=3D/usr/local/slurm/lib'
>>>> > '--with-slurm-include=3D/usr/local/slurm/include' '--enable-debug=3D=
none'
>>>> > '--enable-timing=3Druntime' 'CC=3D/usr/local/GCC/5.3.0/bin/gcc'
>>>> > 'CXX=3D/usr/local/GCC/5.3.0/bin/g++'
>>>> 'FC=3D/usr/local/GCC/5.3.0/bin/gfortran'
>>>> > 'F77=3D/usr/local/GCC/5.3.0/bin/gfortran' '--cache-file=3D/dev/null'
>>>> > '--srcdir=3D.' 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/li=
b -L/lib
>>>> > -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
>>>> 'LIBS=3D-libmad
>>>> > -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src
>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include
>>>> -I/include
>>>> > -I/include -I/include -I/include'
>>>> >
>>>> >     Process Manager:                         pmi
>>>> >
>>>> >     Launchers available:                     ssh rsh fork slurm ll
>>>> lsf sge
>>>> > manual persist
>>>> >
>>>> >     Topology libraries available:            hwloc
>>>> >
>>>> >     Resource management kernels available:   user slurm ll lsf sge p=
bs
>>>> > cobalt
>>>> >
>>>> >     Checkpointing libraries available:
>>>> >
>>>> >     Demux engines available:                 poll select
>>>> >
>>>> >
>>>> > On Fri, May 18, 2018 at 1:31 PM Kandes, Martin <mka...@sdsc.edu>
>>>> wrote:
>>>> >>
>>>> >> Hi George,
>>>> >>
>>>> >> I run with different gcc compiler versions inside and outside my MP=
I
>>>> >> containers. So I would be surprised if that is the issue here. I'm
>>>> not sure
>>>> >> I have a good recommendation of where to start debugging your
>>>> problem. But I
>>>> >> might start by double checking the MPI versions match inside and
>>>> outside the
>>>> >> container. e.g. see [1].
>>>> >>
>>>> >> Marty
>>>> >>
>>>> >> [1]
>>>> >>
>>>> >> [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1
>>>> >> --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/b=
ash
>>>> >> srun: job 16364303 queued and waiting for resources
>>>> >> srun: job 16364303 has been allocated resources
>>>> >> [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/
>>>> >> [mkandes@comet-14-06 16364303]$ cp
>>>> >>
>>>> /oasis/scratch/comet/mkandes/temp_project/singularity/images/ubuntu-mv=
apich2.img
>>>> >> ./
>>>> >> [mkandes@comet-14-06 16364303]$ module purge
>>>> >> [mkandes@comet-14-06 16364303]$ module load gnu
>>>> >> [mkandes@comet-14-06 16364303]$ module load mvapich2_ib
>>>> >> [mkandes@comet-14-06 16364303]$ module list
>>>> >> Currently Loaded Modulefiles:
>>>> >>   1) gnu/4.9.2         2) mvapich2_ib/2.1
>>>> >> [mkandes@comet-14-06 16364303]$ module load singularity
>>>> >> [mkandes@comet-14-06 16364303]$ gcc --version
>>>> >> gcc (GCC) 4.9.2
>>>> >> Copyright (C) 2014 Free Software Foundation, Inc.
>>>> >> This is free software; see the source for copying conditions.  Ther=
e
>>>> is NO
>>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>> >> PURPOSE.
>>>> >>
>>>> >> [mkandes@comet-14-06 16364303]$ mpirun --version
>>>> >> HYDRA build details:
>>>> >>     Version:                                 3.1.4
>>>> >>     Release Date:                            Thu Apr  2 17:15:15 ED=
T
>>>> 2015
>>>> >>     CC:                              gcc  -fPIC -O3
>>>> >>     CXX:                             g++  -fPIC -O3
>>>> >>     F77:                             gfortran -fPIC -O3
>>>> >>     F90:                             gfortran -fPIC -O3
>>>> >>     Configure options:
>>>>  '--disable-option-checking'
>>>> >> '--prefix=3D/opt/mvapich2/gnu/ib' '--enable-shared'
>>>> '--enable-sharedlibs=3Dgcc'
>>>> >> '--with-hwloc' '--enable-f77' '--enable-fc' '--enable-hybrid'
>>>> >> '--with-ib-include=3D/usr/include/infiniband'
>>>> '--with-ib-libpath=3D/usr/lib64'
>>>> >> '--enable-fast=3DO3'
>>>> >>
>>>> '--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gn=
u_ib-2.1/../..//cache/build-limic'
>>>> >> '--with-slurm=3D/usr/lib64/slurm' '--with-file-system=3Dlustre' 'CC=
=3Dgcc'
>>>> >> 'CFLAGS=3D-fPIC -O3 -O3' 'CXX=3Dg++' 'CXXFLAGS=3D-fPIC -O3 -O3'
>>>> 'FC=3Dgfortran'
>>>> >> 'FCFLAGS=3D-fPIC -O3 -O3' 'F77=3Dgfortran' 'FFLAGS=3D-L/usr/lib64 -=
L/lib
>>>> -L/lib
>>>> >> -fPIC -O3 -O3' '--cache-file=3D/dev/null' '--srcdir=3D.'
>>>> 'LDFLAGS=3D-L/usr/lib64
>>>> >> -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib
>>>> -L/usr/lib64
>>>> >>
>>>> -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/../../=
/cache/build-limic/lib
>>>> >> -L/lib -L/lib' 'LIBS=3D-libmad -lrdmacm -libumad -libverbs -ldl -lr=
t
>>>> -llimic2
>>>> >> -lm -lpthread ' 'CPPFLAGS=3D-I/usr/include/infiniband
>>>> >>
>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapic=
h2-2.1/src/mpl/include
>>>> >>
>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapic=
h2-2.1/src/mpl/include
>>>> >>
>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapic=
h2-2.1/src/openpa/src
>>>> >>
>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapic=
h2-2.1/src/openpa/src
>>>> >> -D_REENTRANT
>>>> >>
>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvapic=
h2-2.1/src/mpi/romio/include
>>>> >> -I/include -I/include -I/usr/include/infiniband
>>>> >>
>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/../../=
/cache/build-limic/include
>>>> >> -I/include -I/include'
>>>> >>     Process Manager:                         pmi
>>>> >>     Launchers available:                     ssh rsh fork slurm ll
>>>> lsf sge
>>>> >> manual persist
>>>> >>     Topology libraries available:            hwloc
>>>> >>     Resource management kernels available:   user slurm ll lsf sge
>>>> pbs
>>>> >> cobalt
>>>> >>     Checkpointing libraries available:
>>>> >>     Demux engines available:                 poll select
>>>> >> [mkandes@comet-14-06 16364303]$ singularity shell
>>>> ubuntu-mvapich2.img
>>>> >> Singularity: Invoking an interactive shell within container...
>>>> >>
>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> gcc
>>>> --version
>>>> >> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>>> >> Copyright (C) 2015 Free Software Foundation, Inc.
>>>> >> This is free software; see the source for copying conditions.  Ther=
e
>>>> is NO
>>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>> >> PURPOSE.
>>>> >>
>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> mpirun
>>>> >> --version
>>>> >> HYDRA build details:
>>>> >>     Version:                                 3.1.4
>>>> >>     Release Date:                            Thu Apr  2 17:15:15 ED=
T
>>>> 2015
>>>> >>     CC:                              gcc
>>>> >>     CXX:                             g++
>>>> >>     F77:                             gfortran
>>>> >>     F90:                             gfortran
>>>> >>     Configure options:
>>>>  '--disable-option-checking'
>>>> >> '--prefix=3D/opt/mvapich2' '--cache-file=3D/dev/null' '--srcdir=3D.=
'
>>>> 'CC=3Dgcc'
>>>> >> 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/li=
b
>>>> >> -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libm=
ad
>>>> -lrdmacm
>>>> >> -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>>> >> -I/tmp/mvapich2-2.1/src/mpl/include
>>>> -I/tmp/mvapich2-2.1/src/mpl/include
>>>> >> -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/s=
rc
>>>> >> -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include
>>>> -I/include
>>>> >> -I/include -I/include'
>>>> >>     Process Manager:                         pmi
>>>> >>     Launchers available:                     ssh rsh fork slurm ll
>>>> lsf sge
>>>> >> manual persist
>>>> >>     Topology libraries available:            hwloc
>>>> >>     Resource management kernels available:   user slurm ll lsf sge
>>>> pbs
>>>> >> cobalt
>>>> >>     Checkpointing libraries available:
>>>> >>     Demux engines available:                 poll select
>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303>
>>>> >>
>>>> >>
>>>> >> ________________________________
>>>> >> From: George Zaki [georg...@gmail.com]
>>>> >> Sent: Friday, May 18, 2018 6:48 AM
>>>> >> To: singularity
>>>> >> Subject: [Singularity] Running an mpi program with mvapitch
>>>> >>
>>>> >> Hi singularity team,
>>>> >>
>>>> >>
>>>> >> I would like to run an MPI program in a singularity container. The
>>>> program
>>>> >> is compiled using mvapicth2.2 using a gcc version 5.4.
>>>> >>
>>>> >>
>>>> >> I can see that my cluster has a compiled version of mvapitch2.2 wit=
h
>>>> gcc
>>>> >> 5.3
>>>> >>
>>>> >>
>>>> >> When I run:
>>>> >>
>>>> >>
>>>> >> mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o
>>>> >>
>>>> >> the call does not return.
>>>> >>
>>>> >>
>>>> >>
>>>> >> Does the gcc version has to be exactly the same? I tried the switch
>>>> the
>>>> >> compiler in this image:
>>>> >>
>>>> >>
>>>> >> BootStrap: docker
>>>> >> From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04
>>>> >>
>>>> >>
>>>> >> However when gcc 5.3 is used the mvapitch does not build correctly.
>>>> >>
>>>> >>
>>>> >> If that's the problem, Is there a preferred method of switching gcc
>>>> >> version in this container singularity container?
>>>> >>
>>>> >>
>>>> >> Thanks,
>>>> >> George
>>>> >>
>>>> >> --
>>>> >> You received this message because you are subscribed to the Google
>>>> Groups
>>>> >> "singularity" group.
>>>> >> To unsubscribe from this group and stop receiving emails from it,
>>>> send an
>>>> >> email to singu...@lbl.gov.
>>>> >>
>>>> >> --
>>>> >> You received this message because you are subscribed to a topic in
>>>> the
>>>> >> Google Groups "singularity" group.
>>>> >> To unsubscribe from this topic, visit
>>>> >>
>>>> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/un=
subscribe
>>>> .
>>>> >> To unsubscribe from this group and all its topics, send an email to
>>>> >> singu...@lbl.gov.
>>>> >
>>>> > --
>>>> > You received this message because you are subscribed to the Google
>>>> Groups
>>>> > "singularity" group.
>>>> > To unsubscribe from this group and stop receiving emails from it,
>>>> send an
>>>> > email to singu...@lbl.gov.
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>> --
>>> You received this message because you are subscribed to a topic in the
>>> Google Groups "singularity" group.
>>> To unsubscribe from this topic, visit
>>> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/uns=
ubscribe
>>> .
>>> To unsubscribe from this group and all its topics, send an email to
>>> singu...@lbl.gov.
>>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to a topic in the
> Google Groups "singularity" group.
> To unsubscribe from this topic, visit
> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsub=
scribe
> .
> To unsubscribe from this group and all its topics, send an email to
> singu...@lbl.gov.
>

--0000000000000b777b056cb90f95
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Thanks Victor,=C2=A0<div><br></div><div>Here is the result=
s with the path of the executable:</div><div><br></div><div>





<p class=3D"inbox-inbox-p1"><span class=3D"inbox-inbox-s1">Singularity swif=
t-hypervisor-horovod-mvapich.simg:~/mpi-examples/mpi-example&gt; mpiexec -n=
 1 ./mpi-pi.o<span class=3D"inbox-inbox-Apple-converted-space">=C2=A0</span=
></span></p>
<p class=3D"inbox-inbox-p1"><span class=3D"inbox-inbox-s1">[mpiexec@cn3112]=
 HYDU_create_process (utils/launch/launch.c:75): execvp error on file srun =
(No such file or directory)</span></p><p class=3D"inbox-inbox-p1"><span cla=
ss=3D"inbox-inbox-s1"><br></span></p></div></div><br><div class=3D"gmail_qu=
ote"><div dir=3D"ltr">On Mon, May 21, 2018 at 9:18 AM victor sv &lt;<a href=
=3D"mailto:vict...@gmail.com">vict...@gmail.com</a>&gt; wrote:<br></div><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #=
ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi George,=C2=A0<div><br></div=
><div>please check that you are calling the right program. Is the executabl=
e path in the PATH environment variable? If not you have the prepend the pa=
th to call the executable.</div><div><br></div><div>Take a look to this:</d=
iv><div><br></div><div><a href=3D"https://stackoverflow.com/questions/47472=
153/mpi-mpirun-execvp-error-no-such-file-or-directory" target=3D"_blank">ht=
tps://stackoverflow.com/questions/47472153/mpi-mpirun-execvp-error-no-such-=
file-or-directory</a><br></div><div><br></div><div>BR,</div><div>V=C3=ADcto=
r.</div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">201=
8-05-21 14:58 GMT+02:00 George Zaki <span dir=3D"ltr">&lt;<a href=3D"mailto=
:georg...@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt;</span>:<b=
r><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:=
1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Jason,=C2=A0<div><br><=
/div><div>I was not able to run the program even without singularity when I=
 use mvapitch. I am in contact with our system admin</div><div><br></div><d=
iv>Here is the=C2=A0 output I got when I run mpiexec within singularity:=C2=
=A0</div><div><br></div><div>mpiexec -n 1 mpi-pi.o<span class=3D"m_75419581=
81832643763m_-6547978355576408365inbox-inbox-Apple-converted-space">=C2=A0<=
/span><br></div><div>
<p class=3D"m_7541958181832643763m_-6547978355576408365inbox-inbox-p1"><spa=
n class=3D"m_7541958181832643763m_-6547978355576408365inbox-inbox-s1">[mpie=
xec@cn3137] HYDU_create_process (utils/launch/launch.c:75): execvp error on=
 file srun (No such file or directory)</span></p><p class=3D"m_754195818183=
2643763m_-6547978355576408365inbox-inbox-p1">Best regards,</p><p class=3D"m=
_7541958181832643763m_-6547978355576408365inbox-inbox-p1">George.</p></div>=
</div><div class=3D"m_7541958181832643763HOEnZb"><div class=3D"m_7541958181=
832643763h5"><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Mon, May 21=
, 2018 at 3:04 AM victor sv &lt;<a href=3D"mailto:vict...@gmail.com" target=
=3D"_blank">vict...@gmail.com</a>&gt; wrote:<br></div><blockquote class=3D"=
gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-=
left:1ex"><div dir=3D"ltr"><div><div><div><div>Hi George,<br><br></div>not =
any experience with mvapitch. I think the compiler version has no effect he=
re. <br><br>Jason solution and is a good starting point to check if the con=
tainer MPI works in a single node (not in several nodes).<br><br></div>To r=
un the hybrid MPI approach you should take into account that both version a=
nd vendor of MPI and PMI must match. Can you check if PMI libraries match?<=
br><br></div>BR,<br></div>V=C3=ADctor.<br></div><div class=3D"gmail_extra">=
<br><div class=3D"gmail_quote">2018-05-18 20:14 GMT+02:00 Jason Stover <spa=
n dir=3D"ltr">&lt;<a href=3D"mailto:jason...@gmail.com" target=3D"_blank">j=
ason...@gmail.com</a>&gt;</span>:<br><blockquote class=3D"gmail_quote" styl=
e=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi Geor=
ge,<br>
<br>
=C2=A0 Can you run it from inside the container? For example:<br>
<br>
=C2=A0 =C2=A0 singularity exec<br>
/data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1<br>
mpi-pi.o<br>
<span class=3D"m_7541958181832643763m_-6547978355576408365m_-65914935373110=
87044HOEnZb"><font color=3D"#888888"><br>
-J<br>
</font></span><div class=3D"m_7541958181832643763m_-6547978355576408365m_-6=
591493537311087044HOEnZb"><div class=3D"m_7541958181832643763m_-65479783555=
76408365m_-6591493537311087044h5"><br>
<br>
On Fri, May 18, 2018 at 12:56 PM, George Zaki &lt;<a href=3D"mailto:georg..=
.@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt; wrote:<br>
&gt; Thanks Marty<br>
&gt;<br>
&gt; Below are the values I got, any obvious mismatch? I git this working f=
ine<br>
&gt; with OpenMPI.<br>
&gt;<br>
&gt; Here is also what I try to run:<br>
&gt;<br>
&gt; singularity exec /data/zakigf/candle/swift-hypervisor-horovod-mvapich.=
simg<br>
&gt; mpicc mpi-pi.c -o=C2=A0 mpi-pi.o<br>
&gt;<br>
&gt; [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec<br>
&gt; /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o<br>
&gt;<br>
&gt; Then I kill after no response:<br>
&gt;<br>
&gt; ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested<br>
&gt;<br>
&gt; [mpiexec@cn2360] Press Ctrl-C again to force abort<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error =
(Bad<br>
&gt; file descriptor)<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal (pm/pmiserv/pmiserv_cb.c=
:169):<br>
&gt; unable to write data to proxy<br>
&gt;<br>
&gt; [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to s=
end<br>
&gt; signal downstream<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event<br>
&gt; (tools/demux/demux_poll.c:76): callback returned error status<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmci_wait_for_completion<br>
&gt; (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event<br>
&gt;<br>
&gt; [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error<=
br>
&gt; waiting for completion<br>
&gt;<br>
&gt;<br>
&gt; Now about the versions:<br>
&gt;<br>
&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; mpiexec --vers=
ion<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3DNONE&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--sr=
cdir=3D.&#39; &#39;CC=3Dgcc&#39; &#39;CFLAGS=3D<br>
&gt; -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,=
-rpath,/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -libumad=
 -libverbs -ldl<br>
&gt; -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/=
include<br>
&gt; -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/src=
<br>
&gt; -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT<br>
&gt; -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include -I/inc=
lude<br>
&gt; -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; gcc --version<=
br>
&gt;<br>
&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;<br>
&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; exit<br>
&gt;<br>
&gt; exit<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0<br>
&gt;<br>
&gt; [+] Loading mvapich2 2.2 for GCC 5.3.0<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ gcc --version<br>
&gt;<br>
&gt; gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)<br>
&gt;<br>
&gt; Copyright (C) 2010 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ mpiexec --version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 /usr/local/GCC/5.3=
.0/bin/gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0&#39;<br>
&gt; &#39;--with-slurm-lib=3D/usr/local/slurm/lib&#39;<br>
&gt; &#39;--with-slurm-include=3D/usr/local/slurm/include&#39; &#39;--enabl=
e-debug=3Dnone&#39;<br>
&gt; &#39;--enable-timing=3Druntime&#39; &#39;CC=3D/usr/local/GCC/5.3.0/bin=
/gcc&#39;<br>
&gt; &#39;CXX=3D/usr/local/GCC/5.3.0/bin/g++&#39; &#39;FC=3D/usr/local/GCC/=
5.3.0/bin/gfortran&#39;<br>
&gt; &#39;F77=3D/usr/local/GCC/5.3.0/bin/gfortran&#39; &#39;--cache-file=3D=
/dev/null&#39;<br>
&gt; &#39;--srcdir=3D.&#39; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#=
39;LDFLAGS=3D-L/lib -L/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;=
LIBS=3D-libmad<br>
&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include -I/includ=
e<br>
&gt; -I/include -I/include -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt;<br>
&gt; On Fri, May 18, 2018 at 1:31 PM Kandes, Martin &lt;<a href=3D"mailto:m=
ka...@sdsc.edu" target=3D"_blank">mka...@sdsc.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi George,<br>
&gt;&gt;<br>
&gt;&gt; I run with different gcc compiler versions inside and outside my M=
PI<br>
&gt;&gt; containers. So I would be surprised if that is the issue here. I&#=
39;m not sure<br>
&gt;&gt; I have a good recommendation of where to start debugging your prob=
lem. But I<br>
&gt;&gt; might start by double checking the MPI versions match inside and o=
utside the<br>
&gt;&gt; container. e.g. see [1].<br>
&gt;&gt;<br>
&gt;&gt; Marty<br>
&gt;&gt;<br>
&gt;&gt; [1]<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1<=
br>
&gt;&gt; --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash<br>
&gt;&gt; srun: job 16364303 queued and waiting for resources<br>
&gt;&gt; srun: job 16364303 has been allocated resources<br>
&gt;&gt; [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ cp<br>
&gt;&gt; /oasis/scratch/comet/mkandes/temp_project/singularity/images/ubunt=
u-mvapich2.img<br>
&gt;&gt; ./<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module purge<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load gnu<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load mvapich2_ib<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module list<br>
&gt;&gt; Currently Loaded Modulefiles:<br>
&gt;&gt;=C2=A0 =C2=A01) gnu/4.9.2=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A02) mvapi=
ch2_ib/2.1<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load singularity<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ gcc --version<br>
&gt;&gt; gcc (GCC) 4.9.2<br>
&gt;&gt; Copyright (C) 2014 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ mpirun --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc=C2=A0 -f=
PIC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++=C2=A0 -fP=
IC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2/gnu/ib&#39; &#39;--enable-shared&#39=
; &#39;--enable-sharedlibs=3Dgcc&#39;<br>
&gt;&gt; &#39;--with-hwloc&#39; &#39;--enable-f77&#39; &#39;--enable-fc&#39=
; &#39;--enable-hybrid&#39;<br>
&gt;&gt; &#39;--with-ib-include=3D/usr/include/infiniband&#39; &#39;--with-=
ib-libpath=3D/usr/lib64&#39;<br>
&gt;&gt; &#39;--enable-fast=3DO3&#39;<br>
&gt;&gt; &#39;--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/sdsc-mva=
pich2_gnu_ib-2.1/../..//cache/build-limic&#39;<br>
&gt;&gt; &#39;--with-slurm=3D/usr/lib64/slurm&#39; &#39;--with-file-system=
=3Dlustre&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D-fPIC -O3 -O3&#39; &#39;CXX=3Dg++&#39; &#39;CXXFLAGS=
=3D-fPIC -O3 -O3&#39; &#39;FC=3Dgfortran&#39;<br>
&gt;&gt; &#39;FCFLAGS=3D-fPIC -O3 -O3&#39; &#39;F77=3Dgfortran&#39; &#39;FF=
LAGS=3D-L/usr/lib64 -L/lib -L/lib<br>
&gt;&gt; -fPIC -O3 -O3&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--srcdi=
r=3D.&#39; &#39;LDFLAGS=3D-L/usr/lib64<br>
&gt;&gt; -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr=
/lib64<br>
&gt;&gt; -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/..=
/..//cache/build-limic/lib<br>
&gt;&gt; -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -lrdmacm -libumad -libverbs=
 -ldl -lrt -llimic2<br>
&gt;&gt; -lm -lpthread &#39; &#39;CPPFLAGS=3D-I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpl/include<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpl/include<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/openpa/src<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/openpa/src<br>
&gt;&gt; -D_REENTRANT<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpi/romio/include<br>
&gt;&gt; -I/include -I/include -I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/..=
/..//cache/build-limic/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.=
img<br>
&gt;&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt; gcc =
--version<br>
&gt;&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt; mpir=
un<br>
&gt;&gt; --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2&#39; &#39;--cache-file=3D/dev/null&#=
39; &#39;--srcdir=3D.&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib=
 -L/lib -L/lib<br>
&gt;&gt; -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIB=
S=3D-libmad -lrdmacm<br>
&gt;&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<=
br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/in=
clude<br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/=
src<br>
&gt;&gt; -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include =
-I/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ________________________________<br>
&gt;&gt; From: George Zaki [<a href=3D"mailto:georg...@gmail.com" target=3D=
"_blank">georg...@gmail.com</a>]<br>
&gt;&gt; Sent: Friday, May 18, 2018 6:48 AM<br>
&gt;&gt; To: singularity<br>
&gt;&gt; Subject: [Singularity] Running an mpi program with mvapitch<br>
&gt;&gt;<br>
&gt;&gt; Hi singularity team,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I would like to run an MPI program in a singularity container. The=
 program<br>
&gt;&gt; is compiled using mvapicth2.2 using a gcc version 5.4.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I can see that my cluster has a compiled version of mvapitch2.2 wi=
th gcc<br>
&gt;&gt; 5.3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; When I run:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o<br>
&gt;&gt;<br>
&gt;&gt; the call does not return.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Does the gcc version has to be exactly the same? I tried the switc=
h the<br>
&gt;&gt; compiler in this image:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; BootStrap: docker<br>
&gt;&gt; From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However when gcc 5.3 is used the mvapitch does not build correctly=
.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; If that&#39;s the problem, Is there a preferred method of switchin=
g gcc<br>
&gt;&gt; version in this container singularity container?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; George<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singu...@lbl.gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to a topic in=
 the<br>
&gt;&gt; Google Groups &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this topic, visit<br>
&gt;&gt; <a href=3D"https://groups.google.com/a/lbl.gov/d/topic/singularity=
/A6I5mZxnmFU/unsubscribe" rel=3D"noreferrer" target=3D"_blank">https://grou=
ps.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe</a>.<br=
>
&gt;&gt; To unsubscribe from this group and all its topics, send an email t=
o<br>
&gt;&gt; <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu=
...@lbl.gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singu...@lbl.gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu.=
..@lbl.gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscri=
be</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscri=
be</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div>

--0000000000000b777b056cb90f95--
