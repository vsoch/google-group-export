X-Received: by 10.129.153.205 with SMTP id q196mr13045643ywg.11.1465434403613;
        Wed, 08 Jun 2016 18:06:43 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.134.219 with SMTP id q88ls279866ioi.109.gmail; Wed, 08 Jun
 2016 18:06:43 -0700 (PDT)
X-Received: by 10.98.93.145 with SMTP id n17mr1542935pfj.66.1465434403093;
        Wed, 08 Jun 2016 18:06:43 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id i2si1131686pfc.224.2016.06.08.18.06.42
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 08 Jun 2016 18:06:43 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.192.72 as permitted sender) client-ip=209.85.192.72;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.192.72 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2G3AQDkv1hXfUjAVdFehBR9BoM2pnCEao03PgQXAQaFdQKBOwc7EQEBAQEBAQEDDwEBCQsLCSEvgjo5CjIBAQEBAQEBAQEBAQEBAQEaAj4SAhkBAQEDARIICSswCwkCCw0gCgICIQEPAwEFARwGCAcEARoCBAGHcwMPCAWPWY9CgTE+MYs7jQ4NhAoBAQEBIhCJYYEDgkOBTxEBgx2CWQWHfAEHhlk+hCCEYDQBhgKGKYF6gjeMaId2hi0SHoEPDyWCJxyBaxwyB4g9BAsXgR4BAQE
X-IronPort-AV: E=Sophos;i="5.26,442,1459839600"; 
   d="scan'208,217";a="25877515"
Received: from mail-qg0-f72.google.com ([209.85.192.72])
  by fe4.lbl.gov with ESMTP; 08 Jun 2016 18:06:41 -0700
Received: by mail-qg0-f72.google.com with SMTP id p34so42605453qgp.3
        for <singu...@lbl.gov>; Wed, 08 Jun 2016 18:06:41 -0700 (PDT)
X-Gm-Message-State: ALyK8tK99sfaHwdzJ79IOoQGTH4hunqhPXROfdzQuWSr/hcozBMrIaaKAqiGCQ5dt56T0MhRyEH+kV1HBuIGDSaQFmTu1qRy4SauXamf/QMi//L7mT+6ez7Szsk+JaOnA8bEgbncx9sD3Va/6lGB0IuP33c=
X-Received: by 10.129.27.9 with SMTP id b9mr4412641ywb.173.1465434400847;
        Wed, 08 Jun 2016 18:06:40 -0700 (PDT)
X-Received: by 10.129.27.9 with SMTP id b9mr4412635ywb.173.1465434400600; Wed,
 08 Jun 2016 18:06:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.13.240.194 with HTTP; Wed, 8 Jun 2016 18:06:40 -0700 (PDT)
In-Reply-To: <CA+zw9q2ftBoz3UJgOmU6qsDyESh8kShr3YfX1jtbF4+4WcwcbQ@mail.gmail.com>
References: <CA+zw9q3gSaqZgo3tzOXyn5kwfeUS-LdCuyTnmWyPgfVjmxGBqA@mail.gmail.com>
 <CAN7etTz3V1eSH2r7Yd9y0MFFt24H2vuAB9FcViKbDAsdxtt9Pg@mail.gmail.com>
 <CA+zw9q0zaF0ajbLuPeT0H-Ag5t-ZfV9ux94hm+9m6XQ_JEQ7LA@mail.gmail.com>
 <CAN7etTykdT1UXi42d3+E=OKaDKpXapb5CFcyMfrRic0xXf6RWA@mail.gmail.com> <CA+zw9q2ftBoz3UJgOmU6qsDyESh8kShr3YfX1jtbF4+4WcwcbQ@mail.gmail.com>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Wed, 8 Jun 2016 18:06:40 -0700
Message-ID: <CAN7etTxXZ+yp_0wEwx7UsBCgjn9nisvWHe+tpnjdpjJCUJ=hXA@mail.gmail.com>
Subject: Re: [Singularity] Singularity with Slurm and PMIx
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a1142a41ae3e8440534ce08d7

--001a1142a41ae3e8440534ce08d7
Content-Type: text/plain; charset=UTF-8

My pleasure! I'm glad to hear it is working for you now!

Greg

On Wed, Jun 8, 2016 at 4:56 PM, yiannis georgiou <goh...@gmail.com> wrote:

> On Wed, Jun 8, 2016 at 10:57 PM, Gregory M. Kurtzer <gmku...@lbl.gov>
> wrote:
>
>>
>> On Wed, Jun 8, 2016 at 1:48 PM, yiannis georgiou <goh...@gmail.com>
>> wrote:
>>
>>> Hi Greg,
>>>
>>> bullseye!!! it worked for me like this:
>>>
>>
>> Excellent!!
>>
>>
>>>
>>> [georgioy@rio11 ~]$ salloc -n4 -N2
>>> [georgioy@rio11 ~]$ export SINGULARITY_NO_NAMESPACE_PID=1
>>> [georgioy@rio11 ~]$ srun --mpi=pmix -n 4 -N2
>>> /usr/local/singularity_v2/bin/singularity exec /tmp/ubuntu_v4.img
>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>> Hello world from processor rio13, rank 2 out of 4 processors
>>> Hello world from processor rio13, rank 3 out of 4 processors
>>> Hello world from processor rio12, rank 0 out of 4 processors
>>> Hello world from processor rio12, rank 1 out of 4 processors
>>> ERROR: Could not clear loop device
>>>
>>> Any idea how I can correct the "ERROR: Could not clear loop device"
>>> appearing at the end of the execution?
>>>
>>
>> Hrmm.. Interesting, I wonder on which system it got that error. Is it
>> possible to try and replicate the error on a single new (e.g. -N1)? The
>> other thing to try is to run losetup (-a) on both rio12 and rio13 and see
>> if /dev/loop0 is still bound somewhere.
>>
> It was related with a previously failed srun I think. When I did a new
> clean salloc with different srun all worked without problem.
>
>>
>>
>>> And when you have a moment could you explain me how this magic
>>> environment variable make it work! I'm not sure I got it.
>>>
>>
>> While most container systems are built around the idea of complete
>> isolation, Singularity is focused on application portability so you can
>> disable some of the namespaces as needed (with the PID namespace being the
>> culprit here for OpenMPI's shared memory model).
>>
> Ok I see.
>
>>
>>
>>>
>>> One more question, I've noticed that there have been some changes in the
>>> latest OpenMPI to improve performance when using singularity. Are they done
>>> for PMIx or for all PMI versions?
>>>
>>
>> Indeed. I'm hoping Ralph will chime in when he has a moment and can
>> address this and possibly a better long term fix (and if it can't be done
>> via the MPI side, I can do it on the Singularity side).
>>
> Ok
>
> Thanks for your answers and the fast solution!
>
> Yiannis
>
>>
>>
>>>
>>> Thanks a lot!
>>>
>>
>> My pleasure!
>>
>> Greg
>>
>>
>>
>>
>>>
>>> Yiannis
>>>
>>>
>>> On Wed, Jun 8, 2016 at 9:16 PM, Gregory M. Kurtzer <gmku...@lbl.gov>
>>> wrote:
>>>
>>>> Hi Yiannis,
>>>>
>>>> I have a quick thing to test... The address not mapped error seems
>>>> consistent with something else that I've seen when testing OpenMPI with
>>>> shared memory and the NEWPID namespace. Try to disable the NEWPID namespace
>>>> by exporting this environment variable:
>>>>
>>>> SINGULARITY_NO_NAMESPACE_PID=1
>>>>
>>>> Now you will need to export it in a place where all Singularity
>>>> contexts will see it, maybe something like this:
>>>>
>>>> $ srun --mpi=pmix -n 4 -N 2 SINGULARITY_NO_NAMESPACE_PID=1 singularity
>>>> exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>
>>>> There is an OpenMPI plugin which sets this automatically in the current
>>>> codebase, but I'm not sure how it will come into play in this usage
>>>> scenario.
>>>>
>>>> Let me know how that works for ya!
>>>>
>>>> Thanks,
>>>>
>>>> Greg
>>>>
>>>> On Wed, Jun 8, 2016 at 12:09 PM, yiannis georgiou <goh...@gmail.com>
>>>> wrote:
>>>>
>>>>> Hello,
>>>>>
>>>>> I'm trying to execute an MPI application from within a singularity
>>>>> image through Slurm and pmix using the following command
>>>>>
>>>>> srun --mpi=pmix -n 4 -N 2 singularity exec /tmp/ubuntu_v4.img
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>>
>>>>> The slurmstepd process is spawned on the compute node and it seems to
>>>>> have launched correctly the application from what we see on one of the
>>>>> compute nodes:
>>>>>
>>>>> ========================================
>>>>> [root@rio12 ~]# ps -aux
>>>>> root     35147  0.1  0.0 562284  4016 ?        Sl   20:06   0:00
>>>>> slurmstepd: [83.0]
>>>>> georgioy 35158  0.2  0.0  10428   896 ?        S    20:06   0:00
>>>>> Singularity: namespace
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>> root     35162  0.0  0.0      0     0 ?        S<   20:06   0:00
>>>>> [loop0]
>>>>> georgioy 35163  0.0  0.0  10428   400 ?        S    20:06   0:00
>>>>> Singularity: exec
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>> root     35164  0.0  0.0      0     0 ?        S    20:06   0:00
>>>>> [jbd2/loop0-8]
>>>>> root     35165  0.0  0.0      0     0 ?        S    20:06   0:00
>>>>> [ext4-dio-unwrit]
>>>>> georgioy 35166  102  0.0 285476 14828 ?        RLl  20:06   0:12
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>> ==========================================
>>>>>
>>>>> and I see the pmix related functions starting correctly in the slurmd
>>>>> log files :
>>>>>
>>>>> ========================================
>>>>>
>>>>> [2016-06-08T20:12:08.004] [86.0] debug:  (null) [0] mpi_pmix.c:90
>>>>> [p_mpi_hook_slurmstepd_prefork] mpi/pmix: start
>>>>> [2016-06-08T20:12:08.004] [86.0] debug:  mpi/pmix: setup sockets
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:78
>>>>> [errhandler_reg_callbk] mpi/pmix: Error handler registration callback is
>>>>> called with status=0, ref=0
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:581
>>>>> [pmixp_libpmix_job_set] mpi/pmix: task initialization
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:220
>>>>> [_agent_thread] mpi/pmix: Start agent thread
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:313
>>>>> [pmixp_agent_start] mpi/pmix: agent thread started: tid = 139672278615808
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84
>>>>> [_conn_readable] mpi/pmix: fd = 9
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84
>>>>> [_conn_readable] mpi/pmix: fd = 19
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:256
>>>>> [_pmix_timer_thread] mpi/pmix: Start timer thread
>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:335
>>>>> [pmixp_agent_start] mpi/pmix: timer thread started: tid = 139672277563136
>>>>>
>>>>> =======================================
>>>>>
>>>>> However the application is never actually started and the srun fails
>>>>> completely with the following output:
>>>>>
>>>>> =========================================
>>>>> [georgioy@rio11 ~]$ srun --mpi=pmix -n 4 -N 2 singularity exec
>>>>> /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>> [rio12:00001] *** Process received signal ***
>>>>> [rio12:00001] Signal: Segmentation fault (11)
>>>>> [rio12:00001] Signal code: Address not mapped (1)
>>>>> [rio12:00001] Failing at address: 0x7fbd937e6010
>>>>> [rio12:00001] [ 0] [rio12:00001] *** Process received signal ***
>>>>> /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f26aeb233d0]
>>>>> [rio12:00001] [ 1]
>>>>> /usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f269f964035]
>>>>> [rio12:00001] [ 2]
>>>>> /usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f269f964583]
>>>>> [rio12:00001] [ 3]
>>>>> /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7f269fb68771]
>>>>> [rio12:00001] [ 4]
>>>>> /usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f26a451fbe7]
>>>>> [rio12:00001] [ 5]
>>>>> /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f269ef35662]
>>>>> [rio12:00001] [ 6]
>>>>> /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f26aed75d31]
>>>>> [rio12:00001] [ 7]
>>>>> /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26aed9bdb9]
>>>>> [rio12:00001] [ 8]
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]
>>>>> [rio12:00001] [ 9]
>>>>> /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f26ae769830]
>>>>> [rio12:00001] Signal: Segmentation fault (11)
>>>>> [rio12:00001] Signal code: Address not mapped (1)
>>>>> [rio12:00001] Failing at address: 0x7fbd937e6010
>>>>> [rio12:00001] [10]
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]
>>>>> [rio12:00001] *** End of error message ***
>>>>> [rio12:00001] [ 0]
>>>>> /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f3f89c063d0]
>>>>> [rio12:00001] [ 1]
>>>>> /usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f3f7eb1d035]
>>>>> [rio12:00001] [ 2]
>>>>> /usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f3f7eb1d583]
>>>>> [rio12:00001] [ 3]
>>>>> /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7f3f7ed21771]
>>>>> [rio12:00001] [ 4]
>>>>> /usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f3f7f5f3be7]
>>>>> [rio12:00001] [ 5]
>>>>> /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f3f7e0ee662]
>>>>> [rio12:00001] [ 6]
>>>>> /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f3f89e58d31]
>>>>> [rio12:00001] [ 7]
>>>>> /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7edb9]
>>>>> [rio12:00001] [ 8]
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]
>>>>> [rio12:00001] [ 9]
>>>>> /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f3f8984c830]
>>>>> [rio12:00001] [10]
>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]
>>>>> [rio12:00001] *** End of error message ***
>>>>> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix:
>>>>> ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
>>>>> srun: Job step aborted: Waiting up to 32 seconds for job step to
>>>>> finish.
>>>>> slurmstepd: error: *** STEP 86.0 ON rio12 CANCELLED AT
>>>>> 2016-06-08T20:12:08 ***
>>>>> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix:
>>>>> ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
>>>>> slurmstepd: error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix:
>>>>> ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
>>>>> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix:
>>>>> ERROR: Error handler invoked: status = -25, nranges = 0: Success (0)
>>>>> srun: error: rio12: tasks 0-2: Killed
>>>>> srun: error: rio13: task 3: Killed
>>>>>
>>>>> ============================================
>>>>>
>>>>> the results I get when launching with mpirun instead of srun are more
>>>>> or less the same.
>>>>>
>>>>> From my understanding, the orted process, or in my case slurmstepd
>>>>> process that launches the singularity container
>>>>> and the MPI application, enables the communication of MPI libraries
>>>>> and orted (or slurmstepd) through PMI (in my case here PMIx).
>>>>> So I suppose the problem I see should be related with the mapping of
>>>>> PMI from the container towards the orted or slurmstepd.
>>>>>
>>>>> What do you think?
>>>>>
>>>>> To give some more details, my singularity container has OpenMPI and
>>>>> PMIx installed but not Slurm. I don't think that Slurm needs to reside
>>>>> within the container in that context.
>>>>> But I wasn't sure if OpenMPI and PMIx are needed both inside and
>>>>> outside of the container.
>>>>> So, I've tried using the exact same version of PMIx and OpenMPI in and
>>>>> out of the container and the problem still persists.
>>>>> The experiments have been done using RedHat hosts with CentOS or
>>>>> Ubuntu singularity containers and the latest github versions of slurm,
>>>>> OpenMPI, PMIx and singularity.
>>>>>
>>>>> By the way, do you have any MPI example for singularity v2?
>>>>> Because the MPI example that you show here :
>>>>> http://singularity.lbl.gov/#hpc
>>>>>
>>>>> is actually done using singularity v1, no?
>>>>> In my understanding with singularity v2 we actually build a complete
>>>>> image with OS not just the application with its libraries. Is this correct?
>>>>>
>>>>> Sorry for the long email and
>>>>> thanks a lot for any extra info you can provide regarding singularity
>>>>> v2 and MPI.
>>>>>
>>>>> Best Regards,
>>>>> Yiannis
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a1142a41ae3e8440534ce08d7
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">My pleasure! I&#39;m glad to hear it is working for you no=
w!<div><br></div><div>Greg</div></div><div class=3D"gmail_extra"><br><div c=
lass=3D"gmail_quote">On Wed, Jun 8, 2016 at 4:56 PM, yiannis georgiou <span=
 dir=3D"ltr">&lt;<a href=3D"mailto:goh...@gmail.com" target=3D"_blank">goh.=
..@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div d=
ir=3D"ltr"><div class=3D"gmail_extra"><div class=3D"gmail_quote"><span clas=
s=3D"">On Wed, Jun 8, 2016 at 10:57 PM, Gregory M. Kurtzer <span dir=3D"ltr=
">&lt;<a href=3D"mailto:gmku...@lbl.gov" target=3D"_blank">gmku...@lbl.gov<=
/a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:=
0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><d=
iv class=3D"gmail_extra"><br><div class=3D"gmail_quote"><span>On Wed, Jun 8=
, 2016 at 1:48 PM, yiannis georgiou <span dir=3D"ltr">&lt;<a href=3D"mailto=
:goh...@gmail.com" target=3D"_blank">goh...@gmail.com</a>&gt;</span> wrote:=
<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;bor=
der-left-width:1px;border-left-style:solid;border-left-color:rgb(204,204,20=
4);padding-left:1ex"><div dir=3D"ltr"><div><div><div><div><div><div>Hi Greg=
,<br><br></div>bullseye!!! it worked for me like this:<br></div></div></div=
></div></div></div></blockquote><div><br></div></span><div>Excellent!!</div=
><span><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0=
px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-=
color:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><div><div><d=
iv><div><br>[georgioy@rio11 ~]$ salloc -n4 -N2<br>[georgioy@rio11 ~]$ expor=
t SINGULARITY_NO_NAMESPACE_PID=3D1<br>[georgioy@rio11 ~]$ srun --mpi=3Dpmix=
 -n 4 -N2 /usr/local/singularity_v2/bin/singularity exec /tmp/ubuntu_v4.img=
 /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>Hello world from process=
or rio13, rank 2 out of 4 processors<br>Hello world from processor rio13, r=
ank 3 out of 4 processors<br>Hello world from processor rio12, rank 0 out o=
f 4 processors<br>Hello world from processor rio12, rank 1 out of 4 process=
ors<br>ERROR: Could not clear loop device<br><br></div>Any idea how I can c=
orrect the &quot;ERROR: Could not clear loop device&quot; appearing at the =
end of the execution?<br></div></div></div></div></div></blockquote><div><b=
r></div></span><div>Hrmm.. Interesting, I wonder on which system it got tha=
t error. Is it possible to try and replicate the error on a single new (e.g=
. -N1)? The other thing to try is to run losetup (-a) on both rio12 and rio=
13 and see if /dev/loop0 is still bound somewhere.</div></div></div></div><=
/blockquote></span><div>It was related with a previously failed srun I thin=
k. When I did a new clean salloc with different srun all worked without pro=
blem. <br></div><span class=3D""><blockquote class=3D"gmail_quote" style=3D=
"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D=
"ltr"><div class=3D"gmail_extra"><div class=3D"gmail_quote"><span><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8e=
x;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,2=
04,204);padding-left:1ex"><div dir=3D"ltr"><div><div><div><div></div>And wh=
en you have a moment could you explain me how this magic environment variab=
le make it work! I&#39;m not sure I got it.<br></div></div></div></div></bl=
ockquote><div><br></div></span><div>While most container systems are built =
around the idea of complete isolation, Singularity is focused on applicatio=
n portability so you can disable some of the namespaces as needed (with the=
 PID namespace being the culprit here for OpenMPI&#39;s shared memory model=
).</div></div></div></div></blockquote></span><div>Ok I see. <br></div><spa=
n class=3D""><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div class=3D"=
gmail_extra"><div class=3D"gmail_quote"><span><div>=C2=A0</div><blockquote =
class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1=
px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:=
1ex"><div dir=3D"ltr"><div><div><div><br></div>One more question, I&#39;ve =
noticed that there have been some changes in the latest OpenMPI to improve =
performance when using singularity. Are they done for PMIx or for all PMI v=
ersions?<br></div></div></div></blockquote><div><br></div></span><div>Indee=
d. I&#39;m hoping Ralph will chime in when he has a moment and can address =
this and possibly a better long term fix (and if it can&#39;t be done via t=
he MPI side, I can do it on the Singularity side).</div></div></div></div><=
/blockquote></span><div>Ok<br><br></div><div>Thanks for your answers and th=
e fast solution!<span class=3D"HOEnZb"><font color=3D"#888888"><br></font><=
/span></div><span class=3D"HOEnZb"><font color=3D"#888888"><div><br></div><=
div>Yiannis <br></div></font></span><div><div class=3D"h5"><blockquote clas=
s=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pad=
ding-left:1ex"><div dir=3D"ltr"><div class=3D"gmail_extra"><div class=3D"gm=
ail_quote"><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"marg=
in:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-l=
eft-color:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><div><br=
></div>Thanks a lot!</div></div></blockquote><div><br></div><div>My pleasur=
e!</div><div><br></div><div>Greg</div><div><div><div><br></div><div><br></d=
iv><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0=
px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-colo=
r:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><span><font colo=
r=3D"#888888"><br></font></span></div><span><font color=3D"#888888">Yiannis=
<br><div><div><div><div><div><div><div><div><br></div></div></div></div></d=
iv></div></div></div></font></span></div><div><div><div class=3D"gmail_extr=
a"><br><div class=3D"gmail_quote">On Wed, Jun 8, 2016 at 9:16 PM, Gregory M=
. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"mailto:gmku...@lbl.gov" target=
=3D"_blank">gmku...@lbl.gov</a>&gt;</span> wrote:<br><blockquote class=3D"g=
mail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-=
left-style:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div =
dir=3D"ltr">Hi Yiannis,<div><br></div><div>I have a quick thing to test... =
The address not mapped error seems consistent with something else that I&#3=
9;ve seen when testing OpenMPI with shared memory and the NEWPID namespace.=
 Try to disable the NEWPID namespace by exporting this environment variable=
:</div><div><br></div><div>SINGULARITY_NO_NAMESPACE_PID=3D1<br></div><div><=
br></div><div>Now you will need to export it in a place where all Singulari=
ty contexts will see it, maybe something like this:</div><div><br></div><di=
v>$ srun --mpi=3Dpmix -n 4 -N 2 SINGULARITY_NO_NAMESPACE_PID=3D1 singularit=
y exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello</d=
iv><div><br></div><div>There is an OpenMPI plugin which sets this automatic=
ally in the current codebase, but I&#39;m not sure how it will come into pl=
ay in this usage scenario.</div><div><br></div><div>Let me know how that wo=
rks for ya!</div><div><br></div><div>Thanks,</div><div><br></div><div>Greg<=
/div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote"><div><=
div>On Wed, Jun 8, 2016 at 12:09 PM, yiannis georgiou <span dir=3D"ltr">&lt=
;<a href=3D"mailto:goh...@gmail.com" target=3D"_blank">goh...@gmail.com</a>=
&gt;</span> wrote:<br></div></div><blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;=
border-left-color:rgb(204,204,204);padding-left:1ex"><div><div><div dir=3D"=
ltr"><div><div><div>Hello,<br><br></div>I&#39;m trying to execute an MPI ap=
plication from within a singularity image through Slurm and pmix using the =
following command <br><br></div>srun --mpi=3Dpmix -n 4 -N 2 singularity exe=
c
 /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br><br><=
/div>The
 slurmstepd process is spawned on the compute node and it seems to have=20
launched correctly the application from what we see on one of the=20
compute nodes:<br><div><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br=
>[root@rio12 ~]# ps -aux<br>root=C2=A0=C2=A0=C2=A0=C2=A0 35147=C2=A0 0.1=C2=
=A0 0.0 562284=C2=A0 4016 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 Sl=C2=
=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 slurmstepd: [83.0]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 <br>georgioy
 35158=C2=A0 0.2=C2=A0 0.0=C2=A0 10428=C2=A0=C2=A0 896 ?=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 Singula=
rity:=20
namespace=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=20
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>root=C2=A0=C2=A0=C2=A0=C2=
=A0 35162=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S&lt;=C2=A0=C2=
=A0 20:06=C2=A0=C2=A0 0:00 [loop0]<br>georgioy
 35163=C2=A0 0.0=C2=A0 0.0=C2=A0 10428=C2=A0=C2=A0 400 ?=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 Singula=
rity:=20
exec=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=20
/home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>root=C2=A0=C2=A0=C2=A0=C2=
=A0 35164=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=
=A0 20:06=C2=A0=C2=A0 0:00 [jbd2/loop0-8]<br>root=C2=A0=C2=A0=C2=A0=C2=A0 3=
5165=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0=C2=A0=
=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20=
:06=C2=A0=C2=A0 0:00 [ext4-dio-unwrit]<br>georgioy 35166=C2=A0 102=C2=A0 0.=
0 285476 14828 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 RLl=C2=A0 20:06=
=C2=A0=C2=A0 0:12 /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<br>=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br></div><div>and I see t=
he pmix related functions starting correctly in the slurmd log files :<br><=
br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br>[2016-06-08T20:12:0=
8.004] [86.0] debug:=C2=A0 (null) [0] mpi_pmix.c:90 [p_mpi_hook_slurmstepd_=
prefork] mpi/pmix: start<br>[2016-06-08T20:12:08.004] [86.0] debug:=C2=A0 m=
pi/pmix: setup sockets<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_client.c:78 [errhandler_reg_callbk]=20
mpi/pmix: Error handler registration callback is called with status=3D0,=20
ref=3D0<br>[2016-06-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_cl=
ient.c:581 [pmixp_libpmix_job_set] mpi/pmix: task initialization<br>[2016-0=
6-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:220 [_agent_=
thread] mpi/pmix: Start agent thread<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:313 [pmixp_agent_start]=20
mpi/pmix: agent thread started: tid =3D 139672278615808<br>[2016-06-08T20:1=
2:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:84 [_conn_readable] m=
pi/pmix: fd =3D 9<br>[2016-06-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0=
] pmixp_agent.c:84 [_conn_readable] mpi/pmix: fd =3D 19<br>[2016-06-08T20:1=
2:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:256 [_pmix_timer_thre=
ad] mpi/pmix: Start timer thread<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:335 [pmixp_agent_start]=20
mpi/pmix: timer thread started: tid =3D 139672277563136<br><br>=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br></div><div><div><br></div><div>However th=
e application is never actually started and the srun fails completely with =
the following output:<br></div><div><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D<br>[georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N 2 singular=
ity exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello<=
br>[rio12:00001] *** Process received signal ***<br>[rio12:00001] Signal: S=
egmentation fault (11)<br>[rio12:00001] Signal code: Address not mapped (1)=
<br>[rio12:00001] Failing at address: 0x7fbd937e6010<br>[rio12:00001] [ 0] =
[rio12:00001] *** Process received signal ***<br>/lib/x86_64-linux-gnu/libp=
thread.so.0(+0x113d0)[0x7f26aeb233d0]<br>[rio12:00001] [ 1] /usr/local/lib/=
libmca_common_sm.so.0(+0x1035)[0x7f269f964035]<br>[rio12:00001] [ 2] /usr/l=
ocal/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x7f269f964583]=
<br>[rio12:00001] [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_=
procs+0x5f1)[0x7f269fb68771]<br>[rio12:00001] [ 4] /usr/local/lib/openmpi/m=
ca_bml_r2.so(+0x2be7)[0x7f26a451fbe7]<br>[rio12:00001] [ 5] /usr/local/lib/=
openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x7f269ef35662]<br>[rio1=
2:00001] [ 6] /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f26aed75d3=
1]<br>[rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26ae=
d9bdb9]<br>[rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hell=
o[0x4007ec]<br>[rio12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_st=
art_main+0xf0)[0x7f26ae769830]<br>[rio12:00001] Signal: Segmentation fault =
(11)<br>[rio12:00001] Signal code: Address not mapped (1)<br>[rio12:00001] =
Failing at address: 0x7fbd937e6010<br>[rio12:00001] [10] /home_nfs/georgioy=
/BENCHS/mpi-openmp/mpi_hello[0x400709]<br>[rio12:00001] *** End of error me=
ssage ***<br>[rio12:00001] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x11=
3d0)[0x7f3f89c063d0]<br>[rio12:00001] [ 1] /usr/local/lib/libmca_common_sm.=
so.0(+0x1035)[0x7f3f7eb1d035]<br>[rio12:00001] [ 2] /usr/local/lib/libmca_c=
ommon_sm.so.0(common_sm_mpool_create+0xa3)[0x7f3f7eb1d583]<br>[rio12:00001]=
 [ 3] /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7f=
3f7ed21771]<br>[rio12:00001] [ 4] /usr/local/lib/openmpi/mca_bml_r2.so(+0x2=
be7)[0x7f3f7f5f3be7]<br>[rio12:00001] [ 5] /usr/local/lib/openmpi/mca_pml_o=
b1.so(mca_pml_ob1_add_procs+0xd2)[0x7f3f7e0ee662]<br>[rio12:00001] [ 6] /us=
r/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f3f89e58d31]<br>[rio12:0000=
1] [ 7] /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7edb9]<br>[rio12=
:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]<br>[r=
io12:00001] [ 9] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x=
7f3f8984c830]<br>[rio12:00001] [10] /home_nfs/georgioy/BENCHS/mpi-openmp/mp=
i_hello[0x400709]<br>[rio12:00001] *** End of error message ***<br>slurmste=
pd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>srun: Job s=
tep aborted: Waiting up to 32 seconds for job step to finish.<br>slurmstepd=
: error: *** STEP 86.0 ON rio12 CANCELLED AT 2016-06-08T20:12:08 ***<br>slu=
rmstepd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>slurmstepd:=
=20
error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>slurmstepd:=
=20
error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>srun: error:=
 rio12: tasks 0-2: Killed<br>srun: error: rio13: task 3: Killed<br><br>=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br></div><div>th=
e results I get when launching with mpirun instead of srun are more or less=
 the same. <br><br></div><div>From my understanding, the orted process, or =
in my case slurmstepd process that launches the singularity container <br><=
/div><div>and
 the MPI application, enables the communication of MPI libraries and=20
orted (or slurmstepd) through PMI (in my case here PMIx).<br></div><div>So =
I suppose the problem I see should be related with the mapping of PMI from =
the container towards the orted or slurmstepd.<br><br></div><div>What do yo=
u think?<br></div><div><br></div><div>To
 give some more details, my singularity container has OpenMPI and PMIx=20
installed but not Slurm. I don&#39;t think that Slurm needs to reside withi=
n
 the container in that context.<br></div><div>But I wasn&#39;t sure if Open=
MPI and PMIx are needed both inside and outside of the container.<br></div>=
<div>So, I&#39;ve tried using the exact same version of PMIx and OpenMPI in=
 and out of the container and the problem still persists. <br></div><div>Th=
e
 experiments have been done using RedHat hosts with CentOS or Ubuntu=20
singularity containers and the latest github versions of slurm, OpenMPI,
 PMIx and singularity.<br></div><div><br></div><div>By the way, do you have=
 any MPI example for singularity v2? <br></div><div>Because the MPI example=
 that you show here : <a href=3D"http://singularity.lbl.gov/#hpc" target=3D=
"_blank">http://singularity.lbl.gov/#hpc</a><br><br></div><div>is actually =
done using singularity v1, no? <br>In
 my understanding with singularity v2 we actually build a complete image
 with OS not just the application with its libraries. Is this correct?<br><=
/div><div><br></div><div>Sorry for the long email and<br>thanks a lot for a=
ny extra info you can provide regarding singularity v2 and MPI.<br><br></di=
v><div>Best Regards,<br></div><div>Yiannis</div></div></div></div></div><sp=
an><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<span><font color=3D"#888888"><br>
</font></span></font></span></blockquote></div><span><font color=3D"#888888=
"><br><br clear=3D"all"><div><br></div>-- <br><div data-smartmail=3D"gmail_=
signature"><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Com=
puting Services (HPCS)<br>University of California<br>Lawrence Berkeley Nat=
ional Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div=
>
</font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</font></span></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div></div></div><div><div><br><br clear=3D"all">=
<div><br></div>-- <br><div data-smartmail=3D"gmail_signature"><div dir=3D"l=
tr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HPCS)<b=
r>University of California<br>Lawrence Berkeley National Laboratory<br>One =
Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div></div></div><br></div></div><div class=3D"HO=
EnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>

--001a1142a41ae3e8440534ce08d7--
