X-Received: by 10.157.8.21 with SMTP id 21mr3806613oty.111.1477613743159;
        Thu, 27 Oct 2016 17:15:43 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.127.23 with SMTP id r23ls213343itc.15.canary-gmail; Thu, 27
 Oct 2016 17:15:42 -0700 (PDT)
X-Received: by 10.99.112.5 with SMTP id l5mr10158123pgc.40.1477613742170;
        Thu, 27 Oct 2016 17:15:42 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id h63si10388683pge.110.2016.10.27.17.15.42
        for <singu...@lbl.gov>;
        Thu, 27 Oct 2016 17:15:42 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) client-ip=209.85.215.70;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.70 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
IronPort-PHdr: 9a23:RgEPvxUc5S+qKAx002zedrqv8DHV8LGtZVwlr6E/grcLSJyIuqrYZxSOt8tkgFKBZ4jH8fUM07OQ6PG6HzNaqs3Y+Fk5M7V0HycfjssXmwFySOWkMmbcaMDQUiohAc5ZX0Vk9XzoeWJcGcL5ekGA6ibqtW1aJBzzOEJPK/jvHcaK1oLshrr0pMeYOFwArQH+SIs6FA+xowTVu5teqqpZAYF19CH0pGBVcf9d32JiKAHbtR/94sCt4MwrqHwI6Loc7coIbYHWN+R9E/0LRAkgKH0/scjitB3fSlmU530TT2EfiBtUEkvY6grnVIz6qCrwu8J50i3cMsroHowzDC+j6ah2TBbyiTsWf2oi8WfYl8h5lqNHsTqlrRg5zInKNtK7Lv17K+nyZ94VQnBQFuMXHw9cGJ+uYpFFR74OOOVFtZXvqkEmqRG6QwarGrW8mXdzmnbq0PhigKwaGgbc0VllQd8=
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FqAQCDlxJYgEbXVdFdRwEBFgEBBQEBgjs3AQEBAQE7Om0QB4M4oHWCV4xShRaBRRsnHQEGgkmDNgKBfgc/FAEBAQEBAQEBAQEBAhABAQkNCQkbMoIzBAIBAhEFBAE5CjIBAQEBAQEBAQEBAQEBAQEaAg0xAysBAQMBEhErMAsLCzcCAiIPAwEFARwGCAcEAQcTAgQBiCoIBaQAgTI/MotGjHsBAQEBBgEBAQEBASEQiX2BBYQZEQEIgxiCPh0Fj0KKVAGGLIl2gW5Oh1mFb40Lgj8THoERDw9jhVseNAeFXA4XMDGBMQEBAQ
X-IronPort-AV: E=Sophos;i="5.31,406,1473145200"; 
   d="scan'208,217";a="54067255"
Received: from mail-lf0-f70.google.com ([209.85.215.70])
  by fe3.lbl.gov with ESMTP; 27 Oct 2016 17:15:40 -0700
Received: by mail-lf0-f70.google.com with SMTP id x79so13191864lff.2
        for <singu...@lbl.gov>; Thu, 27 Oct 2016 17:15:40 -0700 (PDT)
X-Gm-Message-State: ABUngvdcZiyuTIJKY6FGFmia+u+1pq4ikxACjrqTF15Dgpwx6UfJVHLF/M8eeIgKYZw2nTLx7VrfUhZxSvF28GcfCK7Zchg9rdzr+0KLePydXVYrUjl2SQXCEM6s5iEfMk8IWqH51LcEBG5vuWvH7XtzdHA=
X-Received: by 10.25.203.143 with SMTP id b137mr7114365lfg.145.1477613739775;
        Thu, 27 Oct 2016 17:15:39 -0700 (PDT)
X-Received: by 10.25.203.143 with SMTP id b137mr7114355lfg.145.1477613739499;
 Thu, 27 Oct 2016 17:15:39 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.167.67 with HTTP; Thu, 27 Oct 2016 17:15:38 -0700 (PDT)
In-Reply-To: <32b1caf2-ebd0-45d9-9283-dd568259821d@lbl.gov>
References: <32b1caf2-ebd0-45d9-9283-dd568259821d@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 27 Oct 2016 17:15:38 -0700
Message-ID: <CAN7etTzURthsJN7fKtZko1YQqt3utRg34MatBSmomiicWFpRtw@mail.gmail.com>
Subject: Re: [Singularity] Singularity for archiving large amounts of (small) files
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=94eb2c19ebe80eff02053fe1c225

--94eb2c19ebe80eff02053fe1c225
Content-Type: text/plain; charset=UTF-8

I will add another thought to the mix... A loopback compressed file system
like SquashFS might be interesting. They are read only, but easily dealt
with at a system level. Additionally Brian has an RFE in for Singularity to
support images as bind mounts.

On Thu, Oct 27, 2016 at 5:27 AM, Stefan Vollmar <vol...@sf.mpg.de> wrote:

> We have started to use singularity containers to ensure consistent HPC
> results on different compute cluster and this has a huge potential for some
> of our scientific projects - thanks again for a great tool! -
>
> We now think about a completely different use case where singularity might
> shine: some of our experiments generate lots of small files (e.g. 0.5 M
> files of 0.2 MB for one run - this is not our software and we have to live
> with it) and for many medical applications (e.g. human brain scanners) you
> cannot avoid handling lots of small DICOM files when working in a clinical
> setting (e.g. 2 GB in 2 K files).
>
> We use HSM storage systems where handling large amounts of small(ish)
> files is usually painful so we have been using a number of techniques over
> the years to reduce the number of files: it boils down to creating gzipped
> tar-archives of reasonable sizes and also calculating checksums while we
> are at it (HDF5 is usually not an option).
>
> While this is of course feasible, it requires unpacking the files if you
> want to re-run some analysis on the data (which should remain read-only).
> However, putting the data on a suitable file system (maybe ZFS with
> on-the-fly compression) inside a container could remedy this and might be
> more user-friendly/less error-prone and has some additional benefits
> (including some analysis software, database tools, etc.).
>
> Maybe someone can think of better solutions? Any comments are appreciated
> - thanks!
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--94eb2c19ebe80eff02053fe1c225
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">I will add another thought to the mix... A loopback compre=
ssed file system like SquashFS might be interesting. They are read only, bu=
t easily dealt with at a system level. Additionally Brian has an RFE in for=
 Singularity to support images as bind mounts.</div><div class=3D"gmail_ext=
ra"><br><div class=3D"gmail_quote">On Thu, Oct 27, 2016 at 5:27 AM, Stefan =
Vollmar <span dir=3D"ltr">&lt;<a href=3D"mailto:vol...@sf.mpg.de" target=3D=
"_blank">vol...@sf.mpg.de</a>&gt;</span> wrote:<br><blockquote class=3D"gma=
il_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-lef=
t:1ex"><div dir=3D"ltr"><div style=3D"color:rgb(0,0,0);font-family:monospac=
e;font-size:18px">We have started to use singularity containers to ensure c=
onsistent HPC results on different compute cluster and=C2=A0this has a huge=
 potential for some of our scientific projects - thanks again for a great t=
ool! -=C2=A0</div><div style=3D"color:rgb(0,0,0);font-family:monospace;font=
-size:18px"><br></div><div style=3D"color:rgb(0,0,0);font-family:monospace;=
font-size:18px">We now think about a completely different use case where si=
ngularity might shine: some of our experiments generate lots of small files=
 (e.g. 0.5 M files of 0.2 MB for one run - this is not our software and we =
have to live with it) and for many medical applications (e.g. human brain s=
canners) you cannot avoid handling lots of small DICOM files when working i=
n a clinical setting (e.g. 2 GB in 2 K files).</div><div style=3D"color:rgb=
(0,0,0);font-family:monospace;font-size:18px"><br></div><div style=3D"color=
:rgb(0,0,0);font-family:monospace;font-size:18px">We use HSM storage system=
s where handling large amounts of small(ish) files is usually painful so we=
 have been using a number of techniques over the years to reduce the number=
 of files: it boils down to creating gzipped tar-archives of reasonable siz=
es and also calculating checksums while we are at it (HDF5 is usually not a=
n option).</div><div style=3D"color:rgb(0,0,0);font-family:monospace;font-s=
ize:18px"><br></div><div style=3D"color:rgb(0,0,0);font-family:monospace;fo=
nt-size:18px">While this is of course feasible, it requires unpacking the f=
iles if you want to re-run some analysis on the data (which should remain r=
ead-only). However, putting the data on a suitable file system (maybe ZFS w=
ith on-the-fly compression) inside a container could remedy this and might =
be more user-friendly/less error-prone and has some additional benefits (in=
cluding some analysis software, database tools, etc.).=C2=A0</div><div styl=
e=3D"color:rgb(0,0,0);font-family:monospace;font-size:18px"><br></div><div =
style=3D"color:rgb(0,0,0);font-family:monospace;font-size:18px">Maybe someo=
ne can think of better solutions? Any comments are appreciated - thanks!</d=
iv></div><span class=3D"HOEnZb"><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div di=
r=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"=
><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sy=
stems Architect and Technology Developer</div><div>Lawrence Berkeley Nation=
al Laboratory HPCS<br>University of California Berkeley Research IT<br>Sing=
ularity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targ=
et=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster M=
anagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http=
://warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.=
com/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<sp=
an style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitt=
er.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twit=
ter.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div>=
</div></div>
</div>

--94eb2c19ebe80eff02053fe1c225--
