Date: Mon, 12 Jun 2017 13:38:48 -0700 (PDT)
From: Stefan Kombrink <stefan....@googlemail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <b3d1a25d-6e5b-4a47-a2ed-26b5fd144f79@lbl.gov>
In-Reply-To: <CAKjpbPgtBHN7iNK9tzABD=A-1K8qKPmzHBsBpeSWRiGyFvKdVQ@mail.gmail.com>
References: <cc84e6d1-49ad-416d-8480-78863790f5fd@lbl.gov> <6117a8ba-063e-4799-aeb3-38a2558a0458@lbl.gov>
 <CAKjpbPgtBHN7iNK9tzABD=A-1K8qKPmzHBsBpeSWRiGyFvKdVQ@mail.gmail.com>
Subject: Re: [Singularity] Re: Is there any way to run mpirun command inside
 container for multi-host system?
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_3773_1482568528.1497299928108"

------=_Part_3773_1482568528.1497299928108
Content-Type: multipart/alternative; 
	boundary="----=_Part_3774_1575444331.1497299928108"

------=_Part_3774_1575444331.1497299928108
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable


Here is the wrapper script which replaces ssh: https://pastebin.com/vqXXRzb=
5

Am Samstag, 10. Juni 2017 11:17:04 UTC+2 schrieb Guohua Li:
>
> Really thanks for your reply!=20
>
> Please post the script when you find it.=20
>
> Now I'm testing every MPI version which host installed MPI version can=20
> support much MPI versions inside the container!=20
>
> Hope your script can solve our problem!
>
>
> =EC=98=81=EC=9B=90=ED=9E=88 =EC=82=B4 =EA=B2=83 =EC=B2=98=EB=9F=BC =EA=BF=
=88=EA=BE=B8=EA=B3=A0 =EB=82=B4=EC=9D=BC =EC=A3=BD=EC=9D=84 =EA=B2=83 =EC=
=B2=98=EB=9F=BC =EC=82=B4=EC=9E=90! =20
> This too shall pass away!
>
> =E2=80=BB =EC=9D=B4=EA=B5=AD=ED=99=94 LI GUOHUA
> =E2=80=BB =EA=B1=B4=EA=B5=AD=EB=8C=80=ED=95=99=EA=B5=90 =EB=8C=80=ED=95=
=99=EC=9B=90 =EC=8B=A0=EA=B8=B0=EC=88=A0=EC=9C=B5=ED=95=A9=ED=95=99=EA=B3=
=BC =EB=B0=95=EC=82=AC=EA=B3=BC=EC=A0=95
> =E2=80=BB =EC=9C=B5=ED=95=A9 =EC=BB=B4=ED=93=A8=ED=8C=85 =EC=97=B0=EA=B5=
=AC=EC=8B=A4 (IC Lab)
> =E2=80=BB iIT, Department of Advanced Technology Fusion, Konkuk Universit=
y
> =E2=80=BB Industry-University Cooperation Bldg. 805
> =E2=80=BB KonKuk University, Hwayang-dong, Gwangjin-Gu, Seoul 143-701 Sou=
th Korea
> =E2=80=BB Cell Phone: +82) 10-3666-8263
> =E2=80=BB E-mail: heave...@gmail.com <javascript:>
>
> 2017-06-10 18:07 GMT+09:00 'Stefan Kombrink' via singularity <
> si...@lbl.gov <javascript:>>:
>
>> I've got some experience on running multi node jobs with mpirun inside=
=20
>> the containers.
>> Greg is right that it will be difficult to find an general purpose=20
>> solution for arbitary HPC systems and software.
>> I made it work for two heavily used applications on our cluster one of=
=20
>> which used IntelMPI and the other OpenMPI 1.6
>> The main thing was to replace /bin/ssh in order to tweak mpirun to wrap=
=20
>> "ssh hostname cmd" to "ssh 'singularity ContainerName exec cmd'
>> I had to apply some more workarounds to make IB work properly (mainly=20
>> envvars to configure MPI properly) but essentially the efford was=20
>> justifiable.
>>
>> Sorry cant find the link to the ssh wrapper script right now but I can=
=20
>> post it later when you are interested in it. It is really just a few lin=
es=20
>> of bash code.
>>
>> Stefan
>>
>>
>> Am Donnerstag, 8. Juni 2017 15:17:30 UTC+2 schrieb Guohua Li:
>>>
>>>
>>> For our HPC system, really need run mpirun command inside the container=
=20
>>> on multi-host.=20
>>>
>>> How can we fix the issues?=20
>>>
>>> In my experience, when I build mpi job on multihost, everytime I change=
=20
>>> the version of MPI inside the container, I have to change the version o=
f=20
>>> MPI on the host.=20
>>>
>>> Is there any solution for run mpi command inside the container on=20
>>> multi-host system?=20
>>>
>>>
>>>
>>>
>>> --=20
>> You received this message because you are subscribed to the Google Group=
s=20
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n=20
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
------=_Part_3774_1575444331.1497299928108
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br>Here is the wrapper script which replaces ssh: https:/=
/pastebin.com/vqXXRzb5<br><br>Am Samstag, 10. Juni 2017 11:17:04 UTC+2 schr=
ieb Guohua Li:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-l=
eft: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr"=
>Really thanks for your reply!=C2=A0<div><br></div><div>Please post the scr=
ipt when you find it.=C2=A0<div><br></div><div>Now I&#39;m testing every MP=
I version which host installed MPI version can support much MPI versions in=
side the container!=C2=A0</div></div><div><br></div><div>Hope your script c=
an solve our problem!</div><div><br></div></div><div><br clear=3D"all"><div=
><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><sp=
an style=3D"color:rgb(182,182,182);font-family:&#39;Microsoft Yahei&#39;,Ar=
ial;text-align:center">=EC=98=81=EC=9B=90=ED=9E=88=C2=A0=EC=82=B4=C2=A0=EA=
=B2=83=C2=A0=EC=B2=98=EB=9F=BC=C2=A0=EA=BF=88=EA=BE=B8=EA=B3=A0=C2=A0=EB=82=
=B4=EC=9D=BC=C2=A0=EC=A3=BD=EC=9D=84=C2=A0=EA=B2=83=C2=A0=EC=B2=98=EB=9F=BC=
=C2=A0=EC=82=B4=EC=9E=90!</span><span style=3D"color:rgb(182,182,182);font-=
family:&#39;Microsoft Yahei&#39;,Arial;text-align:center">=C2=A0</span><wbr=
>=C2=A0</div><div><font color=3D"#999999">This too shall pass away!<br></fo=
nt><br><font style=3D"background-color:rgb(255,255,255)" face=3D"&#39;comic=
 sans ms&#39;, sans-serif" color=3D"#6666cc">=E2=80=BB =EC=9D=B4=EA=B5=AD=
=ED=99=94 LI GUOHUA<br>=E2=80=BB =EA=B1=B4=EA=B5=AD=EB=8C=80=ED=95=99=EA=B5=
=90 =EB=8C=80=ED=95=99=EC=9B=90 =EC=8B=A0=EA=B8=B0=EC=88=A0=EC=9C=B5=ED=95=
=A9=ED=95=99=EA=B3=BC =EB=B0=95=EC=82=AC=EA=B3=BC=EC=A0=95</font></div><div=
><span style=3D"color:rgb(102,102,204);font-family:&#39;comic sans ms&#39;,=
sans-serif;background-color:rgb(255,255,255)">=E2=80=BB</span><font color=
=3D"#3366ff">=C2=A0</font><font color=3D"#6666cc">=EC=9C=B5=ED=95=A9 =EC=BB=
=B4=ED=93=A8=ED=8C=85 =EC=97=B0=EA=B5=AC=EC=8B=A4 (IC Lab)</font><font styl=
e=3D"background-color:rgb(255,255,255)" face=3D"&#39;comic sans ms&#39;, sa=
ns-serif" color=3D"#6666cc"><br>=E2=80=BB iIT, Department of Advanced Techn=
ology Fusion, Konkuk University<br>=E2=80=BB Industry-University Cooperatio=
n Bldg. 805</font></div>
<div><font style=3D"background-color:rgb(255,255,255)" face=3D"&#39;comic s=
ans ms&#39;, sans-serif"><font color=3D"#6666cc">=E2=80=BB KonKuk Universit=
y, Hwayang-dong, Gwangjin-Gu, Seoul 143-701 South Korea<br>=E2=80=BB Cell P=
hone: +82) 10-3666-8263</font><br><font color=3D"#9999ff">=E2=80=BB E-mail:=
 heavenkong</font><a style=3D"color:rgb(153,153,255)" href=3D"javascript:" =
target=3D"_blank" gdf-obfuscated-mailto=3D"RKUxmt-oAQAJ" rel=3D"nofollow" o=
nmousedown=3D"this.href=3D&#39;javascript:&#39;;return true;" onclick=3D"th=
is.href=3D&#39;javascript:&#39;;return true;">...@gmail.com</a></font></div=
></div></div></div></div></div></div></div>
<br><div class=3D"gmail_quote">2017-06-10 18:07 GMT+09:00 &#39;Stefan Kombr=
ink&#39; via singularity <span dir=3D"ltr">&lt;<a href=3D"javascript:" targ=
et=3D"_blank" gdf-obfuscated-mailto=3D"RKUxmt-oAQAJ" rel=3D"nofollow" onmou=
sedown=3D"this.href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.h=
ref=3D&#39;javascript:&#39;;return true;">si...@lbl.gov</a>&gt;</span>:<br>=
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">I&#39;ve got some experienc=
e on running multi node jobs with mpirun inside the containers.<br>Greg is =
right that it will be difficult to find an general purpose solution for arb=
itary HPC systems and software.<br>I made it work for two heavily used appl=
ications on our cluster one of which used IntelMPI and the other OpenMPI 1.=
6<br>The main thing was to replace /bin/ssh in order to tweak mpirun to wra=
p &quot;ssh hostname cmd&quot; to &quot;ssh &#39;singularity ContainerName =
exec cmd&#39;<br>I had to apply some more workarounds to make IB work prope=
rly (mainly envvars to configure MPI properly) but essentially the efford w=
as justifiable.<br><br>Sorry cant find the link to the ssh wrapper script r=
ight now but I can post it later when you are interested in it. It is reall=
y just a few lines of bash code.<span><font color=3D"#888888"><br><br>Stefa=
n</font></span><div><div><br><br>Am Donnerstag, 8. Juni 2017 15:17:30 UTC+2=
 schrieb Guohua Li:<blockquote class=3D"gmail_quote" style=3D"margin:0;marg=
in-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
><br><div><font size=3D"4">For our HPC system, really need run mpirun comma=
nd inside the container on multi-host.=C2=A0</font></div><div><font size=3D=
"4"><br></font></div><div><font size=3D"4">How can we fix the issues?=C2=A0=
</font></div><div><font size=3D"4"><br></font></div><div><font size=3D"4">I=
n my experience, when I build mpi job on multihost, everytime I change the =
version of MPI inside the container, I have to change the version of MPI on=
 the host.=C2=A0</font></div><div><font size=3D"4"><br></font></div><div><f=
ont size=3D"4">Is there any solution for run mpi=C2=A0command inside the co=
ntainer on multi-host system?=C2=A0</font></div><div><font size=3D"4"><br><=
/font></div><div><font size=3D"4"><br></font></div><div><font size=3D"4"><b=
r></font></div><div><font size=3D"4"><br></font></div></div></blockquote></=
div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
RKUxmt-oAQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>
</blockquote></div>
------=_Part_3774_1575444331.1497299928108--

------=_Part_3773_1482568528.1497299928108--
