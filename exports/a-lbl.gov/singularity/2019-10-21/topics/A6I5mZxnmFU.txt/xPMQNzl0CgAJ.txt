X-Received: by 2002:a63:a357:: with SMTP id v23-v6mr4842482pgn.179.1526886233300;
        Mon, 21 May 2018 00:03:53 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a63:7346:: with SMTP id d6-v6ls1197970pgn.26.gmail; Mon, 21
 May 2018 00:03:52 -0700 (PDT)
X-Received: by 2002:a62:3dc9:: with SMTP id x70-v6mr18532896pfj.85.1526886231829;
        Mon, 21 May 2018 00:03:51 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1526886231; cv=none;
        d=google.com; s=arc-20160816;
        b=ChGC6PbunYCwRXwnbxvjYp8BT14hyOnO7keR3RI0kAK/v/8x1L4ClZs74H7roeepDf
         Qn3bfaeate98xa7p4pyBHNHoGtaDlfu/lb7o6BSD8XKpEYNpkypdBPj6fnJ4/0zjL86G
         K3UguIHMCiowDaRY/+ij1ziqTIeuK8ElWk9oe+sivfUxOGgz6tHzmrl0jlSoOR61FDgx
         OJsxRAIorh3h1/eV/+scm1+z1kRDPfNjGyyF2GnF9SBwGkEpUMfHr8rKJcpUHtNU4Fnq
         u5JfAex9eDWyxuf7mO6HCNnYrMaL3rlWhHCf9QJfVV4HP84xDupm8nchZHy4kxfbKXVJ
         jipg==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=AuUjHmTNsn28aeJTtozXDlNvX7HGZVQxV4zREC7wsbg=;
        b=LXTXAQPOOj+jGZ1f02bKr2paG8JMQHf+p0YEwOloiCFeK+OPVROuSsAKpHoh8ZAHrV
         JYVJl9LaXpctNMdBwCaAjRrUWfakIX7ZA2Sv9TOnW9x1NpWHb+HgzeQgE9OixmBK23QQ
         sIKTB5i2pAHQuTtEs8BplU77Vf3BnRPW4a+OqWbR8fJYPJYkWnORkgiVKuaBnJJwqtJU
         n6uegbo1WyuWi9glbLlPDf+A6v203WQQlfFV76QsPOUXtEKFb+p1s7yfHVgMOBfrC1O0
         aNhGGf3P/cfgUTXaFRJvqIHS7hFy0pUWRwdU75rdMkFnpjB0+kZz5s6tgxSTyzQMPhCL
         A4sQ==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=JxETnGhR;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.215.52 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id h131-v6si12923327pfc.206.2018.05.21.00.03.51
        for <singu...@lbl.gov>;
        Mon, 21 May 2018 00:03:51 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 209.85.215.52 as permitted sender) client-ip=209.85.215.52;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=JxETnGhR;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.215.52 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2G7AQBxbgJbhjTXVdFcGwEBAQEDAQEBC?=
 =?us-ascii?q?QEBAYMYgQx9KINvBoEdglCEF4xygXmBD5M2FIEOAxg4AyMBDAmBSYIvRgKCFCE?=
 =?us-ascii?q?0GAECAQEBAQEBAgEBAhABAQEICwsIKCMMgjUFAgMCHgUESysBMAEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEBARQCDCFDAQEBAwEaCR0BDQ4eAwELBgMCCwYDAQEBARULCgI?=
 =?us-ascii?q?CIQEBDgMBBQELCQgOBwQBGgIEgwECKIE+AQMNCAUKimaQAjyLBYF/BQEXgnAFg?=
 =?us-ascii?q?0gKGSYNVFeCBwIBBRKII4FUP4EPgl4ugk9CAQECgRkKCAESAT8NEoJBglQCh2C?=
 =?us-ascii?q?QKRcsCYVqhW6Cf4x9iV9KhAeCODCBBBxsLnFwFTsxggIBAQENgXAkGoNOilQ9M?=
 =?us-ascii?q?AEPjWxHgXABAQ?=
X-IronPort-AV: E=Sophos;i="5.49,426,1520924400"; 
   d="scan'208,217";a="23823283"
Received: from mail-lf0-f52.google.com ([209.85.215.52])
  by fe4.lbl.gov with ESMTP; 21 May 2018 00:03:48 -0700
Received: by mail-lf0-f52.google.com with SMTP id b18-v6so22338194lfa.9
        for <singu...@lbl.gov>; Mon, 21 May 2018 00:03:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=AuUjHmTNsn28aeJTtozXDlNvX7HGZVQxV4zREC7wsbg=;
        b=JxETnGhR5yfjwudhQWPF1bUYqN3igvg8yDCzz7JOBdnU2YRKrGqXt4l862fRe+aUys
         4CJVsxWt0AADmQZwEQhYv6O6kODPvX0A0NOJdTXhFKKpX9yGOCTh2xki4s0Adfz7PPtL
         zFhqZ2SSwhUp2bXVDrL8XDM6k6y9VKfCc5vEajOhh+QxEQcbtOf88reYW5TN8Tn4t5nV
         ZX7FF1wGP5ND0DyhV9cpvYjmazMJPPGyGi0Nj/B/YFS+ZZhizBAGBZrVbCFk9G2RUXh2
         6rUn6H/z/ARNpAlwnlAyImEuYzeoSNC3Y9FA/t7M+KSx1y8fe283VU38Tx/0EPpylxIM
         WGyQ==
X-Gm-Message-State: ALKqPwf4fZjg2mz7bFrV4CP2GIqGuz6UlxmwfUqP89g2qqxrnUJSX76h
	ruujoG9G1taqhi68bG9OkY0Ew9rDURZk4gxk3ovQ9Q==
X-Received: by 2002:a19:544b:: with SMTP id i72-v6mr20985110lfb.4.1526886227232;
 Mon, 21 May 2018 00:03:47 -0700 (PDT)
MIME-Version: 1.0
Received: by 2002:a19:1891:0:0:0:0:0 with HTTP; Mon, 21 May 2018 00:03:46
 -0700 (PDT)
In-Reply-To: <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com>
References: <167671ff-657d-4560-a35f-87091223fe29@lbl.gov> <B58197C146EC324AA00A2A07DC082602C2CBA0C3@XMAIL-MBX-BT1.AD.UCSD.EDU>
 <CAB6eJZ+6PDjs3POSQPNoLdaMys5d7iYDYcUeQFpMyzJ6DEP31w@mail.gmail.com> <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com>
From: victor sv <vict...@gmail.com>
Date: Mon, 21 May 2018 09:03:46 +0200
Message-ID: <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com>
Subject: Re: [Singularity] Running an mpi program with mvapitch
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="0000000000002fcc73056cb1e703"

--0000000000002fcc73056cb1e703
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi George,

not any experience with mvapitch. I think the compiler version has no
effect here.

Jason solution and is a good starting point to check if the container MPI
works in a single node (not in several nodes).

To run the hybrid MPI approach you should take into account that both
version and vendor of MPI and PMI must match. Can you check if PMI
libraries match?

BR,
V=C3=ADctor.

2018-05-18 20:14 GMT+02:00 Jason Stover <jason...@gmail.com>:

> Hi George,
>
>   Can you run it from inside the container? For example:
>
>     singularity exec
> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1
> mpi-pi.o
>
> -J
>
>
> On Fri, May 18, 2018 at 12:56 PM, George Zaki <georg...@gmail.com>
> wrote:
> > Thanks Marty
> >
> > Below are the values I got, any obvious mismatch? I git this working fi=
ne
> > with OpenMPI.
> >
> > Here is also what I try to run:
> >
> > singularity exec /data/zakigf/candle/swift-hypervisor-horovod-mvapich.
> simg
> > mpicc mpi-pi.c -o  mpi-pi.o
> >
> > [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec
> > /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o
> >
> > Then I kill after no response:
> >
> > ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested
> >
> > [mpiexec@cn2360] Press Ctrl-C again to force abort
> >
> > [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error
> (Bad
> > file descriptor)
> >
> > [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal
> (pm/pmiserv/pmiserv_cb.c:169):
> > unable to write data to proxy
> >
> > [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to
> send
> > signal downstream
> >
> > [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event
> > (tools/demux/demux_poll.c:76): callback returned error status
> >
> > [mpiexec@cn2360] HYD_pmci_wait_for_completion
> > (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event
> >
> > [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error
> > waiting for completion
> >
> >
> > Now about the versions:
> >
> > Singularity: Invoking an interactive shell within container...
> >
> >
> > Singularity swift-hypervisor-horovod-mvapich.simg:~> mpiexec --version
> >
> > HYDRA build details:
> >
> >     Version:                                 3.1.4
> >
> >     Release Date:                            Wed Sep  7 14:33:43 EDT 20=
16
> >
> >     CC:                              gcc
> >
> >     CXX:                             g++
> >
> >     F77:                             gfortran
> >
> >     F90:                             gfortran
> >
> >     Configure options:                       '--disable-option-checking=
'
> > '--prefix=3DNONE' '--cache-file=3D/dev/null' '--srcdir=3D.' 'CC=3Dgcc' =
'CFLAGS=3D
> > -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,-rpath,/l=
ib
> > -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad -libumad -libverb=
s
> -ldl
> > -lrt -lm -lpthread ' 'CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/include
> > -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/src
> > -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT
> > -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include
> -I/include
> > -I/include'
> >
> >     Process Manager:                         pmi
> >
> >     Launchers available:                     ssh rsh fork slurm ll lsf
> sge
> > manual persist
> >
> >     Topology libraries available:            hwloc
> >
> >     Resource management kernels available:   user slurm ll lsf sge pbs
> > cobalt
> >
> >     Checkpointing libraries available:
> >
> >     Demux engines available:                 poll select
> >
> > Singularity swift-hypervisor-horovod-mvapich.simg:~> gcc --version
> >
> > gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
> >
> > Copyright (C) 2015 Free Software Foundation, Inc.
> >
> > This is free software; see the source for copying conditions.  There is
> NO
> >
> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
> PURPOSE.
> >
> >
> > Singularity swift-hypervisor-horovod-mvapich.simg:~> exit
> >
> > exit
> >
> >
> > [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0
> >
> > [+] Loading mvapich2 2.2 for GCC 5.3.0
> >
> > [zakigf@cn2360 ~]$ gcc --version
> >
> > gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
> >
> > Copyright (C) 2010 Free Software Foundation, Inc.
> >
> > This is free software; see the source for copying conditions.  There is
> NO
> >
> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
> PURPOSE.
> >
> >
> > [zakigf@cn2360 ~]$ mpiexec --version
> >
> > HYDRA build details:
> >
> >     Version:                                 3.1.4
> >
> >     Release Date:                            Wed Sep  7 14:33:43 EDT 20=
16
> >
> >     CC:                              /usr/local/GCC/5.3.0/bin/gcc
> >
> >     CXX:                             /usr/local/GCC/5.3.0/bin/g++
> >
> >     F77:                             /usr/local/GCC/5.3.0/bin/gfortran
> >
> >     F90:                             /usr/local/GCC/5.3.0/bin/gfortran
> >
> >     Configure options:                       '--disable-option-checking=
'
> > '--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0'
> > '--with-slurm-lib=3D/usr/local/slurm/lib'
> > '--with-slurm-include=3D/usr/local/slurm/include' '--enable-debug=3Dnon=
e'
> > '--enable-timing=3Druntime' 'CC=3D/usr/local/GCC/5.3.0/bin/gcc'
> > 'CXX=3D/usr/local/GCC/5.3.0/bin/g++' 'FC=3D/usr/local/GCC/5.3.0/bin/
> gfortran'
> > 'F77=3D/usr/local/GCC/5.3.0/bin/gfortran' '--cache-file=3D/dev/null'
> > '--srcdir=3D.' 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -=
L/lib
> > -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
> 'LIBS=3D-libmad
> > -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src
> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT
> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include -I/include
> > -I/include -I/include -I/include'
> >
> >     Process Manager:                         pmi
> >
> >     Launchers available:                     ssh rsh fork slurm ll lsf
> sge
> > manual persist
> >
> >     Topology libraries available:            hwloc
> >
> >     Resource management kernels available:   user slurm ll lsf sge pbs
> > cobalt
> >
> >     Checkpointing libraries available:
> >
> >     Demux engines available:                 poll select
> >
> >
> > On Fri, May 18, 2018 at 1:31 PM Kandes, Martin <mka...@sdsc.edu> wrote:
> >>
> >> Hi George,
> >>
> >> I run with different gcc compiler versions inside and outside my MPI
> >> containers. So I would be surprised if that is the issue here. I'm not
> sure
> >> I have a good recommendation of where to start debugging your problem.
> But I
> >> might start by double checking the MPI versions match inside and
> outside the
> >> container. e.g. see [1].
> >>
> >> Marty
> >>
> >> [1]
> >>
> >> [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1
> >> --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/bash
> >> srun: job 16364303 queued and waiting for resources
> >> srun: job 16364303 has been allocated resources
> >> [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/
> >> [mkandes@comet-14-06 16364303]$ cp
> >> /oasis/scratch/comet/mkandes/temp_project/singularity/
> images/ubuntu-mvapich2.img
> >> ./
> >> [mkandes@comet-14-06 16364303]$ module purge
> >> [mkandes@comet-14-06 16364303]$ module load gnu
> >> [mkandes@comet-14-06 16364303]$ module load mvapich2_ib
> >> [mkandes@comet-14-06 16364303]$ module list
> >> Currently Loaded Modulefiles:
> >>   1) gnu/4.9.2         2) mvapich2_ib/2.1
> >> [mkandes@comet-14-06 16364303]$ module load singularity
> >> [mkandes@comet-14-06 16364303]$ gcc --version
> >> gcc (GCC) 4.9.2
> >> Copyright (C) 2014 Free Software Foundation, Inc.
> >> This is free software; see the source for copying conditions.  There i=
s
> NO
> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
> >> PURPOSE.
> >>
> >> [mkandes@comet-14-06 16364303]$ mpirun --version
> >> HYDRA build details:
> >>     Version:                                 3.1.4
> >>     Release Date:                            Thu Apr  2 17:15:15 EDT
> 2015
> >>     CC:                              gcc  -fPIC -O3
> >>     CXX:                             g++  -fPIC -O3
> >>     F77:                             gfortran -fPIC -O3
> >>     F90:                             gfortran -fPIC -O3
> >>     Configure options:                       '--disable-option-checkin=
g'
> >> '--prefix=3D/opt/mvapich2/gnu/ib' '--enable-shared'
> '--enable-sharedlibs=3Dgcc'
> >> '--with-hwloc' '--enable-f77' '--enable-fc' '--enable-hybrid'
> >> '--with-ib-include=3D/usr/include/infiniband'
> '--with-ib-libpath=3D/usr/lib64'
> >> '--enable-fast=3DO3'
> >> '--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/
> sdsc-mvapich2_gnu_ib-2.1/../..//cache/build-limic'
> >> '--with-slurm=3D/usr/lib64/slurm' '--with-file-system=3Dlustre' 'CC=3D=
gcc'
> >> 'CFLAGS=3D-fPIC -O3 -O3' 'CXX=3Dg++' 'CXXFLAGS=3D-fPIC -O3 -O3' 'FC=3D=
gfortran'
> >> 'FCFLAGS=3D-fPIC -O3 -O3' 'F77=3Dgfortran' 'FFLAGS=3D-L/usr/lib64 -L/l=
ib
> -L/lib
> >> -fPIC -O3 -O3' '--cache-file=3D/dev/null' '--srcdir=3D.'
> 'LDFLAGS=3D-L/usr/lib64
> >> -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr/lib=
64
> >> -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/../..//cache/build-limic/lib
> >> -L/lib -L/lib' 'LIBS=3D-libmad -lrdmacm -libumad -libverbs -ldl -lrt
> -llimic2
> >> -lm -lpthread ' 'CPPFLAGS=3D-I/usr/include/infiniband
> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/mvapich2-2.1/src/mpl/include
> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/mvapich2-2.1/src/mpl/include
> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/mvapich2-2.1/src/openpa/src
> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/mvapich2-2.1/src/openpa/src
> >> -D_REENTRANT
> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/mvapich2-2.1/src/mpi/romio/include
> >> -I/include -I/include -I/usr/include/infiniband
> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
> ib-2.1/../..//cache/build-limic/include
> >> -I/include -I/include'
> >>     Process Manager:                         pmi
> >>     Launchers available:                     ssh rsh fork slurm ll lsf
> sge
> >> manual persist
> >>     Topology libraries available:            hwloc
> >>     Resource management kernels available:   user slurm ll lsf sge pbs
> >> cobalt
> >>     Checkpointing libraries available:
> >>     Demux engines available:                 poll select
> >> [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.img
> >> Singularity: Invoking an interactive shell within container...
> >>
> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> gcc
> --version
> >> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
> >> Copyright (C) 2015 Free Software Foundation, Inc.
> >> This is free software; see the source for copying conditions.  There i=
s
> NO
> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
> >> PURPOSE.
> >>
> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> mpirun
> >> --version
> >> HYDRA build details:
> >>     Version:                                 3.1.4
> >>     Release Date:                            Thu Apr  2 17:15:15 EDT
> 2015
> >>     CC:                              gcc
> >>     CXX:                             g++
> >>     F77:                             gfortran
> >>     F90:                             gfortran
> >>     Configure options:                       '--disable-option-checkin=
g'
> >> '--prefix=3D/opt/mvapich2' '--cache-file=3D/dev/null' '--srcdir=3D.' '=
CC=3Dgcc'
> >> 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib
> >> -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad
> -lrdmacm
> >> -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
> >> -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/includ=
e
> >> -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/src
> >> -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include
> -I/include
> >> -I/include -I/include'
> >>     Process Manager:                         pmi
> >>     Launchers available:                     ssh rsh fork slurm ll lsf
> sge
> >> manual persist
> >>     Topology libraries available:            hwloc
> >>     Resource management kernels available:   user slurm ll lsf sge pbs
> >> cobalt
> >>     Checkpointing libraries available:
> >>     Demux engines available:                 poll select
> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303>
> >>
> >>
> >> ________________________________
> >> From: George Zaki [georg...@gmail.com]
> >> Sent: Friday, May 18, 2018 6:48 AM
> >> To: singularity
> >> Subject: [Singularity] Running an mpi program with mvapitch
> >>
> >> Hi singularity team,
> >>
> >>
> >> I would like to run an MPI program in a singularity container. The
> program
> >> is compiled using mvapicth2.2 using a gcc version 5.4.
> >>
> >>
> >> I can see that my cluster has a compiled version of mvapitch2.2 with g=
cc
> >> 5.3
> >>
> >>
> >> When I run:
> >>
> >>
> >> mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o
> >>
> >> the call does not return.
> >>
> >>
> >>
> >> Does the gcc version has to be exactly the same? I tried the switch th=
e
> >> compiler in this image:
> >>
> >>
> >> BootStrap: docker
> >> From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04
> >>
> >>
> >> However when gcc 5.3 is used the mvapitch does not build correctly.
> >>
> >>
> >> If that's the problem, Is there a preferred method of switching gcc
> >> version in this container singularity container?
> >>
> >>
> >> Thanks,
> >> George
> >>
> >> --
> >> You received this message because you are subscribed to the Google
> Groups
> >> "singularity" group.
> >> To unsubscribe from this group and stop receiving emails from it, send
> an
> >> email to singu...@lbl.gov.
> >>
> >> --
> >> You received this message because you are subscribed to a topic in the
> >> Google Groups "singularity" group.
> >> To unsubscribe from this topic, visit
> >> https://groups.google.com/a/lbl.gov/d/topic/singularity/
> A6I5mZxnmFU/unsubscribe.
> >> To unsubscribe from this group and all its topics, send an email to
> >> singu...@lbl.gov.
> >
> > --
> > You received this message because you are subscribed to the Google Grou=
ps
> > "singularity" group.
> > To unsubscribe from this group and stop receiving emails from it, send =
an
> > email to singu...@lbl.gov.
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--0000000000002fcc73056cb1e703
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div><div>Hi George,<br><br></div>not any experi=
ence with mvapitch. I think the compiler version has no effect here. <br><b=
r>Jason solution and is a good starting point to check if the container MPI=
 works in a single node (not in several nodes).<br><br></div>To run the hyb=
rid MPI approach you should take into account that both version and vendor =
of MPI and PMI must match. Can you check if PMI libraries match?<br><br></d=
iv>BR,<br></div>V=C3=ADctor.<br></div><div class=3D"gmail_extra"><br><div c=
lass=3D"gmail_quote">2018-05-18 20:14 GMT+02:00 Jason Stover <span dir=3D"l=
tr">&lt;<a href=3D"mailto:jason...@gmail.com" target=3D"_blank">jason...@gm=
ail.com</a>&gt;</span>:<br><blockquote class=3D"gmail_quote" style=3D"margi=
n:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi George,<br>
<br>
=C2=A0 Can you run it from inside the container? For example:<br>
<br>
=C2=A0 =C2=A0 singularity exec<br>
/data/zakigf/candle/swift-<wbr>hypervisor-horovod-mvapich.<wbr>simg mpiexec=
 -n 1<br>
mpi-pi.o<br>
<span class=3D"HOEnZb"><font color=3D"#888888"><br>
-J<br>
</font></span><div class=3D"HOEnZb"><div class=3D"h5"><br>
<br>
On Fri, May 18, 2018 at 12:56 PM, George Zaki &lt;<a href=3D"mailto:georg..=
.@gmail.com">georg...@gmail.com</a>&gt; wrote:<br>
&gt; Thanks Marty<br>
&gt;<br>
&gt; Below are the values I got, any obvious mismatch? I git this working f=
ine<br>
&gt; with OpenMPI.<br>
&gt;<br>
&gt; Here is also what I try to run:<br>
&gt;<br>
&gt; singularity exec /data/zakigf/candle/swift-<wbr>hypervisor-horovod-mva=
pich.<wbr>simg<br>
&gt; mpicc mpi-pi.c -o=C2=A0 mpi-pi.o<br>
&gt;<br>
&gt; [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec<br>
&gt; /data/zakigf/candle/swift-<wbr>hypervisor-horovod-mvapich.<wbr>simg mp=
i-pi.o<br>
&gt;<br>
&gt; Then I kill after no response:<br>
&gt;<br>
&gt; ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested<br>
&gt;<br>
&gt; [mpiexec@cn2360] Press Ctrl-C again to force abort<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error =
(Bad<br>
&gt; file descriptor)<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal (pm/pmiserv/pmiserv_cb.c=
:169):<br>
&gt; unable to write data to proxy<br>
&gt;<br>
&gt; [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79)<wbr>: unable=
 to send<br>
&gt; signal downstream<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event<br>
&gt; (tools/demux/demux_poll.c:76): callback returned error status<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmci_wait_for_completion<br>
&gt; (pm/pmiserv/pmiserv_pmci.c:<wbr>198): error waiting for event<br>
&gt;<br>
&gt; [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error<=
br>
&gt; waiting for completion<br>
&gt;<br>
&gt;<br>
&gt; Now about the versions:<br>
&gt;<br>
&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; mpiexec -=
-version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3DNONE&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--sr=
cdir=3D.&#39; &#39;CC=3Dgcc&#39; &#39;CFLAGS=3D<br>
&gt; -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,=
-rpath,/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -libumad=
 -libverbs -ldl<br>
&gt; -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/=
<wbr>include<br>
&gt; -I/tmp/mvapich2-2.2/src/mpl/<wbr>include -I/tmp/mvapich2-2.2/src/<wbr>=
openpa/src<br>
&gt; -I/tmp/mvapich2-2.2/src/<wbr>openpa/src -D_REENTRANT<br>
&gt; -I/tmp/mvapich2-2.2/src/mpi/<wbr>romio/include -I/include -I/include -=
I/include<br>
&gt; -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; gcc --ver=
sion<br>
&gt;<br>
&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;<br>
&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; exit<br>
&gt;<br>
&gt; exit<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0<br>
&gt;<br>
&gt; [+] Loading mvapich2 2.2 for GCC 5.3.0<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ gcc --version<br>
&gt;<br>
&gt; gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)<br>
&gt;<br>
&gt; Copyright (C) 2010 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ mpiexec --version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 /usr/local/GCC/5.3=
.0/bin/gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/<wbr>gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/<wbr>gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3D/usr/local/MVAPICH2/<wbr>2.2/gcc-5.3.0&#39;<br>
&gt; &#39;--with-slurm-lib=3D/usr/local/<wbr>slurm/lib&#39;<br>
&gt; &#39;--with-slurm-include=3D/usr/<wbr>local/slurm/include&#39; &#39;--=
enable-debug=3Dnone&#39;<br>
&gt; &#39;--enable-timing=3Druntime&#39; &#39;CC=3D/usr/local/GCC/5.3.0/bin=
/<wbr>gcc&#39;<br>
&gt; &#39;CXX=3D/usr/local/GCC/5.3.0/bin/<wbr>g++&#39; &#39;FC=3D/usr/local=
/GCC/5.3.0/bin/<wbr>gfortran&#39;<br>
&gt; &#39;F77=3D/usr/local/GCC/5.3.0/bin/<wbr>gfortran&#39; &#39;--cache-fi=
le=3D/dev/null&#39;<br>
&gt; &#39;--srcdir=3D.&#39; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#=
39;LDFLAGS=3D-L/lib -L/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;=
LIBS=3D-libmad<br>
&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/openpa/src<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/openpa/src -D_REENTRAN=
T<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpi/romio/<wbr>include=
 -I/include<br>
&gt; -I/include -I/include -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt;<br>
&gt; On Fri, May 18, 2018 at 1:31 PM Kandes, Martin &lt;<a href=3D"mailto:m=
ka...@sdsc.edu">mka...@sdsc.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi George,<br>
&gt;&gt;<br>
&gt;&gt; I run with different gcc compiler versions inside and outside my M=
PI<br>
&gt;&gt; containers. So I would be surprised if that is the issue here. I&#=
39;m not sure<br>
&gt;&gt; I have a good recommendation of where to start debugging your prob=
lem. But I<br>
&gt;&gt; might start by double checking the MPI versions match inside and o=
utside the<br>
&gt;&gt; container. e.g. see [1].<br>
&gt;&gt;<br>
&gt;&gt; Marty<br>
&gt;&gt;<br>
&gt;&gt; [1]<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1<=
br>
&gt;&gt; --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash<br>
&gt;&gt; srun: job 16364303 queued and waiting for resources<br>
&gt;&gt; srun: job 16364303 has been allocated resources<br>
&gt;&gt; [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ cp<br>
&gt;&gt; /oasis/scratch/comet/mkandes/<wbr>temp_project/singularity/<wbr>im=
ages/ubuntu-mvapich2.img<br>
&gt;&gt; ./<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module purge<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load gnu<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load mvapich2_ib<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module list<br>
&gt;&gt; Currently Loaded Modulefiles:<br>
&gt;&gt;=C2=A0 =C2=A01) gnu/4.9.2=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A02) mvapi=
ch2_ib/2.1<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load singularity<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ gcc --version<br>
&gt;&gt; gcc (GCC) 4.9.2<br>
&gt;&gt; Copyright (C) 2014 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ mpirun --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc=C2=A0 -f=
PIC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++=C2=A0 -fP=
IC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2/gnu/<wbr>ib&#39; &#39;--enable-share=
d&#39; &#39;--enable-sharedlibs=3Dgcc&#39;<br>
&gt;&gt; &#39;--with-hwloc&#39; &#39;--enable-f77&#39; &#39;--enable-fc&#39=
; &#39;--enable-hybrid&#39;<br>
&gt;&gt; &#39;--with-ib-include=3D/usr/<wbr>include/infiniband&#39; &#39;--=
with-ib-libpath=3D/usr/lib64&#39;<br>
&gt;&gt; &#39;--enable-fast=3DO3&#39;<br>
&gt;&gt; &#39;--with-limic2=3D/state/<wbr>partition1/git/mpi-roll/BUILD/<wb=
r>sdsc-mvapich2_gnu_ib-2.1/../..<wbr>//cache/build-limic&#39;<br>
&gt;&gt; &#39;--with-slurm=3D/usr/lib64/<wbr>slurm&#39; &#39;--with-file-sy=
stem=3Dlustre&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D-fPIC -O3 -O3&#39; &#39;CXX=3Dg++&#39; &#39;CXXFLAGS=
=3D-fPIC -O3 -O3&#39; &#39;FC=3Dgfortran&#39;<br>
&gt;&gt; &#39;FCFLAGS=3D-fPIC -O3 -O3&#39; &#39;F77=3Dgfortran&#39; &#39;FF=
LAGS=3D-L/usr/lib64 -L/lib -L/lib<br>
&gt;&gt; -fPIC -O3 -O3&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--srcdi=
r=3D.&#39; &#39;LDFLAGS=3D-L/usr/lib64<br>
&gt;&gt; -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr=
/lib64<br>
&gt;&gt; -L/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/../..//cache/build-<wbr>limic/lib<br>
&gt;&gt; -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -lrdmacm -libumad -libverbs=
 -ldl -lrt -llimic2<br>
&gt;&gt; -lm -lpthread &#39; &#39;CPPFLAGS=3D-I/usr/include/<wbr>infiniband=
<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpl/<wbr>include<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpl/<wbr>include<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/<wbr>openpa/src<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/<wbr>openpa/src<br>
&gt;&gt; -D_REENTRANT<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpi/<wbr>romio/include<br>
&gt;&gt; -I/include -I/include -I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/../..//cache/build-<wbr>limic/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.=
img<br>
&gt;&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
 gcc --version<br>
&gt;&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
 mpirun<br>
&gt;&gt; --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2&#39; &#39;--cache-file=3D/dev/null&#=
39; &#39;--srcdir=3D.&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib=
 -L/lib -L/lib<br>
&gt;&gt; -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIB=
S=3D-libmad -lrdmacm<br>
&gt;&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<=
br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/mpl/<wbr>include -I/tmp/mvapich2-2.1/src/m=
pl/<wbr>include<br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/<wbr>openpa/src -I/tmp/mvapich2-2.1/src/<w=
br>openpa/src<br>
&gt;&gt; -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/<wbr>romio/include -I/inc=
lude -I/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ______________________________<wbr>__<br>
&gt;&gt; From: George Zaki [<a href=3D"mailto:georg...@gmail.com">georg...@=
gmail.com</a>]<br>
&gt;&gt; Sent: Friday, May 18, 2018 6:48 AM<br>
&gt;&gt; To: singularity<br>
&gt;&gt; Subject: [Singularity] Running an mpi program with mvapitch<br>
&gt;&gt;<br>
&gt;&gt; Hi singularity team,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I would like to run an MPI program in a singularity container. The=
 program<br>
&gt;&gt; is compiled using mvapicth2.2 using a gcc version 5.4.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I can see that my cluster has a compiled version of mvapitch2.2 wi=
th gcc<br>
&gt;&gt; 5.3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; When I run:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o<br>
&gt;&gt;<br>
&gt;&gt; the call does not return.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Does the gcc version has to be exactly the same? I tried the switc=
h the<br>
&gt;&gt; compiler in this image:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; BootStrap: docker<br>
&gt;&gt; From: nvidia/cuda:8.0-cudnn6-devel-<wbr>ubuntu16.04<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However when gcc 5.3 is used the mvapitch does not build correctly=
.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; If that&#39;s the problem, Is there a preferred method of switchin=
g gcc<br>
&gt;&gt; version in this container singularity container?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; George<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov">singularity+un=
subscribe@lbl.<wbr>gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to a topic in=
 the<br>
&gt;&gt; Google Groups &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this topic, visit<br>
&gt;&gt; <a href=3D"https://groups.google.com/a/lbl.gov/d/topic/singularity=
/A6I5mZxnmFU/unsubscribe" rel=3D"noreferrer" target=3D"_blank">https://grou=
ps.google.com/a/<wbr>lbl.gov/d/topic/singularity/<wbr>A6I5mZxnmFU/unsubscri=
be</a>.<br>
&gt;&gt; To unsubscribe from this group and all its topics, send an email t=
o<br>
&gt;&gt; <a href=3D"mailto:singularity%...@lbl.gov">singularity+unsubscribe=
@lbl.<wbr>gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov">singularity+unsubs=
cribe@lbl.<wbr>gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov">singularity+unsubscribe@=
lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--0000000000002fcc73056cb1e703--
