X-Received: by 2002:a17:902:8a8f:: with SMTP id p15mr831596plo.75.1551406304821;
        Thu, 28 Feb 2019 18:11:44 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a63:f10:: with SMTP id e16ls2718280pgl.3.gmail; Thu, 28 Feb
 2019 18:11:43 -0800 (PST)
X-Received: by 2002:a62:20c9:: with SMTP id m70mr2947434pfj.118.1551406303447;
        Thu, 28 Feb 2019 18:11:43 -0800 (PST)
ARC-Seal: i=1; a=rsa-sha256; t=1551406303; cv=none;
        d=google.com; s=arc-20160816;
        b=A2rxJVpKz/2Wbe0Lc+JSkR7aEzcE1eSLKrCuszQydbeOiquuYTI6BrgFm/vI2f6J2f
         yUXwqGJqe/Tgv5POaUnEOfakULc5tnUl7KR6pPwGVVKb+kM8cCSLyZ9UqOiQSMlvMRLh
         6QpLGCv397rXULo8RCSH2Cl1Xu5IjGV86hwOcq2UodWr7wI9mjaG54Yny8bpjDfIILJK
         7I2zUEj9My9QN0W6r1j8IY3nSf5n1VDB4cHUehfN07kkditIfTxOIBT0mAdnywriKzFV
         csoFqOvRbu3ULni1o8sZKXfhTU9YZV4HHTQi1ZOOa3DysyNyGFJg5GKfcwqd+r4RhHKp
         kT8A==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature;
        bh=rWQIVv7SFC/4cg323FIHfwPcxY9X+sOh7RuW/Bmgfm0=;
        b=pVXPYvP84l1mZnrpcHX95X2MiyhNTJpyQQWs4EHtU/s0KjL0FE2baH4XsL5WH+mHiV
         w/kxk7w8zGqDsQjwVWpBrB/deEfUyb50caPUowdLmaKfM41CegyniD588vHpLYRTXYhq
         gnZhUQJauOi9PR6kVcxoX8QjmwEYyYEgAqNzYgsrygMTD4Z2nVmie4YEl5EyRrjrCHOw
         vVzeL+vTAMw4Yrdb+Va3TT3fhD45kYa7+kuqUciesdS8Sphvm/yBExcwZ/hnn3rO4iWh
         IoT1tfteH4jnhzLvZhq21Fzfzz52wD+QPEqGpmbqA5JFX9C+4/nur+7hM9tCzIIXVFQs
         BGbg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=G2kHFdPG;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.160.169 as permitted sender) smtp.mailfrom=gmku...@gmail.com
Return-Path: <gmku...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [131.243.228.52])
        by mx.google.com with ESMTP id h78si20397444pfj.70.2019.02.28.18.11.43
        for <singu...@lbl.gov>;
        Thu, 28 Feb 2019 18:11:43 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@gmail.com designates 209.85.160.169 as permitted sender) client-ip=209.85.160.169;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=G2kHFdPG;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.160.169 as permitted sender) smtp.mailfrom=gmku...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2GrCACClHhchqmgVdFlgheBVgVRgUQnh?=
 =?us-ascii?q?AEHgR2CXo9ugWsFHYk/iG+FchSBKzwlARWBSYJ1AoQVIjQJDQEDAQEDAQMCAQE?=
 =?us-ascii?q?CEAEBAQgLCwgbDiMMgjoFAgMfBw5NOzABAQEBAQEBAQEBHwItKQEaAQICASMEG?=
 =?us-ascii?q?QENDh4DAQsGBQs3AgIhAQEOAwEFARwGCAcEAQcVBIJ/JAGBNQEDDQgFoBo8iV0?=
 =?us-ascii?q?agSZ8FgUBF4J5BYRCChknDV+BNwIGEoMfiRcXPoFBg3UugleBdwESAYMpgjUiA?=
 =?us-ascii?q?ooKh1WRWjMHAgKLM4N4gz0Zkx+RY4srMIEmbjBxcIEBgjuCKA4JE4M4inQhMBC?=
 =?us-ascii?q?NVkeBdwEB?=
X-IronPort-AV: E=Sophos;i="5.58,425,1544515200"; 
   d="scan'208,217";a="145460328"
Received: from mail-qt1-f169.google.com ([209.85.160.169])
  by fe3.lbl.gov with ESMTP; 28 Feb 2019 18:11:42 -0800
Received: by mail-qt1-f169.google.com with SMTP id p48so26106918qtk.2
        for <singu...@lbl.gov>; Thu, 28 Feb 2019 18:11:42 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=rWQIVv7SFC/4cg323FIHfwPcxY9X+sOh7RuW/Bmgfm0=;
        b=G2kHFdPGJrF2DPD6h4Q774+UBUNWVyGnzwoE4YSvp9bRqzR7Nq3EUllXQlGaCc2UCh
         xeE7FSqqYRerOSm69Vxl5d8ttYrb0zwVFZV0xHynKGIpZ5fXmhWMglcVHDmofUwJc1e3
         9ID8YKAh8Ptk/BxVFo01jELoOI5UkUN+bXExJF6V9qad9fUsPcEbECfbLmbz1xZnHO9A
         Fka89xE9P1MdX9ykmaWl72No0QLgjWUF+ma012D+BglMz35cgBSifeUN+NsUlB+Q79Yk
         3cbDXs/aEV871DgRMBvjod+eL8VGyM0xZo/KDe6dC6D+ZbSI0LUZ82zJsxebBoreAFsG
         NGGg==
X-Gm-Message-State: APjAAAVMaOy5zVuTXmeBXx79WeYHeuLBgbnuibyqgK7QdqZjS5SIOcDy
	tuoShiOE9tVfwDi5I8i/SYRo7E9SL+MqrSPcI4vzKp0k
X-Received: by 2002:ac8:37d5:: with SMTP id e21mr2061677qtc.214.1551406301166;
 Thu, 28 Feb 2019 18:11:41 -0800 (PST)
MIME-Version: 1.0
References: <a94c4768-8cd7-43aa-8bb9-b21b3ccf950c@lbl.gov> <CAApQTTh9E=oNwfTLFmjmH6rAXDH2Q+X29D0Ur_Y3VMXvGvsDtg@mail.gmail.com>
 <710b4b5e-f209-41da-bfcb-c19043201fa4@lbl.gov>
In-Reply-To: <710b4b5e-f209-41da-bfcb-c19043201fa4@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@gmail.com>
Date: Thu, 28 Feb 2019 18:11:29 -0800
Message-ID: <CAApQTTj0BV3GfLJbhH7uT4Deo28W9fnPP0tWB1DU42gVO5NwNA@mail.gmail.com>
Subject: Re: [Singularity] Singularity image download syncronization during
 multiple job startup
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary="0000000000007bc8a00582feedd0"

--0000000000007bc8a00582feedd0
Content-Type: text/plain; charset="UTF-8"

Hi Mike,

I'd have the same recommendation, of first downloading and caching the
container image, and then executing it in parallel, which is what most
others are doing in my experience.

$ singularity pull ~/container.sif shub://sregistry/image
$ qsub -q all.q -t1-500 singularity exec ~/container.sif <application>

Not sure if that is possible given your workload, but that would be the
easiest and most performant way to handle this. If not, let me know, happy
to think through this in other ways.

Greg

On Thu, Feb 28, 2019 at 8:36 AM Mike Moore <wxdu...@gmail.com> wrote:

> Hi Greg,
>
>   So, our current workloads are batch jobs. There is very little MPI at
> all currently. We use Univa Grid Engine for our scheduler. Our users submit
> job arrays with 100->1000s of tasks per job.  Each task is similar, just on
> different data sets. We do have multiple (up to 20) tasks that can start on
> a node almost simultaneously.
>
>   We are deploying singularity images to convert our very large and
> customized bare metal image into a container. This is the first step in
> decoupling our applications from our bare metal installation and start a
> path to computing in the cloud. We went with Singularity because our shared
> environment makes docker a non-starter with our security dept.
>
>   I have been trying to get Sregistry working fast enough to move the
> project out of a POC for functional testing and into production use.
> However, the time to download the image to the system is unacceptably
> slow.  I've patched Singularity 3.1.0 to address the missing Shub local
> caching support, which makes everything better if the image is locally
> cached.  If the image is not cached and begins to download by the first
> task on a node, any additional tasks that are released to the node will
> fail when starting singularity because the image is incomplete. I have test
> cases (up to 500 tasks) with only a handful of tasks successfully
> completing because the first tasks are downloading the image and the
> additional tasks die because the download is in progress.  Our image is on
> the order of 1.5-9.0GB depending on the software included and the function
> of the image. So, time to download the image is significant.
>
>   So, the way we have essentially been running our jobs are:
>
> qsub -q all.q -t1-500 singularity run shub://sregistry/image <application>
>
> Once the scheduler releases the job to the compute node, it starts
> singularity which downloads image from sregistry. We have written a wrapper
> that we will be using to force all jobs to run in singularity once our
> users have completed acceptance testing. But the issue of singularity
> crashing because the cached image is incomplete is a blocker.
>
> I was trying to get an idea of what others were doing for storing and
> starting their containers in their HPC environments and if they have a
> similar issue with trying to start multiple instances of a container on the
> same system at the same time if the container is not cached yet.
>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>


-- 
Gregory M. Kurtzer
CEO, Sylabs Inc.

--0000000000007bc8a00582feedd0
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Mike,<div><br></div><div>I&#39;d have the same recommen=
dation, of first downloading and caching the container image, and then exec=
uting it in parallel, which is what most others are doing in my experience.=
=C2=A0</div><div><br></div><div>$ singularity pull ~/container.sif shub://s=
registry/image</div><div>$ qsub -q all.q -t1-500 singularity exec ~/contain=
er.sif &lt;application&gt;</div><div><br></div><div>Not sure if that is pos=
sible given your workload, but that would be the easiest and most performan=
t way to handle this. If not, let me know, happy to think through this in o=
ther ways.</div><div><br></div><div>Greg</div></div><br><div class=3D"gmail=
_quote"><div dir=3D"ltr" class=3D"gmail_attr">On Thu, Feb 28, 2019 at 8:36 =
AM Mike Moore &lt;<a href=3D"mailto:wxdu...@gmail.com">wxdu...@gmail.com</a=
>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px=
 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><di=
v dir=3D"ltr">Hi Greg,<br><br>=C2=A0 So, our current workloads are batch jo=
bs. There is very little MPI at all currently. We use Univa Grid Engine for=
 our scheduler. Our users submit job arrays with 100-&gt;1000s of tasks per=
 job.=C2=A0 Each task is similar, just on different data sets. We do have m=
ultiple (up to 20) tasks that can start on a node almost simultaneously. <b=
r><br>=C2=A0 We are deploying singularity images to convert our very large =
and customized bare metal image into a container. This is the first step in=
 decoupling our applications from our bare metal installation and start a p=
ath to computing in the cloud. We went with Singularity because our shared =
environment makes docker a non-starter with our security dept. <br><br>=C2=
=A0 I have been trying to get Sregistry working fast enough to move the pro=
ject out of a POC for functional testing and into production use. However, =
the time to download the image to the system is unacceptably slow.=C2=A0 I&=
#39;ve patched Singularity 3.1.0 to address the missing Shub local caching =
support, which makes everything better if the image is locally cached.=C2=
=A0 If the image is not cached and begins to download by the first task on =
a node, any additional tasks that are released to the node will fail when s=
tarting singularity because the image is incomplete. I have test cases (up =
to 500 tasks) with only a handful of tasks successfully completing because =
the first tasks are downloading the image and the additional tasks die beca=
use the download is in progress.=C2=A0 Our image is on the order of 1.5-9.0=
GB depending on the software included and the function of the image. So, ti=
me to download the image is significant. <br><br>=C2=A0 So, the way we have=
 essentially been running our jobs are:<br><br>qsub -q all.q -t1-500 singul=
arity run shub://sregistry/image &lt;application&gt;<br><br>Once the schedu=
ler releases the job to the compute node, it starts singularity which downl=
oads image from sregistry. We have written a wrapper that we will be using =
to force all jobs to run in singularity once our users have completed accep=
tance testing. But the issue of singularity crashing because the cached ima=
ge is incomplete is a blocker.<br><br>I was trying to get an idea of what o=
thers were doing for storing and starting their containers in their HPC env=
ironments and if they have a similar issue with trying to start multiple in=
stances of a container on the same system at the same time if the container=
 is not cached yet.<br><br><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Ku=
rtzer</div><div>CEO, Sylabs Inc.</div></div></div></div></div></div></div><=
/div></div></div></div>

--0000000000007bc8a00582feedd0--
