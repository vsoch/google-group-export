Date: Fri, 29 Apr 2016 12:21:16 -0700 (PDT)
From: Oleksandr Moskalenko <moska...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <170441c6-e26e-46f5-89cc-bb7c9f9b2894@lbl.gov>
Subject: HPC usage: centralized cache location
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_6_1831963862.1461957676513"

------=_Part_6_1831963862.1461957676513
Content-Type: multipart/alternative; 
	boundary="----=_Part_7_1584927628.1461957676513"

------=_Part_7_1584927628.1461957676513
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

First of all, singularity is a great idea and we'd love to adopt it. I've=
=20
built our first container and am trying to get it to run as shown in a=20
github issue I opened. However, in addition to run-time issues I have some=
=20
questions that pertain to using singularity in a shared cluster environment=
.

It looks like singularity creates a container cache when 'singularity=20
install som.sapp' is run in $HOME/.singularity-cache. However in our usage=
=20
module /home directories are small and application or job data are=20
relegated to high-performance networked filesystems. We also use=20
environment modules to provide access to software. In a nutshell, the=20
singularity configuration for any given containerized application we'd like=
=20
to have would be something along the lines of

/apps/singularity/cache/

$ tree -L 2 apps/singularity/cache/*
/apps/singularity/cache/containers
=E2=94=94=E2=94=80=E2=94=80 d3ddf754-e3e4-4aa9-a2c6-fe2b21f59daf
    =E2=94=9C=E2=94=80=E2=94=80 build_dist
    =E2=94=9C=E2=94=80=E2=94=80 build_kernel
    =E2=94=9C=E2=94=80=E2=94=80 c
    =E2=94=9C=E2=94=80=E2=94=80 files
    =E2=94=9C=E2=94=80=E2=94=80 installed
    =E2=94=9C=E2=94=80=E2=94=80 maintainer
    =E2=94=9C=E2=94=80=E2=94=80 name
    =E2=94=9C=E2=94=80=E2=94=80 spec
    =E2=94=9C=E2=94=80=E2=94=80 summary
    =E2=94=94=E2=94=80=E2=94=80 uuid
/apps/singularity/cache/locks
=E2=94=94=E2=94=80=E2=94=80 d3ddf754-e3e4-4aa9-a2c6-fe2b21f59daf.lock
/apps/singularity/cache/names
=E2=94=94=E2=94=80=E2=94=80 xpra
/apps/singularity/cache/runkeys

2 directories, 11 file

which would be run by singularity after an environment module for that=20
application is loaded.


I am also wondering about the 'locks' sub-directory. If we're running a=20
containerized application just for portability why would we want to lock=20
the container? It's all read-only stuff anyway. Shouldn't there be a build=
=20
argument that creates 'unlocked' containers?

Thanks,

Alex

------=_Part_7_1584927628.1461957676513
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">First of all, singularity is a great idea and we&#39;d lov=
e to adopt it. I&#39;ve built our first container and am trying to get it t=
o run as shown in a github issue I opened. However, in addition to run-time=
 issues I have some questions that pertain to using singularity in a shared=
 cluster environment.<br><br>It looks like singularity creates a container =
cache when &#39;singularity install som.sapp&#39; is run in $HOME/.singular=
ity-cache. However in our usage module /home directories are small and appl=
ication or job data are relegated to high-performance networked filesystems=
. We also use environment modules to provide access to software. In a nutsh=
ell, the singularity configuration for any given containerized application =
we&#39;d like to have would be something along the lines of<br><br>/apps/si=
ngularity/cache/<br><br>$ tree -L 2 apps/singularity/cache/*<br>/apps/singu=
larity/cache/containers<br>=E2=94=94=E2=94=80=E2=94=80 d3ddf754-e3e4-4aa9-a=
2c6-fe2b21f59daf<br>=C2=A0=C2=A0=C2=A0 =E2=94=9C=E2=94=80=E2=94=80 build_di=
st<br>=C2=A0=C2=A0=C2=A0 =E2=94=9C=E2=94=80=E2=94=80 build_kernel<br>=C2=A0=
=C2=A0=C2=A0 =E2=94=9C=E2=94=80=E2=94=80 c<br>=C2=A0=C2=A0=C2=A0 =E2=94=9C=
=E2=94=80=E2=94=80 files<br>=C2=A0=C2=A0=C2=A0 =E2=94=9C=E2=94=80=E2=94=80 =
installed<br>=C2=A0=C2=A0=C2=A0 =E2=94=9C=E2=94=80=E2=94=80 maintainer<br>=
=C2=A0=C2=A0=C2=A0 =E2=94=9C=E2=94=80=E2=94=80 name<br>=C2=A0=C2=A0=C2=A0 =
=E2=94=9C=E2=94=80=E2=94=80 spec<br>=C2=A0=C2=A0=C2=A0 =E2=94=9C=E2=94=80=
=E2=94=80 summary<br>=C2=A0=C2=A0=C2=A0 =E2=94=94=E2=94=80=E2=94=80 uuid<br=
>/apps/singularity/cache/locks<br>=E2=94=94=E2=94=80=E2=94=80 d3ddf754-e3e4=
-4aa9-a2c6-fe2b21f59daf.lock<br>/apps/singularity/cache/names<br>=E2=94=94=
=E2=94=80=E2=94=80 xpra<br>/apps/singularity/cache/runkeys<br><br>2 directo=
ries, 11 file<br><br>which would be run by singularity after an environment=
 module for that application is loaded.<br><br><br>I am also wondering abou=
t the &#39;locks&#39; sub-directory. If we&#39;re running a containerized a=
pplication just for portability why would we want to lock the container? It=
&#39;s all read-only stuff anyway. Shouldn&#39;t there be a build argument =
that creates &#39;unlocked&#39; containers?<br><br>Thanks,<br><br>Alex<br><=
/div>
------=_Part_7_1584927628.1461957676513--

------=_Part_6_1831963862.1461957676513--
