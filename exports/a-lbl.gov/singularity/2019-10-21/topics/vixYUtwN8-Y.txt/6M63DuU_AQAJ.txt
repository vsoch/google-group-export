X-Received: by 10.129.33.4 with SMTP id h4mr12803781ywh.24.1468531812330;
        Thu, 14 Jul 2016 14:30:12 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.167.3 with SMTP id q3ls976486ioe.49.gmail; Thu, 14 Jul
 2016 14:30:11 -0700 (PDT)
X-Received: by 10.98.64.193 with SMTP id f62mr14708911pfd.141.1468531811752;
        Thu, 14 Jul 2016 14:30:11 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id q11si5724181pfd.42.2016.07.14.14.30.11
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 14 Jul 2016 14:30:11 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) client-ip=74.125.82.72;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FtAADNA4hXf0hSfUpUCIJwgSR8BoM2qRuMGYF7hhkCgSgHOBQBAQEBAQEBAw8BAQkLCwkfMYRcAQEEARIIAQgrMAsJAgsNIAEJAgIhAQ8DAQUBCxEGCAcEARwEAYd0Aw8IBZUNj0KBMT4xizuKHw2EDwEBCAEBAQEBIhCKZ4JDgU8MBQGDHYI9HQWIFQdfhQx1P4QkhQw0AYt/Q4IWgWsXh3CFP4ZdgUOGOhIegQ8egj8cgWwcMgeGMoE1AQEB
X-IronPort-AV: E=Sophos;i="5.28,364,1464678000"; 
   d="scan'208,217";a="29693987"
Received: from mail-wm0-f72.google.com ([74.125.82.72])
  by fe4.lbl.gov with ESMTP; 14 Jul 2016 14:30:07 -0700
Received: by mail-wm0-f72.google.com with SMTP id o80so2051935wme.1
        for <singu...@lbl.gov>; Thu, 14 Jul 2016 14:30:07 -0700 (PDT)
X-Gm-Message-State: ALyK8tKZ+wiWEASmu/hkT3orSYtVEmN2pXqIvXLBuCICils0KmHkyljf48J36WFYZZmPcoOarv9A9wf4n6jjRPKu8x5RhIc2c51Shjf/1oMbiuALLa4M2+AwcMSObaERWHbBDqlP4EH+9bj5e8/DB/10Il0=
X-Received: by 10.25.87.130 with SMTP id l124mr8647656lfb.170.1468531806705;
        Thu, 14 Jul 2016 14:30:06 -0700 (PDT)
X-Received: by 10.25.87.130 with SMTP id l124mr8647646lfb.170.1468531806424;
 Thu, 14 Jul 2016 14:30:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.158 with HTTP; Thu, 14 Jul 2016 14:30:05 -0700 (PDT)
In-Reply-To: <c4e5864c-cbaa-4269-9522-db61d63d7cff@lbl.gov>
References: <03a19fb0-27ce-43c4-9400-8e58cf726500@lbl.gov> <CAN7etTwRbSe1MMh9wdQAMYoKVJTb_SGJeHto+WrZ=aU7NoBmhQ@mail.gmail.com>
 <90295845-ad9b-4670-97f2-91a76798ef5d@lbl.gov> <CAN7etTyqGkWy1P57-cVJgyru5BT_DvnhwDzLe1p38BV8z_PPww@mail.gmail.com>
 <66b82c74-2778-44f2-ae5c-87e01ec8885d@lbl.gov> <CAN7etTz09uzfP-NpZ3-+HnijfrC+u+=pOuBQDTShXG+unxgOVg@mail.gmail.com>
 <c4e5864c-cbaa-4269-9522-db61d63d7cff@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Thu, 14 Jul 2016 14:30:05 -0700
Message-ID: <CAN7etTztBSE1YXY3etq9ipMNnPSKh2Eatz5i-QQOH=ecdNDVCg@mail.gmail.com>
Subject: Re: [Singularity] Image is locked by another process
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a1141f9eea9f07a05379f3465

--001a1141f9eea9f07a05379f3465
Content-Type: text/plain; charset=UTF-8

Very odd...

I'm concerned about the Buffer I/O errors. It is almost like the image
itself has a problem with it. Did you copy this image from another system?
I wonder if it is the sparseness...

Does the directory exist for /proc/2299/ ?

Can you kill it with a -9?

On Thu, Jul 14, 2016 at 2:27 PM, Steve Mehlberg <sgmeh...@gmail.com>
wrote:

> Seems that process isn't running any more.  I did find this in the
> /var/log/messages file:
>
> [root@mach0 ~]# cat /var/log/messages | grep 2299
> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> Command=exec,
> Container=c7, CWD=/root, Arg1=/usr/bin/ring
> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1, logical
> block 22299
> Jul 14 11:37:17 mach0 kernel: Buffer I/O error on device loop1, logical
> block 22990
> ...
>
> [root@mach0 ~]# ps -ef |grep 2299
> root     10545  2002  0 14:15 pts/0    00:00:00 grep --color=auto 2299
> [root@mach0 ~]# ps |grep 2299
>
> On Thursday, July 14, 2016 at 2:59:26 PM UTC-6, Gregory M. Kurtzer wrote:
>
>> What is running at PID 2299?
>>
>> On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <sg...@gmail.com>
>> wrote:
>>
>>> Ok, I see my error with shell vs exec.  I'm running on a VM with my own
>>> space, no NFS involved.  Here is the debug command run and lsofl
>>>
>>> [root@mach0 ~]# df -h
>>> Filesystem             Size  Used Avail Use% Mounted on
>>> /dev/mapper/vg1-lv001  5.7G  4.3G  1.1G  80% /
>>> devtmpfs               911M     0  911M   0% /dev
>>> tmpfs                  920M     0  920M   0% /dev/shm
>>> tmpfs                  920M   41M  880M   5% /run
>>> tmpfs                  920M     0  920M   0% /sys/fs/cgroup
>>> /dev/vda1              190M  110M   67M  63% /boot
>>> /dev/mapper/vg1-lv002   12G  203M   11G   2% /var
>>> tmpfs                  184M     0  184M   0% /run/user/0
>>> [root@mach0 ~]# singularity --debug shell -w c7
>>> enabling debugging
>>> ending argument loop
>>> Exec'ing: /usr/local/libexec/singularity/cli/shell.exec -w+ '[' -f
>>> /usr/local/etc/singularity/init ']'
>>> + . /usr/local/etc/singularity/init
>>> ++ unset module
>>> ++
>>> PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/usr/bin:/usr/sbin
>>> ++ HISTFILE=/dev/null
>>> ++ export PATH HISTFILE
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> ++ '[' -n '' ']'
>>> + true
>>> + case ${1:-} in
>>> + shift
>>> + SINGULARITY_WRITABLE=1
>>> + export SINGULARITY_WRITABLE
>>> + true
>>> + case ${1:-} in
>>> + break
>>> + '[' -z c7 ']'
>>> + SINGULARITY_IMAGE=c7
>>> + export SINGULARITY_IMAGE
>>> + shift
>>> + exec /usr/local/libexec/singularity/sexec
>>> VERBOSE [U=0,P=8045]       message.c:52:init()                        :
>>> Set messagelevel to: 5
>>> DEBUG   [U=0,P=8045]       sexec.c:127:main()                         :
>>> Gathering and caching user info.
>>> DEBUG   [U=0,P=8045]       privilege.c:43:get_user_privs()            :
>>> Called get_user_privs(struct s_privinfo *uinfo)
>>> DEBUG   [U=0,P=8045]       privilege.c:54:get_user_privs()            :
>>> Returning get_user_privs(struct s_privinfo *uinfo) = 0
>>> DEBUG   [U=0,P=8045]       sexec.c:134:main()                         :
>>> Checking if we can escalate privs properly.
>>> DEBUG   [U=0,P=8045]       privilege.c:61:escalate_privs()            :
>>> Called escalate_privs(void)
>>> DEBUG   [U=0,P=8045]       privilege.c:73:escalate_privs()            :
>>> Returning escalate_privs(void) = 0
>>> DEBUG   [U=0,P=8045]       sexec.c:141:main()                         :
>>> Setting privs to calling user
>>> DEBUG   [U=0,P=8045]       privilege.c:79:drop_privs()                :
>>> Called drop_privs(struct s_privinfo *uinfo)
>>> DEBUG   [U=0,P=8045]       privilege.c:87:drop_privs()                :
>>> Dropping privileges to GID = '0'
>>> DEBUG   [U=0,P=8045]       privilege.c:93:drop_privs()                :
>>> Dropping privileges to UID = '0'
>>> DEBUG   [U=0,P=8045]       privilege.c:103:drop_privs()               :
>>> Confirming we have correct GID
>>> DEBUG   [U=0,P=8045]       privilege.c:109:drop_privs()               :
>>> Confirming we have correct UID
>>> DEBUG   [U=0,P=8045]       privilege.c:115:drop_privs()               :
>>> Returning drop_privs(struct s_privinfo *uinfo) = 0
>>> DEBUG   [U=0,P=8045]       sexec.c:146:main()                         :
>>> Obtaining user's homedir
>>> DEBUG   [U=0,P=8045]       sexec.c:150:main()                         :
>>> Obtaining file descriptor to current directory
>>> DEBUG   [U=0,P=8045]       sexec.c:155:main()                         :
>>> Getting current working directory path string
>>> DEBUG   [U=0,P=8045]       sexec.c:161:main()                         :
>>> Obtaining SINGULARITY_COMMAND from environment
>>> DEBUG   [U=0,P=8045]       sexec.c:168:main()                         :
>>> Obtaining SINGULARITY_IMAGE from environment
>>> DEBUG   [U=0,P=8045]       sexec.c:174:main()                         :
>>> Checking container image is a file: c7
>>> DEBUG   [U=0,P=8045]       sexec.c:180:main()                         :
>>> Building configuration file location
>>> DEBUG   [U=0,P=8045]       sexec.c:183:main()                         :
>>> Config location: /usr/local/etc/singularity/singularity.conf
>>> DEBUG   [U=0,P=8045]       sexec.c:185:main()                         :
>>> Checking Singularity configuration is a file:
>>> /usr/local/etc/singularity/singularity.conf
>>> DEBUG   [U=0,P=8045]       sexec.c:191:main()                         :
>>> Checking Singularity configuration file is owned by root
>>> DEBUG   [U=0,P=8045]       sexec.c:197:main()                         :
>>> Opening Singularity configuration file
>>> DEBUG   [U=0,P=8045]       sexec.c:210:main()                         :
>>> Checking Singularity configuration for 'sessiondir prefix'
>>> DEBUG   [U=0,P=8045]       config_parser.c:47:config_get_key_value()  :
>>> Called config_get_key_value(fp, sessiondir prefix)
>>> DEBUG   [U=0,P=8045]       config_parser.c:66:config_get_key_value()  :
>>> Return config_get_key_value(fp, sessiondir prefix) = NULL
>>> DEBUG   [U=0,P=8045]       file.c:48:file_id()                        :
>>> Called file_id(c7)
>>> VERBOSE [U=0,P=8045]       file.c:58:file_id()                        :
>>> Generated file_id: 0.64768.26052
>>> DEBUG   [U=0,P=8045]       file.c:60:file_id()                        :
>>> Returning file_id(c7) = 0.64768.26052
>>> DEBUG   [U=0,P=8045]       sexec.c:217:main()                         :
>>> Set sessiondir to: /tmp/.singularity-session-0.64768.26052
>>> DEBUG   [U=0,P=8045]       sexec.c:221:main()                         :
>>> Set containername to: c7
>>> DEBUG   [U=0,P=8045]       sexec.c:223:main()                         :
>>> Setting loop_dev_* paths
>>> DEBUG   [U=0,P=8045]       config_parser.c:47:config_get_key_value()  :
>>> Called config_get_key_value(fp, container dir)
>>> DEBUG   [U=0,P=8045]       config_parser.c:58:config_get_key_value()  :
>>> Return config_get_key_value(fp, container dir) = /var/singularity/mnt
>>> DEBUG   [U=0,P=8045]       sexec.c:232:main()                         :
>>> Set image mount path to: /var/singularity/mnt
>>> LOG     [U=0,P=8045]       sexec.c:234:main()                         :
>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>> DEBUG   [U=0,P=8045]       sexec.c:247:main()                         :
>>> Set prompt to: Singularity.c7>
>>> DEBUG   [U=0,P=8045]       sexec.c:249:main()                         :
>>> Checking if we are opening image as read/write
>>> DEBUG   [U=0,P=8045]       sexec.c:264:main()                         :
>>> Opening image as read/write only: c7
>>> DEBUG   [U=0,P=8045]       sexec.c:271:main()                         :
>>> Setting exclusive lock on file descriptor: 6
>>> ERROR   [U=0,P=8045]       sexec.c:273:main()                         :
>>> Image is locked by another process
>>> [root@mach0 ~]# lsof c7
>>> [root@mach0 ~]# lslocks
>>> COMMAND           PID  TYPE SIZE MODE  M START END PATH
>>> crond             601 FLOCK   4B WRITE 0     0   0 /run/crond.pid
>>> master            826 FLOCK  33B WRITE 0     0   0
>>> /var/spool/postfix/pid/master.pid
>>> master            826 FLOCK  33B WRITE 0     0   0
>>> /var/lib/postfix/master.lock
>>> lvmetad           478 POSIX   4B WRITE 0     0   0 /run/lvmetad.pid
>>> slurmctld         940 POSIX   4B WRITE 0     0   0 /run/slurmctld.pid
>>> slurmd            966 POSIX   4B WRITE 0     0   0 /run/slurmd.pid
>>> (unknown)        2299 FLOCK   0B READ  0     0   0 /
>>>
>>>
>>> On Thursday, July 14, 2016 at 1:03:50 PM UTC-6, Gregory M. Kurtzer wrote:
>>>
>>>>
>>>>
>>>> On Thu, Jul 14, 2016 at 11:50 AM, Steve Mehlberg <sg...@gmail.com>
>>>> wrote:
>>>>
>>>>> Gregory,
>>>>>
>>>>> Thanks for the quick suggestions.  There don't seem to be any
>>>>> processes attached to the container and I can't seem to run other commands
>>>>> (in read mode). I'm not sure what's going on.
>>>>>
>>>>> Just lazy with root right now, I need to create a normal uid.
>>>>>
>>>>
>>>> Ahh, ok. Been there, done that! lol
>>>>
>>>>
>>>>>
>>>>> Steve
>>>>>
>>>>> [root@mach0 ~]# ls -la c7
>>>>> -rwxr-xr-x 1 root root 1610612769 Jul 14 10:49 c7
>>>>> [root@mach0 ~]# singularity shell -w c7
>>>>> ERROR  : Image is locked by another process
>>>>>
>>>>
>>>> Can you run this command again in --debug mode (singularity --debug
>>>> ....)
>>>>
>>>>
>>>>> [root@mach0 ~]# lsof c7
>>>>>
>>>>
>>>> What about the command "lslocks"?
>>>>
>>>>
>>>>> [root@mach0 ~]# singularity shell c7 whoami
>>>>> /usr/bin/whoami: /usr/bin/whoami: cannot execute binary file
>>>>>
>>>>
>>>> Ahh, this is normal. You are asking the shell script to read in
>>>> /usr/bin/whoami. If you want to use shell to run whoami, you must prefix it
>>>> with the -c (e.g. -c "whoami [args]"), or use the 'exec' Singularity
>>>> subcommand: "singularity exec c7 whoami"
>>>>
>>>>
>>>>> [root@mach0 ~]#
>>>>> [root@mach0 ~]# ps -ef |grep c7
>>>>> root      7479  2002  0 11:49 pts/0    00:00:00 grep --color=auto c7
>>>>> [root@mach0 ~]#
>>>>>
>>>>
>>>> What kind of file system does your image exist on? Is it NFS by chance?
>>>> I wonder if there is a host issue with a locking daemon or something else
>>>> weird going on where it is not giving the exclusive lock properly. If this
>>>> is NFS or other non local file system, can you copy the image to /tmp,
>>>> rerun the MPI command to get it to fail again, and try again?
>>>>
>>>> Thanks!
>>>>
>>>>
>>>>
>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gregory M. Kurtzer
>>>>> wrote:
>>>>>>
>>>>>> Hi Steve,
>>>>>>
>>>>>> That means there is an active file descriptor/process still running
>>>>>> and attached to the container maintaining a shared lock. You can run other
>>>>>> commands against the container as long as long as the container is not
>>>>>> being requested as --writable(-w), because that will try and obtain an
>>>>>> exclusive lock and it will fail if there are any active shared locks. Try
>>>>>> an "lsof /path/to/c7" to see what processes are attached to it. You may see
>>>>>> a list like:
>>>>>>
>>>>>> # lsof /tmp/Demo-2.img
>>>>>> COMMAND    PID USER   FD   TYPE DEVICE   SIZE/OFF      NODE NAME
>>>>>> sexec   107975 root    6rR  REG  253,0 1073741856 202112247
>>>>>> /tmp/Demo-2.img
>>>>>> sexec   107977 root    6r   REG  253,0 1073741856 202112247
>>>>>> /tmp/Demo-2.img
>>>>>> bash    107982 root    6r   REG  253,0 1073741856 202112247
>>>>>> /tmp/Demo-2.img
>>>>>>
>>>>>> Notice the two top ones are 'sexec' which are part of the Singularity
>>>>>> process stack. Kill the bottom one, and those should go away naturally.
>>>>>>
>>>>>> BTW, as long as you have installed Singularity as root, there is no
>>>>>> need to run Singularity commands as root (unless you want to make system
>>>>>> changes within the container).
>>>>>>
>>>>>> Hope that helps!
>>>>>>
>>>>>>
>>>>>> On Thu, Jul 14, 2016 at 10:56 AM, Steve Mehlberg <sg...@gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Running mpirun tests, when an abort occurs, my image ends up
>>>>>>> locked.  Is there a way to clear the lock without rebooting?  I looked for
>>>>>>> processes that I could kill, but didn't see anything worthy.
>>>>>>>
>>>>>>> I'm using singularity v2.1 on Centos 7.2 (both host and container).
>>>>>>>
>>>>>>> Regards,
>>>>>>>
>>>>>>> Steve
>>>>>>>
>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 2 -H mach1,mach2
>>>>>>> singularity exec c7 /usr/bin/ring
>>>>>>> Process 0 sending 10 to 1, tag 201 (2 processes in ring)
>>>>>>> Process 0 sent to 1
>>>>>>> Process 0 decremented value: 9
>>>>>>> Process 0 decremented value: 8
>>>>>>> Process 0 decremented value: 7
>>>>>>> Process 0 decremented value: 6
>>>>>>> Process 0 decremented value: 5
>>>>>>> Process 0 decremented value: 4
>>>>>>> Process 0 decremented value: 3
>>>>>>> Process 0 decremented value: 2
>>>>>>> Process 0 decremented value: 1
>>>>>>> Process 0 decremented value: 0
>>>>>>> Process 0 exiting
>>>>>>> Process 1 exiting
>>>>>>> [root@mach0 ~]# mpirun --allow-run-as-root -n 3 -H
>>>>>>> mach0,mach1,mach2 singularity exec c7 /usr/bin/ring
>>>>>>>
>>>>>>> --------------------------------------------------------------------------
>>>>>>> It appears as if there is not enough space for
>>>>>>> /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory
>>>>>>> backing
>>>>>>> file). It is likely that your MPI job will now either abort or
>>>>>>> experience
>>>>>>> performance degradation.
>>>>>>>
>>>>>>>   Local host:  mach0
>>>>>>>   Space Requested: 4194312 B
>>>>>>>   Space Available: 0 B
>>>>>>>
>>>>>>> --------------------------------------------------------------------------
>>>>>>> [mach0:02308] create_and_attach: unable to create shared memory BTL
>>>>>>> coordinating structure :: size 134217728
>>>>>>> [mach0:02291] 2 more processes have sent help message
>>>>>>> help-opal-shmem-mmap.txt / target full
>>>>>>> [mach0:02291] Set MCA parameter "orte_base_help_aggregate" to 0 to
>>>>>>> see all help / error messages
>>>>>>> ^CKilled by signal 2.
>>>>>>> Killed by signal 2.
>>>>>>> Singularity is sending SIGKILL to child pid: 2308
>>>>>>> Singularity is sending SIGKILL to child pid: 2309
>>>>>>> [warn] Epoll ADD(4) on fd 31 failed.  Old events were 0; read change
>>>>>>> was 0 (none); write change was 1 (add): Bad file descriptor
>>>>>>> ^C[root@mach0 ~]singularity shell -w c7
>>>>>>> ERROR  : Image is locked by another process
>>>>>>> [root@mach0 ~]# tail -30 /var/log/messages
>>>>>>> Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon management.
>>>>>>> Jul 14 10:42:17 mach0 systemd: Reached target Multi-User System.
>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Multi-User System.
>>>>>>> Jul 14 10:42:17 mach0 systemd: Starting Update UTMP about System
>>>>>>> Runlevel Changes...
>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Stop Read-Ahead Data
>>>>>>> Collection 10s After Completed Startup.
>>>>>>> Jul 14 10:42:17 mach0 systemd: Started Update UTMP about System
>>>>>>> Runlevel Changes.
>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded kdump kernel
>>>>>>> Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]
>>>>>>> Jul 14 10:42:18 mach0 systemd: Started Crash recovery kernel arming.
>>>>>>> Jul 14 10:42:18 mach0 systemd: Startup finished in 415ms (kernel) +
>>>>>>> 1.100s (initrd) + 4.931s (userspace) = 6.446s.
>>>>>>> Jul 14 10:42:34 mach0 systemd: Created slice user-0.slice.
>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting user-0.slice.
>>>>>>> Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user root.
>>>>>>> Jul 14 10:42:34 mach0 systemd: Started Session 1 of user root.
>>>>>>> Jul 14 10:42:34 mach0 systemd: Starting Session 1 of user root.
>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2023)> Command=exec,
>>>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Command=exec,
>>>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>> Jul 14 10:42:36 mach0 Singularity: sexec (U=0,P=2024)> Image is
>>>>>>> locked by another process
>>>>>>> Jul 14 10:42:36 mach0 kernel: loop: module loaded
>>>>>>> Jul 14 10:43:38 mach0 Singularity: sexec (U=0,P=2050)>
>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>> Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted filesystem
>>>>>>> with ordered data mode. Opts: discard
>>>>>>> Jul 14 10:49:17 mach0 Singularity: sexec (U=0,P=2203)>
>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>> Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted filesystem
>>>>>>> with ordered data mode. Opts: discard
>>>>>>> Jul 14 10:50:39 mach0 Singularity: sexec (U=0,P=2244)> Command=exec,
>>>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>> Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted filesystem
>>>>>>> with ordered data mode. Opts: discard
>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2299)> Command=exec,
>>>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>> Jul 14 10:51:34 mach0 Singularity: sexec (U=0,P=2300)> Command=exec,
>>>>>>> Container=c7, CWD=/root, Arg1=/usr/bin/ring
>>>>>>> Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted filesystem
>>>>>>> with ordered data mode. Opts: discard
>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)>
>>>>>>> Command=shell, Container=c7, CWD=/root, Arg1=(null)
>>>>>>> Jul 14 10:51:57 mach0 Singularity: sexec (U=0,P=2322)> Image is
>>>>>>> locked by another process
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> You received this message because you are subscribed to the Google
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Gregory M. Kurtzer
>>>>>> High Performance Computing Services (HPCS)
>>>>>> University of California
>>>>>> Lawrence Berkeley National Laboratory
>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> High Performance Computing Services (HPCS)
>>>> University of California
>>>> Lawrence Berkeley National Laboratory
>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a1141f9eea9f07a05379f3465
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Very odd...<div><br></div><div>I&#39;m concerned about the=
 Buffer I/O errors. It is almost like the image itself has a problem with i=
t. Did you copy this image from another system? I wonder if it is the spars=
eness...</div><div><br></div><div>Does the directory exist for /proc/2299/ =
?</div><div><br></div><div>Can you kill it with a -9?</div></div><div class=
=3D"gmail_extra"><br><div class=3D"gmail_quote">On Thu, Jul 14, 2016 at 2:2=
7 PM, Steve Mehlberg <span dir=3D"ltr">&lt;<a href=3D"mailto:sgmeh...@gmail=
.com" target=3D"_blank">sgmeh...@gmail.com</a>&gt;</span> wrote:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc =
solid;padding-left:1ex"><div dir=3D"ltr"><div>Seems that process isn&#39;t =
running any more.=C2=A0 I did find this in the /var/log/messages file:</div=
><div><br></div><div><font face=3D"courier new,monospace">[root@mach0 ~]# c=
at /var/log/messages | grep 2299<span class=3D""><br>Jul 14 10:51:34 mach0 =
Singularity: sexec (U=3D0,P=3D2299)&gt; Command=3Dexec, Container=3Dc7, CWD=
=3D/root, Arg1=3D/usr/bin/ring<br></span>Jul 14 11:37:17 mach0 kernel: Buff=
er I/O error on device loop1, logical block 22299<br>Jul 14 11:37:17 mach0 =
kernel: Buffer I/O error on device loop1, logical block 22990<br>...</font>=
</div><div><font face=3D"courier new,monospace"><br></font></div><div><font=
 face=3D"courier new,monospace">[root@mach0 ~]# ps -ef |grep 2299<br>root=
=C2=A0=C2=A0=C2=A0=C2=A0 10545=C2=A0 2002=C2=A0 0 14:15 pts/0=C2=A0=C2=A0=
=C2=A0 00:00:00 grep --color=3Dauto 2299<br>[root@mach0 ~]# ps |grep 2299</=
font><br></div><span class=3D""><div><br>On Thursday, July 14, 2016 at 2:59=
:26 PM UTC-6, Gregory M. Kurtzer wrote:</div></span><blockquote class=3D"gm=
ail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-c=
olor:rgb(204,204,204);border-left-width:1px;border-left-style:solid"><span =
class=3D""><div dir=3D"ltr">What is running at PID=C2=A0<span style=3D"font=
-size:12.8px">2299?</span></div></span><div><div class=3D"h5"><div><br><div=
 class=3D"gmail_quote">On Thu, Jul 14, 2016 at 1:35 PM, Steve Mehlberg <spa=
n dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a>&gt;</span> wrote:=
<br><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;pad=
ding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;bord=
er-left-style:solid"><div dir=3D"ltr">Ok, I see my error with shell vs exec=
.=C2=A0 I&#39;m running on a VM with my own space, no NFS involved.=C2=A0 H=
ere is the debug command run and lsofl<br><br>[root@mach0 ~]# df -h<br>File=
system=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 Size=C2=A0 Used Avail Use% Mounted on<br>/dev/mapper/vg1-lv001=C2=A0 5.=
7G=C2=A0 4.3G=C2=A0 1.1G=C2=A0 80% /<br>devtmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 911M=C2=A0=C2=A0=
=C2=A0=C2=A0 0=C2=A0 911M=C2=A0=C2=A0 0% /dev<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 920M=C2=A0=C2=A0 0% /dev/shm<br=
>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0 41M=C2=A0 880M=C2=A0=C2=
=A0 5% /run<br>tmpfs=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 920M=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0 920M=C2=A0=C2=A0 0% /sys/fs/cgroup<br>/dev/vda1=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 190M=C2=A0 =
110M=C2=A0=C2=A0 67M=C2=A0 63% /boot<br>/dev/mapper/vg1-lv002=C2=A0=C2=A0 1=
2G=C2=A0 203M=C2=A0=C2=A0 11G=C2=A0=C2=A0 2% /var<br>tmpfs=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 184M=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0 184M=C2=A0=C2=A0 0% /run/=
user/0<br>[root@mach0 ~]# singularity --debug shell -w c7<br>enabling debug=
ging<br>ending argument loop<br>Exec&#39;ing: /usr/local/libexec/singularit=
y/cli/shell.exec -w+ &#39;[&#39; -f /usr/local/etc/singularity/init &#39;]&=
#39;<br>+ . /usr/local/etc/singularity/init<br>++ unset module<br>++ PATH=
=3D/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/bin:/sbin:/=
usr/bin:/usr/sbin<br>++ HISTFILE=3D/dev/null<br>++ export PATH HISTFILE<br>=
++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#=
39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n =
&#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#=
39;[&#39; -n &#39;&#39; &#39;]&#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&=
#39;<br>++ &#39;[&#39; -n &#39;&#39; &#39;]&#39;<br>+ true<br>+ case ${1:-}=
 in<br>+ shift<br>+ SINGULARITY_WRITABLE=3D1<br>+ export SINGULARITY_WRITAB=
LE<br>+ true<br>+ case ${1:-} in<br>+ break<br>+ &#39;[&#39; -z c7 &#39;]&#=
39;<br>+ SINGULARITY_IMAGE=3Dc7<br>+ export SINGULARITY_IMAGE<br>+ shift<br=
>+ exec /usr/local/libexec/singularity/sexec<br>VERBOSE [U=3D0,P=3D8045]=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 message.c:52:init()=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Set messagelevel to: 5<br>DEBU=
G=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:=
127:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Gathering and caching user info.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8=
045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:43:get_user_privs()=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called ge=
t_user_privs(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:54:get_user_privs()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning ge=
t_user_privs(struct s_privinfo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:134:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking if we =
can escalate privs properly.<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:61:escalate_privs()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called escalate_privs=
(void)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 privilege.c:73:escalate_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning escalate_privs(void) =3D 0<br>DE=
BUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.=
c:141:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Setting privs to calling user<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045=
]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:79:drop_privs()=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Called drop_privs(struct s_privinfo *uinfo)<br>DEBUG=C2=A0=C2=A0 [=
U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege.c:87:drop_pri=
vs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Dropping privileges to GID =3D &#39;0&#39;<br>DEBUG=
=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 privilege=
.c:93:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Dropping privileges to UID =3D &#39;0&#=
39;<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 privilege.c:103:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct GID=
<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 =
privilege.c:109:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Confirming we have correct UID<br=
>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 pri=
vilege.c:115:drop_privs()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning drop_privs(struct s_privin=
fo *uinfo) =3D 0<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 sexec.c:146:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining user&#39;s homedir<br>DEBUG=C2=
=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:150:=
main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 : Obtaining file descriptor to current directory<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:155:main()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Getting c=
urrent working directory path string<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:161:main()=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY=
_COMMAND from environment<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:168:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Obtaining SINGULARITY_IMAGE from =
environment<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 sexec.c:174:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking container image is a file: c7<br>DEB=
UG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c=
:180:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 : Building configuration file location<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:183:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Config location=
: /usr/local/etc/singularity/singularity.conf<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:185:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking Singul=
arity configuration is a file: /usr/local/etc/singularity/singularity.conf<=
br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 s=
exec.c:191:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 : Checking Singularity configuration file is owned by root<br>=
DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexe=
c.c:197:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 : Opening Singularity configuration file<br>DEBUG=C2=A0=C2=A0 [U=
=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:210:main()=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Checking =
Singularity configuration for &#39;sessiondir prefix&#39;<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:47=
:config_get_key_value()=C2=A0 : Called config_get_key_value(fp, sessiondir =
prefix)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 config_parser.c:66:config_get_key_value()=C2=A0 : Return config_get_=
key_value(fp, sessiondir prefix) =3D NULL<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8=
045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:48:file_id()=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Called file_id(c7)<br>VE=
RBOSE [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:58:file_i=
d()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Genera=
ted file_id: 0.64768.26052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 file.c:60:file_id()=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Returning file_id(c7) =3D 0.64768.26=
052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:217:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Set sessiondir to: /tmp/.singularity-session-0.64768.2=
6052<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:221:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Set containername to: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:223:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting loop_de=
v_* paths<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 config_parser.c:47:config_get_key_value()=C2=A0 : Called config_g=
et_key_value(fp, container dir)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 config_parser.c:58:config_get_key_value()=C2=
=A0 : Return config_get_key_value(fp, container dir) =3D /var/singularity/m=
nt<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:232:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Set image mount path to: /var/singularity/mnt<br>LOG=
=C2=A0=C2=A0=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 sexec.c:234:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 : Command=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(=
null)<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 sexec.c:247:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 : Set prompt to: Singularity.c7&gt;<br>DEBUG=C2=A0=C2=
=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:249:main()=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Che=
cking if we are opening image as read/write<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=
=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sexec.c:264:main()=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Opening image a=
s read/write only: c7<br>DEBUG=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 sexec.c:271:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 : Setting exclusive lock on file descr=
iptor: 6<br>ERROR=C2=A0=C2=A0 [U=3D0,P=3D8045]=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 sexec.c:273:main()=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 : Image is locked by another process<span><br>[roo=
t@mach0 ~]# lsof c7<br></span>[root@mach0 ~]# lslocks<br>COMMAND=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 PID=C2=A0 TYPE SIZE MOD=
E=C2=A0 M START END PATH<br>crond=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 601 FLOCK=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=
=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/crond.pid<br>master=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 826 FLOCK=C2=A0 33B WRITE 0=
=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/spool/postfix/pid/master.pid<=
br>master=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 826 FLOCK=C2=A0 33B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /var/l=
ib/postfix/master.lock<br>lvmetad=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 478 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=A0=
 0=C2=A0=C2=A0 0 /run/lvmetad.pid<br>slurmctld=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 940 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=A0=C2=A0=C2=A0=C2=
=A0 0=C2=A0=C2=A0 0 /run/slurmctld.pid<br>slurmd=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 966 POSIX=C2=A0=C2=A0 4B WRITE 0=C2=
=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /run/slurmd.pid<br>(unknown)=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2299 FLOCK=C2=A0=C2=A0 0B READ=C2=A0 0=C2=
=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=A0 0 /<br><br><br>On Thursday, July 14, 20=
16 at 1:03:50 PM UTC-6, Gregory M. Kurtzer wrote:<div><div><blockquote clas=
s=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border=
-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:solid"=
><div dir=3D"ltr"><br><div><br><div class=3D"gmail_quote">On Thu, Jul 14, 2=
016 at 11:50 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">s=
g...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" s=
tyle=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204=
,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr">G=
regory,<br><br>Thanks for the quick suggestions.=C2=A0 There don&#39;t seem=
 to be any processes attached to the container and I can&#39;t seem to run =
other commands (in read mode). I&#39;m not sure what&#39;s going on.<br><br=
>Just lazy with root right now, I need to create a normal uid.<br></div></b=
lockquote><div><br></div><div>Ahh, ok. Been there, done that! lol</div><div=
>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width=
:1px;border-left-style:solid"><div dir=3D"ltr"><br>Steve<br><br><span style=
=3D"font-family:courier new,monospace">[root@mach0 ~]# ls -la c7<br>-rwxr-x=
r-x 1 root root 1610612769 Jul 14 10:49 c7<br>[root@mach0 ~]# singularity s=
hell -w c7<span><br>ERROR=C2=A0 : Image is locked by another process<br></s=
pan></span></div></blockquote><div><br></div><div>Can you run this command =
again in --debug mode (singularity --debug ....)</div><div>=C2=A0</div><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-lef=
t:1ex;border-left-color:rgb(204,204,204);border-left-width:1px;border-left-=
style:solid"><div dir=3D"ltr"><span style=3D"font-family:courier new,monosp=
ace"><span></span>[root@mach0 ~]# lsof c7<br></span></div></blockquote><div=
><br></div><div>What about the command &quot;lslocks&quot;?</div><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8e=
x;padding-left:1ex;border-left-color:rgb(204,204,204);border-left-width:1px=
;border-left-style:solid"><div dir=3D"ltr"><span style=3D"font-family:couri=
er new,monospace">[root@mach0 ~]# singularity shell c7 whoami<br>/usr/bin/w=
hoami: /usr/bin/whoami: cannot execute binary file<br></span></div></blockq=
uote><div><br></div><div>Ahh, this is normal. You are asking the shell scri=
pt to read in /usr/bin/whoami. If you want to use shell to run whoami, you =
must prefix it with the -c (e.g. -c &quot;whoami [args]&quot;), or use the =
&#39;exec&#39; Singularity subcommand: &quot;singularity exec c7 whoami&quo=
t;</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:=
0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);borde=
r-left-width:1px;border-left-style:solid"><div dir=3D"ltr"><span style=3D"f=
ont-family:courier new,monospace">[root@mach0 ~]#<br>[root@mach0 ~]# ps -ef=
 |grep c7<br>root=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 7479=C2=A0 2002=C2=A0 0 11:=
49 pts/0=C2=A0=C2=A0=C2=A0 00:00:00 grep --color=3Dauto c7<br>[root@mach0 ~=
]#</span></div></blockquote><div><br></div><div>What kind of file system do=
es your image exist on? Is it NFS by chance? I wonder if there is a host is=
sue with a locking daemon or something else weird going on where it is not =
giving the exclusive lock properly. If this is NFS or other non local file =
system, can you copy the image to /tmp, rerun the MPI command to get it to =
fail again, and try again?=C2=A0</div><div><br></div><div>Thanks!</div><div=
><br></div><div><br></div><div>=C2=A0</div><blockquote class=3D"gmail_quote=
" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(=
204,204,204);border-left-width:1px;border-left-style:solid"><div dir=3D"ltr=
"><span><br><br><br><br>On Thursday, July 14, 2016 at 12:30:01 PM UTC-6, Gr=
egory M. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0px 0px 0px 0.8ex;padding-left:1ex;border-left-color:rgb(204,204,204);=
border-left-width:1px;border-left-style:solid"><span><div dir=3D"ltr">Hi St=
eve,<div><br></div><div>That means there is an active file descriptor/proce=
ss still running and attached to the container maintaining a shared lock. Y=
ou can run other commands against the container as long as long as the cont=
ainer is not being requested as --writable(-w), because that will try and o=
btain an exclusive lock and it will fail if there are any active shared loc=
ks. Try an &quot;lsof /path/to/c7&quot; to see what processes are attached =
to it. You may see a list like:</div><div><br></div><div><div># lsof /tmp/D=
emo-2.img=C2=A0</div><div>COMMAND =C2=A0 =C2=A0PID USER =C2=A0 FD =C2=A0 TY=
PE DEVICE =C2=A0 SIZE/OFF =C2=A0 =C2=A0 =C2=A0NODE NAME</div><div>sexec =C2=
=A0 107975 root =C2=A0 =C2=A06rR =C2=A0REG =C2=A0253,0 1073741856 202112247=
 /tmp/Demo-2.img</div><div>sexec =C2=A0 107977 root =C2=A0 =C2=A06r =C2=A0 =
REG =C2=A0253,0 1073741856 202112247 /tmp/Demo-2.img</div><div>bash =C2=A0 =
=C2=A0107982 root =C2=A0 =C2=A06r =C2=A0 REG =C2=A0253,0 1073741856 2021122=
47 /tmp/Demo-2.img</div></div><div><br></div><div>Notice the two top ones a=
re &#39;sexec&#39; which are part of the Singularity process stack. Kill th=
e bottom one, and those should go away naturally.</div><div><br></div><div>=
BTW, as long as you have installed Singularity as root, there is no need to=
 run Singularity commands as root (unless you want to make system changes w=
ithin the container).</div><div><br></div><div>Hope that helps!</div><div><=
br></div></div></span><div><br><div class=3D"gmail_quote"><div><div>On Thu,=
 Jul 14, 2016 at 10:56 AM, Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"n=
ofollow">sg...@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;padding-left:1ex;bor=
der-left-color:rgb(204,204,204);border-left-width:1px;border-left-style:sol=
id"><div><div><div dir=3D"ltr">Running mpirun tests, when an abort occurs, =
my image ends up locked.=C2=A0 Is there a way to clear the lock without reb=
ooting?=C2=A0 I looked for processes that I could kill, but didn&#39;t see =
anything worthy.<br><br>I&#39;m using singularity v2.1 on Centos 7.2 (both =
host and container).<br><br>Regards,<br><br>Steve<br><br><span style=3D"fon=
t-family:courier new,monospace">[root@mach0 ~]# mpirun --allow-run-as-root =
-n 2 -H mach1,mach2 singularity exec c7 /usr/bin/ring<br>Process 0 sending =
10 to 1, tag 201 (2 processes in ring)<br>Process 0 sent to 1<br>Process 0 =
decremented value: 9<br>Process 0 decremented value: 8<br>Process 0 decreme=
nted value: 7<br>Process 0 decremented value: 6<br>Process 0 decremented va=
lue: 5<br>Process 0 decremented value: 4<br>Process 0 decremented value: 3<=
br>Process 0 decremented value: 2<br>Process 0 decremented value: 1<br>Proc=
ess 0 decremented value: 0<br>Process 0 exiting<br>Process 1 exiting<br>[ro=
ot@mach0 ~]# mpirun --allow-run-as-root -n 3 -H mach0,mach1,mach2 singulari=
ty exec c7 /usr/bin/ring<br>-----------------------------------------------=
---------------------------<br>It appears as if there is not enough space f=
or /tmp/ompi.mach0.2291/54935/1/0/vader_segment.mach0.0 (the shared-memory =
backing<br>file). It is likely that your MPI job will now either abort or e=
xperience<br>performance degradation.<br><br>=C2=A0 Local host:=C2=A0 mach0=
<br>=C2=A0 Space Requested: 4194312 B<br>=C2=A0 Space Available: 0 B<br>---=
-----------------------------------------------------------------------<br>=
[mach0:02308] create_and_attach: unable to create shared memory BTL coordin=
ating structure :: size 134217728<br>[mach0:02291] 2 more processes have se=
nt help message help-opal-shmem-mmap.txt / target full<br>[mach0:02291] Set=
 MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / =
error messages<br>^CKilled by signal 2.<br>Killed by signal 2.<br>Singulari=
ty is sending SIGKILL to child pid: 2308<br>Singularity is sending SIGKILL =
to child pid: 2309<br>[warn] Epoll ADD(4) on fd 31 failed.=C2=A0 Old events=
 were 0; read change was 0 (none); write change was 1 (add): Bad file descr=
iptor<br>^C[root@mach0 ~]singularity shell -w c7<br>ERROR=C2=A0 : Image is =
locked by another process<br>[root@mach0 ~]# tail -30 /var/log/messages<br>=
Jul 14 10:42:17 mach0 systemd: Started LSB: slurm daemon management.<br>Jul=
 14 10:42:17 mach0 systemd: Reached target Multi-User System.<br>Jul 14 10:=
42:17 mach0 systemd: Starting Multi-User System.<br>Jul 14 10:42:17 mach0 s=
ystemd: Starting Update UTMP about System Runlevel Changes...<br>Jul 14 10:=
42:17 mach0 systemd: Started Stop Read-Ahead Data Collection 10s After Comp=
leted Startup.<br>Jul 14 10:42:17 mach0 systemd: Started Update UTMP about =
System Runlevel Changes.<br>Jul 14 10:42:18 mach0 kdumpctl: kexec: loaded k=
dump kernel<br>Jul 14 10:42:18 mach0 kdumpctl: Starting kdump: [OK]<br>Jul =
14 10:42:18 mach0 systemd: Started Crash recovery kernel arming.<br>Jul 14 =
10:42:18 mach0 systemd: Startup finished in 415ms (kernel) + 1.100s (initrd=
) + 4.931s (userspace) =3D 6.446s.<br>Jul 14 10:42:34 mach0 systemd: Create=
d slice user-0.slice.<br>Jul 14 10:42:34 mach0 systemd: Starting user-0.sli=
ce.<br>Jul 14 10:42:34 mach0 systemd-logind: New session 1 of user root.<br=
>Jul 14 10:42:34 mach0 systemd: Started Session 1 of user root.<br>Jul 14 1=
0:42:34 mach0 systemd: Starting Session 1 of user root.<br>Jul 14 10:42:36 =
mach0 Singularity: sexec (U=3D0,P=3D2023)&gt; Command=3Dexec, Container=3Dc=
7, CWD=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:42:36 mach0 Singularity: =
sexec (U=3D0,P=3D2024)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg=
1=3D/usr/bin/ring<br>Jul 14 10:42:36 mach0 Singularity: sexec (U=3D0,P=3D20=
24)&gt; Image is locked by another process<br>Jul 14 10:42:36 mach0 kernel:=
 loop: module loaded<br>Jul 14 10:43:38 mach0 Singularity: sexec (U=3D0,P=
=3D2050)&gt; Command=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(null)<br=
>Jul 14 10:43:38 mach0 kernel: EXT4-fs (loop0): mounted filesystem with ord=
ered data mode. Opts: discard<br>Jul 14 10:49:17 mach0 Singularity: sexec (=
U=3D0,P=3D2203)&gt; Command=3Dshell, Container=3Dc7, CWD=3D/root, Arg1=3D(n=
ull)<br>Jul 14 10:49:17 mach0 kernel: EXT4-fs (loop0): mounted filesystem w=
ith ordered data mode. Opts: discard<br>Jul 14 10:50:39 mach0 Singularity: =
sexec (U=3D0,P=3D2244)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg=
1=3D/usr/bin/ring<br>Jul 14 10:50:39 mach0 kernel: EXT4-fs (loop0): mounted=
 filesystem with ordered data mode. Opts: discard<br>Jul 14 10:51:34 mach0 =
Singularity: sexec (U=3D0,P=3D2299)&gt; Command=3Dexec, Container=3Dc7, CWD=
=3D/root, Arg1=3D/usr/bin/ring<br>Jul 14 10:51:34 mach0 Singularity: sexec =
(U=3D0,P=3D2300)&gt; Command=3Dexec, Container=3Dc7, CWD=3D/root, Arg1=3D/u=
sr/bin/ring<br>Jul 14 10:51:34 mach0 kernel: EXT4-fs (loop0): mounted files=
ystem with ordered data mode. Opts: discard<br>Jul 14 10:51:57 mach0 Singul=
arity: sexec (U=3D0,P=3D2322)&gt; Command=3Dshell, Container=3Dc7, CWD=3D/r=
oot, Arg1=3D(null)<br>Jul 14 10:51:57 mach0 Singularity: sexec (U=3D0,P=3D2=
322)&gt; Image is locked by another process</span><span><font color=3D"#888=
888"><br><br><br></font></span></div></div></div><span><font color=3D"#8888=
88"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance C=
omputing Services (HPCS)<br>University of California<br>Lawrence Berkeley N=
ational Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></d=
iv>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div></div>
</blockquote></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</div></div></blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>

--001a1141f9eea9f07a05379f3465--
