Date: Tue, 13 Jun 2017 01:37:40 -0700 (PDT)
From: victor sv <vict...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <e701dd05-18fd-43ab-860b-085661b8a1e5@lbl.gov>
In-Reply-To: <CAN7etTxXZ+yp_0wEwx7UsBCgjn9nisvWHe+tpnjdpjJCUJ=hXA@mail.gmail.com>
References: <CA+zw9q3gSaqZgo3tzOXyn5kwfeUS-LdCuyTnmWyPgfVjmxGBqA@mail.gmail.com>
 <CAN7etTz3V1eSH2r7Yd9y0MFFt24H2vuAB9FcViKbDAsdxtt9Pg@mail.gmail.com>
 <CA+zw9q0zaF0ajbLuPeT0H-Ag5t-ZfV9ux94hm+9m6XQ_JEQ7LA@mail.gmail.com>
 <CAN7etTykdT1UXi42d3+E=OKaDKpXapb5CFcyMfrRic0xXf6RWA@mail.gmail.com> <CA+zw9q2ftBoz3UJgOmU6qsDyESh8kShr3YfX1jtbF4+4WcwcbQ@mail.gmail.com>
 <CAN7etTxXZ+yp_0wEwx7UsBCgjn9nisvWHe+tpnjdpjJCUJ=hXA@mail.gmail.com>
Subject: Re: [Singularity] Singularity with Slurm and PMIx
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_3824_53927777.1497343060477"

------=_Part_3824_53927777.1497343060477
Content-Type: multipart/alternative; 
	boundary="----=_Part_3825_1644207319.1497343060478"

------=_Part_3825_1644207319.1497343060478
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Dear all,

first of all, congratulations for your great work with singularity!

I'm experiencing some issues running singularity with slurm.

I've several images based on ubuntu and within several versions of OpenMPI=
=20
(1,10.X, 2.0.2, 2.1). I'm able to run them at least with mpirun from, at=20
least, the same version of OpenMPI outside the container. But when I try to=
=20
reproduce the same process with srun it is not succesful.

Some time it crashes with an MPI ORTE error.=20

With a particular test with OpenMPI ring example I get the following output=
:

srun -n2 -p thinnodes -t 00:01:00 singularity exec ring_slurm.img ring
srun: job 787739 queued and waiting for resources
srun: job 787739 has been allocated resources
Process 0 sending 10 to 0, tag 201 (1 processes in ring)
Process 0 sent to 0
Process 0 decremented value: 9
Process 0 decremented value: 8
Process 0 decremented value: 7
Process 0 decremented value: 6
Process 0 decremented value: 5
Process 0 decremented value: 4
Process 0 decremented value: 3
Process 0 decremented value: 2
Process 0 decremented value: 1
Process 0 decremented value: 0
Process 0 exiting
Process 0 sending 10 to 0, tag 201 (1 processes in ring)
Process 0 sent to 0
Process 0 decremented value: 9
Process 0 decremented value: 8
Process 0 decremented value: 7
Process 0 decremented value: 6
Process 0 decremented value: 5
Process 0 decremented value: 4
Process 0 decremented value: 3
Process 0 decremented value: 2
Process 0 decremented value: 1
Process 0 decremented value: 0
Process 0 exiting

It seems that there is no communication between tasks.

Some info about slurm:

$ srun --version
slurm 14.11.10-Bull.1.0

$ srun --mpi=3Dlist
srun: MPI types are...
srun: mpi/mvapich
srun: mpi/openmpi
srun: mpi/lam
srun: mpi/pmi2
srun: mpi/mpichgm
srun: mpi/mpich1_shmem
srun: mpi/none
srun: mpi/mpichmx
srun: mpi/mpich1_p4

I'm a little bit lost with this issue ... can someone give me some lights?

Thanks in advance,
V=C3=ADctor.

El jueves, 9 de junio de 2016, 3:06:43 (UTC+2), Gregory M. Kurtzer escribi=
=C3=B3:
>
> My pleasure! I'm glad to hear it is working for you now!
>
> Greg
>
> On Wed, Jun 8, 2016 at 4:56 PM, yiannis georgiou <g...@gmail.com=20
> <javascript:>> wrote:
>
>> On Wed, Jun 8, 2016 at 10:57 PM, Gregory M. Kurtzer <gm...@lbl.gov=20
>> <javascript:>> wrote:
>>
>>>
>>> On Wed, Jun 8, 2016 at 1:48 PM, yiannis georgiou <g...@gmail.com=20
>>> <javascript:>> wrote:
>>>
>>>> Hi Greg,
>>>>
>>>> bullseye!!! it worked for me like this:
>>>>
>>>
>>> Excellent!!
>>> =20
>>>
>>>>
>>>> [georgioy@rio11 ~]$ salloc -n4 -N2
>>>> [georgioy@rio11 ~]$ export SINGULARITY_NO_NAMESPACE_PID=3D1
>>>> [georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N2=20
>>>> /usr/local/singularity_v2/bin/singularity exec /tmp/ubuntu_v4.img=20
>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>> Hello world from processor rio13, rank 2 out of 4 processors
>>>> Hello world from processor rio13, rank 3 out of 4 processors
>>>> Hello world from processor rio12, rank 0 out of 4 processors
>>>> Hello world from processor rio12, rank 1 out of 4 processors
>>>> ERROR: Could not clear loop device
>>>>
>>>> Any idea how I can correct the "ERROR: Could not clear loop device"=20
>>>> appearing at the end of the execution?
>>>>
>>>
>>> Hrmm.. Interesting, I wonder on which system it got that error. Is it=
=20
>>> possible to try and replicate the error on a single new (e.g. -N1)? The=
=20
>>> other thing to try is to run losetup (-a) on both rio12 and rio13 and s=
ee=20
>>> if /dev/loop0 is still bound somewhere.
>>>
>> It was related with a previously failed srun I think. When I did a new=
=20
>> clean salloc with different srun all worked without problem.=20
>>
>>> =20
>>>
>>>> And when you have a moment could you explain me how this magic=20
>>>> environment variable make it work! I'm not sure I got it.
>>>>
>>>
>>> While most container systems are built around the idea of complete=20
>>> isolation, Singularity is focused on application portability so you can=
=20
>>> disable some of the namespaces as needed (with the PID namespace being =
the=20
>>> culprit here for OpenMPI's shared memory model).
>>>
>> Ok I see.=20
>>
>>> =20
>>>
>>>>
>>>> One more question, I've noticed that there have been some changes in=
=20
>>>> the latest OpenMPI to improve performance when using singularity. Are =
they=20
>>>> done for PMIx or for all PMI versions?
>>>>
>>>
>>> Indeed. I'm hoping Ralph will chime in when he has a moment and can=20
>>> address this and possibly a better long term fix (and if it can't be do=
ne=20
>>> via the MPI side, I can do it on the Singularity side).
>>>
>> Ok
>>
>> Thanks for your answers and the fast solution!
>>
>> Yiannis=20
>>
>>> =20
>>>
>>>>
>>>> Thanks a lot!
>>>>
>>>
>>> My pleasure!
>>>
>>> Greg
>>>
>>>
>>> =20
>>>
>>>>
>>>> Yiannis
>>>>
>>>>
>>>> On Wed, Jun 8, 2016 at 9:16 PM, Gregory M. Kurtzer <gm...@lbl.gov=20
>>>> <javascript:>> wrote:
>>>>
>>>>> Hi Yiannis,
>>>>>
>>>>> I have a quick thing to test... The address not mapped error seems=20
>>>>> consistent with something else that I've seen when testing OpenMPI wi=
th=20
>>>>> shared memory and the NEWPID namespace. Try to disable the NEWPID nam=
espace=20
>>>>> by exporting this environment variable:
>>>>>
>>>>> SINGULARITY_NO_NAMESPACE_PID=3D1
>>>>>
>>>>> Now you will need to export it in a place where all Singularity=20
>>>>> contexts will see it, maybe something like this:
>>>>>
>>>>> $ srun --mpi=3Dpmix -n 4 -N 2 SINGULARITY_NO_NAMESPACE_PID=3D1 singul=
arity=20
>>>>> exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hell=
o
>>>>>
>>>>> There is an OpenMPI plugin which sets this automatically in the=20
>>>>> current codebase, but I'm not sure how it will come into play in this=
 usage=20
>>>>> scenario.
>>>>>
>>>>> Let me know how that works for ya!
>>>>>
>>>>> Thanks,
>>>>>
>>>>> Greg
>>>>>
>>>>> On Wed, Jun 8, 2016 at 12:09 PM, yiannis georgiou <g...@gmail.com=20
>>>>> <javascript:>> wrote:
>>>>>
>>>>>> Hello,
>>>>>>
>>>>>> I'm trying to execute an MPI application from within a singularity=
=20
>>>>>> image through Slurm and pmix using the following command=20
>>>>>>
>>>>>> srun --mpi=3Dpmix -n 4 -N 2 singularity exec /tmp/ubuntu_v4.img=20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>>>
>>>>>> The slurmstepd process is spawned on the compute node and it seems t=
o=20
>>>>>> have launched correctly the application from what we see on one of t=
he=20
>>>>>> compute nodes:
>>>>>>
>>>>>> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>>>>>> [root@rio12 ~]# ps -aux
>>>>>> root     35147  0.1  0.0 562284  4016 ?        Sl   20:06   0:00=20
>>>>>> slurmstepd: [83.0]                 =20
>>>>>> georgioy 35158  0.2  0.0  10428   896 ?        S    20:06   0:00=20
>>>>>> Singularity: namespace                             =20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>>> root     35162  0.0  0.0      0     0 ?        S<   20:06   0:00=20
>>>>>> [loop0]
>>>>>> georgioy 35163  0.0  0.0  10428   400 ?        S    20:06   0:00=20
>>>>>> Singularity: exec                                  =20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>>> root     35164  0.0  0.0      0     0 ?        S    20:06   0:00=20
>>>>>> [jbd2/loop0-8]
>>>>>> root     35165  0.0  0.0      0     0 ?        S    20:06   0:00=20
>>>>>> [ext4-dio-unwrit]
>>>>>> georgioy 35166  102  0.0 285476 14828 ?        RLl  20:06   0:12=20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>>> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>>>>>>
>>>>>> and I see the pmix related functions starting correctly in the slurm=
d=20
>>>>>> log files :
>>>>>>
>>>>>> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>>>>>>
>>>>>> [2016-06-08T20:12:08.004] [86.0] debug:  (null) [0] mpi_pmix.c:90=20
>>>>>> [p_mpi_hook_slurmstepd_prefork] mpi/pmix: start
>>>>>> [2016-06-08T20:12:08.004] [86.0] debug:  mpi/pmix: setup sockets
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:78=
=20
>>>>>> [errhandler_reg_callbk] mpi/pmix: Error handler registration callbac=
k is=20
>>>>>> called with status=3D0, ref=3D0
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_client.c:58=
1=20
>>>>>> [pmixp_libpmix_job_set] mpi/pmix: task initialization
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:220=
=20
>>>>>> [_agent_thread] mpi/pmix: Start agent thread
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:313=
=20
>>>>>> [pmixp_agent_start] mpi/pmix: agent thread started: tid =3D 13967227=
8615808
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84=
=20
>>>>>> [_conn_readable] mpi/pmix: fd =3D 9
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:84=
=20
>>>>>> [_conn_readable] mpi/pmix: fd =3D 19
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:256=
=20
>>>>>> [_pmix_timer_thread] mpi/pmix: Start timer thread
>>>>>> [2016-06-08T20:12:08.005] [86.0] debug:  rio12 [0] pmixp_agent.c:335=
=20
>>>>>> [pmixp_agent_start] mpi/pmix: timer thread started: tid =3D 13967227=
7563136
>>>>>>
>>>>>> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>>>>>>
>>>>>> However the application is never actually started and the srun fails=
=20
>>>>>> completely with the following output:
>>>>>>
>>>>>> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>>>>>> [georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N 2 singularity exec=20
>>>>>> /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello
>>>>>> [rio12:00001] *** Process received signal ***
>>>>>> [rio12:00001] Signal: Segmentation fault (11)
>>>>>> [rio12:00001] Signal code: Address not mapped (1)
>>>>>> [rio12:00001] Failing at address: 0x7fbd937e6010
>>>>>> [rio12:00001] [ 0] [rio12:00001] *** Process received signal ***
>>>>>> /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f26aeb233d0]
>>>>>> [rio12:00001] [ 1]=20
>>>>>> /usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f269f964035]
>>>>>> [rio12:00001] [ 2]=20
>>>>>> /usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x=
7f269f964583]
>>>>>> [rio12:00001] [ 3]=20
>>>>>> /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7=
f269fb68771]
>>>>>> [rio12:00001] [ 4]=20
>>>>>> /usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f26a451fbe7]
>>>>>> [rio12:00001] [ 5]=20
>>>>>> /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x=
7f269ef35662]
>>>>>> [rio12:00001] [ 6]=20
>>>>>> /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f26aed75d31]
>>>>>> [rio12:00001] [ 7]=20
>>>>>> /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f26aed9bdb9]
>>>>>> [rio12:00001] [ 8]=20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]
>>>>>> [rio12:00001] [ 9]=20
>>>>>> /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f26ae7698=
30]
>>>>>> [rio12:00001] Signal: Segmentation fault (11)
>>>>>> [rio12:00001] Signal code: Address not mapped (1)
>>>>>> [rio12:00001] Failing at address: 0x7fbd937e6010
>>>>>> [rio12:00001] [10]=20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]
>>>>>> [rio12:00001] *** End of error message ***
>>>>>> [rio12:00001] [ 0]=20
>>>>>> /lib/x86_64-linux-gnu/libpthread.so.0(+0x113d0)[0x7f3f89c063d0]
>>>>>> [rio12:00001] [ 1]=20
>>>>>> /usr/local/lib/libmca_common_sm.so.0(+0x1035)[0x7f3f7eb1d035]
>>>>>> [rio12:00001] [ 2]=20
>>>>>> /usr/local/lib/libmca_common_sm.so.0(common_sm_mpool_create+0xa3)[0x=
7f3f7eb1d583]
>>>>>> [rio12:00001] [ 3]=20
>>>>>> /usr/local/lib/openmpi/mca_btl_sm.so(mca_btl_sm_add_procs+0x5f1)[0x7=
f3f7ed21771]
>>>>>> [rio12:00001] [ 4]=20
>>>>>> /usr/local/lib/openmpi/mca_bml_r2.so(+0x2be7)[0x7f3f7f5f3be7]
>>>>>> [rio12:00001] [ 5]=20
>>>>>> /usr/local/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_add_procs+0xd2)[0x=
7f3f7e0ee662]
>>>>>> [rio12:00001] [ 6]=20
>>>>>> /usr/local/lib/libmpi.so.0(ompi_mpi_init+0xa51)[0x7f3f89e58d31]
>>>>>> [rio12:00001] [ 7]=20
>>>>>> /usr/local/lib/libmpi.so.0(MPI_Init+0xb9)[0x7f3f89e7edb9]
>>>>>> [rio12:00001] [ 8]=20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x4007ec]
>>>>>> [rio12:00001] [ 9]=20
>>>>>> /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f3f8984c8=
30]
>>>>>> [rio12:00001] [10]=20
>>>>>> /home_nfs/georgioy/BENCHS/mpi-openmp/mpi_hello[0x400709]
>>>>>> [rio12:00001] *** End of error message ***
>>>>>> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler]=20
>>>>>> mpi/pmix: ERROR: Error handler invoked: status =3D -25, nranges =3D =
0: Success=20
>>>>>> (0)
>>>>>> srun: Job step aborted: Waiting up to 32 seconds for job step to=20
>>>>>> finish.
>>>>>> slurmstepd: error: *** STEP 86.0 ON rio12 CANCELLED AT=20
>>>>>> 2016-06-08T20:12:08 ***
>>>>>> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler]=20
>>>>>> mpi/pmix: ERROR: Error handler invoked: status =3D -25, nranges =3D =
0: Success=20
>>>>>> (0)
>>>>>> slurmstepd: error: rio13 [1] pmixp_client.c:241 [errhandler]=20
>>>>>> mpi/pmix: ERROR: Error handler invoked: status =3D -25, nranges =3D =
0: Success=20
>>>>>> (0)
>>>>>> slurmstepd: error: rio12 [0] pmixp_client.c:241 [errhandler]=20
>>>>>> mpi/pmix: ERROR: Error handler invoked: status =3D -25, nranges =3D =
0: Success=20
>>>>>> (0)
>>>>>> srun: error: rio12: tasks 0-2: Killed
>>>>>> srun: error: rio13: task 3: Killed
>>>>>>
>>>>>> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
>>>>>>
>>>>>> the results I get when launching with mpirun instead of srun are mor=
e=20
>>>>>> or less the same.=20
>>>>>>
>>>>>> From my understanding, the orted process, or in my case slurmstepd=
=20
>>>>>> process that launches the singularity container=20
>>>>>> and the MPI application, enables the communication of MPI libraries=
=20
>>>>>> and orted (or slurmstepd) through PMI (in my case here PMIx).
>>>>>> So I suppose the problem I see should be related with the mapping of=
=20
>>>>>> PMI from the container towards the orted or slurmstepd.
>>>>>>
>>>>>> What do you think?
>>>>>>
>>>>>> To give some more details, my singularity container has OpenMPI and=
=20
>>>>>> PMIx installed but not Slurm. I don't think that Slurm needs to resi=
de=20
>>>>>> within the container in that context.
>>>>>> But I wasn't sure if OpenMPI and PMIx are needed both inside and=20
>>>>>> outside of the container.
>>>>>> So, I've tried using the exact same version of PMIx and OpenMPI in=
=20
>>>>>> and out of the container and the problem still persists.=20
>>>>>> The experiments have been done using RedHat hosts with CentOS or=20
>>>>>> Ubuntu singularity containers and the latest github versions of slur=
m,=20
>>>>>> OpenMPI, PMIx and singularity.
>>>>>>
>>>>>> By the way, do you have any MPI example for singularity v2?=20
>>>>>> Because the MPI example that you show here :=20
>>>>>> http://singularity.lbl.gov/#hpc
>>>>>>
>>>>>> is actually done using singularity v1, no?=20
>>>>>> In my understanding with singularity v2 we actually build a complete=
=20
>>>>>> image with OS not just the application with its libraries. Is this c=
orrect?
>>>>>>
>>>>>> Sorry for the long email and
>>>>>> thanks a lot for any extra info you can provide regarding singularit=
y=20
>>>>>> v2 and MPI.
>>>>>>
>>>>>> Best Regards,
>>>>>> Yiannis
>>>>>>
>>>>>> --=20
>>>>>> You received this message because you are subscribed to the Google=
=20
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,=20
>>>>>> send an email to singu...@lbl.gov <javascript:>.
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --=20
>>>>> Gregory M. Kurtzer
>>>>> High Performance Computing Services (HPCS)
>>>>> University of California
>>>>> Lawrence Berkeley National Laboratory
>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>
>>>>> --=20
>>>>> You received this message because you are subscribed to the Google=20
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d=20
>>>>> an email to singu...@lbl.gov <javascript:>.
>>>>>
>>>>
>>>> --=20
>>>> You received this message because you are subscribed to the Google=20
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>>> an email to singu...@lbl.gov <javascript:>.
>>>>
>>>
>>>
>>>
>>> --=20
>>> Gregory M. Kurtzer
>>> High Performance Computing Services (HPCS)
>>> University of California
>>> Lawrence Berkeley National Laboratory
>>> One Cyclotron Road, Berkeley, CA 94720
>>>
>>> --=20
>>> You received this message because you are subscribed to the Google=20
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send=
=20
>>> an email to singu...@lbl.gov <javascript:>.
>>>
>>
>> --=20
>> You received this message because you are subscribed to the Google Group=
s=20
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n=20
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
>
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>

------=_Part_3825_1644207319.1497343060478
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Dear all,<br><br>first of all, congratulations for your gr=
eat work with singularity!<br><br>I&#39;m experiencing some issues running =
singularity with slurm.<br><br>I&#39;ve several images based on ubuntu and =
within several versions of OpenMPI (1,10.X, 2.0.2, 2.1). I&#39;m able to ru=
n them at least with mpirun from, at least, the same version of OpenMPI out=
side the container. But when I try to reproduce the same process with srun =
it is not succesful.<br><br>Some time it crashes with an MPI ORTE error. <b=
r><br>With a particular test with OpenMPI ring example I get the following =
output:<br><br>srun -n2 -p thinnodes -t 00:01:00 singularity exec ring_slur=
m.img ring<br>srun: job 787739 queued and waiting for resources<br>srun: jo=
b 787739 has been allocated resources<br>Process 0 sending 10 to 0, tag 201=
 (1 processes in ring)<br>Process 0 sent to 0<br>Process 0 decremented valu=
e: 9<br>Process 0 decremented value: 8<br>Process 0 decremented value: 7<br=
>Process 0 decremented value: 6<br>Process 0 decremented value: 5<br>Proces=
s 0 decremented value: 4<br>Process 0 decremented value: 3<br>Process 0 dec=
remented value: 2<br>Process 0 decremented value: 1<br>Process 0 decremente=
d value: 0<br>Process 0 exiting<br>Process 0 sending 10 to 0, tag 201 (1 pr=
ocesses in ring)<br>Process 0 sent to 0<br>Process 0 decremented value: 9<b=
r>Process 0 decremented value: 8<br>Process 0 decremented value: 7<br>Proce=
ss 0 decremented value: 6<br>Process 0 decremented value: 5<br>Process 0 de=
cremented value: 4<br>Process 0 decremented value: 3<br>Process 0 decrement=
ed value: 2<br>Process 0 decremented value: 1<br>Process 0 decremented valu=
e: 0<br>Process 0 exiting<br><br>It seems that there is no communication be=
tween tasks.<br><br>Some info about slurm:<br><br>$ srun --version<br>slurm=
 14.11.10-Bull.1.0<br><br>$ srun --mpi=3Dlist<br>srun: MPI types are...<br>=
srun: mpi/mvapich<br>srun: mpi/openmpi<br>srun: mpi/lam<br>srun: mpi/pmi2<b=
r>srun: mpi/mpichgm<br>srun: mpi/mpich1_shmem<br>srun: mpi/none<br>srun: mp=
i/mpichmx<br>srun: mpi/mpich1_p4<br><br>I&#39;m a little bit lost with this=
 issue ... can someone give me some lights?<br><br>Thanks in advance,<br>V=
=C3=ADctor.<br><br>El jueves, 9 de junio de 2016, 3:06:43 (UTC+2), Gregory =
M. Kurtzer  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin=
: 0;margin-left: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div=
 dir=3D"ltr">My pleasure! I&#39;m glad to hear it is working for you now!<d=
iv><br></div><div>Greg</div></div><div><br><div class=3D"gmail_quote">On We=
d, Jun 8, 2016 at 4:56 PM, yiannis georgiou <span dir=3D"ltr">&lt;<a href=
=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"SoWRhWgIDAAJ" r=
el=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;return tru=
e;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">g...@gmail.c=
om</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"marg=
in:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
><div><div class=3D"gmail_quote"><span>On Wed, Jun 8, 2016 at 10:57 PM, Gre=
gory M. Kurtzer <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_bl=
ank" gdf-obfuscated-mailto=3D"SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D=
"this.href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D&#3=
9;javascript:&#39;;return true;">gm...@lbl.gov</a>&gt;</span> wrote:<br><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #=
ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><br><div class=3D"gmail_q=
uote"><span>On Wed, Jun 8, 2016 at 1:48 PM, yiannis georgiou <span dir=3D"l=
tr">&lt;<a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">g...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote=
" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style=
:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr=
"><div><div><div><div><div><div>Hi Greg,<br><br></div>bullseye!!! it worked=
 for me like this:<br></div></div></div></div></div></div></blockquote><div=
><br></div></span><div>Excellent!!</div><span><div>=C2=A0</div><blockquote =
class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1=
px;border-left-style:solid;border-left-color:rgb(204,204,204);padding-left:=
1ex"><div dir=3D"ltr"><div><div><div><div><div><br>[georgioy@rio11 ~]$ sall=
oc -n4 -N2<br>[georgioy@rio11 ~]$ export SINGULARITY_NO_NAMESPACE_PID=3D1<b=
r>[georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N2 /usr/local/singularity_v2/=
bin/<wbr>singularity exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-=
<wbr>openmp/mpi_hello<br>Hello world from processor rio13, rank 2 out of 4 =
processors<br>Hello world from processor rio13, rank 3 out of 4 processors<=
br>Hello world from processor rio12, rank 0 out of 4 processors<br>Hello wo=
rld from processor rio12, rank 1 out of 4 processors<br>ERROR: Could not cl=
ear loop device<br><br></div>Any idea how I can correct the &quot;ERROR: Co=
uld not clear loop device&quot; appearing at the end of the execution?<br><=
/div></div></div></div></div></blockquote><div><br></div></span><div>Hrmm..=
 Interesting, I wonder on which system it got that error. Is it possible to=
 try and replicate the error on a single new (e.g. -N1)? The other thing to=
 try is to run losetup (-a) on both rio12 and rio13 and see if /dev/loop0 i=
s still bound somewhere.</div></div></div></div></blockquote></span><div>It=
 was related with a previously failed srun I think. When I did a new clean =
salloc with different srun all worked without problem. <br></div><span><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #c=
cc solid;padding-left:1ex"><div dir=3D"ltr"><div><div class=3D"gmail_quote"=
><span><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0=
px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-=
color:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><div><div><d=
iv></div>And when you have a moment could you explain me how this magic env=
ironment variable make it work! I&#39;m not sure I got it.<br></div></div><=
/div></div></blockquote><div><br></div></span><div>While most container sys=
tems are built around the idea of complete isolation, Singularity is focuse=
d on application portability so you can disable some of the namespaces as n=
eeded (with the PID namespace being the culprit here for OpenMPI&#39;s shar=
ed memory model).</div></div></div></div></blockquote></span><div>Ok I see.=
 <br></div><span><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8=
ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><div =
class=3D"gmail_quote"><span><div>=C2=A0</div><blockquote class=3D"gmail_quo=
te" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-sty=
le:solid;border-left-color:rgb(204,204,204);padding-left:1ex"><div dir=3D"l=
tr"><div><div><div><br></div>One more question, I&#39;ve noticed that there=
 have been some changes in the latest OpenMPI to improve performance when u=
sing singularity. Are they done for PMIx or for all PMI versions?<br></div>=
</div></div></blockquote><div><br></div></span><div>Indeed. I&#39;m hoping =
Ralph will chime in when he has a moment and can address this and possibly =
a better long term fix (and if it can&#39;t be done via the MPI side, I can=
 do it on the Singularity side).</div></div></div></div></blockquote></span=
><div>Ok<br><br></div><div>Thanks for your answers and the fast solution!<s=
pan><font color=3D"#888888"><br></font></span></div><span><font color=3D"#8=
88888"><div><br></div><div>Yiannis <br></div></font></span><div><div><block=
quote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc=
 solid;padding-left:1ex"><div dir=3D"ltr"><div><div class=3D"gmail_quote"><=
div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0=
px 0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rg=
b(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div><div><br></div>Thank=
s a lot!</div></div></blockquote><div><br></div><div>My pleasure!</div><div=
><br></div><div>Greg</div><div><div><div><br></div><div><br></div><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8e=
x;border-left-width:1px;border-left-style:solid;border-left-color:rgb(204,2=
04,204);padding-left:1ex"><div dir=3D"ltr"><div><span><font color=3D"#88888=
8"><br></font></span></div><span><font color=3D"#888888">Yiannis<br><div><d=
iv><div><div><div><div><div><div><br></div></div></div></div></div></div></=
div></div></font></span></div><div><div><div><br><div class=3D"gmail_quote"=
>On Wed, Jun 8, 2016 at 9:16 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<=
a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"SoWRhWgID=
AAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;retu=
rn true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">gm...@=
lbl.gov</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D=
"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-style:solid;bor=
der-left-color:rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr">Hi Yiann=
is,<div><br></div><div>I have a quick thing to test... The address not mapp=
ed error seems consistent with something else that I&#39;ve seen when testi=
ng OpenMPI with shared memory and the NEWPID namespace. Try to disable the =
NEWPID namespace by exporting this environment variable:</div><div><br></di=
v><div>SINGULARITY_NO_NAMESPACE_PID=3D1<br></div><div><br></div><div>Now yo=
u will need to export it in a place where all Singularity contexts will see=
 it, maybe something like this:</div><div><br></div><div>$ srun --mpi=3Dpmi=
x -n 4 -N 2 SINGULARITY_NO_NAMESPACE_PID=3D1 singularity exec /tmp/ubuntu_v=
4.img /home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_hello</div><div><br></d=
iv><div>There is an OpenMPI plugin which sets this automatically in the cur=
rent codebase, but I&#39;m not sure how it will come into play in this usag=
e scenario.</div><div><br></div><div>Let me know how that works for ya!</di=
v><div><br></div><div>Thanks,</div><div><br></div><div>Greg</div></div><div=
><br><div class=3D"gmail_quote"><div><div>On Wed, Jun 8, 2016 at 12:09 PM, =
yiannis georgiou <span dir=3D"ltr">&lt;<a href=3D"javascript:" target=3D"_b=
lank" gdf-obfuscated-mailto=3D"SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=
=3D"this.href=3D&#39;javascript:&#39;;return true;" onclick=3D"this.href=3D=
&#39;javascript:&#39;;return true;">g...@gmail.com</a>&gt;</span> wrote:<br=
></div></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;border-left-width:1px;border-left-style:solid;border-left-color:rgb(2=
04,204,204);padding-left:1ex"><div><div><div dir=3D"ltr"><div><div><div>Hel=
lo,<br><br></div>I&#39;m trying to execute an MPI application from within a=
 singularity image through Slurm and pmix using the following command <br><=
br></div>srun --mpi=3Dpmix -n 4 -N 2 singularity exec
 /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_hello<br>=
<br></div>The
 slurmstepd process is spawned on the compute node and it seems to have=20
launched correctly the application from what we see on one of the=20
compute nodes:<br><div><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<wbr>=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D<br>[root@rio12 ~]# ps -aux<br>root=C2=A0=C2=A0=C2=A0=C2=A0 35147=C2=A0 =
0.1=C2=A0 0.0 562284=C2=A0 4016 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 Sl=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 slurmstepd: [83.0]=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0 <br>georgioy
 35158=C2=A0 0.2=C2=A0 0.0=C2=A0 10428=C2=A0=C2=A0 896 ?=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 Singula=
rity:=20
namespace=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0<wbr>=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=20
/home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_hello<br>root=C2=A0=C2=A0=C2=
=A0=C2=A0 35162=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=
=C2=A0=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S&lt;=C2=
=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 [loop0]<br>georgioy
 35163=C2=A0 0.0=C2=A0 0.0=C2=A0 10428=C2=A0=C2=A0 400 ?=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 Singula=
rity:=20
exec=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0<wbr>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=20
/home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_hello<br>root=C2=A0=C2=A0=C2=
=A0=C2=A0 35164=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=
=C2=A0=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=
=A0=C2=A0 20:06=C2=A0=C2=A0 0:00 [jbd2/loop0-8]<br>root=C2=A0=C2=A0=C2=A0=
=C2=A0 35165=C2=A0 0.0=C2=A0 0.0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 0=C2=A0=C2=
=A0=C2=A0=C2=A0 0 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 S=C2=A0=C2=A0=
=C2=A0 20:06=C2=A0=C2=A0 0:00 [ext4-dio-unwrit]<br>georgioy 35166=C2=A0 102=
=C2=A0 0.0 285476 14828 ?=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 RLl=C2=
=A0 20:06=C2=A0=C2=A0 0:12 /home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_he=
llo<br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D<wbr>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br></=
div><div>and I see the pmix related functions starting correctly in the slu=
rmd log files :<br><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<wbr>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<=
br><br>[2016-06-08T20:12:08.004] [86.0] debug:=C2=A0 (null) [0] mpi_pmix.c:=
90 [p_mpi_hook_slurmstepd_<wbr>prefork] mpi/pmix: start<br>[2016-06-08T20:1=
2:08.004] [86.0] debug:=C2=A0 mpi/pmix: setup sockets<br>[2016-06-08T20:12:=
08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_client.c:78 [errhandler_reg_callbk]=20
mpi/pmix: Error handler registration callback is called with status=3D0,=20
ref=3D0<br>[2016-06-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_cl=
ient.c:581 [pmixp_libpmix_job_set] mpi/pmix: task initialization<br>[2016-0=
6-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:220 [_agent_=
thread] mpi/pmix: Start agent thread<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:313 [pmixp_agent_start]=20
mpi/pmix: agent thread started: tid =3D 139672278615808<br>[2016-06-08T20:1=
2:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:84 [_conn_readable] m=
pi/pmix: fd =3D 9<br>[2016-06-08T20:12:08.005] [86.0] debug:=C2=A0 rio12 [0=
] pmixp_agent.c:84 [_conn_readable] mpi/pmix: fd =3D 19<br>[2016-06-08T20:1=
2:08.005] [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:256 [_pmix_timer_thre=
ad] mpi/pmix: Start timer thread<br>[2016-06-08T20:12:08.005]
 [86.0] debug:=C2=A0 rio12 [0] pmixp_agent.c:335 [pmixp_agent_start]=20
mpi/pmix: timer thread started: tid =3D 139672277563136<br><br>=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D<wbr>=3D=3D=3D=3D=3D=3D=3D=3D=3D<br></div><div><div><br></div><div>Howev=
er the application is never actually started and the srun fails completely =
with the following output:<br></div><div><br>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<wbr>=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D<br>[georgioy@rio11 ~]$ srun --mpi=3Dpmix -n 4 -N 2=
 singularity exec /tmp/ubuntu_v4.img /home_nfs/georgioy/BENCHS/mpi-<wbr>ope=
nmp/mpi_hello<br>[rio12:00001] *** Process received signal ***<br>[rio12:00=
001] Signal: Segmentation fault (11)<br>[rio12:00001] Signal code: Address =
not mapped (1)<br>[rio12:00001] Failing at address: 0x7fbd937e6010<br>[rio1=
2:00001] [ 0] [rio12:00001] *** Process received signal ***<br>/lib/x86_64-=
linux-gnu/<wbr>libpthread.so.0(+0x113d0)[<wbr>0x7f26aeb233d0]<br>[rio12:000=
01] [ 1] /usr/local/lib/libmca_common_<wbr>sm.so.0(+0x1035)[<wbr>0x7f269f96=
4035]<br>[rio12:00001] [ 2] /usr/local/lib/libmca_common_<wbr>sm.so.0(commo=
n_sm_mpool_<wbr>create+0xa3)[0x7f269f964583]<br>[rio12:00001] [ 3] /usr/loc=
al/lib/openmpi/mca_<wbr>btl_sm.so(mca_btl_sm_add_<wbr>procs+0x5f1)[0x7f269f=
b68771]<br>[rio12:00001] [ 4] /usr/local/lib/openmpi/mca_<wbr>bml_r2.so(+0x=
2be7)[<wbr>0x7f26a451fbe7]<br>[rio12:00001] [ 5] /usr/local/lib/openmpi/mca=
_<wbr>pml_ob1.so(mca_pml_ob1_add_<wbr>procs+0xd2)[0x7f269ef35662]<br>[rio12=
:00001] [ 6] /usr/local/lib/libmpi.so.0(<wbr>ompi_mpi_init+0xa51)[<wbr>0x7f=
26aed75d31]<br>[rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(<wbr>MPI_Init+=
0xb9)[0x7f26aed9bdb9]<br>[rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mpi-<=
wbr>openmp/mpi_hello[0x4007ec]<br>[rio12:00001] [ 9] /lib/x86_64-linux-gnu/=
libc.so.<wbr>6(__libc_start_main+0xf0)[<wbr>0x7f26ae769830]<br>[rio12:00001=
] Signal: Segmentation fault (11)<br>[rio12:00001] Signal code: Address not=
 mapped (1)<br>[rio12:00001] Failing at address: 0x7fbd937e6010<br>[rio12:0=
0001] [10] /home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_hello[0x400709]<br=
>[rio12:00001] *** End of error message ***<br>[rio12:00001] [ 0] /lib/x86_=
64-linux-gnu/<wbr>libpthread.so.0(+0x113d0)[<wbr>0x7f3f89c063d0]<br>[rio12:=
00001] [ 1] /usr/local/lib/libmca_common_<wbr>sm.so.0(+0x1035)[<wbr>0x7f3f7=
eb1d035]<br>[rio12:00001] [ 2] /usr/local/lib/libmca_common_<wbr>sm.so.0(co=
mmon_sm_mpool_<wbr>create+0xa3)[0x7f3f7eb1d583]<br>[rio12:00001] [ 3] /usr/=
local/lib/openmpi/mca_<wbr>btl_sm.so(mca_btl_sm_add_<wbr>procs+0x5f1)[0x7f3=
f7ed21771]<br>[rio12:00001] [ 4] /usr/local/lib/openmpi/mca_<wbr>bml_r2.so(=
+0x2be7)[<wbr>0x7f3f7f5f3be7]<br>[rio12:00001] [ 5] /usr/local/lib/openmpi/=
mca_<wbr>pml_ob1.so(mca_pml_ob1_add_<wbr>procs+0xd2)[0x7f3f7e0ee662]<br>[ri=
o12:00001] [ 6] /usr/local/lib/libmpi.so.0(<wbr>ompi_mpi_init+0xa51)[<wbr>0=
x7f3f89e58d31]<br>[rio12:00001] [ 7] /usr/local/lib/libmpi.so.0(<wbr>MPI_In=
it+0xb9)[0x7f3f89e7edb9]<br>[rio12:00001] [ 8] /home_nfs/georgioy/BENCHS/mp=
i-<wbr>openmp/mpi_hello[0x4007ec]<br>[rio12:00001] [ 9] /lib/x86_64-linux-g=
nu/libc.so.<wbr>6(__libc_start_main+0xf0)[<wbr>0x7f3f8984c830]<br>[rio12:00=
001] [10] /home_nfs/georgioy/BENCHS/mpi-<wbr>openmp/mpi_hello[0x400709]<br>=
[rio12:00001] *** End of error message ***<br>slurmstepd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>srun: Job s=
tep aborted: Waiting up to 32 seconds for job step to finish.<br>slurmstepd=
: error: *** STEP 86.0 ON rio12 CANCELLED AT 2016-06-08T20:12:08 ***<br>slu=
rmstepd:
 error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error
 handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>slurmstepd:=
=20
error: rio13 [1] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>slurmstepd:=
=20
error: rio12 [0] pmixp_client.c:241 [errhandler] mpi/pmix: ERROR: Error=20
handler invoked: status =3D -25, nranges =3D 0: Success (0)<br>srun: error:=
 rio12: tasks 0-2: Killed<br>srun: error: rio13: task 3: Killed<br><br>=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D<wbr>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D<br><br></div><d=
iv>the results I get when launching with mpirun instead of srun are more or=
 less the same. <br><br></div><div>From my understanding, the orted process=
, or in my case slurmstepd process that launches the singularity container =
<br></div><div>and
 the MPI application, enables the communication of MPI libraries and=20
orted (or slurmstepd) through PMI (in my case here PMIx).<br></div><div>So =
I suppose the problem I see should be related with the mapping of PMI from =
the container towards the orted or slurmstepd.<br><br></div><div>What do yo=
u think?<br></div><div><br></div><div>To
 give some more details, my singularity container has OpenMPI and PMIx=20
installed but not Slurm. I don&#39;t think that Slurm needs to reside withi=
n
 the container in that context.<br></div><div>But I wasn&#39;t sure if Open=
MPI and PMIx are needed both inside and outside of the container.<br></div>=
<div>So, I&#39;ve tried using the exact same version of PMIx and OpenMPI in=
 and out of the container and the problem still persists. <br></div><div>Th=
e
 experiments have been done using RedHat hosts with CentOS or Ubuntu=20
singularity containers and the latest github versions of slurm, OpenMPI,
 PMIx and singularity.<br></div><div><br></div><div>By the way, do you have=
 any MPI example for singularity v2? <br></div><div>Because the MPI example=
 that you show here : <a href=3D"http://singularity.lbl.gov/#hpc" target=3D=
"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;http://www.google=
.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\=
x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0phcop4-SUwsYEjw&#39;;return true;" onclic=
k=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingulari=
ty.lbl.gov%2F%23hpc\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNEKXGCj-HN-lC0ph=
cop4-SUwsYEjw&#39;;return true;">http://singularity.lbl.gov/#<wbr>hpc</a><b=
r><br></div><div>is actually done using singularity v1, no? <br>In
 my understanding with singularity v2 we actually build a complete image
 with OS not just the application with its libraries. Is this correct?<br><=
/div><div><br></div><div>Sorry for the long email and<br>thanks a lot for a=
ny extra info you can provide regarding singularity v2 and MPI.<br><br></di=
v><div>Best Regards,<br></div><div>Yiannis</div></div></div></div></div><sp=
an><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<span><font color=3D"#888888"><br>
</font></span></font></span></blockquote></div><span><font color=3D"#888888=
"><br><br clear=3D"all"><div><br></div>-- <br><div><div dir=3D"ltr"><div>Gr=
egory M. Kurtzer<br>High Performance Computing Services (HPCS)<br>Universit=
y of California<br>Lawrence Berkeley National Laboratory<br>One Cyclotron R=
oad, Berkeley, CA 94720</div></div></div>
</font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div></div></div><div><div><br><br clear=3D"all">=
<div><br></div>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High=
 Performance Computing Services (HPCS)<br>University of California<br>Lawre=
nce Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720<=
/div></div></div>
</div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div></div></div><br></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
SoWRhWgIDAAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing=
 Services (HPCS)<br>University of California<br>Lawrence Berkeley National =
Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>
</blockquote></div>
------=_Part_3825_1644207319.1497343060478--

------=_Part_3824_53927777.1497343060477--
