X-Received: by 10.157.10.164 with SMTP id 33mr3586299otq.45.1463158418134;
        Fri, 13 May 2016 09:53:38 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.180.20 with SMTP id d20ls1012428iof.80.gmail; Fri, 13 May
 2016 09:53:37 -0700 (PDT)
X-Received: by 10.66.78.73 with SMTP id z9mr24546529paw.4.1463158417738;
        Fri, 13 May 2016 09:53:37 -0700 (PDT)
Return-Path: <shapov...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id c127si25354692pfa.69.2016.05.13.09.53.37
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 13 May 2016 09:53:37 -0700 (PDT)
Received-SPF: pass (google.com: domain of shapov...@gmail.com designates 74.125.82.46 as permitted sender) client-ip=74.125.82.46;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of shapov...@gmail.com designates 74.125.82.46 as permitted sender) smtp.mailfrom=shapov...@gmail.com
X-Ironport-SBRS: 4.3
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2HzAAA3BjZXjy5SfUpeg1WBNQanD4c2hhSFBYF2hhECgSUHORMBAQEBAQEBAw8BAQEBBwsLCSEvgjc5EFUCK0EBAQQSER0BGx4DDAYFBAc3AgIiAREBBQEcO4dyAQMXBaMegTE+MYs7gWqCWAWHTQoZJw1Sg1EpAgYQimKHP4JZBY5PiViOHo8ZjgMSHoEOIgGCOA0RCoFOOTKIWgEBAQ
X-IronPort-AV: E=Sophos;i="5.24,614,1455004800"; 
   d="scan'208,217";a="23949493"
Received: from mail-wm0-f46.google.com ([74.125.82.46])
  by fe3.lbl.gov with ESMTP; 13 May 2016 09:53:23 -0700
Received: by mail-wm0-f46.google.com with SMTP id n129so29979092wmn.1
        for <singu...@lbl.gov>; Fri, 13 May 2016 09:53:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=QRk3dk4Dw0+ZcHCiXOEJEXgojL91l+kB+ZImuE4l5LI=;
        b=Iu4+cfg2Fk4YMRJhOBOtXPcTdsNgu8oAOiPub+eUeBfxJ8PgLRR19iMiGKUZYyKC/G
         GrnDukfDG2ZaAWadEOw+enJ5cro9waiSuEmuuTaItrCJl6vqMwmrRyIZZCB2d1YvRhLi
         Do/1Lp8pFBmNaQprQeHukZbZI5gmuA2xqnfYd9Abp2Jt855ebyXQyR+a0o4vZ6ks0zt4
         EkrABAe9rHnxXEfbDameFctrMssXprvEWdolRnKLtY4bver0SnVIgqfu4xzLPpyNap8W
         cfmotngDHqWXPSUeSRUnRXC6dKcM4sZwaEXzx4LH6l3F5DoFgkGf7dZ85v5OWl01BWMY
         8JSA==
X-Gm-Message-State: AOPr4FWAYAhYND3Pvng7cL+07ntXw4CiaH+HJ6QZma3KqXPyarfKfZEk4Z3wZyqTXWYPaj93c2adteYmAyjX6g==
X-Received: by 10.194.216.33 with SMTP id on1mr16930425wjc.120.1463158403250;
 Fri, 13 May 2016 09:53:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.37.230 with HTTP; Fri, 13 May 2016 09:52:43 -0700 (PDT)
In-Reply-To: <CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com>
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov> <CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com>
From: Taras Shapovalov <shapov...@gmail.com>
Date: Fri, 13 May 2016 19:52:43 +0300
Message-ID: <CAAS-_CBb3sy393W1Bga5Wnr9-EvSHC6NPNaOx58Mpdw-LiFq8g@mail.gmail.com>
Subject: Re: [Singularity] SIngularity and MPI implementations
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=089e0149359ee040580532bc1ce2

--089e0149359ee040580532bc1ce2
Content-Type: text/plain; charset=UTF-8

Hi Ralph and Gregory,

Thank you the both for the so detailed answers! I see your replies
complement each other. Although I am a bit confused now with the whole
picture, so could you confirm that I get the ideas correctly:

1. All implementations of MPI by default should work with Singularity
containers (maybe not as optimal as could be, but should start and finish
correctly always). Actually I've tested recently MPICH+Singularity with
several workload managers, worked fine (did not benchmark it comparing with
Open MPI). I did not manage to make Singularity+MPI work in LSF, but this
is a different story that deserves a separate thread.

2. MPI process calls dl_open, thus the more MPI processes starts on a node,
the more times dl_open will be called. Open MPI 2.0.1 somehow solves this
magically (I don't get how) and dl_open is called only once per node. Other
implementations of MPI and older Open MPI are not Singularity aware, thus
they still will call dl_open each time when MPI process spawns.

3. dl_open issue affects only process start time and does not effect the
process execution, so on small scale with long running processes there is
no difference between Open MPI 2.0.1 and older Open MPI versions (as well
as other MPI implementations).

4. When sapp is built then Singularity detects Open MPI (even older then
2.0.1, right?) and resolves all dependencies automatically adding all files
to the sapp. But with, say, MVAPICH2 the dependencies are not resolved
automatically, so user should add some stuff manually.

5. Apart of solving dl_open issue Open MPI 2.0.1 does some splitting
between the host and the container, which allows user/admin to not optimize
Open MPI for a target platform. I really don't get how Singularity does
this, but I get the problem. Could you explain what Singularity or Open MPI
2.0.1 does for that specificaly?

Best regards,

Taras

--089e0149359ee040580532bc1ce2
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div><div><div><div>Hi Ralph and Gregory,<br><br=
></div>Thank you the both for the so detailed answers! I see your replies c=
omplement each other. Although I am a bit confused now with the whole pictu=
re, so could you confirm that I get the ideas correctly:<br><br></div>1. Al=
l implementations of MPI by default should work with Singularity containers=
 (maybe not as optimal as could be, but should start and finish correctly a=
lways). Actually I&#39;ve tested recently MPICH+Singularity with several wo=
rkload managers, worked fine (did not benchmark it comparing with Open MPI)=
. I did not manage to make Singularity+MPI work in LSF, but this is a diffe=
rent story that deserves a separate thread.<br><br></div>2. MPI process cal=
ls dl_open, thus the more MPI processes starts on a node, the more times dl=
_open will be called. Open MPI 2.0.1 somehow solves this magically (I don&#=
39;t get how) and dl_open is called only once per node. Other implementatio=
ns of MPI and older Open MPI are not Singularity aware, thus they still wil=
l call dl_open each time when MPI process spawns. <br><br>3. dl_open issue =
affects only process start time and does not effect the process execution, =
so on small scale with long running processes there is no difference betwee=
n Open MPI 2.0.1 and older Open MPI versions (as well as other MPI implemen=
tations).<br><br></div><div>4. When sapp is built then Singularity detects =
Open MPI (even older then 2.0.1, right?) and resolves all dependencies auto=
matically adding all files to the sapp. But with, say, MVAPICH2 the depende=
ncies are not resolved automatically, so user should add some stuff manuall=
y.<br></div><div><br></div>5. Apart of solving dl_open issue Open MPI 2.0.1=
 does some splitting between the host and the container, which allows user/=
admin to not optimize Open MPI for a target platform. I really don&#39;t ge=
t how Singularity does this, but I get the problem. Could you explain what =
Singularity or Open MPI 2.0.1 does for that specificaly?<br><br></div>Best =
regards,<br><br></div>Taras<br><br></div>

--089e0149359ee040580532bc1ce2--
