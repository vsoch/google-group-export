X-Received: by 10.99.49.149 with SMTP id x143mr7397238pgx.2.1487807586534;
        Wed, 22 Feb 2017 15:53:06 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.134.99 with SMTP id i96ls720353iod.1.gmail; Wed, 22 Feb
 2017 15:53:05 -0800 (PST)
X-Received: by 10.84.132.33 with SMTP id 30mr50917208ple.44.1487807584868;
        Wed, 22 Feb 2017 15:53:04 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id n11si2528834plg.275.2017.02.22.15.53.04
        for <singu...@lbl.gov>;
        Wed, 22 Feb 2017 15:53:04 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.213.200 as permitted sender) client-ip=209.85.213.200;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.213.200 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FUAQC+I65Yf8jVVdFdHAEBBAEBCgEBFwEBBAEBCgEBgkOBQ3gRB4NMCIoIkVqCZIUoh3yFLIFKGygfAQaBc0OBbIFaAoMGBz8YAQEBAQEBAQEBAQECEAEBCQsLChsxgjMEAgMZBQQEPQoDLgEBAQEBAQEBAQEBAQEBAQEaAg0iDwMpAQEBAwEjKzALCQILDSoCAiEBDwMBBQELEQYIBwQBEwkEiHJKAw0IBZFckRY/jAOCJodBDYN+AQEBBwEBAQEBAQEBIBKLKYJRgVUQAgGDIYJfBYkZYYZRhRuFcDoBhnOHEIQfgk6BB407ikGHHRQegRUPEIECLwgZCjdNFwWDdjYDHYICHzUHiCaBZwEBAQ
X-IronPort-AV: E=Sophos;i="5.35,196,1484035200"; 
   d="scan'208,217";a="65030405"
Received: from mail-yb0-f200.google.com ([209.85.213.200])
  by fe4.lbl.gov with ESMTP; 22 Feb 2017 15:53:01 -0800
Received: by mail-yb0-f200.google.com with SMTP id h190so19123143ybb.6
        for <singu...@lbl.gov>; Wed, 22 Feb 2017 15:53:01 -0800 (PST)
X-Gm-Message-State: AMke39m+BtJuQknLVoDEgjQ9uOS/RtXoduaIjoALvfpqQ5OGNEibFsFA8bxCrY/Ew1+YRX4VIaTJdxv+rm2t+kqGEchGb4iK1ouNmTOlNJiyZVV+KbX1jNznI5MsZKOwo6MSLX5NevImzQJOp4nHyyWEBL0=
X-Received: by 10.13.228.134 with SMTP id n128mr29481435ywe.76.1487807581269;
        Wed, 22 Feb 2017 15:53:01 -0800 (PST)
X-Received: by 10.13.228.134 with SMTP id n128mr29481422ywe.76.1487807580997;
 Wed, 22 Feb 2017 15:53:00 -0800 (PST)
MIME-Version: 1.0
Received: by 10.129.110.69 with HTTP; Wed, 22 Feb 2017 15:53:00 -0800 (PST)
In-Reply-To: <887cfc8c-48ed-4720-8040-989e407f4203@lbl.gov>
References: <d0a10fdc-f912-4e9c-8681-a54f5d53fd72@lbl.gov> <CAN7etTxxeYYxY7aB93H7E686y-8Qru-c_H3t1ANNQ_4oE1C-aA@mail.gmail.com>
 <887cfc8c-48ed-4720-8040-989e407f4203@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Wed, 22 Feb 2017 15:53:00 -0800
Message-ID: <CAN7etTwgGhbKjQw3EpWXMQN7jcQURt2tTHhWZn3FgZjij_=GDA@mail.gmail.com>
Subject: Re: [Singularity] Performance impact: My experience
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=94eb2c0352605c239b05492732ee

--94eb2c0352605c239b05492732ee
Content-Type: text/plain; charset=UTF-8

Hi Chih-Song,

Haha, every now and then I get lucky when a new mail comes in and is at the
top of my mbox and I have a moment.

In summary, yes. The MPI inside the container must also link against the IB
libraries (also within the container).

Hopefully that helps!

On Wed, Feb 22, 2017 at 10:58 AM, Chih-Song Kuo <chihs...@gmail.com>
wrote:

> Hi Greg,
>
> That reply was very prompt! Anyways, my answer follows.
>
> > * Make sure the MPI inside the container is properly linking against the
> IB libraries
> Did that mean that I need to install IB libraries (like Mellanox OFED)
> into the container? I guess not?
> But apparently openib is missing in the container.
> [me@cn03 ompi-rhel6-host]$ ompi_info | grep openib
>                  MCA btl: openib (MCA v2.1.0, API v3.0.0, Component v3.0.0)
>
> Singularity.container-centos6-demo.img> ompi_info | grep openib
> Singularity.container-centos6-demo.img>
> Singularity.container-centos6-demo.img> ls /etc/infiniband/openib.conf
> ls: cannot access /etc/infiniband/openib.conf: No such file or directory
>
> > * Make sure that the MPI configuration is configured to use MPI
> I think you meant "to use IB" instead.
> But still, did you mean that OpenMPI should be "configured"
> "--with-verbs"? (Did you do so or you never had my problem?)
> I did not use this flag when compiling Open MPI either on the host or in
> the container.
> > * Make sure you have Singularity configured properly to share the
> devices properly, tmp, and you are *NOT* using the IPC or PID namespaces
> Can you provide more hint on how that can be done?
>
> Thank you once again for your time.
> Chih-Song
>
> On Wednesday, February 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer
> wrote:
>>
>> There are various things that *could* go wrong, and usage of containers
>> (any of the technologies) actually introduce complexities in kernel and
>> user space alignment that normally we don't consider. For that reason, my
>> first suspicion is that your IB fabric is not being properly utilized by
>> the MPI within the container. That could be due to anything from build
>> errors within the container to IB library/kmod API misalignment.
>>
>> Things I would look at and check:
>>
>> * Make sure the MPI inside the container is properly linking against the
>> IB libraries
>> * Make sure that the IB libraries inside the container are compatible
>> with the host kernel
>> * Make sure that the MPI configuration is configured to use MPI
>> * Make sure you have Singularity configured properly to share the devices
>> properly, tmp, and you are *NOT* using the IPC or PID namespaces
>>
>> Hope that helps.
>>
>> Greg
>>
>> On Wed, Feb 22, 2017 at 10:19 AM, Chih-Song Kuo <chi...@gmail.com>
>> wrote:
>>
>>> Hello,
>>>
>>> This is again Chih-Song from Fujitsu. I decided to make another post to
>>> share my experience of performance impact with two kernel benchmarks: High
>>> Performance Linpack (HPL) and the OSU MPI benchmark suit.
>>>
>>> Overall, there was no noticeable performance difference for benchmarks
>>> running on a single node. But for benchmarks running across nodes, I did
>>> observe some difference, which was against to the claim of Singularity.
>>> Have anybody done any similar exercise? What are your findings? Can you
>>> suspect whether I was doing anything wrong?
>>>
>>> Host configuration:
>>> 2x Intel E5-2680v2 (Ivybridge)
>>> 64GB memory
>>> Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)
>>> RHEL 6.7
>>> OpenMPI development master branch (8.2.17)
>>> Intel MKL 2017.0 community edition
>>> gcc 4.4
>>>
>>> Container:
>>> Centos 6 and 7 both tested without noticeable performance difference
>>> OpenMPI development master branch (8.2.17)
>>> Intel MKL 2017.0 community edition
>>> gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)
>>>
>>> Benchmarks:
>>> 1. LINPACK 2.2
>>> 2. OSU 5.3.2
>>>
>>> <LINPACK>
>>> Single node. N=40000, P=5, Q=4
>>> Container: 368 GFlops
>>> Host: 368 GFLOPS
>>> #A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x 8 *
>>> 2.8 = 448 GFlops
>>> Efficiency = 368 / 448 = 82%. Not bad (given the small N (matrix size)
>>> and that gcc instead of icc was used, and the executable was dynamically
>>> linked -- by purpose)
>>>
>>> Dual-node, N=60000, P=8, Q=5
>>> Container: 702 GFLOPS
>>> Host:737 GFLOPS
>>> There is roughly 5% of performance degradation with the container.
>>>
>>> <OSU-P2P-Bandwidth>
>>> The container only saw 50-65% of the total bandwidth.
>>>
>>> Container:
>>> Msg size(bytes) BW (MB/s)
>>> 65536                2142.28
>>> 131072               2363.45
>>> 262144               1705.79
>>> 524288               1592.56
>>> 1048576              1721.88
>>> 2097152              1557.42
>>> 4194304              1655.90
>>>
>>> Host:
>>> Msg size(bytes) BW (MB/s)
>>> 65536                3722.32
>>> 131072               3751.33
>>> 262144               3771.13
>>> 524288               3774.33
>>> 1048576              3781.43
>>> 2097152              3775.00
>>> 4194304              3773.68
>>>
>>> <OSU-P2P-Latency>
>>> Here the container was significantly slower.
>>>
>>> Container:
>>> Msg size(bytes)  Latency (us)
>>> 0                      31.59
>>> 1                      31.86
>>> 2                      31.83
>>>
>>> Host:
>>> Msg size(bytes)  Latency (us)
>>> 0                       1.55
>>> 1                       1.63
>>> 2                       1.63
>>>
>>> Note 1: Run-to-run variation of performance was much smaller than the
>>> difference on the host and in the container.
>>> Note 2: When Singularity was used, I could not instruct mpirun to use
>>> the ofed by specifying "--mca btl openib,self,vader" in the mpirun
>>> parameter list. Doing so would give me an error message stating that the
>>> openib component is missing. However, from the bandwidth measured above,
>>> the container did seem to be able to use InfiniBand, otherwise the
>>> bandwidth would not be so high (the nodes only had InfiniBand and 1G
>>> Ethernet). Maybe container was using IPoIB? I did not check that yet.
>>>
>>> Reference: How the benchmarks were executed:
>>> mpirun -n 20 -hostfile hostfile singularity exec
>>> /home/chih/containers/container-centos6-demo.img xhpl
>>> mpirun -n 2 -hostfile hostfile singularity exec
>>> /home/chih/containers/container-centos6-demo.img osu_bw
>>> mpirun -n 2 -hostfile hostfile singularity exec
>>> /home/chih/containers/container-centos6-demo.img osu_latency
>>>
>>> Chih-Song
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--94eb2c0352605c239b05492732ee
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Chih-Song,<div><br></div><div>Haha, every now and then =
I get lucky when a new mail comes in and is at the top of my mbox and I hav=
e a moment.</div><div><br></div><div>In summary, yes. The MPI inside the co=
ntainer must also link against the IB libraries (also within the container)=
.</div><div><br></div><div>Hopefully that helps!</div></div><div class=3D"g=
mail_extra"><br><div class=3D"gmail_quote">On Wed, Feb 22, 2017 at 10:58 AM=
, Chih-Song Kuo <span dir=3D"ltr">&lt;<a href=3D"mailto:chihs...@gmail.com"=
 target=3D"_blank">chihs...@gmail.com</a>&gt;</span> wrote:<br><blockquote =
class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid=
;padding-left:1ex"><div dir=3D"ltr">Hi Greg,<br><br>That reply was very pro=
mpt! Anyways, my answer follows.<span class=3D""><br><br>&gt; * Make sure t=
he MPI inside the container is properly linking against the IB libraries<br=
></span>Did that mean that I need to install IB libraries (like Mellanox OF=
ED) into the container? I guess not?<br>But apparently openib is missing in=
 the container.<br>[me@cn03 ompi-rhel6-host]$ ompi_info | grep openib<br>=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 MCA btl: openib (MCA v2.1.0, API v3.0.0, Component v3=
.0.0)<br><br>Singularity.container-centos6-<wbr>demo.img&gt; ompi_info | gr=
ep openib<br>Singularity.container-centos6-<wbr>demo.img&gt;<br>Singularity=
.container-centos6-<wbr>demo.img&gt; ls /etc/infiniband/openib.conf<br>ls: =
cannot access /etc/infiniband/openib.conf: No such file or directory<span c=
lass=3D""><br><br>&gt; * Make sure that the MPI configuration is configured=
 to use MPI<br></span>I think you meant &quot;to use IB&quot; instead. <br>=
But still, did you mean that OpenMPI should be &quot;configured&quot; &quot=
;--with-verbs&quot;? (Did you do so or you never had my problem?)<br>I did =
not use this flag when compiling Open MPI either on the host or in the cont=
ainer.<span class=3D""><br>&gt; * Make sure you have Singularity configured=
 properly to share the=20
devices properly, tmp, and you are *NOT* using the IPC or PID namespaces<br=
></span>Can you provide more hint on how that can be done?<br><br>Thank you=
 once again for your time.<br>Chih-Song<span class=3D""><br><br>On Wednesda=
y, February 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer wrote:</span><=
blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border=
-left:1px #ccc solid;padding-left:1ex"><span class=3D""><div dir=3D"ltr">Th=
ere are various things that *could* go wrong, and usage of containers (any =
of the technologies) actually introduce complexities in kernel and user spa=
ce alignment that normally we don&#39;t consider. For that reason, my first=
 suspicion is that your IB fabric is not being properly utilized by the MPI=
 within the container. That could be due to anything from build errors with=
in the container to IB library/kmod API misalignment.<div><br></div><div>Th=
ings I would look at and check:</div><div><br></div><div>* Make sure the MP=
I inside the container is properly linking against the IB libraries</div><d=
iv>* Make sure that the IB libraries inside the container are compatible wi=
th the host kernel</div><div>* Make sure that the MPI configuration is conf=
igured to use MPI</div><div>* Make sure you have Singularity configured pro=
perly to share the devices properly, tmp, and you are *NOT* using the IPC o=
r PID namespaces</div><div><br></div><div>Hope that helps.</div><div><br></=
div><div>Greg</div></div></span><div><br><div class=3D"gmail_quote"><div><d=
iv class=3D"h5">On Wed, Feb 22, 2017 at 10:19 AM, Chih-Song Kuo <span dir=
=3D"ltr">&lt;<a rel=3D"nofollow">chi...@gmail.com</a>&gt;</span> wrote:<br>=
</div></div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div><div class=3D"h5"><div dir=
=3D"ltr">Hello,<br><br>This is again Chih-Song from Fujitsu. I decided to m=
ake another post to share my experience of performance impact with two kern=
el benchmarks: High Performance Linpack (HPL) and the OSU MPI benchmark sui=
t.<br><br>Overall, there was no noticeable performance difference for bench=
marks running on a single node. But for benchmarks running across nodes, I =
did observe some difference, which was against to the claim of Singularity.=
 Have anybody done any similar exercise? What are your findings? Can you su=
spect whether I was doing anything wrong?<br><br>Host configuration:<br>2x =
Intel E5-2680v2 (Ivybridge)<br>64GB memory<br>Mellanox ConnectX-3 FDR adapt=
er (but connects to a Mellanox QDR switch)<br>RHEL 6.7<br>OpenMPI developme=
nt master branch (8.2.17)<br>Intel MKL 2017.0 community edition<br>gcc 4.4<=
br><br>Container:<br>Centos 6 and 7 both tested without noticeable performa=
nce difference<br>OpenMPI development master branch (8.2.17)<br>Intel MKL 2=
017.0 community edition<br>gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)<br><b=
r>Benchmarks:<br>1. LINPACK 2.2<br>2. OSU 5.3.2<br><br>&lt;LINPACK&gt;<br>S=
ingle node. N=3D40000, P=3D5, Q=3D4<br>Container: 368 GFlops<br>Host: 368 G=
FLOPS<br>#A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 =
x 8 * 2.8 =3D 448 GFlops<br>Efficiency =3D 368 / 448 =3D 82%. Not bad (give=
n the small N (matrix size) and that gcc instead of icc was used, and the e=
xecutable was dynamically linked -- by purpose)<br><br>Dual-node, N=3D60000=
, P=3D8, Q=3D5<br>Container: 702 GFLOPS<br>Host:737 GFLOPS<br>There is roug=
hly 5% of performance degradation with the container.<br><br>&lt;OSU-P2P-Ba=
ndwidth&gt;<br>The container only saw 50-65% of the total bandwidth.<br><br=
>Container: <br>Msg size(bytes) BW (MB/s)<br>65536=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2142.28<=
br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 2363.45<br>262144=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1705.79<br>524288=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 15=
92.56<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 1721.88<br>2097152=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1557.42<br>4194304=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1655.=
90<br><br>Host:<br>Msg size(bytes) BW (MB/s)<br>65536=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3722.=
32<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3771.13<br>524288=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
 3774.33<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3775.00<br>4194304=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
73.68<br><br>&lt;OSU-P2P-Latency&gt;<br>Here the container was significantl=
y slower.<br><br>Container: <br>Msg size(bytes)=C2=A0 Latency (us)<br>0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.59<br>1=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.86<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 31.83<br><br>Host:<br>Msg size(bytes)=C2=A0 Latenc=
y (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.55<=
br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.63<br>2=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1.63<br><br>Note =
1: Run-to-run variation of performance was much smaller than the difference=
 on the host and in the container.<br>Note 2: When Singularity was used, I =
could not instruct mpirun to use the ofed by specifying &quot;--mca btl ope=
nib,self,vader&quot; in the mpirun parameter list. Doing so would give me a=
n error message stating that the openib component is missing. However, from=
 the bandwidth measured above, the container did seem to be able to use Inf=
iniBand, otherwise the bandwidth would not be so high (the nodes only had I=
nfiniBand and 1G Ethernet). Maybe container was using IPoIB? I did not chec=
k that yet. <br><br>Reference: How the benchmarks were executed:<br>mpirun =
-n 20 -hostfile hostfile singularity exec /home/chih/containers/containe<wb=
r>r-centos6-demo.img xhpl<br>mpirun -n 2 -hostfile hostfile singularity exe=
c /home/chih/containers/containe<wbr>r-centos6-demo.img osu_bw<br>mpirun -n=
 2 -hostfile hostfile singularity exec /home/chih/containers/containe<wbr>r=
-centos6-demo.img osu_latency<span><font color=3D"#888888"><br><br>Chih-Son=
g<br><br></font></span></div></div></div><span><font color=3D"#888888"><div=
><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span class=3D""><br><br clear=3D"all"><di=
v><br></div>-- <br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div di=
r=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Greg=
ory M. Kurtzer</div><div>HPC Systems Architect and Technology Developer</di=
v><div>Lawrence Berkeley National Laboratory HPCS<br>University of Californ=
ia Berkeley Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"ht=
tp://singularity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singul=
arity<wbr>.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a hre=
f=3D"http://warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://wa=
rewulf.lb<wbr>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.=
com/gmkurtzer" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<wb=
r>urtzer</a>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a=
 href=3D"https://twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"n=
ofollow" target=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></di=
v></div></div></div></div></div></div></div></div></div>
</span></div>
</blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr">=
<div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sys=
tems Architect and Technology Developer</div><div>Lawrence Berkeley Nationa=
l Laboratory HPCS<br>University of California Berkeley Research IT<br>Singu=
larity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targe=
t=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster Ma=
nagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http:=
//warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.c=
om/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<spa=
n style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitte=
r.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twitt=
er.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div><=
/div></div>
</div>

--94eb2c0352605c239b05492732ee--
