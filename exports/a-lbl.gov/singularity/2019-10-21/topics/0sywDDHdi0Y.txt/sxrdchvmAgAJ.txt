X-Received: by 10.157.17.54 with SMTP id g51mr7369946ote.37.1466687353036;
        Thu, 23 Jun 2016 06:09:13 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.46.32 with SMTP id i32ls1217654ioo.30.gmail; Thu, 23 Jun
 2016 06:09:12 -0700 (PDT)
X-Received: by 10.98.49.133 with SMTP id x127mr39670134pfx.90.1466687352408;
        Thu, 23 Jun 2016 06:09:12 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id xq1si120041pab.11.2016.06.23.06.09.12
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Thu, 23 Jun 2016 06:09:12 -0700 (PDT)
Received-SPF: pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.49 as permitted sender) client-ip=209.85.220.49;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.49 as permitted sender) smtp.mailfrom=rhc.o...@gmail.com
X-Ironport-SBRS: 3.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2DcAAAU3mtXhzHcVdFdgnCBJH2DPIEMoRgBAQaDW4RxjAaCEQEGhXkCgS06EgEBAQEBAQESAQEBCA0JCSEvhEwBAQEDARIRHQENGRMDAQsBBQUYIAcDAgIhEAMBBQELEQ4HBAEcBAGHdAMPCAWoLoExPjGLO4REBYdPJw2EAQEcAgYQhU8GBYI0glaCQ4FPEQEGTIJLK4IvBYhzhXqEXoUANIYIhiuDZ06EBYJ9DoVdiA+GMDCBDw8WDFiBRoIVTgeIaIE1AQEB
X-IronPort-AV: E=Sophos;i="5.26,516,1459839600"; 
   d="scan'208,217";a="27215800"
Received: from mail-pa0-f49.google.com ([209.85.220.49])
  by fe4.lbl.gov with ESMTP; 23 Jun 2016 06:09:10 -0700
Received: by mail-pa0-f49.google.com with SMTP id bz2so27487357pad.1
        for <singu...@lbl.gov>; Thu, 23 Jun 2016 06:09:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=sender:from:message-id:mime-version:subject:date:references:to
         :in-reply-to;
        bh=QOvnZ/yRCBg+SU+liUIg+bxJozRwUp2ZkuVylZLcg2Q=;
        b=c54QX9T8PPD3RqBVi/B6cPeXL0RMHPfsBDv91h+exgodxJVaMRaSzkKRj/VRwGB2dY
         sRJjnu1YnFWx4HQICudbSYL0fXHBMhGVO1HpZi4yIPTNyes4z0+ePHm0aR5aLIweOmcX
         tvjK5pieoFnCBmqoTecfw65FfIREBpToV/3VOKy1tcKoh9e61zMaYjCBm/p6/0/ZEXdx
         Yosk6CbhOBoFxTlwXgbbci8e3Wgz4KKmirq1kk0eX4LlYZSJ+jmayLBGQ17SOiuZXL5b
         A91eKDMPL8Th51TBt7gyCNyOllhWiPkaHGMyIKPuQq1KMq6Qr3t3ZCmhhL/lv64NNenX
         G9SA==
X-Gm-Message-State: ALyK8tLo49/uwFLa/qdA1AkKqz0tYzUA6j9Scm8ElMPU83EUU1Qn1HNS+igjP3Al6iP5eA==
X-Received: by 10.66.249.161 with SMTP id yv1mr42541715pac.39.1466687350104;
        Thu, 23 Jun 2016 06:09:10 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from [192.168.0.6] ([208.100.172.177])
        by smtp.gmail.com with ESMTPSA id k22sm203917pfj.16.2016.06.23.06.09.08
        for <singu...@lbl.gov>
        (version=TLS1 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
        Thu, 23 Jun 2016 06:09:08 -0700 (PDT)
Sender: Ralph Castain <rhc.o...@gmail.com>
From: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary="Apple-Mail=_6400A11E-0667-49BF-9EA7-783821499710"
Message-Id: <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
Mime-Version: 1.0 (Mac OS X Mail 9.3 \(3124\))
Subject: Re: [Singularity] Communication between singularity containers
Date: Thu, 23 Jun 2016 06:09:07 -0700
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov> <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com> <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov> <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com> <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov> <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com> <054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov>
To: singularity@lbl.gov
In-Reply-To: <054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov>
X-Mailer: Apple Mail (2.3124)

--Apple-Mail=_6400A11E-0667-49BF-9EA7-783821499710
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

I think you are misunderstanding the basic nature of the Singularity =E2=80=
=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overlay. So =E2=80=
=9Csharing=E2=80=9D a container is no different than running on a node wher=
e the procs all see the same file system. Thus, having multiple containers =
that are identical makes no sense - it=E2=80=99s all the same file system.

Now if you want to run different containers (e.g., with different libraries=
 or OS in them), then you would use mpirun=E2=80=99s MPMD syntax - for exam=
ple:

mpirun -n 1 <container1> : -n 1 <container2>

HTH
Ralph

> On Jun 23, 2016, at 1:53 AM, Raimon Bosch <raimo...@gmail.com> wrote:
>=20
>=20
> One last question: What if I want to execute more than one container in t=
he same host? With this technique I am bounded always to the same container=
. One of our experiments was based in measuring performance of several cont=
ainers working in parallel in the same node. Also we had experiments with N=
 containers per host in a multihost environment.
>=20
> El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Kurt=
zer escribi=C3=B3:
> Hi Raimon,
>=20
> Sorry I wasn't clear. I am not yet at my computer and thinking while typi=
ng on an iPhone hinders my mental processes. Lol
>=20
> If I understand your example properly, you have a docker or VM infrastruc=
ture already set up and you are invoking the mpirun commands from within th=
e virtual environment. Singularity works on a very different premis because=
 integrating a virtual cluster into an existing cluster and scheduling syst=
em is a mess.
>=20
> So starting from the physical nodes, which already have access to all oth=
er nodes in the cluster, and already scheduled properly, and in direct acce=
ss to optimized hardware and file systems.... You call mpirun.=20
>=20
> The mpirun command will take the standard format as you illustrated with =
the following change to call Singularity inline:
>=20
> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img trac=
e.sh bt.C.4
>=20
> This assumes the following:
>=20
> 1. The container image which contains the program's you want to run is at=
 ~/container.img and accessible at this path on all nodes referenced in hos=
ts.txt
> 2. The hosts.txt references other physical nodes you want to run on
> 3. The executables trace.sh and bt.C.4 are both inside the container and =
in a standard path
>=20
> In this case we are performing one execution and MPI + singularity is man=
aging all of the communication between processes, nodes and containers. Als=
o it is now using any optimized hardware (eg. Infiniband) and existing high=
 performance file systems (which should not be accessible via a virtualized=
 or Docker'ized cluster for security reasons).
>=20
> This way is actually MUCH simpler then what you are proposing because the=
re is no need to manage any virtual nodes, virtual networks, or resource ma=
nager hacks. It really is as easy as just running any other MPI process on =
an existing cluster.=20
>=20
> Hope that helps better!
>=20
>=20
>=20
> On Wednesday, June 22, 2016, Raimon Bosch <raimon...@ <>gmail.com <http:/=
/gmail.com/>> wrote:
>=20
>=20
> Hi Gregory,
>=20
> I'm not sure if I would achieve the same with your commands. In an enviro=
nment based on dockers or virtual machines we would do something like this =
[non applicable to Singularity]:
>=20
> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh ./bt.C=
.4
>=20
> where hosts.txt* is:
>=20
> >vm-ip-01-on-host01 slots=3D2
> >vm-ip-01-on-host02 slots=3D2
>=20
> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>=20
> and trace.sh is:
>=20
> >#!/bin/bash
> >
> >export EXTRAE_HOME=3D/opt/extrae/
> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
> >
> >## Run the desired program
> >$*
>=20
> As you see we only perform one execution and OpenMPI transparently manage=
s communication between containers or virtual machines. This command would =
work well rather VMs are on the same host or not.
>=20
> What I understand from your response is that now we should execute OpenMP=
I on each host and then merge results manually. I don't know yet how to do =
this merge step or if it is any way to centralize everything like I would d=
o with VMs.
>=20
> Thanks in advance,
>=20
> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Kurt=
zer escribi=C3=B3:
> Hi Raimon,
>=20
> The quick answer is you have mpirun handle that as you would normally whe=
re the container file lives on a shared file system:
>=20
> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>=20
> Let the MPI outside the container launch the singularity container on eac=
h host as it would normally launch any MPI program. Then it will call Singu=
lairty and Singularity will launch the MPI program inside the container on =
each of your hosts/servers.=20
>=20
> Hope that helps!
>=20
>=20
>=20
> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com <>> wrote:
>=20
> Hi Gregory,
>=20
> Thank you for your answer. One of our experiments needs to run OpenMPI am=
ong several servers. This means that we should put one of our containers in=
 host01, another in host02 and another in host03 and collect the results.=
=20
>=20
> How can I do this execution in parallel if I need to communicate with mor=
e than one server?
>=20
> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer escr=
ibi=C3=B3:
> Hi Raimon,
>=20
> The communication model of a Singularity container is very different from=
 that of a Docker implementation. This is because Docker for all practical =
purposes emulates a virtual machine as each container has it's own IP addre=
ss and thus it's own ssh server. It also carries its own set of complexitie=
s, for example networks need to be segregated/VLan'ed, DNS/host resolution =
needs to be dynamic and passed down to the containers (so they can reach ea=
ch other), ssh daemons and other process running inside the containers, man=
agement via an existing scheduling system, and the list goes on and on.
>=20
> Think of it this way, Singularity does not do any of that... It runs a pr=
ogram within the container as if it were running on the host itself, so to =
communicate between containers is as easy as communicating between programs=
. So for MPI, it would happen with the MPI on the physical host (outside th=
e container) invoking the container subsystem which then invokes the MPI pr=
ograms within the container and the MPI programs within the container commu=
nicate back to the MPI (orted) outside the container on the host to get acc=
ess to the host resources. In this model all available resources and infras=
tructure can be leveraged at full bandwidth by the contained processes and =
all of the aforementioned complexities akin to running on a virtualized min=
i-cluster are circumvented.
>=20
> There is additional information I have written at:
>=20
> http://singularity.lbl.gov/#hpc <http://singularity.lbl.gov/#hpc>
>=20
> That page is still coming along, and needs more information still but if =
you have any questions, comments or change proposals please let us know!
>=20
> Thanks and hope that helps!
>=20
>=20
>=20
> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com <>> wrote=
:
>=20
>=20
> Hi,
>=20
> We are trying to run experiments using singularity containers. The idea i=
s to run OpenMPI among several containers and check performance results.=20
>=20
> How can I communicate with another container? In docker this is clear bec=
ause every container gets an assigned IP and you can ping there, but what i=
s the situation in the case of singularity? Is it possible to assign an IP =
to each container? Can I connect via ssh to them?
>=20
> Thanks in advance,
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <>.
>=20
>=20
>=20
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <>.
>=20
>=20
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <>.
>=20
>=20
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.


--Apple-Mail=_6400A11E-0667-49BF-9EA7-783821499710
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html charset=
=3Dutf-8"></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space;" class=3D""><div>I think you a=
re misunderstanding the basic nature of the Singularity =E2=80=9Ccontainer=
=E2=80=9D. It=E2=80=99s just a file system overlay. So =E2=80=9Csharing=E2=
=80=9D a container is no different than running on a node where the procs a=
ll see the same file system. Thus, having multiple containers that are iden=
tical makes no sense - it=E2=80=99s all the same file system.</div><div><br=
 class=3D""></div><div>Now if you want to run different containers (e.g., w=
ith different libraries or OS in them), then you would use mpirun=E2=80=99s=
 MPMD syntax - for example:</div><div><br class=3D""></div><div>mpirun -n 1=
 &lt;container1&gt; : -n 1 &lt;container2&gt;</div><div><br class=3D""></di=
v><div>HTH</div><div>Ralph</div><div><br class=3D""><blockquote type=3D"cit=
e" class=3D""><div class=3D"">On Jun 23, 2016, at 1:53 AM, Raimon Bosch &lt=
;<a href=3D"mailto:raimo...@gmail.com" class=3D"">raimo...@gmail.com</a>&gt=
; wrote:</div><br class=3D"Apple-interchange-newline"><div class=3D""><div =
dir=3D"ltr" style=3D"font-family: Helvetica; font-size: 12px; font-style: n=
ormal; font-variant-caps: normal; font-weight: normal; letter-spacing: norm=
al; orphans: auto; text-align: start; text-indent: 0px; text-transform: non=
e; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-strok=
e-width: 0px;" class=3D""><br class=3D"">One last question: What if I want =
to execute more than one container in the same host? With this technique I =
am bounded always to the same container. One of our experiments was based i=
n measuring performance of several containers working in parallel in the sa=
me node. Also we had experiments with N containers per host in a multihost =
environment.<br class=3D""><br class=3D"">El mi=C3=A9rcoles, 22 de junio de=
 2016, 16:41:58 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=
=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8ex; border-left-width: 1px=
; border-left-color: rgb(204, 204, 204); border-left-style: solid; padding-=
left: 1ex;">Hi Raimon,<div class=3D""><br class=3D""></div><div class=3D"">=
Sorry I wasn't clear. I am not yet at my computer and thinking while typing=
 on an iPhone hinders my mental processes. Lol</div><div class=3D""><br cla=
ss=3D""></div><div class=3D"">If I understand your example properly, you ha=
ve a docker or VM infrastructure already set up and you are invoking the mp=
irun commands from within the virtual environment. Singularity works on a v=
ery different premis because integrating a virtual cluster into an existing=
 cluster and scheduling system is a mess.</div><div class=3D""><br class=3D=
""></div><div class=3D"">So starting from the physical nodes, which already=
 have access to all other nodes in the cluster, and already scheduled prope=
rly, and in direct access to optimized hardware and file systems.... You ca=
ll mpirun.&nbsp;</div><div class=3D""><br class=3D""></div><div class=3D"">=
The mpirun command will take the standard format as you illustrated with th=
e following change to call Singularity inline:</div><div class=3D""><br cla=
ss=3D""></div><div class=3D"">$ mpirun -np 4 --hostfile hosts.txt singulari=
ty exec ~/container.img trace.sh bt.C.4</div><div class=3D""><br class=3D""=
></div><div class=3D"">This assumes the following:</div><div class=3D""><br=
 class=3D""></div><div class=3D"">1. The container image which contains the=
 program's you want to run is at ~/container.img and accessible at this pat=
h&nbsp;on all nodes referenced in hosts.txt</div><div class=3D"">2. The hos=
ts.txt references other physical nodes you want to run on</div><div class=
=3D"">3. The executables trace.sh and bt.C.4 are both inside the container =
and in a standard path</div><br class=3D""><div class=3D"">In this case we =
are performing one execution and MPI + singularity is managing all of the c=
ommunication between processes, nodes and containers. Also it is now using =
any optimized hardware (eg. Infiniband)&nbsp;and existing high performance&=
nbsp;file systems (which should not be accessible via a virtualized or Dock=
er'ized&nbsp;cluster for security reasons).</div><div class=3D""><br class=
=3D""></div><div class=3D"">This way is actually MUCH simpler then what you=
 are proposing because there is no need to manage any virtual nodes, virtua=
l networks, or resource manager hacks. It really is as easy as just running=
 any other MPI&nbsp;process on an existing cluster.&nbsp;</div><div class=
=3D""><br class=3D""></div><div class=3D"">Hope that helps better!</div><di=
v class=3D""><br class=3D""></div><div class=3D""><span class=3D""></span><=
br class=3D""><br class=3D"">On Wednesday, June 22, 2016, Raimon Bosch &lt;=
<a target=3D"_blank" gdf-obfuscated-mailto=3D"xDydkJacAgAJ" rel=3D"nofollow=
" class=3D"">raimon...@</a><a href=3D"http://gmail.com/" class=3D"">gmail.c=
om</a>&gt; wrote:<br class=3D""><blockquote class=3D"gmail_quote" style=3D"=
margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(2=
04, 204, 204); border-left-style: solid; padding-left: 1ex;"><div dir=3D"lt=
r" class=3D""><br class=3D""><br class=3D"">Hi Gregory,<br class=3D""><br c=
lass=3D"">I'm not sure if I would achieve the same with your commands. In a=
n environment based on dockers or virtual machines we would do something li=
ke this [non applicable to Singularity]:<br class=3D""><br class=3D"">&gt; =
cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile hosts.txt ./trace.sh ./=
bt.C.4<br class=3D""><br class=3D"">where hosts.txt* is:<br class=3D""><br =
class=3D"">&gt;vm-ip-01-on-<b class=3D"">host01</b><span class=3D"Apple-con=
verted-space">&nbsp;</span>slots=3D2<br class=3D"">&gt;vm-ip-01-on-<b class=
=3D"">host02</b><span class=3D"Apple-converted-space">&nbsp;</span>slots=3D=
2<br class=3D""><br class=3D"">* vm-ip-XX-on-hostXX<b class=3D""><span clas=
s=3D"Apple-converted-space">&nbsp;</span></b>are IPs i.e. 172.100.60.XX<br =
class=3D""><br class=3D"">and trace.sh is:<br class=3D""><br class=3D"">&gt=
;#!/bin/bash<br class=3D"">&gt;<br class=3D"">&gt;export EXTRAE_HOME=3D/opt=
/extrae/<br class=3D"">&gt;export EXTRAE_CONFIG_FILE=3D/extrae.xml<br class=
=3D"">&gt;export LD_PRELOAD=3D${EXTRAE_HOME}/lib/<wbr class=3D"">libmpitrac=
e.so<br class=3D"">&gt;<br class=3D"">&gt;## Run the desired program<br cla=
ss=3D"">&gt;$*<br class=3D""><br class=3D"">As you see we only perform one =
execution and OpenMPI transparently manages communication between container=
s or virtual machines. This command would work well rather VMs are on the s=
ame host or not.<br class=3D""><br class=3D"">What I understand from your r=
esponse is that now we should execute OpenMPI on each host and then merge r=
esults manually. I don't know yet how to do this merge step or if it is any=
 way to centralize everything like I would do with VMs.<br class=3D""><br c=
lass=3D"">Thanks in advance,<br class=3D""><br class=3D"">El mi=C3=A9rcoles=
, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<=
blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8ex; border=
-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style:=
 solid; padding-left: 1ex;">Hi Raimon,<div class=3D""><br class=3D""></div>=
<div class=3D"">The quick answer is you have mpirun handle that as you woul=
d normally where the container file lives on a shared file system:</div><di=
v class=3D""><br class=3D""></div><div class=3D"">$ mpirun singularity exec=
 ~/container.img mpi_prog_in_container</div><div class=3D""><br class=3D"">=
</div><div class=3D"">Let the MPI outside the container launch the singular=
ity container on each host as it would normally launch any MPI program. The=
n it will call Singulairty and Singularity will launch the MPI program insi=
de the container on each of your hosts/servers.&nbsp;</div><div class=3D"">=
<br class=3D""></div><div class=3D"">Hope that helps!</div><div class=3D"">=
<br class=3D""></div><div class=3D""><span class=3D""></span><br class=3D""=
><br class=3D"">On Wednesday, June 22, 2016, Raimon Bosch &lt;<a rel=3D"nof=
ollow" class=3D"">rai...@gmail.com</a>&gt; wrote:<br class=3D""><blockquote=
 class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8ex; border-left-widt=
h: 1px; border-left-color: rgb(204, 204, 204); border-left-style: solid; pa=
dding-left: 1ex;"><div dir=3D"ltr" class=3D""><br class=3D"">Hi Gregory,<br=
 class=3D""><br class=3D"">Thank you for your answer. One of our experiment=
s needs to run OpenMPI among several servers. This means that we should put=
 one of our containers in host01, another in host02 and another in host03 a=
nd collect the results.<span class=3D"Apple-converted-space">&nbsp;</span><=
br class=3D""><br class=3D"">How can I do this execution in parallel if I n=
eed to communicate with more than one server?<br class=3D""><br class=3D"">=
El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer escrib=
i=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8e=
x; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-le=
ft-style: solid; padding-left: 1ex;"><div dir=3D"ltr" class=3D"">Hi Raimon,=
<div class=3D""><br class=3D""></div><div class=3D"">The communication mode=
l of a Singularity container is very different from that of a Docker implem=
entation. This is because Docker for all practical purposes emulates a virt=
ual machine as each container has it's own IP address and thus it's own ssh=
 server. It also carries its own set of complexities, for example networks =
need to be segregated/VLan'ed, DNS/host resolution needs to be dynamic and =
passed down to the containers (so they can reach each other), ssh daemons a=
nd other process running inside the containers, management via an existing =
scheduling system, and the list goes on and on.</div><div class=3D""><br cl=
ass=3D""></div><div class=3D"">Think of it this way, Singularity does not d=
o any of that... It runs a program within the container as if it were runni=
ng on the host itself, so to communicate between containers is as easy as c=
ommunicating between programs. So for MPI, it would happen with the MPI on =
the physical host (outside the container) invoking the container subsystem =
which then invokes the MPI programs within the container and the MPI progra=
ms within the container communicate back to the MPI (orted) outside the con=
tainer on the host to get access to the host resources. In this model all a=
vailable resources and infrastructure can be leveraged at full bandwidth by=
 the contained processes and all of the aforementioned complexities akin to=
 running on a virtualized mini-cluster are circumvented.</div><div class=3D=
""><br class=3D""></div><div class=3D"">There is additional information I h=
ave written at:</div><div class=3D""><br class=3D""></div><div class=3D""><=
a href=3D"http://singularity.lbl.gov/#hpc" rel=3D"nofollow" target=3D"_blan=
k" class=3D"">http://singularity.lbl.gov/#<wbr class=3D"">hpc</a><br class=
=3D""></div><div class=3D""><br class=3D""></div><div class=3D"">That page =
is still coming along, and needs more information still but if you have any=
 questions, comments or change proposals please let us know!</div><div clas=
s=3D""><br class=3D""></div><div class=3D"">Thanks and hope that helps!</di=
v><div class=3D""><br class=3D""></div><div class=3D""><br class=3D""></div=
></div><div class=3D""><br class=3D""><div class=3D"gmail_quote">On Tue, Ju=
n 21, 2016 at 7:37 AM, Raimon Bosch<span class=3D"Apple-converted-space">&n=
bsp;</span><span dir=3D"ltr" class=3D"">&lt;<a rel=3D"nofollow" class=3D"">=
rai...@gmail.com</a>&gt;</span><span class=3D"Apple-converted-space">&nbsp;=
</span>wrote:<br class=3D""><blockquote class=3D"gmail_quote" style=3D"marg=
in: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, =
204, 204); border-left-style: solid; padding-left: 1ex;"><div dir=3D"ltr" c=
lass=3D""><br class=3D""><br class=3D"">Hi,<br class=3D""><br class=3D"">We=
 are trying to run experiments using singularity containers. The idea is to=
 run OpenMPI among several containers and check performance results.<span c=
lass=3D"Apple-converted-space">&nbsp;</span><br class=3D""><br class=3D"">H=
ow can I communicate with another container? In docker this is clear becaus=
e every container gets an assigned IP and you can ping there, but what is t=
he situation in the case of singularity? Is it possible to assign an IP to =
each container? Can I connect via ssh to them?<br class=3D""><br class=3D""=
>Thanks in advance,<span class=3D""><font color=3D"#888888" class=3D""><br =
class=3D""></font></span></div><span class=3D""><font color=3D"#888888" cla=
ss=3D""><div class=3D""><br class=3D"webkit-block-placeholder"></div>--<spa=
n class=3D"Apple-converted-space">&nbsp;</span><br class=3D"">You received =
this message because you are subscribed to the Google Groups "singularity" =
group.<br class=3D"">To unsubscribe from this group and stop receiving emai=
ls from it, send an email to<span class=3D"Apple-converted-space">&nbsp;</s=
pan><a rel=3D"nofollow" class=3D"">singu...@lbl.gov</a>.<br class=3D""></fo=
nt></span></blockquote></div><br class=3D""><br clear=3D"all" class=3D""><d=
iv class=3D""><br class=3D""></div>--<span class=3D"Apple-converted-space">=
&nbsp;</span><br class=3D""><div class=3D""><div dir=3D"ltr" class=3D""><di=
v class=3D"">Gregory M. Kurtzer<br class=3D"">High Performance Computing Se=
rvices (HPCS)<br class=3D"">University of California<br class=3D"">Lawrence=
 Berkeley National Laboratory<br class=3D"">One Cyclotron Road, Berkeley, C=
A 94720</div></div></div></div></blockquote></div><div class=3D""><br class=
=3D"webkit-block-placeholder"></div>--<span class=3D"Apple-converted-space"=
>&nbsp;</span><br class=3D"">You received this message because you are subs=
cribed to the Google Groups "singularity" group.<br class=3D"">To unsubscri=
be from this group and stop receiving emails from it, send an email to<span=
 class=3D"Apple-converted-space">&nbsp;</span><a class=3D"">singularity+uns=
ubscribe@lbl.<wbr class=3D"">gov</a>.<br class=3D""></blockquote></div><br =
class=3D""><br class=3D"">--<span class=3D"Apple-converted-space">&nbsp;</s=
pan><br class=3D""><div dir=3D"ltr" class=3D""><div class=3D"">Gregory M. K=
urtzer<br class=3D"">High Performance Computing Services (HPCS)<br class=3D=
"">University of California<br class=3D"">Lawrence Berkeley National Labora=
tory<br class=3D"">One Cyclotron Road, Berkeley, CA 94720</div></div><br cl=
ass=3D""></blockquote></div><div class=3D""><br class=3D"webkit-block-place=
holder"></div>--<span class=3D"Apple-converted-space">&nbsp;</span><br clas=
s=3D"">You received this message because you are subscribed to the Google G=
roups "singularity" group.<br class=3D"">To unsubscribe from this group and=
 stop receiving emails from it, send an email to<span class=3D"Apple-conver=
ted-space">&nbsp;</span><a class=3D"">singularity+unsubscribe@lbl.<wbr clas=
s=3D"">gov</a>.<br class=3D""></blockquote></div><br class=3D""><br class=
=3D"">--<span class=3D"Apple-converted-space">&nbsp;</span><br class=3D""><=
div dir=3D"ltr" class=3D""><div class=3D"">Gregory M. Kurtzer<br class=3D""=
>High Performance Computing Services (HPCS)<br class=3D"">University of Cal=
ifornia<br class=3D"">Lawrence Berkeley National Laboratory<br class=3D"">O=
ne Cyclotron Road, Berkeley, CA 94720</div></div><br class=3D""></blockquot=
e></div><div style=3D"font-family: Helvetica; font-size: 12px; font-style: =
normal; font-variant-caps: normal; font-weight: normal; letter-spacing: nor=
mal; orphans: auto; text-align: start; text-indent: 0px; text-transform: no=
ne; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-stro=
ke-width: 0px;" class=3D""><br class=3D"webkit-block-placeholder"></div><sp=
an style=3D"font-family: Helvetica; font-size: 12px; font-style: normal; fo=
nt-variant-caps: normal; font-weight: normal; letter-spacing: normal; orpha=
ns: auto; text-align: start; text-indent: 0px; text-transform: none; white-=
space: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: =
0px; float: none; display: inline !important;" class=3D"">--<span class=3D"=
Apple-converted-space">&nbsp;</span></span><br style=3D"font-family: Helvet=
ica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-w=
eight: normal; letter-spacing: normal; orphans: auto; text-align: start; te=
xt-indent: 0px; text-transform: none; white-space: normal; widows: auto; wo=
rd-spacing: 0px; -webkit-text-stroke-width: 0px;" class=3D""><span style=3D=
"font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-=
caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; t=
ext-align: start; text-indent: 0px; text-transform: none; white-space: norm=
al; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float:=
 none; display: inline !important;" class=3D"">You received this message be=
cause you are subscribed to the Google Groups "singularity" group.</span><b=
r style=3D"font-family: Helvetica; font-size: 12px; font-style: normal; fon=
t-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphan=
s: auto; text-align: start; text-indent: 0px; text-transform: none; white-s=
pace: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0=
px;" class=3D""><span style=3D"font-family: Helvetica; font-size: 12px; fon=
t-style: normal; font-variant-caps: normal; font-weight: normal; letter-spa=
cing: normal; orphans: auto; text-align: start; text-indent: 0px; text-tran=
sform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-=
text-stroke-width: 0px; float: none; display: inline !important;" class=3D"=
">To unsubscribe from this group and stop receiving emails from it, send an=
 email to<span class=3D"Apple-converted-space">&nbsp;</span></span><a href=
=3D"mailto:singu...@lbl.gov" style=3D"font-family: Helvetica; font-size: 12=
px; font-style: normal; font-variant-caps: normal; font-weight: normal; let=
ter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; te=
xt-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -=
webkit-text-stroke-width: 0px;" class=3D"">singu...@lbl.gov</a><span style=
=3D"font-family: Helvetica; font-size: 12px; font-style: normal; font-varia=
nt-caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto=
; text-align: start; text-indent: 0px; text-transform: none; white-space: n=
ormal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; flo=
at: none; display: inline !important;" class=3D"">.</span></div></blockquot=
e></div><br class=3D""></body></html>
--Apple-Mail=_6400A11E-0667-49BF-9EA7-783821499710--
