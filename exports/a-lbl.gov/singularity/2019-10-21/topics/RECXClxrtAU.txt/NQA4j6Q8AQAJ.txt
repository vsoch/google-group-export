X-Received: by 2002:aa7:8589:: with SMTP id w9-v6mr1871273pfn.39.1530180500607;
        Thu, 28 Jun 2018 03:08:20 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a65:600c:: with SMTP id m12-v6ls1127334pgu.15.gmail; Thu, 28
 Jun 2018 03:08:19 -0700 (PDT)
X-Received: by 2002:a65:61a7:: with SMTP id i7-v6mr5159476pgv.219.1530180499465;
        Thu, 28 Jun 2018 03:08:19 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1530180499; cv=none;
        d=google.com; s=arc-20160816;
        b=CfdN+TtU59b1BnT4P/36obJCsRiJ5ybc8qf8Lbv/tqfeLpMWww/ngpmI9xQkZWUHqt
         XbWIABuIvZPbSmyBt+I+A+OFTRLloFGPKX4Lgap5iWgoBuIcj0CCqDYQA2WjVI7thUi+
         WZ8R0bdHdsT3RL0ZyjNRSapA4FFT9YCWrstK7Xp5qLcLe0WPYbkbKgKPYkZKnkrWRWFp
         4Ps1eNBhQiHIh2ivNyI4JPk1K3V+JNAkK05BDz7wt/IZFVGJKf0C0JpvT6N6s3lbv2f8
         pmrDRcENPp2t6j2txqfid0p1WCpk1mHsKWS0ZUfmp0ms3JwMNP67+PqgWh2ahO252Chc
         Vd8Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature:arc-authentication-results;
        bh=JnIP1P++81by0MNhofS33MmC7cnusbRs7U/+JAVzv9Q=;
        b=iKj9tE0LlPOAPRDUtWdpSrayax+JrYjs+G5ucRjBxPXTN/Rr+dNVqSNAPEgNjQrGLr
         617SFjSWH86xFxMk0QzsEuzRqz5MjWt4UQhyRmNYznwQpJIcfqvV8jjllCrAX3k0nHMq
         TBL7ajh3atOH0XR4F921dMEyO8FfN3Xz+iJAlH5PSmjYwv5vAalfpbkKaEsoqfMX1Q6O
         Q54kv2eYaLnfhtTzGJp5Pfiq5jznNPPomC80bBUiiZwrlhYjDyewqknSaLnI2j1go02s
         5mGSBohcQ8CqPVE3Duvxf2eFHg5PdnU1W1+mEwwEyhoDqYtnEzKhE+Z0y7Vk+8nmH216
         l1bA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Z7ZgTKh8;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.223.172 as permitted sender) smtp.mailfrom=vso...@gmail.com
Return-Path: <vso...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id 6-v6si5763969pgg.366.2018.06.28.03.08.19
        for <singu...@lbl.gov>;
        Thu, 28 Jun 2018 03:08:19 -0700 (PDT)
Received-SPF: pass (google.com: domain of vso...@gmail.com designates 209.85.223.172 as permitted sender) client-ip=209.85.223.172;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=Z7ZgTKh8;
       spf=pass (google.com: domain of vso...@gmail.com designates 209.85.223.172 as permitted sender) smtp.mailfrom=vso...@gmail.com
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2CKBAAwszRbgKzfVdFQBAUDHgEGDIMfg?=
 =?us-ascii?q?Qx/KINzBoEdglCQVIIHApUvgWMDIwEOgVCCdQKDGSE3FQECAQECAQECARMBAQk?=
 =?us-ascii?q?NCQgnJQyCNQUCAwIeglcBAQEDARoDBgQZAQ0OCRUDAQsGBQsNIAoCAiEBAQ4DA?=
 =?us-ascii?q?QUBHA4HBAEUBgIEgjRLASiBPgEDDQgFCqEGPIlXEYEggWkWBQEXgnIFaoJtChk?=
 =?us-ascii?q?mDVdXgRQCBhKIW4IVgQ+CEX6CVjcLAQEBAYEfDAENBQEDBgI1DBqCOoJVAodQE?=
 =?us-ascii?q?QgVgTGDGWqEJYc1KwmGAIYKgwmBQEKDRYJqhRmKJ06GcTCBNlUuXQwIcBVsgjg?=
 =?us-ascii?q?JFoF4DBd6AQ6CPIJkgjCCZoJXIR8wAQsEjg4BAQ0XBAMpgW0FAQE?=
X-IronPort-AV: E=Sophos;i="5.51,283,1526367600"; 
   d="scan'208,217";a="27849381"
Received: from mail-io0-f172.google.com ([209.85.223.172])
  by fe4.lbl.gov with ESMTP; 28 Jun 2018 03:08:17 -0700
Received: by mail-io0-f172.google.com with SMTP id l25-v6so4675837ioh.12
        for <singu...@lbl.gov>; Thu, 28 Jun 2018 03:08:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=JnIP1P++81by0MNhofS33MmC7cnusbRs7U/+JAVzv9Q=;
        b=Z7ZgTKh8Cr2lnzJ34fKDxPuTxGJyeNUZ8SLydxrUpLZuDvDtXrfbtgU/uXFW8eOoVl
         xJiBbW3Gd/Ysgt5mlTIWHnvBC2udCFGnpOXP6BGhl8H/Yk/GYUgI4CKHigRowrZw9tYI
         g+FrayjHer5LgsuYLbekcQHZt2i0bjI6jqWESzvo+3jmXcoLCoZWgsl+FFzZ4ANImK3s
         PS3N7Jtj8NTwHk3N4e/wAX29hFaGkEX8ICK9Xc6ADAxVnx2ryLQhj1KbV5v6lQpTSYjR
         eUm0x6L/0YgusfR7jqyB1iYIdS/zkYvyDSge5lV9xddgVSSuv3MjNlLfCbYXy+XN50UF
         rguA==
X-Gm-Message-State: APt69E1lmnplbXvpqz7E4N1ScjIWV/fySxe+1NSnGtFw9v49lSCc8dGm
	1RKr+jOmC6aKhA0w9HDI4N/MLzY6roBFJstucOFZJ8b+
X-Received: by 2002:a6b:ac42:: with SMTP id v63-v6mr7956217ioe.71.1530180496322;
 Thu, 28 Jun 2018 03:08:16 -0700 (PDT)
MIME-Version: 1.0
References: <4d550130-4d54-4d4f-bf9f-a46f34132e96@lbl.gov> <CAJZ53CkoPup6sXotzVLO_toCu2c+rwK-A6Y0+TU277Y9km8N9w@mail.gmail.com>
 <CAM=pu+Jq2HcgVEBoYJ-USVQomnT6=pmd_16UdEpz2nniPQewdw@mail.gmail.com>
 <CADf5cTEmD1BEEg=jQHf2rrK1h+cwPYYqSD3Tz0UKS-RpumNNRA@mail.gmail.com>
 <CAMsq4T1c4vA5Yv6_bWJhpNC=UCze1reP2yPQa2YUtutJRPzQbA@mail.gmail.com>
 <CAM=pu+L-DXgQJ4eSUqSEnriDX9U2+dwOU-e2P2gWFxW3jsPcWQ@mail.gmail.com>
 <CAPqNE2VEL6oOB2NMP89=BxHqQj5k=Mbg2C=7O=oZuRvN5Q-pnQ@mail.gmail.com> <CAPqNE2UYU6+k3k6eef49t1gUegbDRSAD5XyDAf28bQWEBKjCqg@mail.gmail.com>
In-Reply-To: <CAPqNE2UYU6+k3k6eef49t1gUegbDRSAD5XyDAf28bQWEBKjCqg@mail.gmail.com>
From: v <vso...@gmail.com>
Date: Thu, 28 Jun 2018 03:08:04 -0700
Message-ID: <CAM=pu++GTFfSA5riN7F7KmLzGR=U-TdbX1C9hJzD9x=-h-M_cw@mail.gmail.com>
Subject: Re: [Singularity] Research data and containers
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000ecf3a4056fb0e8a9"

--000000000000ecf3a4056fb0e8a9
Content-Type: text/plain; charset="UTF-8"

All the cool kids are signing things these days!
https://docs.docker.com/engine/security/trust/content_trust/#image-tags-and-content-trust
:P

Dataset signing, especially for these fantastic "actually large" datasets,
seems to be taken with the same importance for reproducibility as the tools
that go into the anayses. To play devil's advocate, given that we want to
prove some phenomena, wouldn't we *want *to be able to show the same (or
comparable within some range of confidence) result across different
datasets? It doesn't strengthen the hypothsis to do the same thing again on
the *exact same* data, it strengthens the confidence in the reproducibility
of the pipeline (two separate things). So perhaps a small bit of data could
be stored with the container to validate the pipeline, but then new samples
encouraged to show if the hypothsis holds?

And I'm probably off here, but I think the sharing of datasets is less
about "run it again on exactly the same data" but more about "share the
data so we can all learn from it and increase overall N for better power."
A culture of data sharing, along with validation of method + published
result, also allows for checks and balances ("yes, I ran the pipeline with
your data, and can verify what you claimed in the paper, I have replicated
the result."

On Thu, Jun 28, 2018 at 12:20 AM 'John Hearns' via singularity <
singu...@lbl.gov> wrote:

> Chris Hines mentions signing of tool packages.  In danger of crossing the
> streams, I am an enthusiast for the Julia language.
> the new Pkg3 infrastructure for Julia, which is the default package
> manager now, implements UUIDs for packages
> https://github.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries
>
> On 28 June 2018 at 09:15, John Hearns <hea...@googlemail.com> wrote:
>
>> I think this a very, very interesting topic.  Please allow me to explore
>> and hopefully bring out some responses?
>>
>> Are we talking about large public datasets here - for instance I worked
>> on a CERN experiment (ahem) many years ago.
>> One hopes that huge, public datasets do not change. For instance if I
>> went back to analyze what was tagged at the time with a run number and an
>> event number I hope I would get the same data.
>> OK, it is clear that some large institutiosn worldwide will store that
>> data. And that bitrot or the scarcity of the tape drives to read the data
>> has not rendered it beyond recovery.
>>
>> I also worked with the JASMIN project in the UK, where several institutes
>> have banded together to create a central data store for climate modelling.
>> Again large public datasets.
>>
>> I also worked in Formula 1 with CFD engineers, and managed over a
>> petabyte of data from CFD studies. Those studies were catalogued by year
>> and then case number.
>> Again, we assume that the data is held on tape somewhere and if you ind
>> the correctly named directory you will get the same data back.
>>
>> I guess here we are talking about reproducibility. So we have a
>> Singularity container, which is signed, and captures the packages and
>> libraries needed to run that analysis.
>> We also have to have the same data available to run on.  Do we need to
>> physically keep that data 'beside' the container? Maybe, for certain data
>> and certain values of 'not too large'.
>> but how about analyses of (say) CERN or JASMIN data? Is it enough to make
>> a pointer towards those datasets somehow?
>> I would say URL's or URIs here, but we have seen how the Web is dynamic
>> and sites disappear and links change.
>>
>>
>> Its worth saying that in the UK there is a lot of effort in this space.
>> The Research Councils there mandated that if they fund research the data
>> must be retained for N years (I forget N)
>> So many Universities now have specific services for research data, for
>> instance:
>> http://www.ucl.ac.uk/research-it-services/research-data-service
>> https://www.ukdataservice.ac.uk/
>>
>> Arkivum https://arkivum.com/ works closely with the UK academic network
>> to provide large data storage. I just found this blog on their website -
>> well worth reading.
>>
>> https://arkivum.com/articles/blog-series-part-1-four-economic-factors-make-cost-nothing-expensive-something/
>>
>> https://arkivum.com/pharma/blog-series-part-4-final-economic-fallacy-long-term-data-temporarily-dynamic-path-dependent/
>>
>>
>> To end up, I guess what we are aiming towards is an S3 compatible storage
>> bucket, whether this is hosted in-house, by Amazon or by a compatible
>> provider.
>> Such a bucket would have to be locked, and signed.  Ideas??
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> On 28 June 2018 at 03:44, v <vso...@gmail.com> wrote:
>>
>>> Your description of the different components is on point.
>>>
>>> Finding hosting for large data is, and will continue to be, always one
>>> of the biggest problems. It's expensive. Someone has to pay for it. Nobody
>>> will do it for you for free. We are lucky to have things like Github repos,
>>> Google Drive, Dropbox, that gives some (mediocre) access to small data. For
>>> large data, everyone is pretty much on their own. Most academics have a
>>> university that provides some sort of web space / library data archive, and
>>> if you are publishing, some journals will support it too -->
>>> https://www.nature.com/sdata/policies/repositories. I try to figure out
>>> hacks to get everything hosted, for free, in version control, but that
>>> breaks when anything gets large.
>>>
>>> The entity that manages a data respository, if this is a thing, must
>>> have incentive to do it. The datasets have to bring in money in some way,
>>> or someone has to value it enough to pay for it. That's sort of what most
>>> things come down to, unfortunately.
>>>
>>>
>>> On Wed, Jun 27, 2018 at 6:38 PM Maxime Hebrard <maxime...@gmail.com>
>>> wrote:
>>>
>>>> I start to use singularity / containers and obviously I ran into the
>>>> same question ...
>>>>
>>>> a half solution that I found by exploring existing solution (thanks to
>>>> nf-core and SciLifeLab to share their workflow / containers) is ( as
>>>> far I understand ):
>>>>
>>>> the container encapsulate only the sorftware.
>>>> the data are store in external repositories.
>>>>
>>>> the user receive the container.
>>>>
>>>> case 1:
>>>> the user download the data by himself and point his files to the
>>>> container
>>>> the container use local files as resources
>>>> pros:
>>>>  * the container is slim
>>>>  * user can store the data on machine A (data server) and run the
>>>> software on machine B (analyse server)
>>>> cons:
>>>> * user is involved in downloading the right data
>>>>
>>>> case 2:
>>>> the user do not have the data
>>>> the container download the data from the external repository
>>>> the container use the data it just download
>>>> pro:
>>>> * the container manage everything
>>>> cons:
>>>> * the step of downloading the data require bandwith + disk space =
>>>> time and resources
>>>>
>>>> a general worry for both scenarii is:
>>>> who manage the data repository ? can we ensure the version of the data
>>>> package will be maintain ?
>>>>
>>>> Waiting for your feedback ;)
>>>> Maxime
>>>>
>>>> On Thu, Jun 28, 2018 at 8:15 AM, 'Chris Hines' via singularity
>>>> <singu...@lbl.gov> wrote:
>>>> > To quote 90's band "The Offspring" IMHO "You gotta keep 'em separated"
>>>> >
>>>> > Personally I think this is a larger question: How do we (as research
>>>> > software engineers, or whatever you would like to call us today)
>>>> catalog and
>>>> > attach references (DOI's I guess) to larger data sets. Does it make
>>>> sense to
>>>> > mksquashfs or tar.bz2? What if they are already in a single hdf5
>>>> file? And
>>>> > what about the output data set? we obviously want that to land
>>>> outside the
>>>> > computational container, but how should we package the output so it
>>>> can be
>>>> > referenced directly? How do we make that whole thing discoverable and
>>>> > encourage researchers to attach sufficient metadata to make it
>>>> searchable?
>>>> >
>>>> > I'm wondering if part of the solution is some sort of "meta
>>>> container" that
>>>> > downloads a set of versioned data with specific checksums, and a
>>>> versioned
>>>> > tool container with a specific checksum, and attaches them in a
>>>> reproducible
>>>> > way.
>>>> >
>>>> > To address Dominque's question directly: I'd keep the
>>>> "semi-separated".
>>>> > Whatever your researchers use for archiving/cataloging/managing their
>>>> data
>>>> > sets, keep that in place and add the singularity container to the
>>>> data set
>>>> > (rather than adding the data set to the singularity container).
>>>> >
>>>> > Regards,
>>>> > --
>>>> > Chris.
>>>> > Monash eResearch Centre, Monash University, Australia
>>>> >
>>>> > On Thu, 28 Jun 2018 at 01:19, v <vso...@gmail.com> wrote:
>>>> >>
>>>> >> If you are making containers with singularity (and using Squashfs
>>>> anyway)
>>>> >> it wouldn't be so nutty to just compress with mksquashfs and then
>>>> mount
>>>> >> where needed - I did this with a huge dataset and it worked pretty
>>>> nicely
>>>> >> :_) It relies on FUSE then and all the issues around that, but it's
>>>> an
>>>> >> option!
>>>> >>
>>>> >> This is good showing of how use use mksquashfs (it's really
>>>> >> simple,actually!) -->
>>>> >> http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.html
>>>> >> And then mount -->
>>>> >> https://vsoch.github.io/datasets/2018/zenodo/#mount-with-sudo
>>>> >>
>>>> >> On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker
>>>> >> <brando...@cornell.edu> wrote:
>>>> >>>
>>>> >>> I seem to recall code ocean may have explored this idea. Sorry I
>>>> can't
>>>> >>> say more, at the moment.
>>>> >>>
>>>> >>> On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen
>>>> >>> <dominiqu...@gmail.com> wrote:
>>>> >>>>
>>>> >>>> Hello everyone,
>>>> >>>>
>>>> >>>> I am seeking for tips and experience on how to handle research
>>>> data,
>>>> >>>> especially bigger sets of data (inside the GB range), in
>>>> combination with
>>>> >>>> containers.
>>>> >>>>
>>>> >>>> I am a student assistant working in the IT department of a research
>>>> >>>> institute. And I am involved in building the infrastructure for
>>>> singularity
>>>> >>>> containerization and introducing researchers to the technology. We
>>>> have
>>>> >>>> already build a few base images containing frequently used tools.
>>>> Recently a
>>>> >>>> research group approached us, wishing to integrate data, they
>>>> created for
>>>> >>>> one tool, into a container with the tool. The data takes up
>>>> several GB of
>>>> >>>> disk space and we are unsure how to handle this and similar future
>>>> use
>>>> >>>> cases.
>>>> >>>>
>>>> >>>> Does anyone have a set of best practices or is there an intended
>>>> way to
>>>> >>>> use singularity with big research data?
>>>> >>>>
>>>> >>>> The options we considered are:
>>>> >>>>
>>>> >>>> Moving the data into the container at build time.
>>>> >>>>
>>>> >>>> Pro:
>>>> >>>>
>>>> >>>> Keeps the whole thing mobile
>>>> >>>> Keeps work away from the researchers
>>>> >>>> Can be referenced as a single entity in a publication.
>>>> >>>>
>>>> >>>> Con:
>>>> >>>>
>>>> >>>> Where would you store such big containers, especially several of
>>>> them?
>>>> >>>> What would happen, if separate versions of the tool are needed?
>>>> Keep the
>>>> >>>> old and accumulate redundant data? Delete the old and risk
>>>> reproducibility
>>>> >>>> problems? (Maybe container Apps can be used to minimize this.)
>>>> >>>>
>>>> >>>> Let the researchers mount the data into the container at startup.
>>>> >>>>
>>>> >>>> Pro:
>>>> >>>>
>>>> >>>> Keeps the containers slimmer, the tools more modular.
>>>> >>>>
>>>> >>>> Con:
>>>> >>>>
>>>> >>>> Adds to the workload and the things the researchers have to keep
>>>> track
>>>> >>>> of.
>>>> >>>> Spreads the parts needed for reproduction over at least two points.
>>>> >>>> Hampers mobility.
>>>> >>>>
>>>> >>>> Mounting during early phases and publish with a container,
>>>> containing
>>>> >>>> the data.
>>>> >>>>
>>>> >>>> Pro:
>>>> >>>>
>>>> >>>> Reduces amount of containers with redundant and deprecated  data.
>>>> >>>> Makes reproduction of results easier after publication.
>>>> >>>>
>>>> >>>> Con:
>>>> >>>>
>>>> >>>> Ads to the workload.
>>>> >>>> When is the point when its time to "freeze" the data inside the
>>>> >>>> container?
>>>> >>>> Storage of the container is still problematic.
>>>> >>>> Might introduce reproducibility problems, since you change the
>>>> original
>>>> >>>> container to put the data into it.
>>>> >>>>
>>>> >>>> Are there any recommendations from experience?
>>>> >>>>
>>>> >>>> Thank you and best regards,
>>>> >>>> Dominique
>>>> >>>>
>>>> >>>> --
>>>> >>>> You received this message because you are subscribed to the Google
>>>> >>>> Groups "singularity" group.
>>>> >>>> To unsubscribe from this group and stop receiving emails from it,
>>>> send
>>>> >>>> an email to singu...@lbl.gov.
>>>> >>>
>>>> >>> --
>>>> >>> You received this message because you are subscribed to the Google
>>>> Groups
>>>> >>> "singularity" group.
>>>> >>> To unsubscribe from this group and stop receiving emails from it,
>>>> send an
>>>> >>> email to singu...@lbl.gov.
>>>> >>>
>>>> >>> --
>>>> >>> Vanessa Villamia Sochat
>>>> >>> Stanford University '16
>>>> >>> (603) 321-0676
>>>> >>
>>>> >> --
>>>> >> You received this message because you are subscribed to the Google
>>>> Groups
>>>> >> "singularity" group.
>>>> >> To unsubscribe from this group and stop receiving emails from it,
>>>> send an
>>>> >> email to singu...@lbl.gov.
>>>> >
>>>> > --
>>>> > You received this message because you are subscribed to the Google
>>>> Groups
>>>> > "singularity" group.
>>>> > To unsubscribe from this group and stop receiving emails from it,
>>>> send an
>>>> > email to singu...@lbl.gov.
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>> --
>>> Vanessa Villamia Sochat
>>> Stanford University '16
>>> (603) 321-0676
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>
> --
> Vanessa Villamia Sochat
> Stanford University '16
> (603) 321-0676
>

--000000000000ecf3a4056fb0e8a9
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">All the cool kids are signing things these days!=C2=A0<a h=
ref=3D"https://docs.docker.com/engine/security/trust/content_trust/#image-t=
ags-and-content-trust">https://docs.docker.com/engine/security/trust/conten=
t_trust/#image-tags-and-content-trust</a> :P<div><br></div><div>Dataset sig=
ning, especially for these fantastic &quot;actually large&quot; datasets, s=
eems to be taken with the same importance for reproducibility as the tools =
that go into the anayses. To play devil&#39;s advocate, given that we want =
to prove some phenomena, wouldn&#39;t we <i>want </i>to be able to show the=
 same (or comparable within some range of confidence) result across differe=
nt datasets? It doesn&#39;t strengthen the hypothsis to do the same thing a=
gain on the <i>exact same</i>=C2=A0data, it strengthens the confidence in t=
he reproducibility of the pipeline (two separate things). So perhaps a smal=
l bit of data could be stored with the container to validate the pipeline, =
but then new samples encouraged to show if the hypothsis holds?</div><div><=
br></div><div>And I&#39;m probably off here, but I think the sharing of dat=
asets is less about &quot;run it again on exactly the same data&quot; but m=
ore about &quot;share the data so we can all learn from it and increase ove=
rall N for better power.&quot; A culture of data sharing, along with valida=
tion of method=C2=A0+ published result, also allows for checks and balances=
 (&quot;yes, I ran the pipeline with your data, and can verify what you cla=
imed in the paper, I have replicated the result.&quot;</div></div><br><div =
class=3D"gmail_quote"><div dir=3D"ltr">On Thu, Jun 28, 2018 at 12:20 AM &#3=
9;John Hearns&#39; via singularity &lt;<a href=3D"mailto:singu...@lbl.gov">=
singu...@lbl.gov</a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" =
style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><di=
v dir=3D"ltr"><div>Chris Hines mentions signing of tool packages.=C2=A0 In =
danger of crossing the streams, I am an enthusiast for the Julia language.<=
/div><div>the new Pkg3 infrastructure for Julia, which is the default packa=
ge manager now, implements UUIDs for packages</div><div><a href=3D"https://=
github.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries" target=3D"_blan=
k">https://github.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries</a><b=
r></div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On =
28 June 2018 at 09:15, John Hearns <span dir=3D"ltr">&lt;<a href=3D"mailto:=
hea...@googlemail.com" target=3D"_blank">hea...@googlemail.com</a>&gt;</spa=
n> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>I think t=
his a very, very interesting topic.=C2=A0 Please allow me to explore and ho=
pefully bring out some responses?</div><div><br></div><div>Are we talking a=
bout large public datasets here - for instance I worked on a CERN experimen=
t (ahem) many years ago.</div><div>One hopes that huge, public datasets do =
not change. For instance if I went back to analyze what was tagged at the t=
ime with a run number and an event number I hope I would get the same data.=
</div><div>OK, it is clear that some large institutiosn worldwide will stor=
e that data. And that bitrot or the scarcity of the tape drives to read the=
 data has not rendered it beyond recovery.</div><div><br></div><div>I also =
worked with the JASMIN project in the UK, where several institutes have ban=
ded together to create a central data store for climate modelling. Again la=
rge public datasets.</div><div><br></div><div>I also worked in Formula 1 wi=
th CFD engineers, and managed over a petabyte of data from CFD studies. Tho=
se studies were catalogued by year and then case number.</div><div>Again, w=
e assume that the data is held on tape somewhere and if you ind the correct=
ly named directory you will get the same data back.</div><div><br></div><di=
v>I guess here we are talking about reproducibility. So we have a Singulari=
ty container, which is signed, and captures the packages and libraries need=
ed to run that analysis.</div><div>We also have to have the same data avail=
able to run on.=C2=A0 Do we need to physically keep that data &#39;beside&#=
39; the container? Maybe, for certain data and certain values of &#39;not t=
oo large&#39;.</div><div>but how about analyses of (say) CERN or JASMIN dat=
a? Is it enough to make a pointer towards those datasets somehow?</div><div=
>I would say URL&#39;s or URIs here, but we have seen how the Web is dynami=
c and sites disappear and links change. <br></div><div><br></div><div><br><=
/div><div>Its worth saying that in the UK there is a lot of effort in this =
space. The Research Councils there mandated that if they fund research the =
data must be retained for N years (I forget N)</div><div>So many Universiti=
es now have specific services for research data, for instance:</div><div><a=
 href=3D"http://www.ucl.ac.uk/research-it-services/research-data-service" t=
arget=3D"_blank">http://www.ucl.ac.uk/research-it-services/research-data-se=
rvice</a></div><div><a href=3D"https://www.ukdataservice.ac.uk/" target=3D"=
_blank">https://www.ukdataservice.ac.uk/</a></div><div><br></div><div>Arkiv=
um <a href=3D"https://arkivum.com/" target=3D"_blank">https://arkivum.com/<=
/a> works closely with the UK academic network to provide large data storag=
e. I just found this blog on their website - well worth reading. <br></div>=
<div><a href=3D"https://arkivum.com/articles/blog-series-part-1-four-econom=
ic-factors-make-cost-nothing-expensive-something/" target=3D"_blank">https:=
//arkivum.com/articles/blog-series-part-1-four-economic-factors-make-cost-n=
othing-expensive-something/</a></div><div><a href=3D"https://arkivum.com/ph=
arma/blog-series-part-4-final-economic-fallacy-long-term-data-temporarily-d=
ynamic-path-dependent/" target=3D"_blank">https://arkivum.com/pharma/blog-s=
eries-part-4-final-economic-fallacy-long-term-data-temporarily-dynamic-path=
-dependent/</a></div><div><br></div><div><br></div><div>To end up, I guess =
what we are aiming towards is an S3 compatible storage bucket, whether this=
 is hosted in-house, by Amazon or by a compatible provider.</div><div>Such =
a bucket would have to be locked, and signed.=C2=A0 Ideas??<br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div><div><br></div><div><br></div><div><br></div><div><=
br></div><div><br></div></div><div class=3D"m_-4458839707498356592HOEnZb"><=
div class=3D"m_-4458839707498356592h5"><div class=3D"gmail_extra"><br><div =
class=3D"gmail_quote">On 28 June 2018 at 03:44, v <span dir=3D"ltr">&lt;<a =
href=3D"mailto:vso...@gmail.com" target=3D"_blank">vso...@gmail.com</a>&gt;=
</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Your desc=
ription of the different components is on point.=C2=A0<div><br></div><div>F=
inding hosting for large data is, and will continue to be, always one of th=
e biggest problems. It&#39;s expensive. Someone has to pay for it. Nobody w=
ill do it for you for free. We are lucky to have things like Github repos, =
Google Drive, Dropbox, that gives some (mediocre) access to small data. For=
 large data, everyone is pretty much on their own. Most academics have a un=
iversity that provides some sort of web space / library data archive, and i=
f you are publishing, some journals will support it too --&gt;=C2=A0<a href=
=3D"https://www.nature.com/sdata/policies/repositories" target=3D"_blank">h=
ttps://www.nature.com/sdata/policies/repositories</a>. I try to figure out =
hacks to get everything hosted, for free, in version control, but that brea=
ks when anything gets large.</div><div><br></div><div>The entity that manag=
es a data respository, if this is a thing, must have incentive to do it. Th=
e datasets have to bring in money in some way, or someone has to value it e=
nough to pay for it. That&#39;s sort of what most things come down to, unfo=
rtunately.</div><div><br></div></div><div class=3D"m_-4458839707498356592m_=
636930356989791511HOEnZb"><div class=3D"m_-4458839707498356592m_63693035698=
9791511h5"><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Wed, Jun 27, =
2018 at 6:38 PM Maxime Hebrard &lt;<a href=3D"mailto:maxime...@gmail.com" t=
arget=3D"_blank">maxime...@gmail.com</a>&gt; wrote:<br></div><blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid=
 rgb(204,204,204);padding-left:1ex">I start to use singularity / containers=
 and obviously I ran into the<br>
same question ...<br>
<br>
a half solution that I found by exploring existing solution (thanks to<br>
nf-core and SciLifeLab to share their workflow / containers) is ( as<br>
far I understand ):<br>
<br>
the container encapsulate only the sorftware.<br>
the data are store in external repositories.<br>
<br>
the user receive the container.<br>
<br>
case 1:<br>
the user download the data by himself and point his files to the container<=
br>
the container use local files as resources<br>
pros:<br>
=C2=A0* the container is slim<br>
=C2=A0* user can store the data on machine A (data server) and run the<br>
software on machine B (analyse server)<br>
cons:<br>
* user is involved in downloading the right data<br>
<br>
case 2:<br>
the user do not have the data<br>
the container download the data from the external repository<br>
the container use the data it just download<br>
pro:<br>
* the container manage everything<br>
cons:<br>
* the step of downloading the data require bandwith + disk space =3D<br>
time and resources<br>
<br>
a general worry for both scenarii is:<br>
who manage the data repository ? can we ensure the version of the data<br>
package will be maintain ?<br>
<br>
Waiting for your feedback ;)<br>
Maxime<br>
<br>
On Thu, Jun 28, 2018 at 8:15 AM, &#39;Chris Hines&#39; via singularity<br>
&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov<=
/a>&gt; wrote:<br>
&gt; To quote 90&#39;s band &quot;The Offspring&quot; IMHO &quot;You gotta =
keep &#39;em separated&quot;<br>
&gt;<br>
&gt; Personally I think this is a larger question: How do we (as research<b=
r>
&gt; software engineers, or whatever you would like to call us today) catal=
og and<br>
&gt; attach references (DOI&#39;s I guess) to larger data sets. Does it mak=
e sense to<br>
&gt; mksquashfs or tar.bz2? What if they are already in a single hdf5 file?=
 And<br>
&gt; what about the output data set? we obviously want that to land outside=
 the<br>
&gt; computational container, but how should we package the output so it ca=
n be<br>
&gt; referenced directly? How do we make that whole thing discoverable and<=
br>
&gt; encourage researchers to attach sufficient metadata to make it searcha=
ble?<br>
&gt;<br>
&gt; I&#39;m wondering if part of the solution is some sort of &quot;meta c=
ontainer&quot; that<br>
&gt; downloads a set of versioned data with specific checksums, and a versi=
oned<br>
&gt; tool container with a specific checksum, and attaches them in a reprod=
ucible<br>
&gt; way.<br>
&gt;<br>
&gt; To address Dominque&#39;s question directly: I&#39;d keep the &quot;se=
mi-separated&quot;.<br>
&gt; Whatever your researchers use for archiving/cataloging/managing their =
data<br>
&gt; sets, keep that in place and add the singularity container to the data=
 set<br>
&gt; (rather than adding the data set to the singularity container).<br>
&gt;<br>
&gt; Regards,<br>
&gt; --<br>
&gt; Chris.<br>
&gt; Monash eResearch Centre, Monash University, Australia<br>
&gt;<br>
&gt; On Thu, 28 Jun 2018 at 01:19, v &lt;<a href=3D"mailto:vso...@gmail.com=
" target=3D"_blank">vso...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; If you are making containers with singularity (and using Squashfs =
anyway)<br>
&gt;&gt; it wouldn&#39;t be so nutty to just compress with mksquashfs and t=
hen mount<br>
&gt;&gt; where needed - I did this with a huge dataset and it worked pretty=
 nicely<br>
&gt;&gt; :_) It relies on FUSE then and all the issues around that, but it&=
#39;s an<br>
&gt;&gt; option!<br>
&gt;&gt;<br>
&gt;&gt; This is good showing of how use use mksquashfs (it&#39;s really<br=
>
&gt;&gt; simple,actually!) --&gt;<br>
&gt;&gt; <a href=3D"http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.h=
tml" rel=3D"noreferrer" target=3D"_blank">http://tldp.org/HOWTO/SquashFS-HO=
WTO/creatingandusing.html</a><br>
&gt;&gt; And then mount --&gt;<br>
&gt;&gt; <a href=3D"https://vsoch.github.io/datasets/2018/zenodo/#mount-wit=
h-sudo" rel=3D"noreferrer" target=3D"_blank">https://vsoch.github.io/datase=
ts/2018/zenodo/#mount-with-sudo</a><br>
&gt;&gt;<br>
&gt;&gt; On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker<br>
&gt;&gt; &lt;<a href=3D"mailto:brando...@cornell.edu" target=3D"_blank">bra=
ndo...@cornell.edu</a>&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I seem to recall code ocean may have explored this idea. Sorry=
 I can&#39;t<br>
&gt;&gt;&gt; say more, at the moment.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen<br>
&gt;&gt;&gt; &lt;<a href=3D"mailto:dominiqu...@gmail.com" target=3D"_blank"=
>dominiqu...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Hello everyone,<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I am seeking for tips and experience on how to handle rese=
arch data,<br>
&gt;&gt;&gt;&gt; especially bigger sets of data (inside the GB range), in c=
ombination with<br>
&gt;&gt;&gt;&gt; containers.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I am a student assistant working in the IT department of a=
 research<br>
&gt;&gt;&gt;&gt; institute. And I am involved in building the infrastructur=
e for singularity<br>
&gt;&gt;&gt;&gt; containerization and introducing researchers to the techno=
logy. We have<br>
&gt;&gt;&gt;&gt; already build a few base images containing frequently used=
 tools. Recently a<br>
&gt;&gt;&gt;&gt; research group approached us, wishing to integrate data, t=
hey created for<br>
&gt;&gt;&gt;&gt; one tool, into a container with the tool. The data takes u=
p several GB of<br>
&gt;&gt;&gt;&gt; disk space and we are unsure how to handle this and simila=
r future use<br>
&gt;&gt;&gt;&gt; cases.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Does anyone have a set of best practices or is there an in=
tended way to<br>
&gt;&gt;&gt;&gt; use singularity with big research data?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; The options we considered are:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Moving the data into the container at build time.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Keeps the whole thing mobile<br>
&gt;&gt;&gt;&gt; Keeps work away from the researchers<br>
&gt;&gt;&gt;&gt; Can be referenced as a single entity in a publication.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Where would you store such big containers, especially seve=
ral of them?<br>
&gt;&gt;&gt;&gt; What would happen, if separate versions of the tool are ne=
eded? Keep the<br>
&gt;&gt;&gt;&gt; old and accumulate redundant data? Delete the old and risk=
 reproducibility<br>
&gt;&gt;&gt;&gt; problems? (Maybe container Apps can be used to minimize th=
is.)<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Let the researchers mount the data into the container at s=
tartup.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Keeps the containers slimmer, the tools more modular.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Adds to the workload and the things the researchers have t=
o keep track<br>
&gt;&gt;&gt;&gt; of.<br>
&gt;&gt;&gt;&gt; Spreads the parts needed for reproduction over at least tw=
o points.<br>
&gt;&gt;&gt;&gt; Hampers mobility.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Mounting during early phases and publish with a container,=
 containing<br>
&gt;&gt;&gt;&gt; the data.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Reduces amount of containers with redundant and deprecated=
=C2=A0 data.<br>
&gt;&gt;&gt;&gt; Makes reproduction of results easier after publication.<br=
>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Ads to the workload.<br>
&gt;&gt;&gt;&gt; When is the point when its time to &quot;freeze&quot; the =
data inside the<br>
&gt;&gt;&gt;&gt; container?<br>
&gt;&gt;&gt;&gt; Storage of the container is still problematic.<br>
&gt;&gt;&gt;&gt; Might introduce reproducibility problems, since you change=
 the original<br>
&gt;&gt;&gt;&gt; container to put the data into it.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Are there any recommendations from experience?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Thank you and best regards,<br>
&gt;&gt;&gt;&gt; Dominique<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; --<br>
&gt;&gt;&gt;&gt; You received this message because you are subscribed to th=
e Google<br>
&gt;&gt;&gt;&gt; Groups &quot;singularity&quot; group.<br>
&gt;&gt;&gt;&gt; To unsubscribe from this group and stop receiving emails f=
rom it, send<br>
&gt;&gt;&gt;&gt; an email to <a href=3D"mailto:singularity%...@lbl.gov" tar=
get=3D"_blank">singu...@lbl.gov</a>.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; You received this message because you are subscribed to the Go=
ogle Groups<br>
&gt;&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt;&gt; To unsubscribe from this group and stop receiving emails from =
it, send an<br>
&gt;&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"=
_blank">singu...@lbl.gov</a>.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; Vanessa Villamia Sochat<br>
&gt;&gt;&gt; Stanford University &#39;16<br>
&gt;&gt;&gt; (603) 321-0676<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singu...@lbl.gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singu...@lbl.gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu.=
..@lbl.gov</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"m_-4458839707498356592m_636930356989791511m_-8585967889488559609g=
mail_signature"><div class=3D"m_-4458839707498356592m_636930356989791511m_-=
8585967889488559609gmail_signature">Vanessa Villamia Sochat<br>Stanford Uni=
versity &#39;16<br><div><div><div>(603) 321-0676</div></div></div></div></d=
iv>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr" class=3D"g=
mail_signature" data-smartmail=3D"gmail_signature"><div class=3D"gmail_sign=
ature" data-smartmail=3D"gmail_signature">Vanessa Villamia Sochat<br>Stanfo=
rd University &#39;16<br><div><div><div>(603) 321-0676</div></div></div></d=
iv></div></blockquote></div>

--000000000000ecf3a4056fb0e8a9--
