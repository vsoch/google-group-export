X-Received: by 10.31.49.196 with SMTP id x187mr20548597vkx.0.1463403907646;
        Mon, 16 May 2016 06:05:07 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.76.149 with SMTP id a143ls669888itb.17.gmail; Mon, 16 May
 2016 06:05:07 -0700 (PDT)
X-Received: by 10.98.101.198 with SMTP id z189mr45934192pfb.76.1463403906994;
        Mon, 16 May 2016 06:05:06 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id b68si45712749pfb.21.2016.05.16.06.05.06
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 May 2016 06:05:06 -0700 (PDT)
Received-SPF: pass (google.com: domain of rhc.o...@gmail.com designates 209.85.192.171 as permitted sender) client-ip=209.85.192.171;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of rhc.o...@gmail.com designates 209.85.192.171 as permitted sender) smtp.mailfrom=rhc.o...@gmail.com
X-Ironport-SBRS: 3.6
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EeAQBXxDlXiKvAVdFdgmyCHoM6ogsGiDmLc4FyGwGHHDkTAQEBAQEBAQMPAQEBCAsLCR8xgjc5EFUCK0ABAQEDARIICR0BDSwDAQsBBQUYIAEGAwICIRADAQUBCxEOBwQBGQMEAYdzAw8IBaBJgTE+MYs7hEIFhz0nDYQzHQIGEIVNBgWCMwiBTIEDgkOBThEBBgKDFCuCLgWOU4RdhEYxiHaDL4NihE+CeQ6FWodchicwgQ4iAXSBDUQbgWtOB4ZCCBeBHgEBAQ
X-IronPort-AV: E=Sophos;i="5.24,627,1455004800"; 
   d="scan'208,217";a="23455332"
Received: from mail-pf0-f171.google.com ([209.85.192.171])
  by fe4.lbl.gov with ESMTP; 16 May 2016 06:05:05 -0700
Received: by mail-pf0-f171.google.com with SMTP id c189so69817572pfb.3
        for <singu...@lbl.gov>; Mon, 16 May 2016 06:05:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=sender:from:message-id:mime-version:subject:date:references:to
         :in-reply-to;
        bh=e1wRm422ed1zOhwOvMeZW+Bfbv3KKAoXtxCt5LsI75I=;
        b=I+1h/Ze/9K93Mvo1D8io8RxC3UrGafs5YzBtCKGoDORF/D0uD+fWRTdPJQZziUU5Q+
         HnQs5mzu5Slzk4H31BUVO8iqVlVQTxUTqstb7pxSV4biv4HMbD+AGYHHJupAO6Y3SAx7
         Hl6mzTgP201GsUGLCYlVARj+JGKxllYXEbKdOQv9f2Pen8LPdsITG3WDCKRzjrnSFsEw
         Wv5CwkLYK1y20Nqn6hZfc842U0yCd2i6WiYMuIuhuHNZ4w+dt8Xx3Xaw5kmcBdwgFIT6
         ksfKiHvvay3Ji+ocA41BJM4l158u+T9BysBYD6BLVikvJ+bsSy0Mc7qJkueAI5+PYcRw
         G58Q==
X-Gm-Message-State: AOPr4FXk8snmDT8wO9GT03hkGwHXsall7w5c+ySQozP6akriEFcubx4MVfDWJlG8y7n4UQ==
X-Received: by 10.98.71.13 with SMTP id u13mr45512361pfa.123.1463403904677;
        Mon, 16 May 2016 06:05:04 -0700 (PDT)
Return-Path: <rhc.o...@gmail.com>
Received: from [192.168.0.8] ([208.100.172.177])
        by smtp.gmail.com with ESMTPSA id lq10sm47477470pab.36.2016.05.16.06.05.02
        for <singu...@lbl.gov>
        (version=TLSv1/SSLv3 cipher=OTHER);
        Mon, 16 May 2016 06:05:03 -0700 (PDT)
Sender: Ralph Castain <rhc.o...@gmail.com>
From: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary="Apple-Mail=_2A865F55-5DCF-42DD-BAF5-32787619DD50"
Message-Id: <46A137E8-DB00-4564-A4CA-AC1B9FE62784@open-mpi.org>
Mime-Version: 1.0 (Mac OS X Mail 9.3 \(3124\))
Subject: Re: [Singularity] SIngularity and MPI implementations
Date: Mon, 16 May 2016 06:05:02 -0700
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov> <CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com> <CAAS-_CBb3sy393W1Bga5Wnr9-EvSHC6NPNaOx58Mpdw-LiFq8g@mail.gmail.com> <414C0039-42A3-4C2E-89F6-3D97D082C742@open-mpi.org> <CAN7etTxQkXe6uEDZgt+kkAex0Hzt9UWYE6+V4qS4vnKJAMgFzQ@mail.gmail.com> <CAAS-_CBsKM3=d_OhVknqcF2De29UiQ-cjOv5imCyR=jv=4Rh7g@mail.gmail.com> <CAN7etTxJkbqv3mdujx0JziZo7y9fTrRkk3eE0GpXnkmKhpTc=g@mail.gmail.com>
To: singularity@lbl.gov
In-Reply-To: <CAN7etTxJkbqv3mdujx0JziZo7y9fTrRkk3eE0GpXnkmKhpTc=g@mail.gmail.com>
X-Mailer: Apple Mail (2.3124)

--Apple-Mail=_2A865F55-5DCF-42DD-BAF5-32787619DD50
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8


> On May 16, 2016, at 5:54 AM, Gregory M. Kurtzer <gmku...@lbl.gov> wrote:
>=20
>=20
>=20
> On Mon, May 16, 2016 at 1:17 AM, Taras Shapovalov <shapov...@gmail.com <m=
ailto:shapov...@gmail.com>> wrote:
> Hi guys,
>=20
> Thanks for the great answers! Now it is more or less clear how it works. =
To be absolutely sure, can you please confirm also these statements (got fr=
om your answers):
>=20
> 1. Ralph's answer mentions mpiexec, but Gregory's answer is about mpirun.=
 So, all the discussed here can be applied to the both utilities included i=
n Open MPI distribution.
>=20
> Ralph can speak definitively here, but I believe my answer applies to bot=
h.

The two names are for the identical binary - in the MPI world, folks use bo=
th names interchangeably

> =20
>=20
> 2. Running Open MPI processes in a single container is impleneted only in=
 Singularity v2. In v1 each Open MPI process still will be executed in diff=
erent containers.
>=20
> For technical Q&A we should probably use the word namespaces in addition =
to containers, I'll explain.
>=20
> Singularity v1 will cache the container on each node, so processes within=
 a node will share the container cache but operate in some different namesp=
aces (the specific namespaces are somewhat application/necessity dependent)=
.
>=20
> Singularity v2 has no need to cache the container, but it does need to bi=
nd it to a loop device. This happens once per node, but again there is no c=
ache so all nodes are sharing the same container image and also operate in =
some separate namespaces (again dependent on need).
> =20
>=20
> 3. Lets compare these 2 scenarios: Singularity runs child processes in a =
single container agains scenario when each child runs in a separate contain=
er each. The optimization with dlopen call happens in the first scenario, b=
ecause the opened library is loaded into the memory per Singularity contain=
er, then dlopen magically returns the same handler for each child process i=
nside the container, which should be faster. Or there is some other low lev=
el optimization occurs in the first scenario regarding dlopen?
>=20
> I am not sure I follow completely, but if you are asking what I think you=
're asking... Singularity v2 will optimize all calls to open() (including d=
lopen()) within the container because what is within the container all exis=
t within a single image (there is no need to make additional metadata reque=
sts to files that exist within the container image). Additionally there is =
no launch penalty taken because there is no need to cache the image. On ave=
rage, launch time when using this method is about .020s on my test system a=
nd writes/changes never require a rebuild.
>=20
> With Singularity v1, files are pulled out of the container archive (SAPP)=
 and spilled out to the storage. If the storage is local to nodes, then cal=
ls to open() and thus the required metadata will not goto shared storage. B=
y default the container is cached to shared storage (unless launching a SAP=
P file directly through Open MPI). Launch time for v1 is about .050s after =
the image has been cached, and caching of the image usually takes anywhere =
from .5s to as high as you want to go depending on image size (I've seen in=
 my testing upwards of 10 seconds).
>=20
> Hopefully that helps!
>=20
>=20
>=20
> Best regards,
>=20
> Taras
>=20
>=20
> On Fri, May 13, 2016 at 9:18 PM, Gregory M. Kurtzer <gmku...@lbl.gov <mai=
lto:gmku...@lbl.gov>> wrote:
>=20
>=20
> On Fri, May 13, 2016 at 10:10 AM, Ralph Castain <r...@open-mpi.org <mailt=
o:r...@open-mpi.org>> wrote:
>=20
>> On May 13, 2016, at 9:52 AM, Taras Shapovalov <shapov...@gmail.com <mail=
to:shapov...@gmail.com>> wrote:
>>=20
>> Hi Ralph and Gregory,
>>=20
>> Thank you the both for the so detailed answers! I see your replies compl=
ement each other. Although I am a bit confused now with the whole picture, =
so could you confirm that I get the ideas correctly:
>>=20
>> 1. All implementations of MPI by default should work with Singularity co=
ntainers (maybe not as optimal as could be, but should start and finish cor=
rectly always). Actually I've tested recently MPICH+Singularity with severa=
l workload managers, worked fine (did not benchmark it comparing with Open =
MPI). I did not manage to make Singularity+MPI work in LSF, but this is a d=
ifferent story that deserves a separate thread.
>=20
>=20
> I wouldn't necessarily say they would all work by default. For example, s=
ome namespaces may necessitate being disabled in order to get proper shared=
 memory IO performance. But ... If you have tested this, that is great news=
 and I'd love to hear more about your findings!
> =20
>=20
> Correct - the LSF issue is likely a problem of getting the required setup=
 info passed by LSF
>=20
>>=20
>> 2. MPI process calls dl_open, thus the more MPI processes starts on a no=
de, the more times dl_open will be called. Open MPI 2.0.1 somehow solves th=
is magically (I don't get how) and dl_open is called only once per node. Ot=
her implementations of MPI and older Open MPI are not Singularity aware, th=
us they still will call dl_open each time when MPI process spawns.=20
>=20
> Not exactly. Singularity will solve the dl_open problem by itself. What t=
he container does is wrap all the dl_open libraries into the container, and=
 so all dl_open calls by the app are locally resolved. Thus, you automatica=
lly resolve the IO node bottleneck scaling issue.
>=20
> What OMPI adds is that it pulls the container only once/node. Other mpiex=
ec implementations will pull the container again for every local process. S=
o if you have 100 procs/node, OMPI will result in 100x fewer =E2=80=9Cpulls=
=E2=80=9D thru that IO node.
>=20
> Yep, and additionally I want to make sure we keep Singularity v1 and v2 f=
eatures separate. Version 2 has several huge benefits (including this) over=
 v1, but it is a departure from using SAPPs (and now uses images).
> =20
>=20
>>=20
>> 3. dl_open issue affects only process start time and does not effect the=
 process execution, so on small scale with long running processes there is =
no difference between Open MPI 2.0.1 and older Open MPI versions (as well a=
s other MPI implementations).
>=20
> Correct
>=20
> Correct, just keep in mind start times at massive scale have been stated =
by several large centers to approach 30 minutes. During that 30 minutes, it=
 basically looks like a distributed denial of service attack to the file sy=
stem metadata server killing file system performance to the rest of the sys=
tem.
> =20
>=20
>>=20
>> 4. When sapp is built then Singularity detects Open MPI (even older then=
 2.0.1, right?) and resolves all dependencies automatically adding all file=
s to the sapp. But with, say, MVAPICH2 the dependencies are not resolved au=
tomatically, so user should add some stuff manually.
>=20
> Correct
>=20
> And in v2, this will get handled either by an RPM installation of Open MP=
I, or the 'singularity exec --writable /path/to/Container.img make install'=
.=20
>=20
>>=20
>> 5. Apart of solving dl_open issue Open MPI 2.0.1 does some splitting bet=
ween the host and the container, which allows user/admin to not optimize Op=
en MPI for a target platform. I really don't get how Singularity does this,=
 but I get the problem. Could you explain what Singularity or Open MPI 2.0.=
1 does for that specificaly?
>=20
> When running under mpiexec with Singularity, OMPI=E2=80=99s local daemon =
on each node actually runs outside of the containers. We then fork/exec the=
 container itself, and the container is defined so it auto-executes the app=
lication process. This allows us to minimize the services overhead, keeping=
 all services outside of your container (and thus shared across all contain=
ers.
>=20
> Other approaches have the daemon -inside- the container, and you get one =
daemon for each container - and thus, one daemon for each local application=
. So you get a higher overhead and therefore lower performance.
>=20
> Maybe this will help to articulate it... I have described this via the MP=
I/Singularity invocation pathway as follows (hopefully it is reasonably cor=
rect and doesn't cause Ralph to kick me). Considering the command:
>=20
> $ mpirun -np X singularity exec ~/Centos-7.img mpi_program
>=20
> 1. From shell (or resource manager) mpirun gets called
> 2. mpirun forks and exec orte daemon
> 3. Orted process creates PMI
> 4. Orted forks =3D=3D to the number of process per node requested
> 5. Orted children exec to original command passed to mpirun (Singularity)
> 6. Each Singularity execs the command passed inside the given container
> 7. Each MPI program links in the dynamic Open MPI libraries (ldd)
> 8. Open MPI libraries continue to open the non-ldd shared libraries (dlop=
en)
> 9. Open MPI libraries connect back to original orted via PMI
> 10. All non-shared memory communication occurs through the PMI and then t=
o local interfaces (e.g. InfiniBand)
>=20
> While the workflow from the MPI perspective seems simpler with the daemon=
 inside the container, it is much more complicated from the system perspect=
ive because each orted process must also know about the other hosts, be abl=
e to communicate with them and mitigate performance factors of host/resourc=
e/interconnect tuning.
>=20
> Hope that helps!
>=20
>=20
> =20
>=20
> HTH
> Ralph
>=20
>=20
>>=20
>> Best regards,
>>=20
>> Taras
>>=20
>>=20
>> --=20
>> You received this message because you are subscribed to the Google Group=
s "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n email to singu...@lbl.gov <mailto:singu...@lbl.gov>.
>=20
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.
>=20
>=20
>=20
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.
>=20
>=20
>=20
> --=20
> /T
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.
>=20
>=20
>=20
> --=20
> Gregory M. Kurtzer
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.


--Apple-Mail=_2A865F55-5DCF-42DD-BAF5-32787619DD50
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html charset=
=3Dutf-8"></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space;" class=3D""><br class=3D""><di=
v><blockquote type=3D"cite" class=3D""><div class=3D"">On May 16, 2016, at =
5:54 AM, Gregory M. Kurtzer &lt;<a href=3D"mailto:gmku...@lbl.gov" class=3D=
"">gmku...@lbl.gov</a>&gt; wrote:</div><br class=3D"Apple-interchange-newli=
ne"><div class=3D""><div dir=3D"ltr" style=3D"font-family: Helvetica; font-=
size: 12px; font-style: normal; font-variant-caps: normal; font-weight: nor=
mal; letter-spacing: normal; orphans: auto; text-align: start; text-indent:=
 0px; text-transform: none; white-space: normal; widows: auto; word-spacing=
: 0px; -webkit-text-stroke-width: 0px;" class=3D""><div class=3D"gmail_extr=
a"><br class=3D"Apple-interchange-newline"><br class=3D""><div class=3D"gma=
il_quote">On Mon, May 16, 2016 at 1:17 AM, Taras Shapovalov<span class=3D"A=
pple-converted-space">&nbsp;</span><span dir=3D"ltr" class=3D"">&lt;<a href=
=3D"mailto:shapov...@gmail.com" target=3D"_blank" class=3D"">shapov...@gmai=
l.com</a>&gt;</span><span class=3D"Apple-converted-space">&nbsp;</span>wrot=
e:<br class=3D""><blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px=
 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); =
border-left-style: solid; padding-left: 1ex;"><div dir=3D"ltr" class=3D""><=
div class=3D""><div class=3D""><div class=3D""><div class=3D""><div class=
=3D"">Hi guys,<br class=3D""><br class=3D""></div>Thanks for the great answ=
ers! Now it is more or less clear how it works. To be absolutely sure, can =
you please confirm also these statements (got from your answers):<br class=
=3D""><br class=3D""></div><div class=3D"">1. Ralph's answer mentions mpiex=
ec, but Gregory's answer is about mpirun. So, all the discussed here can be=
 applied to the both utilities included in Open MPI distribution.<br class=
=3D""></div></div></div></div></div></blockquote><div class=3D""><br class=
=3D""></div><div class=3D"">Ralph can speak definitively here, but I believ=
e my answer applies to both.</div></div></div></div></div></blockquote><div=
><br class=3D""></div>The two names are for the identical binary - in the M=
PI world, folks use both names interchangeably</div><div><br class=3D""><bl=
ockquote type=3D"cite" class=3D""><div class=3D""><div dir=3D"ltr" style=3D=
"font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-=
caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; t=
ext-align: start; text-indent: 0px; text-transform: none; white-space: norm=
al; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=
=3D""><div class=3D"gmail_extra"><div class=3D"gmail_quote"><div class=3D""=
>&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px=
 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); bord=
er-left-style: solid; padding-left: 1ex;"><div dir=3D"ltr" class=3D""><div =
class=3D""><div class=3D""><div class=3D""><div class=3D""></div><div class=
=3D""><br class=3D""></div>2. Running Open MPI processes in a single contai=
ner is impleneted only in Singularity v2. In v1 each Open MPI process still=
 will be executed in different containers.<br class=3D""></div></div></div>=
</div></blockquote><div class=3D""><br class=3D""></div><div class=3D"">For=
 technical Q&amp;A we should probably use the word namespaces in addition t=
o containers, I'll explain.</div><div class=3D""><br class=3D""></div><div =
class=3D"">Singularity v1 will cache the container on each node, so process=
es within a node will share the container cache but operate in some differe=
nt namespaces (the specific namespaces are somewhat application/necessity d=
ependent).</div><div class=3D""><br class=3D""></div><div class=3D"">Singul=
arity v2 has no need to cache the container, but it does need to bind it to=
 a loop device. This happens once per node, but again there is no cache so =
all nodes are sharing the same container image and also operate in some sep=
arate namespaces (again dependent on need).</div><div class=3D"">&nbsp;</di=
v><blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8ex; bor=
der-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-sty=
le: solid; padding-left: 1ex;"><div dir=3D"ltr" class=3D""><div class=3D"">=
<div class=3D""><div class=3D""><br class=3D""></div>3. Lets compare these =
2 scenarios: Singularity runs child processes in a single container agains =
scenario when each child runs in a separate container each. The optimizatio=
n with dlopen call happens in the first scenario, because the opened librar=
y is loaded into the memory per Singularity container, then dlopen magicall=
y returns the same handler for each child process inside the container, whi=
ch should be faster. Or there is some other low level optimization occurs i=
n the first scenario regarding dlopen?<br class=3D""></div></div></div></bl=
ockquote><div class=3D""><br class=3D""></div><div class=3D"">I am not sure=
 I follow completely, but if you are asking what I think you're asking... S=
ingularity v2 will optimize all calls to open() (including dlopen()) within=
 the container because what is within the container all exist within a sing=
le image (there is no need to make additional metadata requests to files th=
at exist within the container image). Additionally there is no launch penal=
ty taken because there is no need to cache the image. On average, launch ti=
me when using this method is about .020s on my test system and writes/chang=
es never require a rebuild.<br class=3D""></div><div class=3D""><br class=
=3D""></div><div class=3D"">With Singularity v1, files are pulled out of th=
e container archive (SAPP) and spilled out to the storage. If the storage i=
s local to nodes, then calls to open() and thus the required metadata will =
not goto shared storage. By default the container is cached to shared stora=
ge (unless launching a SAPP file directly through Open MPI). Launch time fo=
r v1 is about .050s after the image has been cached, and caching of the ima=
ge usually takes anywhere from .5s to as high as you want to go depending o=
n image size (I've seen in my testing upwards of 10 seconds).</div><div cla=
ss=3D""><br class=3D""></div><div class=3D"">Hopefully that helps!</div><di=
v class=3D""><br class=3D""></div><div class=3D""><br class=3D""></div><blo=
ckquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8ex; border-le=
ft-width: 1px; border-left-color: rgb(204, 204, 204); border-left-style: so=
lid; padding-left: 1ex;"><div dir=3D"ltr" class=3D""><div class=3D""><div c=
lass=3D""><br class=3D""></div>Best regards,<br class=3D""><br class=3D""><=
/div>Taras<br class=3D""><div class=3D""><div class=3D""><br class=3D""></d=
iv></div></div><div class=3D"gmail_extra"><div class=3D""><div class=3D"h5"=
><br class=3D""><div class=3D"gmail_quote">On Fri, May 13, 2016 at 9:18 PM,=
 Gregory M. Kurtzer<span class=3D"Apple-converted-space">&nbsp;</span><span=
 dir=3D"ltr" class=3D"">&lt;<a href=3D"mailto:gmku...@lbl.gov" target=3D"_b=
lank" class=3D"">gmku...@lbl.gov</a>&gt;</span><span class=3D"Apple-convert=
ed-space">&nbsp;</span>wrote:<br class=3D""><blockquote class=3D"gmail_quot=
e" style=3D"margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-=
color: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><d=
iv dir=3D"ltr" class=3D""><br class=3D""><div class=3D"gmail_extra"><br cla=
ss=3D""><div class=3D"gmail_quote"><span class=3D"">On Fri, May 13, 2016 at=
 10:10 AM, Ralph Castain<span class=3D"Apple-converted-space">&nbsp;</span>=
<span dir=3D"ltr" class=3D"">&lt;<a href=3D"mailto:r...@open-mpi.org" targe=
t=3D"_blank" class=3D"">r...@open-mpi.org</a>&gt;</span><span class=3D"Appl=
e-converted-space">&nbsp;</span>wrote:<br class=3D""><blockquote class=3D"g=
mail_quote" style=3D"margin: 0px 0px 0px 0.8ex; border-left-width: 1px; bor=
der-left-color: rgb(204, 204, 204); border-left-style: solid; padding-left:=
 1ex;"><div style=3D"word-wrap: break-word;" class=3D""><br class=3D""><div=
 class=3D""><span class=3D""><blockquote type=3D"cite" class=3D""><div clas=
s=3D"">On May 13, 2016, at 9:52 AM, Taras Shapovalov &lt;<a href=3D"mailto:=
shapov...@gmail.com" target=3D"_blank" class=3D"">shapov...@gmail.com</a>&g=
t; wrote:</div><br class=3D""><div class=3D""><div dir=3D"ltr" class=3D""><=
div class=3D""><div class=3D""><div class=3D""><div class=3D""><div class=
=3D""><div class=3D"">Hi Ralph and Gregory,<br class=3D""><br class=3D""></=
div>Thank you the both for the so detailed answers! I see your replies comp=
lement each other. Although I am a bit confused now with the whole picture,=
 so could you confirm that I get the ideas correctly:<br class=3D""><br cla=
ss=3D""></div>1. All implementations of MPI by default should work with Sin=
gularity containers (maybe not as optimal as could be, but should start and=
 finish correctly always). Actually I've tested recently MPICH+Singularity =
with several workload managers, worked fine (did not benchmark it comparing=
 with Open MPI). I did not manage to make Singularity+MPI work in LSF, but =
this is a different story that deserves a separate thread.<br class=3D""></=
div></div></div></div></div></div></blockquote></span></div></div></blockqu=
ote><div class=3D""><br class=3D""></div></span><div class=3D"">I wouldn't =
necessarily say they would all work by default. For example, some namespace=
s may necessitate being disabled in order to get proper shared memory IO pe=
rformance. But ... If you have tested this, that is great news and I'd love=
 to hear more about your findings!</div><span class=3D""><div class=3D"">&n=
bsp;</div><blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.=
8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204); border-=
left-style: solid; padding-left: 1ex;"><div style=3D"word-wrap: break-word;=
" class=3D""><div class=3D""><span class=3D""><blockquote type=3D"cite" cla=
ss=3D""><div class=3D""><div dir=3D"ltr" class=3D""><div class=3D""><div cl=
ass=3D""><div class=3D""><div class=3D""></div></div></div></div></div></di=
v></blockquote><div class=3D""><br class=3D""></div></span>Correct - the LS=
F issue is likely a problem of getting the required setup info passed by LS=
F</div><div class=3D""><span class=3D""><br class=3D""><blockquote type=3D"=
cite" class=3D""><div class=3D""><div dir=3D"ltr" class=3D""><div class=3D"=
"><div class=3D""><div class=3D""><div class=3D""><br class=3D""></div>2. M=
PI process calls dl_open, thus the more MPI processes starts on a node, the=
 more times dl_open will be called. Open MPI 2.0.1 somehow solves this magi=
cally (I don't get how) and dl_open is called only once per node. Other imp=
lementations of MPI and older Open MPI are not Singularity aware, thus they=
 still will call dl_open each time when MPI process spawns.<span class=3D"A=
pple-converted-space">&nbsp;</span><br class=3D""></div></div></div></div><=
/div></blockquote><div class=3D""><br class=3D""></div></span>Not exactly. =
Singularity will solve the dl_open problem by itself. What the container do=
es is wrap all the dl_open libraries into the container, and so all dl_open=
 calls by the app are locally resolved. Thus, you automatically resolve the=
 IO node bottleneck scaling issue.</div><div class=3D""><br class=3D""></di=
v><div class=3D"">What OMPI adds is that it pulls the container only once/n=
ode. Other mpiexec implementations will pull the container again for every =
local process. So if you have 100 procs/node, OMPI will result in 100x fewe=
r =E2=80=9Cpulls=E2=80=9D thru that IO node.</div></div></blockquote><div c=
lass=3D""><br class=3D""></div></span><div class=3D"">Yep, and additionally=
 I want to make sure we keep Singularity v1 and v2 features separate. Versi=
on 2 has several huge benefits (including this) over v1, but it is a depart=
ure from using SAPPs (and now uses images).</div><span class=3D""><div clas=
s=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"margin: 0px 0=
px 0px 0.8ex; border-left-width: 1px; border-left-color: rgb(204, 204, 204)=
; border-left-style: solid; padding-left: 1ex;"><div style=3D"word-wrap: br=
eak-word;" class=3D""><div class=3D""><span class=3D""><br class=3D""><bloc=
kquote type=3D"cite" class=3D""><div class=3D""><div dir=3D"ltr" class=3D""=
><div class=3D""><div class=3D""><div class=3D""><br class=3D"">3. dl_open =
issue affects only process start time and does not effect the process execu=
tion, so on small scale with long running processes there is no difference =
between Open MPI 2.0.1 and older Open MPI versions (as well as other MPI im=
plementations).<br class=3D""></div></div></div></div></div></blockquote><d=
iv class=3D""><br class=3D""></div></span>Correct</div></div></blockquote><=
div class=3D""><br class=3D""></div></span><div class=3D"">Correct, just ke=
ep in mind start times at massive scale have been stated by several large c=
enters to approach 30 minutes. During that 30 minutes, it basically looks l=
ike a distributed denial of service attack to the file system metadata serv=
er killing file system performance to the rest of the system.</div><span cl=
ass=3D""><div class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" styl=
e=3D"margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-color: =
rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><div styl=
e=3D"word-wrap: break-word;" class=3D""><div class=3D""><span class=3D""><b=
r class=3D""><blockquote type=3D"cite" class=3D""><div class=3D""><div dir=
=3D"ltr" class=3D""><div class=3D""><div class=3D""><div class=3D""><br cla=
ss=3D""></div><div class=3D"">4. When sapp is built then Singularity detect=
s Open MPI (even older then 2.0.1, right?) and resolves all dependencies au=
tomatically adding all files to the sapp. But with, say, MVAPICH2 the depen=
dencies are not resolved automatically, so user should add some stuff manua=
lly.<br class=3D""></div></div></div></div></div></blockquote><div class=3D=
""><br class=3D""></div></span>Correct</div></div></blockquote><div class=
=3D""><br class=3D""></div></span><div class=3D"">And in v2, this will get =
handled either by an RPM installation of Open MPI, or the 'singularity exec=
 --writable /path/to/Container.img make install'.&nbsp;</div><span class=3D=
""><blockquote class=3D"gmail_quote" style=3D"margin: 0px 0px 0px 0.8ex; bo=
rder-left-width: 1px; border-left-color: rgb(204, 204, 204); border-left-st=
yle: solid; padding-left: 1ex;"><div style=3D"word-wrap: break-word;" class=
=3D""><div class=3D""><span class=3D""><br class=3D""><blockquote type=3D"c=
ite" class=3D""><div class=3D""><div dir=3D"ltr" class=3D""><div class=3D""=
><div class=3D""><div class=3D""><br class=3D""></div>5. Apart of solving d=
l_open issue Open MPI 2.0.1 does some splitting between the host and the co=
ntainer, which allows user/admin to not optimize Open MPI for a target plat=
form. I really don't get how Singularity does this, but I get the problem. =
Could you explain what Singularity or Open MPI 2.0.1 does for that specific=
aly?<br class=3D""></div></div></div></div></blockquote><div class=3D""><br=
 class=3D""></div></span><div class=3D"">When running under mpiexec with Si=
ngularity, OMPI=E2=80=99s local daemon on each node actually runs outside o=
f the containers. We then fork/exec the container itself, and the container=
 is defined so it auto-executes the application process. This allows us to =
minimize the services overhead, keeping all services outside of your contai=
ner (and thus shared across all containers.</div><div class=3D""><br class=
=3D""></div><div class=3D"">Other approaches have the daemon -inside- the c=
ontainer, and you get one daemon for each container - and thus, one daemon =
for each local application. So you get a higher overhead and therefore lowe=
r performance.</div></div></div></blockquote><div class=3D""><br class=3D""=
></div></span><div class=3D"">Maybe this will help to articulate it... I ha=
ve described this via the MPI/Singularity invocation pathway as follows (ho=
pefully it is reasonably correct and doesn't cause Ralph to kick me). Consi=
dering the command:</div><div class=3D""><br class=3D""></div><div class=3D=
"">$ mpirun -np X singularity exec ~/Centos-7.img mpi_program</div><div cla=
ss=3D""><br class=3D""></div><div class=3D"">1. From shell (or resource man=
ager) mpirun gets called</div><div class=3D"">2. mpirun forks and exec orte=
 daemon</div><div class=3D"">3. Orted process creates PMI</div><div class=
=3D"">4. Orted forks =3D=3D to the number of process per node requested</di=
v><div class=3D"">5. Orted children exec to original command passed to mpir=
un (Singularity)</div><div class=3D"">6. Each Singularity execs the command=
 passed inside the given container</div><div class=3D"">7. Each MPI program=
 links in the dynamic Open MPI libraries (ldd)</div><div class=3D"">8. Open=
 MPI libraries continue to open the non-ldd shared libraries (dlopen)</div>=
<div class=3D"">9. Open MPI libraries connect back to original orted via PM=
I</div><div class=3D"">10. All non-shared memory communication occurs throu=
gh the PMI and then to local interfaces (e.g. InfiniBand)</div><div class=
=3D""><br class=3D""></div><div class=3D"">While the workflow from the MPI =
perspective seems simpler with the daemon inside the container, it is much =
more complicated from the system perspective because each orted process mus=
t also know about the other hosts, be able to communicate with them and mit=
igate performance factors of host/resource/interconnect tuning.</div><div c=
lass=3D""><br class=3D""></div><div class=3D"">Hope that helps!</div><span =
class=3D""><div class=3D""><br class=3D""></div><div class=3D""><br class=
=3D""></div><div class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" s=
tyle=3D"margin: 0px 0px 0px 0.8ex; border-left-width: 1px; border-left-colo=
r: rgb(204, 204, 204); border-left-style: solid; padding-left: 1ex;"><div s=
tyle=3D"word-wrap: break-word;" class=3D""><div class=3D""><div class=3D"">=
<br class=3D""></div><div class=3D"">HTH</div><span class=3D""><font color=
=3D"#888888" class=3D""><div class=3D"">Ralph</div></font></span><span clas=
s=3D""><div class=3D""><br class=3D""></div><br class=3D""><blockquote type=
=3D"cite" class=3D""><div class=3D""><div dir=3D"ltr" class=3D""><div class=
=3D""><div class=3D""><br class=3D""></div>Best regards,<br class=3D""><br =
class=3D""></div>Taras<br class=3D""><br class=3D""></div><div class=3D""><=
br class=3D""></div>--<span class=3D"Apple-converted-space">&nbsp;</span><b=
r class=3D"">You received this message because you are subscribed to the Go=
ogle Groups "singularity" group.<br class=3D"">To unsubscribe from this gro=
up and stop receiving emails from it, send an email to<span class=3D"Apple-=
converted-space">&nbsp;</span><a href=3D"mailto:singu...@lbl.gov" target=3D=
"_blank" class=3D"">singu...@lbl.gov</a>.<br class=3D""></div></blockquote>=
</span></div><br class=3D""></div><div class=3D""><div class=3D""><div clas=
s=3D""><br class=3D"webkit-block-placeholder"></div>--<span class=3D"Apple-=
converted-space">&nbsp;</span><br class=3D"">You received this message beca=
use you are subscribed to the Google Groups "singularity" group.<br class=
=3D"">To unsubscribe from this group and stop receiving emails from it, sen=
d an email to<span class=3D"Apple-converted-space">&nbsp;</span><a href=3D"=
mailto:singu...@lbl.gov" target=3D"_blank" class=3D"">singu...@lbl.gov</a>.=
<br class=3D""></div></div></blockquote></span></div><br class=3D""><br cle=
ar=3D"all" class=3D""><span class=3D""><div class=3D""><br class=3D""></div=
>--<span class=3D"Apple-converted-space">&nbsp;</span><br class=3D""><div c=
lass=3D""><div dir=3D"ltr" class=3D""><div class=3D"">Gregory M. Kurtzer<br=
 class=3D"">High Performance Computing Services (HPCS)<br class=3D"">Univer=
sity of California<br class=3D"">Lawrence Berkeley National Laboratory<br c=
lass=3D"">One Cyclotron Road, Berkeley, CA 94720</div></div></div></span></=
div></div><div class=3D""><br class=3D"webkit-block-placeholder"></div>--<s=
pan class=3D"Apple-converted-space">&nbsp;</span><br class=3D""><div class=
=3D""><div class=3D"">You received this message because you are subscribed =
to the Google Groups "singularity" group.<br class=3D"">To unsubscribe from=
 this group and stop receiving emails from it, send an email to<span class=
=3D"Apple-converted-space">&nbsp;</span><a href=3D"mailto:singu...@lbl.gov"=
 target=3D"_blank" class=3D"">singu...@lbl.gov</a>.<br class=3D""></div></d=
iv></blockquote></div><br class=3D""><br clear=3D"all" class=3D""><br class=
=3D"">--<span class=3D"Apple-converted-space">&nbsp;</span><br class=3D""><=
/div></div><span class=3D"HOEnZb"><font color=3D"#888888" class=3D""><div c=
lass=3D""><div dir=3D"ltr" class=3D"">/T<br class=3D""></div></div></font><=
/span></div><div class=3D"HOEnZb"><div class=3D"h5"><div class=3D""><br cla=
ss=3D"webkit-block-placeholder"></div>--<span class=3D"Apple-converted-spac=
e">&nbsp;</span><br class=3D"">You received this message because you are su=
bscribed to the Google Groups "singularity" group.<br class=3D"">To unsubsc=
ribe from this group and stop receiving emails from it, send an email to<sp=
an class=3D"Apple-converted-space">&nbsp;</span><a href=3D"mailto:singu...@=
lbl.gov" target=3D"_blank" class=3D"">singu...@lbl.gov</a>.<br class=3D""><=
/div></div></blockquote></div><br class=3D""><br clear=3D"all" class=3D""><=
div class=3D""><br class=3D""></div>--<span class=3D"Apple-converted-space"=
>&nbsp;</span><br class=3D""><div class=3D"gmail_signature"><div dir=3D"ltr=
" class=3D""><div class=3D"">Gregory M. Kurtzer<br class=3D"">High Performa=
nce Computing Services (HPCS)<br class=3D"">University of California<br cla=
ss=3D"">Lawrence Berkeley National Laboratory<br class=3D"">One Cyclotron R=
oad, Berkeley, CA 94720</div></div></div></div></div><div style=3D"font-fam=
ily: Helvetica; font-size: 12px; font-style: normal; font-variant-caps: nor=
mal; font-weight: normal; letter-spacing: normal; orphans: auto; text-align=
: start; text-indent: 0px; text-transform: none; white-space: normal; widow=
s: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;" class=3D""><br=
 class=3D"webkit-block-placeholder"></div><span style=3D"font-family: Helve=
tica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-=
weight: normal; letter-spacing: normal; orphans: auto; text-align: start; t=
ext-indent: 0px; text-transform: none; white-space: normal; widows: auto; w=
ord-spacing: 0px; -webkit-text-stroke-width: 0px; float: none; display: inl=
ine !important;" class=3D"">--<span class=3D"Apple-converted-space">&nbsp;<=
/span></span><br style=3D"font-family: Helvetica; font-size: 12px; font-sty=
le: normal; font-variant-caps: normal; font-weight: normal; letter-spacing:=
 normal; orphans: auto; text-align: start; text-indent: 0px; text-transform=
: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-text-=
stroke-width: 0px;" class=3D""><span style=3D"font-family: Helvetica; font-=
size: 12px; font-style: normal; font-variant-caps: normal; font-weight: nor=
mal; letter-spacing: normal; orphans: auto; text-align: start; text-indent:=
 0px; text-transform: none; white-space: normal; widows: auto; word-spacing=
: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !import=
ant;" class=3D"">You received this message because you are subscribed to th=
e Google Groups "singularity" group.</span><br style=3D"font-family: Helvet=
ica; font-size: 12px; font-style: normal; font-variant-caps: normal; font-w=
eight: normal; letter-spacing: normal; orphans: auto; text-align: start; te=
xt-indent: 0px; text-transform: none; white-space: normal; widows: auto; wo=
rd-spacing: 0px; -webkit-text-stroke-width: 0px;" class=3D""><span style=3D=
"font-family: Helvetica; font-size: 12px; font-style: normal; font-variant-=
caps: normal; font-weight: normal; letter-spacing: normal; orphans: auto; t=
ext-align: start; text-indent: 0px; text-transform: none; white-space: norm=
al; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px; float:=
 none; display: inline !important;" class=3D"">To unsubscribe from this gro=
up and stop receiving emails from it, send an email to<span class=3D"Apple-=
converted-space">&nbsp;</span></span><a href=3D"mailto:singu...@lbl.gov" st=
yle=3D"font-family: Helvetica; font-size: 12px; font-style: normal; font-va=
riant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: a=
uto; text-align: start; text-indent: 0px; text-transform: none; white-space=
: normal; widows: auto; word-spacing: 0px; -webkit-text-stroke-width: 0px;"=
 class=3D"">singu...@lbl.gov</a><span style=3D"font-family: Helvetica; font=
-size: 12px; font-style: normal; font-variant-caps: normal; font-weight: no=
rmal; letter-spacing: normal; orphans: auto; text-align: start; text-indent=
: 0px; text-transform: none; white-space: normal; widows: auto; word-spacin=
g: 0px; -webkit-text-stroke-width: 0px; float: none; display: inline !impor=
tant;" class=3D"">.</span></div></blockquote></div><br class=3D""></body></=
html>
--Apple-Mail=_2A865F55-5DCF-42DD-BAF5-32787619DD50--
