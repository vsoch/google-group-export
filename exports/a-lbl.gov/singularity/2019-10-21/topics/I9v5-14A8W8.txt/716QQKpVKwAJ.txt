X-Received: by 10.107.55.11 with SMTP id e11mr20641655ioa.7.1463403250848;
        Mon, 16 May 2016 05:54:10 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.96.70 with SMTP id i67ls664405itc.34.gmail; Mon, 16 May
 2016 05:54:10 -0700 (PDT)
X-Received: by 10.66.152.164 with SMTP id uz4mr29039455pab.9.1463403250281;
        Mon, 16 May 2016 05:54:10 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id f65si45646986pfb.29.2016.05.16.05.54.10
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 May 2016 05:54:10 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) client-ip=74.125.82.72;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 3.6
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EUAQD0wTlXiEhSfUpdhHsPBoM0qkmLc4F2FwGFeQKBGgc5EwEBAQEBAQEDDwEBAQgLCwkfMYRCAQEBAwESESswCwsLDSAKAgIhAQ8DAQUBHAYIBwQBGQMEAYdzAw8IBaAzgTE+MYs7jCoNhCcBCgEBASMQiV+BA4JDgU4RAQYCgxSCWQWOU4RdhEYxAYh1gy+BeYFphE+IYYdchicSHoEOIQGCRhEKgWscMgeGQggXgR4BAQE
X-IronPort-AV: E=Sophos;i="5.24,627,1455004800"; 
   d="scan'208,217";a="23454443"
Received: from mail-wm0-f72.google.com ([74.125.82.72])
  by fe4.lbl.gov with ESMTP; 16 May 2016 05:54:08 -0700
Received: by mail-wm0-f72.google.com with SMTP id r12so42876162wme.0
        for <singu...@lbl.gov>; Mon, 16 May 2016 05:54:08 -0700 (PDT)
X-Gm-Message-State: AOPr4FUum03Oq6xdjvWmsU4xt7cU9uQ62V6lhM+DUJMQyVEy/4gnG/7qdfp2CZ0XAtora3bfRlHEf2qMVOXhO3btJ8igwbt3/FK7ob7v9fVdIzfQPkPA7M1hHtaPNv9h8Gaga+631wSHBb8qpQGeXJz7goU=
X-Received: by 10.28.105.200 with SMTP id z69mr17604552wmh.78.1463403247403;
        Mon, 16 May 2016 05:54:07 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.28.105.200 with SMTP id z69mr17604531wmh.78.1463403247069;
 Mon, 16 May 2016 05:54:07 -0700 (PDT)
Received: by 10.28.144.200 with HTTP; Mon, 16 May 2016 05:54:06 -0700 (PDT)
In-Reply-To: <CAAS-_CBsKM3=d_OhVknqcF2De29UiQ-cjOv5imCyR=jv=4Rh7g@mail.gmail.com>
References: <a5f7347e-4bf8-486c-b06f-f00e2e762747@lbl.gov>
	<CAN7etTwFQcnyMjry5ZvRKVqrWo2FqpAPAwQ1ZfpzJdOk3kRp_A@mail.gmail.com>
	<CAAS-_CBb3sy393W1Bga5Wnr9-EvSHC6NPNaOx58Mpdw-LiFq8g@mail.gmail.com>
	<414C0039-42A3-4C2E-89F6-3D97D082C742@open-mpi.org>
	<CAN7etTxQkXe6uEDZgt+kkAex0Hzt9UWYE6+V4qS4vnKJAMgFzQ@mail.gmail.com>
	<CAAS-_CBsKM3=d_OhVknqcF2De29UiQ-cjOv5imCyR=jv=4Rh7g@mail.gmail.com>
Date: Mon, 16 May 2016 05:54:06 -0700
Message-ID: <CAN7etTxJkbqv3mdujx0JziZo7y9fTrRkk3eE0GpXnkmKhpTc=g@mail.gmail.com>
Subject: Re: [Singularity] SIngularity and MPI implementations
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a11467c0eb48d560532f51e73

--001a11467c0eb48d560532f51e73
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On Mon, May 16, 2016 at 1:17 AM, Taras Shapovalov <shapov...@gmail.com>
wrote:

> Hi guys,
>
> Thanks for the great answers! Now it is more or less clear how it works.
> To be absolutely sure, can you please confirm also these statements (got
> from your answers):
>
> 1. Ralph's answer mentions mpiexec, but Gregory's answer is about mpirun.
> So, all the discussed here can be applied to the both utilities included =
in
> Open MPI distribution.
>

Ralph can speak definitively here, but I believe my answer applies to both.


>
> 2. Running Open MPI processes in a single container is impleneted only in
> Singularity v2. In v1 each Open MPI process still will be executed in
> different containers.
>

For technical Q&A we should probably use the word namespaces in addition to
containers, I'll explain.

Singularity v1 will cache the container on each node, so processes within a
node will share the container cache but operate in some different
namespaces (the specific namespaces are somewhat application/necessity
dependent).

Singularity v2 has no need to cache the container, but it does need to bind
it to a loop device. This happens once per node, but again there is no
cache so all nodes are sharing the same container image and also operate in
some separate namespaces (again dependent on need).


>
> 3. Lets compare these 2 scenarios: Singularity runs child processes in a
> single container agains scenario when each child runs in a separate
> container each. The optimization with dlopen call happens in the first
> scenario, because the opened library is loaded into the memory per
> Singularity container, then dlopen magically returns the same handler for
> each child process inside the container, which should be faster. Or there
> is some other low level optimization occurs in the first scenario regardi=
ng
> dlopen?
>

I am not sure I follow completely, but if you are asking what I think
you're asking... Singularity v2 will optimize all calls to open()
(including dlopen()) within the container because what is within the
container all exist within a single image (there is no need to make
additional metadata requests to files that exist within the container
image). Additionally there is no launch penalty taken because there is no
need to cache the image. On average, launch time when using this method is
about .020s on my test system and writes/changes never require a rebuild.

With Singularity v1, files are pulled out of the container archive (SAPP)
and spilled out to the storage. If the storage is local to nodes, then
calls to open() and thus the required metadata will not goto shared
storage. By default the container is cached to shared storage (unless
launching a SAPP file directly through Open MPI). Launch time for v1 is
about .050s after the image has been cached, and caching of the image
usually takes anywhere from .5s to as high as you want to go depending on
image size (I've seen in my testing upwards of 10 seconds).

Hopefully that helps!



> Best regards,
>
> Taras
>
>
> On Fri, May 13, 2016 at 9:18 PM, Gregory M. Kurtzer <gmku...@lbl.gov>
> wrote:
>
>>
>>
>> On Fri, May 13, 2016 at 10:10 AM, Ralph Castain <r...@open-mpi.org> wrot=
e:
>>
>>>
>>> On May 13, 2016, at 9:52 AM, Taras Shapovalov <shapov...@gmail.com>
>>> wrote:
>>>
>>> Hi Ralph and Gregory,
>>>
>>> Thank you the both for the so detailed answers! I see your replies
>>> complement each other. Although I am a bit confused now with the whole
>>> picture, so could you confirm that I get the ideas correctly:
>>>
>>> 1. All implementations of MPI by default should work with Singularity
>>> containers (maybe not as optimal as could be, but should start and fini=
sh
>>> correctly always). Actually I've tested recently MPICH+Singularity with
>>> several workload managers, worked fine (did not benchmark it comparing =
with
>>> Open MPI). I did not manage to make Singularity+MPI work in LSF, but th=
is
>>> is a different story that deserves a separate thread.
>>>
>>>
>> I wouldn't necessarily say they would all work by default. For example,
>> some namespaces may necessitate being disabled in order to get proper
>> shared memory IO performance. But ... If you have tested this, that is
>> great news and I'd love to hear more about your findings!
>>
>>
>>>
>>> Correct - the LSF issue is likely a problem of getting the required
>>> setup info passed by LSF
>>>
>>>
>>> 2. MPI process calls dl_open, thus the more MPI processes starts on a
>>> node, the more times dl_open will be called. Open MPI 2.0.1 somehow sol=
ves
>>> this magically (I don't get how) and dl_open is called only once per no=
de.
>>> Other implementations of MPI and older Open MPI are not Singularity awa=
re,
>>> thus they still will call dl_open each time when MPI process spawns.
>>>
>>>
>>> Not exactly. Singularity will solve the dl_open problem by itself. What
>>> the container does is wrap all the dl_open libraries into the container=
,
>>> and so all dl_open calls by the app are locally resolved. Thus, you
>>> automatically resolve the IO node bottleneck scaling issue.
>>>
>>> What OMPI adds is that it pulls the container only once/node. Other
>>> mpiexec implementations will pull the container again for every local
>>> process. So if you have 100 procs/node, OMPI will result in 100x fewer
>>> =E2=80=9Cpulls=E2=80=9D thru that IO node.
>>>
>>
>> Yep, and additionally I want to make sure we keep Singularity v1 and v2
>> features separate. Version 2 has several huge benefits (including this)
>> over v1, but it is a departure from using SAPPs (and now uses images).
>>
>>
>>>
>>>
>>> 3. dl_open issue affects only process start time and does not effect th=
e
>>> process execution, so on small scale with long running processes there =
is
>>> no difference between Open MPI 2.0.1 and older Open MPI versions (as we=
ll
>>> as other MPI implementations).
>>>
>>>
>>> Correct
>>>
>>
>> Correct, just keep in mind start times at massive scale have been stated
>> by several large centers to approach 30 minutes. During that 30 minutes,=
 it
>> basically looks like a distributed denial of service attack to the file
>> system metadata server killing file system performance to the rest of th=
e
>> system.
>>
>>
>>>
>>>
>>> 4. When sapp is built then Singularity detects Open MPI (even older the=
n
>>> 2.0.1, right?) and resolves all dependencies automatically adding all f=
iles
>>> to the sapp. But with, say, MVAPICH2 the dependencies are not resolved
>>> automatically, so user should add some stuff manually.
>>>
>>>
>>> Correct
>>>
>>
>> And in v2, this will get handled either by an RPM installation of Open
>> MPI, or the 'singularity exec --writable /path/to/Container.img make
>> install'.
>>
>>>
>>>
>>> 5. Apart of solving dl_open issue Open MPI 2.0.1 does some splitting
>>> between the host and the container, which allows user/admin to not opti=
mize
>>> Open MPI for a target platform. I really don't get how Singularity does
>>> this, but I get the problem. Could you explain what Singularity or Open=
 MPI
>>> 2.0.1 does for that specificaly?
>>>
>>>
>>> When running under mpiexec with Singularity, OMPI=E2=80=99s local daemo=
n on each
>>> node actually runs outside of the containers. We then fork/exec the
>>> container itself, and the container is defined so it auto-executes the
>>> application process. This allows us to minimize the services overhead,
>>> keeping all services outside of your container (and thus shared across =
all
>>> containers.
>>>
>>> Other approaches have the daemon -inside- the container, and you get on=
e
>>> daemon for each container - and thus, one daemon for each local
>>> application. So you get a higher overhead and therefore lower performan=
ce.
>>>
>>
>> Maybe this will help to articulate it... I have described this via the
>> MPI/Singularity invocation pathway as follows (hopefully it is reasonabl=
y
>> correct and doesn't cause Ralph to kick me). Considering the command:
>>
>> $ mpirun -np X singularity exec ~/Centos-7.img mpi_program
>>
>> 1. From shell (or resource manager) mpirun gets called
>> 2. mpirun forks and exec orte daemon
>> 3. Orted process creates PMI
>> 4. Orted forks =3D=3D to the number of process per node requested
>> 5. Orted children exec to original command passed to mpirun (Singularity=
)
>> 6. Each Singularity execs the command passed inside the given container
>> 7. Each MPI program links in the dynamic Open MPI libraries (ldd)
>> 8. Open MPI libraries continue to open the non-ldd shared libraries
>> (dlopen)
>> 9. Open MPI libraries connect back to original orted via PMI
>> 10. All non-shared memory communication occurs through the PMI and then
>> to local interfaces (e.g. InfiniBand)
>>
>> While the workflow from the MPI perspective seems simpler with the daemo=
n
>> inside the container, it is much more complicated from the system
>> perspective because each orted process must also know about the other
>> hosts, be able to communicate with them and mitigate performance factors=
 of
>> host/resource/interconnect tuning.
>>
>> Hope that helps!
>>
>>
>>
>>
>>>
>>> HTH
>>> Ralph
>>>
>>>
>>>
>>> Best regards,
>>>
>>> Taras
>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> High Performance Computing Services (HPCS)
>> University of California
>> Lawrence Berkeley National Laboratory
>> One Cyclotron Road, Berkeley, CA 94720
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> /T
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a11467c0eb48d560532f51e73
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Mon, May 16, 2016 at 1:17 AM, Taras Shapovalov <span dir=3D"ltr">&lt=
;<a href=3D"mailto:shapov...@gmail.com" target=3D"_blank">shapov...@gmail.c=
om</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"marg=
in:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
><div><div><div><div><div>Hi guys,<br><br></div>Thanks for the great answer=
s! Now it is more or less clear how it works. To be absolutely sure, can yo=
u please confirm also these statements (got from your answers):<br><br></di=
v><div>1. Ralph&#39;s answer mentions mpiexec, but Gregory&#39;s answer is =
about mpirun. So, all the discussed here can be applied to the both utiliti=
es included in Open MPI distribution.<br></div></div></div></div></div></bl=
ockquote><div><br></div><div>Ralph can speak definitively here, but I belie=
ve my answer applies to both.</div><div>=C2=A0</div><blockquote class=3D"gm=
ail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-le=
ft:1ex"><div dir=3D"ltr"><div><div><div><div></div><div><br></div>2. Runnin=
g Open MPI processes in a single container is impleneted only in Singularit=
y v2. In v1 each Open MPI process still will be executed in different conta=
iners.<br></div></div></div></div></blockquote><div><br></div><div>For tech=
nical Q&amp;A we should probably use the word namespaces in addition to con=
tainers, I&#39;ll explain.</div><div><br></div><div>Singularity v1 will cac=
he the container on each node, so processes within a node will share the co=
ntainer cache but operate in some different namespaces (the specific namesp=
aces are somewhat application/necessity dependent).</div><div><br></div><di=
v>Singularity v2 has no need to cache the container, but it does need to bi=
nd it to a loop device. This happens once per node, but again there is no c=
ache so all nodes are sharing the same container image and also operate in =
some separate namespaces (again dependent on need).</div><div>=C2=A0</div><=
blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px=
 #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><div><div><br></div>3. =
Lets compare these 2 scenarios: Singularity runs child processes in a singl=
e container agains scenario when each child runs in a separate container ea=
ch. The optimization with dlopen call happens in the first scenario, becaus=
e the opened library is loaded into the memory per Singularity container, t=
hen dlopen magically returns the same handler for each child process inside=
 the container, which should be faster. Or there is some other low level op=
timization occurs in the first scenario regarding dlopen?<br></div></div></=
div></blockquote><div><br></div><div>I am not sure I follow completely, but=
 if you are asking what I think you&#39;re asking... Singularity v2 will op=
timize all calls to open() (including dlopen()) within the container becaus=
e what is within the container all exist within a single image (there is no=
 need to make additional metadata requests to files that exist within the c=
ontainer image). Additionally there is no launch penalty taken because ther=
e is no need to cache the image. On average, launch time when using this me=
thod is about .020s on my test system and writes/changes never require a re=
build.<br></div><div><br></div><div>With Singularity v1, files are pulled o=
ut of the container archive (SAPP) and spilled out to the storage. If the s=
torage is local to nodes, then calls to open() and thus the required metada=
ta will not goto shared storage. By default the container is cached to shar=
ed storage (unless launching a SAPP file directly through Open MPI). Launch=
 time for v1 is about .050s after the image has been cached, and caching of=
 the image usually takes anywhere from .5s to as high as you want to go dep=
ending on image size (I&#39;ve seen in my testing upwards of 10 seconds).</=
div><div><br></div><div>Hopefully that helps!</div><div><br></div><div><br>=
</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-l=
eft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div><div><br></div>B=
est regards,<br><br></div>Taras<br><div><div><br></div></div></div><div cla=
ss=3D"gmail_extra"><div><div class=3D"h5"><br><div class=3D"gmail_quote">On=
 Fri, May 13, 2016 at 9:18 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a =
href=3D"mailto:gmku...@lbl.gov" target=3D"_blank">gmku...@lbl.gov</a>&gt;</=
span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8e=
x;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br><div cl=
ass=3D"gmail_extra"><br><div class=3D"gmail_quote"><span>On Fri, May 13, 20=
16 at 10:10 AM, Ralph Castain <span dir=3D"ltr">&lt;<a href=3D"mailto:r...@=
open-mpi.org" target=3D"_blank">r...@open-mpi.org</a>&gt;</span> wrote:<br>=
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><br><div=
><span><blockquote type=3D"cite"><div>On May 13, 2016, at 9:52 AM, Taras Sh=
apovalov &lt;<a href=3D"mailto:shapov...@gmail.com" target=3D"_blank">shapo=
v...@gmail.com</a>&gt; wrote:</div><br><div><div dir=3D"ltr"><div><div><div=
><div><div><div>Hi Ralph and Gregory,<br><br></div>Thank you the both for t=
he so detailed answers! I see your replies complement each other. Although =
I am a bit confused now with the whole picture, so could you confirm that I=
 get the ideas correctly:<br><br></div>1. All implementations of MPI by def=
ault should work with Singularity containers (maybe not as optimal as could=
 be, but should start and finish correctly always). Actually I&#39;ve teste=
d recently MPICH+Singularity with several workload managers, worked fine (d=
id not benchmark it comparing with Open MPI). I did not manage to make Sing=
ularity+MPI work in LSF, but this is a different story that deserves a sepa=
rate thread.<br></div></div></div></div></div></div></blockquote></span></d=
iv></div></blockquote><div><br></div></span><div>I wouldn&#39;t necessarily=
 say they would all work by default. For example, some namespaces may neces=
sitate being disabled in order to get proper shared memory IO performance. =
But ... If you have tested this, that is great news and I&#39;d love to hea=
r more about your findings!</div><span><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div style=3D"word-wrap:break-word"><div><span><blockquote ty=
pe=3D"cite"><div><div dir=3D"ltr"><div><div><div><div></div></div></div></d=
iv></div></div></blockquote><div><br></div></span>Correct - the LSF issue i=
s likely a problem of getting the required setup info passed by LSF</div><d=
iv><span><br><blockquote type=3D"cite"><div><div dir=3D"ltr"><div><div><div=
><div><br></div>2. MPI process calls dl_open, thus the more MPI processes s=
tarts on a node, the more times dl_open will be called. Open MPI 2.0.1 some=
how solves this magically (I don&#39;t get how) and dl_open is called only =
once per node. Other implementations of MPI and older Open MPI are not Sing=
ularity aware, thus they still will call dl_open each time when MPI process=
 spawns. <br></div></div></div></div></div></blockquote><div><br></div></sp=
an>Not exactly. Singularity will solve the dl_open problem by itself. What =
the container does is wrap all the dl_open libraries into the container, an=
d so all dl_open calls by the app are locally resolved. Thus, you automatic=
ally resolve the IO node bottleneck scaling issue.</div><div><br></div><div=
>What OMPI adds is that it pulls the container only once/node. Other mpiexe=
c implementations will pull the container again for every local process. So=
 if you have 100 procs/node, OMPI will result in 100x fewer =E2=80=9Cpulls=
=E2=80=9D thru that IO node.</div></div></blockquote><div><br></div></span>=
<div>Yep, and additionally I want to make sure we keep Singularity v1 and v=
2 features separate. Version 2 has several huge benefits (including this) o=
ver v1, but it is a departure from using SAPPs (and now uses images).</div>=
<span><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 =
0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wr=
ap:break-word"><div><span><br><blockquote type=3D"cite"><div><div dir=3D"lt=
r"><div><div><div><br>3. dl_open issue affects only process start time and =
does not effect the process execution, so on small scale with long running =
processes there is no difference between Open MPI 2.0.1 and older Open MPI =
versions (as well as other MPI implementations).<br></div></div></div></div=
></div></blockquote><div><br></div></span>Correct</div></div></blockquote><=
div><br></div></span><div>Correct, just keep in mind start times at massive=
 scale have been stated by several large centers to approach 30 minutes. Du=
ring that 30 minutes, it basically looks like a distributed denial of servi=
ce attack to the file system metadata server killing file system performanc=
e to the rest of the system.</div><span><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div style=3D"word-wrap:break-word"><div><span><br><blockquot=
e type=3D"cite"><div><div dir=3D"ltr"><div><div><div><br></div><div>4. When=
 sapp is built then Singularity detects Open MPI (even older then 2.0.1, ri=
ght?) and resolves all dependencies automatically adding all files to the s=
app. But with, say, MVAPICH2 the dependencies are not resolved automaticall=
y, so user should add some stuff manually.<br></div></div></div></div></div=
></blockquote><div><br></div></span>Correct</div></div></blockquote><div><b=
r></div></span><div>And in v2, this will get handled either by an RPM insta=
llation of Open MPI, or the &#39;singularity exec --writable /path/to/Conta=
iner.img make install&#39;.=C2=A0</div><span><blockquote class=3D"gmail_quo=
te" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"=
><div style=3D"word-wrap:break-word"><div><span><br><blockquote type=3D"cit=
e"><div><div dir=3D"ltr"><div><div><div><br></div>5. Apart of solving dl_op=
en issue Open MPI 2.0.1 does some splitting between the host and the contai=
ner, which allows user/admin to not optimize Open MPI for a target platform=
. I really don&#39;t get how Singularity does this, but I get the problem. =
Could you explain what Singularity or Open MPI 2.0.1 does for that specific=
aly?<br></div></div></div></div></blockquote><div><br></div></span><div>Whe=
n running under mpiexec with Singularity, OMPI=E2=80=99s local daemon on ea=
ch node actually runs outside of the containers. We then fork/exec the cont=
ainer itself, and the container is defined so it auto-executes the applicat=
ion process. This allows us to minimize the services overhead, keeping all =
services outside of your container (and thus shared across all containers.<=
/div><div><br></div><div>Other approaches have the daemon -inside- the cont=
ainer, and you get one daemon for each container - and thus, one daemon for=
 each local application. So you get a higher overhead and therefore lower p=
erformance.</div></div></div></blockquote><div><br></div></span><div>Maybe =
this will help to articulate it... I have described this via the MPI/Singul=
arity invocation pathway as follows (hopefully it is reasonably correct and=
 doesn&#39;t cause Ralph to kick me). Considering the command:</div><div><b=
r></div><div>$ mpirun -np X singularity exec ~/Centos-7.img mpi_program</di=
v><div><br></div><div>1. From shell (or resource manager) mpirun gets calle=
d</div><div>2. mpirun forks and exec orte daemon</div><div>3. Orted process=
 creates PMI</div><div>4. Orted forks =3D=3D to the number of process per n=
ode requested</div><div>5. Orted children exec to original command passed t=
o mpirun (Singularity)</div><div>6. Each Singularity execs the command pass=
ed inside the given container</div><div>7. Each MPI program links in the dy=
namic Open MPI libraries (ldd)</div><div>8. Open MPI libraries continue to =
open the non-ldd shared libraries (dlopen)</div><div>9. Open MPI libraries =
connect back to original orted via PMI</div><div>10. All non-shared memory =
communication occurs through the PMI and then to local interfaces (e.g. Inf=
iniBand)</div><div><br></div><div>While the workflow from the MPI perspecti=
ve seems simpler with the daemon inside the container, it is much more comp=
licated from the system perspective because each orted process must also kn=
ow about the other hosts, be able to communicate with them and mitigate per=
formance factors of host/resource/interconnect tuning.</div><div><br></div>=
<div>Hope that helps!</div><span><div><br></div><div><br></div><div>=C2=A0<=
/div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><di=
v><div><br></div><div>HTH</div><span><font color=3D"#888888"><div>Ralph</di=
v></font></span><span><div><br></div><br><blockquote type=3D"cite"><div><di=
v dir=3D"ltr"><div><div><br></div>Best regards,<br><br></div>Taras<br><br><=
/div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></blockquote></span></div><br></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></span></div><br><br clear=3D"all"><span><div><br>=
</div>-- <br><div><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performa=
nce Computing Services (HPCS)<br>University of California<br>Lawrence Berke=
ley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></di=
v></div>
</span></div></div>

<p></p>

-- <br><div><div>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><br>-- <br></div></div=
><span class=3D"HOEnZb"><font color=3D"#888888"><div><div dir=3D"ltr">/T<br=
></div></div>
</font></span></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature"><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>=
High Performance Computing Services (HPCS)<br>University of California<br>L=
awrence Berkeley National Laboratory<br>One Cyclotron Road, Berkeley, CA 94=
720</div></div></div>
</div></div>

--001a11467c0eb48d560532f51e73--
