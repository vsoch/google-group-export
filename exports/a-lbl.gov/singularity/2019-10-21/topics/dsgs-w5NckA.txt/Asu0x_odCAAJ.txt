Date: Mon, 21 May 2018 10:03:29 -0700 (PDT)
From: Samy <smahan...@gmail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <4e62b75e-086e-451b-a4fa-416f00aaee01@lbl.gov>
In-Reply-To: <2d204111-9f70-4f6c-83e2-61209f4481a3@lbl.gov>
References: <2d204111-9f70-4f6c-83e2-61209f4481a3@lbl.gov>
Subject: Re: Running a container in a cluster with mpi and the %environment
 snippet
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_21422_748689590.1526922209213"

------=_Part_21422_748689590.1526922209213
Content-Type: multipart/alternative; 
	boundary="----=_Part_21423_936470973.1526922209213"

------=_Part_21423_936470973.1526922209213
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Thank Daniel for the suggestion. 

I tried this already but unfortunately it didn't work even when putting the 
arguments inside the container. I get the error bellow. Running mpi with 
the same binary without singularity works with no problem. 

I'm wondering if anyone had any ideas. I checked and all the files and the 
binaries are inside the container.

The error i get:

mpiexec.hydra -hostfile ./nodelist -n 4 -ppn 2 singularity run 
container.simg arg arg

 [cli_1]: write_line error; fd=16 buf=:cmd=init pmi_version=1 
pmi_subversion=1
:
system msg for write_line failure : Bad file descriptor
[cli_1]: Unable to write to PMI_fd
[cli_1]: write_line error; fd=16 buf=:cmd=barrier_in
:
system msg for write_line failure : Bad file descriptor
[cli_1]: write_line error; fd=16 buf=:cmd=get_ranks2hosts
:
system msg for write_line failure : Bad file descriptor
[cli_1]: expecting cmd="put_ranks2hosts", got cmd=""
Fatal error in PMPI_Init_thread: Other MPI error, error stack:
MPIR_Init_thread(805): fail failed
MPID_Init(1743)......: channel initialization failed
MPID_Init(2144)......: PMI_Init returned -1
[cli_1]: write_line error; fd=16 buf=:cmd=abort exitcode=68204815
:
system msg for write_line failure : Bad file descriptor
all done!
[cli_0]: write_line error; fd=14 buf=:cmd=init pmi_version=1 
pmi_subversion=1
:
system msg for write_line failure : Bad file descriptor
[cli_0]: Unable to write to PMI_fd
[cli_0]: write_line error; fd=14 buf=:cmd=barrier_in


On Wednesday, May 9, 2018 at 11:29:39 AM UTC-7, Samy wrote:
>
> Hello everyone, I have 2 question please:
>
> So for now, the only way i can run a container with mpi in a cluster is as:
>
> Using the container:
> source /opt/intel/psxe_runtime/linux/bin/compilervars.sh intel64
> mpirun -hostfile nodelist -ppn 48 -np 192 singularity exec container.simg 
> /<containerpath>/$BIN -in <containerpath>/$workload arg arg arg arg 
> ...$workload.log
>
> Using the binary:
> source /opt/intel/psxe_runtime/linux/bin/compilervars.sh intel64
> mpirun -hostfile nodelist -ppn 48 -np 192 $BIN -in $workload arg arg arg 
> arg ...$workload.log
>
> *Q1: Do the environment variables in the %environment get executed as well 
> in this syntax or no? And what else get executed?*
> *Q2: any better way **than this** to run the container with mpi ? can i 
> encapsulate anything inside the container for mpi jobs? *
>
> *It just get very **ugly** and complicated using the exec command with 
> some applications i have.*
>
>
> Thank you, 
>

------=_Part_21423_936470973.1526922209213
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Thank Daniel for the suggestion.=C2=A0<div><br></div><div>=
I tried this already but unfortunately it didn&#39;t work even when putting=
 the arguments inside the container. I get the error bellow. Running mpi wi=
th the same binary without singularity works with no problem.=C2=A0</div><d=
iv><br></div><div>I&#39;m wondering if anyone had any ideas. I checked and =
all the files and the binaries are inside the container.</div><div><br></di=
v><div>The error i get:</div><div><br></div><div><font face=3D"courier new,=
 monospace">mpiexec.hydra -hostfile ./nodelist -n 4 -ppn 2 singularity run =
container.simg arg arg<br></font></div><div><font face=3D"courier new, mono=
space"><br></font></div><div><font face=3D"courier new, monospace">=C2=A0[c=
li_1]: write_line error; fd=3D16 buf=3D:cmd=3Dinit pmi_version=3D1 pmi_subv=
ersion=3D1</font><div><font face=3D"courier new, monospace">:</font></div><=
div><font face=3D"courier new, monospace">system msg for write_line failure=
 : Bad file descriptor</font></div><div><font face=3D"courier new, monospac=
e">[cli_1]: Unable to write to PMI_fd</font></div><div><font face=3D"courie=
r new, monospace">[cli_1]: write_line error; fd=3D16 buf=3D:cmd=3Dbarrier_i=
n</font></div><div><font face=3D"courier new, monospace">:</font></div><div=
><font face=3D"courier new, monospace">system msg for write_line failure : =
Bad file descriptor</font></div><div><font face=3D"courier new, monospace">=
[cli_1]: write_line error; fd=3D16 buf=3D:cmd=3Dget_ranks2hosts</font></div=
><div><font face=3D"courier new, monospace">:</font></div><div><font face=
=3D"courier new, monospace">system msg for write_line failure : Bad file de=
scriptor</font></div><div><font face=3D"courier new, monospace">[cli_1]: ex=
pecting cmd=3D&quot;put_ranks2hosts&quot;, got cmd=3D&quot;&quot;</font></d=
iv><div><font face=3D"courier new, monospace">Fatal error in PMPI_Init_thre=
ad: Other MPI error, error stack:</font></div><div><font face=3D"courier ne=
w, monospace">MPIR_Init_thread(805): fail failed</font></div><div><font fac=
e=3D"courier new, monospace">MPID_Init(1743)......: channel initialization =
failed</font></div><div><font face=3D"courier new, monospace">MPID_Init(214=
4)......: PMI_Init returned -1</font></div><div><font face=3D"courier new, =
monospace">[cli_1]: write_line error; fd=3D16 buf=3D:cmd=3Dabort exitcode=
=3D68204815</font></div><div><font face=3D"courier new, monospace">:</font>=
</div><div><font face=3D"courier new, monospace">system msg for write_line =
failure : Bad file descriptor</font></div><div><font face=3D"courier new, m=
onospace">all done!</font></div><div><font face=3D"courier new, monospace">=
[cli_0]: write_line error; fd=3D14 buf=3D:cmd=3Dinit pmi_version=3D1 pmi_su=
bversion=3D1</font></div><div><font face=3D"courier new, monospace">:</font=
></div><div><font face=3D"courier new, monospace">system msg for write_line=
 failure : Bad file descriptor</font></div><div><font face=3D"courier new, =
monospace">[cli_0]: Unable to write to PMI_fd</font></div><div><font face=
=3D"courier new, monospace">[cli_0]: write_line error; fd=3D14 buf=3D:cmd=
=3Dbarrier_in</font></div><div><font face=3D"courier new, monospace"><br></=
font></div><br>On Wednesday, May 9, 2018 at 11:29:39 AM UTC-7, Samy wrote:<=
blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8ex;bord=
er-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">Hello everyone=
, I have 2 question please:<div><br></div><div>So for now, the only way i c=
an run a container with mpi in a cluster is as:</div><div><br></div><div>Us=
ing the container:</div><div><div><font face=3D"courier new, monospace">sou=
rce /opt/intel/psxe_runtime/linux/<wbr>bin/compilervars.sh intel64</font></=
div><div><font face=3D"courier new, monospace">mpirun -hostfile nodelist -p=
pn 48 -np 192 singularity exec container.simg /&lt;containerpath&gt;/$BIN -=
in &lt;containerpath&gt;/$workload arg arg arg arg ...$workload.log</font><=
/div></div><div><font face=3D"courier new, monospace"><br></font></div><div=
><font face=3D"arial, sans-serif">Using the binary:</font></div><div><div><=
div><font face=3D"courier new, monospace">source /opt/intel/psxe_runtime/li=
nux/<wbr>bin/compilervars.sh intel64</font></div><div><font face=3D"courier=
 new, monospace">mpirun -hostfile nodelist -ppn 48 -np 192 $BIN -in $worklo=
ad arg arg arg arg ...$workload.log</font></div></div></div><div><font face=
=3D"courier new, monospace"><br></font></div><div><font face=3D"arial, sans=
-serif"><b>Q1: Do the environment=C2=A0variables in the %environment get ex=
ecuted as well in this syntax or no? And what else get executed?</b></font>=
</div><div><font face=3D"arial, sans-serif"><b>Q2: any better way=C2=A0</b>=
</font><b style=3D"font-family:arial,sans-serif">than this</b><b style=3D"f=
ont-family:arial,sans-serif">=C2=A0to run the container with mpi ? can i en=
capsulate anything inside the container for mpi jobs?=C2=A0</b></div><div><=
b style=3D"font-family:arial,sans-serif"><br></b></div><div><b style=3D"fon=
t-family:arial,sans-serif">It just get very </b><font face=3D"arial, sans-s=
erif"><b>ugly</b></font><b style=3D"font-family:arial,sans-serif">=C2=A0and=
 complicated using the exec command with some applications i have.</b></div=
><div><font face=3D"arial, sans-serif"><b><br></b></font></div><div><font f=
ace=3D"arial, sans-serif"><b><br></b></font></div><div><font face=3D"arial,=
 sans-serif">Thank you,=C2=A0</font></div></div></blockquote></div></div>
------=_Part_21423_936470973.1526922209213--

------=_Part_21422_748689590.1526922209213--
