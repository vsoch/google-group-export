X-Received: by 10.66.119.39 with SMTP id kr7mr27049115pab.18.1466527149904;
        Tue, 21 Jun 2016 09:39:09 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.130.202 with SMTP id m71ls690906ioi.68.gmail; Tue, 21 Jun
 2016 09:39:09 -0700 (PDT)
X-Received: by 10.98.68.84 with SMTP id r81mr29396569pfa.26.1466527149418;
        Tue, 21 Jun 2016 09:39:09 -0700 (PDT)
Return-Path: <gregw...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id g4si40943602pfj.47.2016.06.21.09.39.09
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 21 Jun 2016 09:39:09 -0700 (PDT)
Received-SPF: pass (google.com: domain of gregw...@gmail.com designates 209.85.214.169 as permitted sender) client-ip=209.85.214.169;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of gregw...@gmail.com designates 209.85.214.169 as permitted sender) smtp.mailfrom=gregw...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2BDAgCpbGlXhqnWVdFdgnCCIQaDNrI5hnsXAYV/AoEwBzwQAQEBAQEBARIBAQEICwsJIS+ESwEBAQMBEhEdAQ0OHgMBCwYFCw0qAgIhAQEOAwEFARwOBwQBHAQBh3MBAw8IBaNIgTE+MYs7gWqCWQWHQgoZJw1SgyMBAQEBBgEBAQEBARkCBhCKZIJDgU8RAYMdgloFjmmJXDSMMoF6jyOIC4YuEh6BDzWCGh6BdR4yB4kMgTUBAQE
X-IronPort-AV: E=Sophos;i="5.26,505,1459839600"; 
   d="scan'208,217";a="26995964"
Received: from mail-ob0-f169.google.com ([209.85.214.169])
  by fe4.lbl.gov with ESMTP; 21 Jun 2016 09:39:08 -0700
Received: by mail-ob0-f169.google.com with SMTP id xn17so30641399obc.0
        for <singu...@lbl.gov>; Tue, 21 Jun 2016 09:39:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=OO8AycSLuAiP5iv+R+1tXeC7UYuhuq7bXQ5MkK45OUo=;
        b=DQfl8Mn4OCdnBl51dV4hPmTnAU1OIL4tZ31j7uAMg8AmFvTLiCS4hvic82dPVu67PB
         5bjv2gEdUTEeUajyg0R83DSqC4D2bQOCsJ8KASZjeicbeOAzC8nnprz4X0q/3Hq2r7Uy
         Bux+kHN6xMg0uPlepd/6S06Z4scmYZnqjHaYIr+MMRlo3kb/8+iy5Ls31LpGuWl7mnYw
         21wdqaFO4bA1C/kXNVb62mZKitUPJhNCBcsrX48zRhqMXRiJ53PuWUA1s3vh655Vg5En
         SUHEEKRfGT3yiVZ1M64JhsUU5rUrzFxMLH9CbZlVzcgVuFft1OT+dvDgZhaLLJdI+jWB
         eJQQ==
X-Gm-Message-State: ALyK8tI8mNn/uTJrn5lM7WjeCENTeQ/Ez0Zm0wKlvQgQiStL09mfgocWtTD/mQiAz5A3HrThkdB6EK63PizJpg==
X-Received: by 10.157.35.20 with SMTP id j20mr14890171otb.24.1466527147872;
 Tue, 21 Jun 2016 09:39:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.52.170 with HTTP; Tue, 21 Jun 2016 09:39:07 -0700 (PDT)
In-Reply-To: <CAPqNE2WY31aOinCS12uqJD-wXQNUGBaEr6irHuSQMGGp=47SDQ@mail.gmail.com>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov> <244D668A-1EB7-412D-9303-E52F5412409C@open-mpi.org>
 <CAPqNE2WY31aOinCS12uqJD-wXQNUGBaEr6irHuSQMGGp=47SDQ@mail.gmail.com>
From: Greg Keller <gregw...@gmail.com>
Date: Tue, 21 Jun 2016 11:39:07 -0500
Message-ID: <CAHCZMOGATFSE2HmNoi=XocaxC6g=V2yeaiegi04RU07j792U0g@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a11354d2ab3d98c0535cc7588

--001a11354d2ab3d98c0535cc7588
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Any chance IntelMPI will, "just work"?

On Tue, Jun 21, 2016 at 10:46 AM, 'John Hearns' via singularity <
singu...@lbl.gov> wrote:

> > We=E2=80=99ll take care of the rest. Our initial studies showed zero pe=
rformance
> degradation by running inside Singularity, and the launch penalty is
> near-zero as well
>
> May I just say - I haz a happee. Lolz.
> Sorry - normal service will be resumed as soon as possible.  And yes I am
> a sad person when the thought of running MPI processes in containers make=
s
> me happy.
>
> On 21 June 2016 at 15:49, Ralph Castain <r...@open-mpi.org> wrote:
>
>> Singularity is fully supported by OMPI (and vice versa). If you grab a
>> copy of the OMPI master and build it
>> =E2=80=94with-singularity=3D<path-to-singularity> (or have the singulari=
ty path in
>> your default path), then all you have to do is use mpirun as you normall=
y
>> do, but provide the container as your =E2=80=9Capp=E2=80=9D.
>>
>> We=E2=80=99ll take care of the rest. Our initial studies showed zero per=
formance
>> degradation by running inside Singularity, and the launch penalty is
>> near-zero as well (and gets better when compared against dl_open=E2=80=
=99d dynamic
>> jobs running at scale). I=E2=80=99ll let Greg answer the question of how=
 to address
>> the running container.
>>
>>
>> On Jun 21, 2016, at 7:37 AM, Raimon Bosch <raimo...@gmail.com> wrote:
>>
>>
>>
>> Hi,
>>
>> We are trying to run experiments using singularity containers. The idea
>> is to run OpenMPI among several containers and check performance results=
.
>>
>> How can I communicate with another container? In docker this is clear
>> because every container gets an assigned IP and you can ping there, but
>> what is the situation in the case of singularity? Is it possible to assi=
gn
>> an IP to each container? Can I connect via ssh to them?
>>
>> Thanks in advance,
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a11354d2ab3d98c0535cc7588
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Any chance IntelMPI will, &quot;just work&quot;?</div><div=
 class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Jun 21, 2016 =
at 10:46 AM, &#39;John Hearns&#39; via singularity <span dir=3D"ltr">&lt;<a=
 href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>&gt=
;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 =
.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><span cl=
ass=3D""><span style=3D"font-size:12.8px">&gt; We=E2=80=99ll take care of t=
he rest. Our initial studies showed zero performance degradation by running=
 inside Singularity, and the launch penalty is near-zero as well</span><br>=
<div><span style=3D"font-size:12.8px"><br></span></div></span><div><span st=
yle=3D"font-size:12.8px">May I just say - I haz a happee. Lolz.</span></div=
><div><span style=3D"font-size:12.8px">Sorry - normal service will be resum=
ed as soon as possible.=C2=A0 And yes I am a sad person when the thought of=
 running MPI processes in containers makes me happy.</span><br></div></div>=
<div class=3D"HOEnZb"><div class=3D"h5"><div class=3D"gmail_extra"><br><div=
 class=3D"gmail_quote">On 21 June 2016 at 15:49, Ralph Castain <span dir=3D=
"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org" target=3D"_blank">r...@open-=
mpi.org</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D=
"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word">Singularity is fully supported by OMPI (and vice =
versa). If you grab a copy of the OMPI master and build it =E2=80=94with-si=
ngularity=3D&lt;path-to-singularity&gt; (or have the singularity path in yo=
ur default path), then all you have to do is use mpirun as you normally do,=
 but provide the container as your =E2=80=9Capp=E2=80=9D.<div><br></div><di=
v>We=E2=80=99ll take care of the rest. Our initial studies showed zero perf=
ormance degradation by running inside Singularity, and the launch penalty i=
s near-zero as well (and gets better when compared against dl_open=E2=80=99=
d dynamic jobs running at scale). I=E2=80=99ll let Greg answer the question=
 of how to address the running container.</div><div><div><div><br></div><di=
v><br><div><blockquote type=3D"cite"><div>On Jun 21, 2016, at 7:37 AM, Raim=
on Bosch &lt;<a href=3D"mailto:raimo...@gmail.com" target=3D"_blank">raimo.=
..@gmail.com</a>&gt; wrote:</div><br><div><div dir=3D"ltr"><br><br>Hi,<br><=
br>We are trying to run experiments using singularity containers. The idea =
is to run OpenMPI among several containers and check performance results. <=
br><br>How can I communicate with another container? In docker this is clea=
r because every container gets an assigned IP and you can ping there, but w=
hat is the situation in the case of singularity? Is it possible to assign a=
n IP to each container? Can I connect via ssh to them?<br><br>Thanks in adv=
ance,<br></div><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></blockquote></div><br></div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

--001a11354d2ab3d98c0535cc7588--
