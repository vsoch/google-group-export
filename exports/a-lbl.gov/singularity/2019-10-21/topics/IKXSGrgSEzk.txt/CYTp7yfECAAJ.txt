X-Received: by 10.107.20.67 with SMTP id 64mr1208569iou.61.1484758501222;
        Wed, 18 Jan 2017 08:55:01 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.37.197 with SMTP id l188ls612580iol.14.gmail; Wed, 18 Jan
 2017 08:55:00 -0800 (PST)
X-Received: by 10.99.113.82 with SMTP id b18mr5096629pgn.118.1484758500431;
        Wed, 18 Jan 2017 08:55:00 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id v65si733520pfi.204.2017.01.18.08.55.00
        for <singu...@lbl.gov>;
        Wed, 18 Jan 2017 08:55:00 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.161.200 as permitted sender) client-ip=209.85.161.200;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.161.200 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2HbAAARnX9Yf8ihVdFdHAEFAQsBGAEFAQsBgkVKAQEBAQF/eBEHg0qKCJICgmCNIYUrgUgbKCaFfAKBegc/GAEBAQEBAQEBAQEBAhABAQkLCwobMoIzGwkEPQoDLwEBAQEBAQEBAQEBAQEBARoCDTEDKQEBAQMBGAtbCwsLDSoCAiISAQUBHAYTFIhnCAWjTj+MAoIlijgBAQEHAQEBASQSiyeEJYMqgl4FiWaGP4scAYZeiwOBd1GEPYlokSkUHoEUDxCBSAgzXQWEHhyCAR01hkSCPQEBAQ
X-IronPort-AV: E=Sophos;i="5.33,249,1477983600"; 
   d="scan'208,217";a="61120356"
Received: from mail-yw0-f200.google.com ([209.85.161.200])
  by fe3.lbl.gov with ESMTP; 18 Jan 2017 08:54:59 -0800
Received: by mail-yw0-f200.google.com with SMTP id q71so19960746ywg.1
        for <singu...@lbl.gov>; Wed, 18 Jan 2017 08:54:59 -0800 (PST)
X-Gm-Message-State: AIkVDXKs8cY2i+1HnsPEibHAmBAVF+BtJ4XRjVZGLoyGG2eCHXq0GBAcf9jrsb+eirWEizBbBCasPS90DBR4StlSWbU4PfgdJZMDYFH78X34ZhuLCzezxScVOMBqbpk39lbpAsRen7qTEcEM3WkO8PtmnJw=
X-Received: by 10.37.71.5 with SMTP id u5mr2939382yba.59.1484758498947;
        Wed, 18 Jan 2017 08:54:58 -0800 (PST)
X-Received: by 10.37.71.5 with SMTP id u5mr2939370yba.59.1484758498737; Wed,
 18 Jan 2017 08:54:58 -0800 (PST)
MIME-Version: 1.0
Received: by 10.129.153.2 with HTTP; Wed, 18 Jan 2017 08:54:58 -0800 (PST)
In-Reply-To: <3589df38-4328-41e6-9c55-ad51fbc8271d@lbl.gov>
References: <8a569f24-bd38-4bf0-b025-5843a1525141@lbl.gov> <CAN7etTz5bYr-TTXJibNH+5qii9z_7pKpU_HXyR-SyNLiLRQk-A@mail.gmail.com>
 <3589df38-4328-41e6-9c55-ad51fbc8271d@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Wed, 18 Jan 2017 08:54:58 -0800
Message-ID: <CAN7etTxw0f+oX1nNOQPD20Dzi-hpM1gLgfujm29d+CABhqBV-w@mail.gmail.com>
Subject: Re: [Singularity] Survey: MPI Hybrid mode vs MPI native?
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a11423600e545d6054661469e

--001a11423600e545d6054661469e
Content-Type: text/plain; charset=UTF-8

On Wed, Jan 18, 2017 at 3:00 AM, 'Stefan Kombrink' via singularity <
singu...@lbl.gov> wrote:

> Hi Greg,
>
>  thanks for your reply.
>
> Am Mittwoch, 18. Januar 2017 04:51:46 UTC+1 schrieb Gregory M. Kurtzer:
>>
>>
>>
>> On Tue, Jan 10, 2017 at 12:25 AM, 'Stefan Kombrink' via singularity <
>> si...@lbl.gov> wrote:
>>
>>> Happy new year, dear Singularity-community,
>>>
>>>  I am asking those of you using the MPI Hybrid mode with OpenMPI 2 for
>>> sharing your experience.
>>> What does the hybrid approach do for you that cannot be done by having
>>> MPI within the container?
>>>
>>
>> A few that come up quick for me are:
>>
>> * if MPI is inside container jobs are typically limited to single node
>> runs
>> * resource managers can not communicate reasonably to MPI inside
>> container (e.g. no coupling of the MPI and RM)
>> * In this model, MPI would try to interface with ssh within the
>> container, and thus expect sshd running within the other containers (which
>> is opening pandora's box on traditional HPC)
>>
>
> That is how I currently run my experimental container jobs. I contain the
> originally installed MPI and bind mount /var/lib/torque.
> The most tricky part was to replace ssh inside the container with a
> wrapper which launches the passed command inside another instance of the
> origin container.
>

I am very curious what you did here. Can you elaborate on this?


> It works now with my IntelMPI and OpenMPI examples and I did not see
> significant changes in runtime.
> We use a single-user policy for our nodes i.e. only one user may submit
> jobs on a compute node at a time.
> I didn't see problems of wrong accounting of the scheduler / RM but then
> again this is just a very small testbed.
>

The RM has to communicate a fair amount of information down to the MPI, and
in the case of SLURM, srun could itself be used as part of the
multi-process/multi-node invocation. When using the hybrid model, the RM
can communicate directly with the host's MPI so a hybrid approach allows
MPI to run exactly as it was intended from the host's perspective. It does
break some aspects of container portability, but it also requires no
changes to the host or containers (as long as things "match up", but some
of that can be handled by site specific additional bootstrap overlays).

I am of course very interested in other approaches so please keep me
informed as you experiment on this further!


-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--001a11423600e545d6054661469e
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><div class=3D"gmail_extra"><br><div class=3D"gmail_quo=
te">On Wed, Jan 18, 2017 at 3:00 AM, &#39;Stefan Kombrink&#39; via singular=
ity <span dir=3D"ltr">&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_bl=
ank">singu...@lbl.gov</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_q=
uote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1e=
x"><div dir=3D"ltr">Hi Greg,<br><br>=C2=A0thanks for your reply.<br><br>Am =
Mittwoch, 18. Januar 2017 04:51:46 UTC+1 schrieb Gregory M. Kurtzer:<span c=
lass=3D""><blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0=
.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br><div=
><br><div class=3D"gmail_quote">On Tue, Jan 10, 2017 at 12:25 AM, &#39;Stef=
an Kombrink&#39; via singularity <span dir=3D"ltr">&lt;<a rel=3D"nofollow">=
si...@lbl.gov</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" st=
yle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div =
dir=3D"ltr">Happy new year, dear Singularity-community,<br><br>=C2=A0I am a=
sking those of you using the MPI Hybrid mode with OpenMPI 2 for sharing you=
r experience.<br>What does the hybrid approach do for you that cannot be do=
ne by having MPI within the container?<br></div></blockquote><div><br></div=
><div>A few that come up quick for me are:</div><div><br></div><div>* if MP=
I is inside container jobs are typically limited to single node runs</div><=
div>* resource managers can not communicate reasonably to MPI inside contai=
ner (e.g. no coupling of the MPI and RM)</div><div>* In this model, MPI wou=
ld try to interface with ssh within the container, and thus expect sshd run=
ning within the other containers (which is opening pandora&#39;s box on tra=
ditional HPC)</div></div></div></div></blockquote></span><div><br>That is h=
ow I currently run my experimental container jobs. I contain the originally=
 installed MPI and bind mount /var/lib/torque.<br>The most tricky part was =
to replace ssh inside the container with a wrapper which launches the passe=
d command inside another instance of the origin container.<br></div></div><=
/blockquote><div><br></div><div>I am very curious what you did here. Can yo=
u elaborate on this?</div><div>=C2=A0</div><blockquote class=3D"gmail_quote=
" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><=
div dir=3D"ltr"><div>It works now with my IntelMPI and OpenMPI examples and=
 I did not see significant changes in runtime.<br>We use a single-user poli=
cy for our nodes i.e. only one user may submit jobs on a compute node at a =
time. <br>I didn&#39;t see problems of wrong accounting of the scheduler / =
RM but then again this is just a very small testbed.</div></div></blockquot=
e><div><br></div><div>The RM has to communicate a fair amount of informatio=
n down to the MPI, and in the case of SLURM, srun could itself be used as p=
art of the multi-process/multi-node invocation. When using the hybrid model=
, the RM can communicate directly with the host&#39;s MPI so a hybrid appro=
ach allows MPI to run exactly as it was intended from the host&#39;s perspe=
ctive. It does break some aspects of container portability, but it also req=
uires no changes to the host or containers (as long as things &quot;match u=
p&quot;, but some of that can be handled by site specific additional bootst=
rap overlays).</div><div><br></div><div>I am of course very interested in o=
ther approaches so please keep me informed as you experiment on this furthe=
r!</div><div><br></div><div><br></div></div>-- <br><div class=3D"gmail_sign=
ature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D=
"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Systems Architect and Techno=
logy Developer</div><div>Lawrence Berkeley National Laboratory HPCS<br>Univ=
ersity of California Berkeley Research IT<br>Singularity Linux Containers=
=C2=A0(<a href=3D"http://singularity.lbl.gov/" target=3D"_blank">http://sin=
gularity.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=
=3D"http://warewulf.lbl.gov/" target=3D"_blank">http://warewulf.lbl.gov/</a=
>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" target=
=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<span style=3D"font-size=
:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurtzer" sty=
le=3D"font-size:12.8px" target=3D"_blank">https://twitter.com/gmkurtzer</a>=
</div></div></div></div></div></div></div></div></div></div></div>
</div></div>

--001a11423600e545d6054661469e--
