X-Received: by 10.99.122.25 with SMTP id v25mr1747053pgc.96.1488937881702;
        Tue, 07 Mar 2017 17:51:21 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.200.209 with SMTP id y200ls659827iof.26.gmail; Tue, 07 Mar
 2017 17:51:21 -0800 (PST)
X-Received: by 10.84.236.67 with SMTP id h3mr4826753pln.12.1488937880818;
        Tue, 07 Mar 2017 17:51:20 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id s3si1657935plj.94.2017.03.07.17.51.20
        for <singu...@lbl.gov>;
        Tue, 07 Mar 2017 17:51:20 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.217.198 as permitted sender) client-ip=209.85.217.198;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.217.198 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FaAAAhY79Yf8bZVdFdHAEBBAEBCgEBFwEBBAEBCgEBgkOBRHgSB4NQCIoMkUuCZIpgEIRHM4JpgUobKB8BDIFtgi+BWgKCJAc/GAEBAQEBAQEBAQEBAhABAQkLCwgmMYIzBAIDAR0EBEYmAQIuAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBEgINIgINAw8CGAEBAQMBIyswCwsLDSoCAiIPAwEFAQsRBggHBAEcBIh8WggFCaN6P4wDgiaKfgEBCAEBAQEBIxKKIoEJgxeBDxEBgyKCXwWJBQkEB2iGVRA8hFWGOQGGdYVnhVqBe1OBCY1IiEOJMRQegRUPEIEEMAgZCzhlBYF1AoIIOR2CBB81B4deR4FnAQEB
X-IronPort-AV: E=Sophos;i="5.36,261,1486454400"; 
   d="scan'208,217";a="66311231"
Received: from mail-ua0-f198.google.com ([209.85.217.198])
  by fe4.lbl.gov with ESMTP; 07 Mar 2017 17:51:00 -0800
Received: by mail-ua0-f198.google.com with SMTP id 72so33975268uaf.7
        for <singu...@lbl.gov>; Tue, 07 Mar 2017 17:51:00 -0800 (PST)
X-Gm-Message-State: AMke39nx2F7HkN1iXEULkk3ODtzrsqQY3Y4AeDY3C3qQJnmiltr52ODJv0ZtrtDaYeyvbVOZIy8WK54l9wAeq4hF4ruxrAYpdqaPcSFwOp1cQjChappE7nzmsumQ+CyjHKLFr81/v/EPxJavddDDSjphRDs=
X-Received: by 10.129.161.199 with SMTP id y190mr786720ywg.190.1488937859723;
        Tue, 07 Mar 2017 17:50:59 -0800 (PST)
X-Received: by 10.129.161.199 with SMTP id y190mr786714ywg.190.1488937859319;
 Tue, 07 Mar 2017 17:50:59 -0800 (PST)
MIME-Version: 1.0
Received: by 10.129.110.69 with HTTP; Tue, 7 Mar 2017 17:50:58 -0800 (PST)
In-Reply-To: <57a275c4-71cc-4dba-8c10-027862b820b7@lbl.gov>
References: <4a675b1d-5116-4702-a9e4-cc0024dcb436@lbl.gov> <CAN7etTzUMksoSB_H1ZgA3+-fFvM2pO+XzeD3E-pLqLiiL69c4w@mail.gmail.com>
 <5c39ff15-22d6-4b4f-8566-a670d2910e6a@lbl.gov> <2e6d299f-12d6-42ea-aacb-55f0d8924e36@lbl.gov>
 <CAN7etTyX+_RYBb8m4eNFORisfa848Bksm6s5vWZZjMgsdD1WHg@mail.gmail.com>
 <104158ea-3718-400a-a536-b0c64ccad1fa@lbl.gov> <CAN7etTx65za_GXuaYMvY44iYQc3erzYQWgYqSNAR_ZDiaCRqfA@mail.gmail.com>
 <57a275c4-71cc-4dba-8c10-027862b820b7@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Tue, 7 Mar 2017 17:50:58 -0800
Message-ID: <CAN7etTzBWHcBS_NzFOi0KCt3Zt47s9R0jvWi8KZPv+2ekCsbMQ@mail.gmail.com>
Subject: Re: [Singularity] job with multiple singularity calls (mixed
 MPI/serial) fails
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a114f853a32d190054a2e5c03

--001a114f853a32d190054a2e5c03
Content-Type: text/plain; charset=UTF-8

Hi Geert,

It seems there are many Singularity processes running, but initially I am
seeing various warnings/errors about not being able to bind mount /user
within the container. Is that where your home directory is located? What
happens if you create "/user" within the container?

Thanks!

On Tue, Mar 7, 2017 at 6:38 AM, Geert Jan BEX <geert...@uhasselt.be>
wrote:

> Dear Gregory,
>
> Sorry this took a while, I first wanted to get the user in production with
> a standard build.
>
> You'll find the debug output on Google Drive
> <https://drive.google.com/file/d/0BxlSeep1Z7B-aFdYMXZud2lQREE/view?usp=sharing>
> .
>
> Thanks in advance, best regards, -gjb-
>
> On Friday, March 3, 2017 at 4:10:22 AM UTC+1, Gregory M. Kurtzer wrote:
>>
>> Heya, I don't know off the top of my head. Can you provide some debugging
>> output (both from the MPI program and Singularity on the errors you are
>> getting)?
>>
>> Thanks!
>>
>> On Fri, Feb 24, 2017 at 5:52 AM, Geert Jan BEX <gee...@uhasselt.be>
>> wrote:
>>
>>> Dear Greg,
>>>
>>> Thanks for the suggestion.
>>>
>>> Some update: my initial impression that this problem had to do with
>>> multiple singularity calls was wrong.  I've reproduced this in isolation as
>>> well.
>>>
>>> When I try your suggestion with Singularity 2.2, I get the following
>>> error:
>>> Primary job  terminated normally, but 1 process returned
>>> a non-zero exit code.. Per user-direction, the job has been aborted.
>>> -------------------------------------------------------
>>> ------------------------------------------------------------
>>> --------------
>>> A system call failed during shared memory initialization that should
>>> not have.  It is likely that your MPI job will now either abort or
>>> experience performance degradation.
>>>
>>>   Local host:  r04i15n1
>>>   System call: open(2)
>>>   Error:       No such file or directory (errno 2)
>>>
>>> [r04i15n1:29725] 9 more processes have sent help message
>>> help-opal-shmem-mmap.txt / sys call fail
>>>
>>> This is the same type of error I get with Singularity 2.2.1, without
>>> setting SINGULARITY_NOSESSIONCLEANUP..  That would indeed imply that
>>> the session directory cleanup is fixed in 2.2.1, but now there seems to be
>>> an issue with the shared memory file of OpenMPI.
>>>
>>> Any ideas?
>>>
>>> Thanks in advance, best regards, Geert Jan Bex
>>>
>>>
>>>
>>> On Thursday, February 23, 2017 at 4:23:08 PM UTC+1, Gregory M. Kurtzer
>>> wrote:
>>>>
>>>> While I think you are hitting some locking bug in Singularity, that I
>>>> think/hope is already fixed in one of our development branches, I do think
>>>> there is a workaround you can use. Modify your MPI line to include:
>>>>
>>>> SINGULARITY_NOSESSIONCLEANUP=1
>>>>
>>>> I think you can do it like:
>>>>
>>>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" SINGULARITY_NOSE
>>>> SSIONCLEANUP=1 singularity --debug exec -B ${SIMDIR}:/wrf/run
>>>> $VSC_SCRATCH/containers/wrfrun-latest.img bash -c 'cd
>>>> /wrf/run/wrfwkdir; real.exe'
>>>>
>>>> Or you can make sure that the variable is set on every node running the
>>>> process. A better fix is included (or rather will be included) in the
>>>> latest version of Open MPI that will automatically set that variable, and
>>>> allow Open MPI to clean up the Singularity sessionfiles directly after the
>>>> job runs. Presently those optimizations exist in Open MPI's master branch.
>>>> So if you can build that yourself and test, that is also an option.
>>>>
>>>> Sorry for the delay, and hope that helps!
>>>>
>>>> Greg
>>>>
>>>> On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BEX <gee...@uhasselt.be>
>>>> wrote:
>>>>
>>>>> Dear Gregory,
>>>>>
>>>>> Any feedback you can give me?
>>>>>
>>>>> Thanks in advance, best regards, -gjb-
>>>>>
>>>>>
>>>>> On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:
>>>>>>
>>>>>> Dear Gregory,
>>>>>>
>>>>>> Thanks for looking into this.
>>>>>>
>>>>>> The /tmp is on the local disks of the compute nodes, so no NFS mounts
>>>>>> involved.
>>>>>>
>>>>>> The /tmp is also used for the MCA orte_tmpdir_base for mpirun.
>>>>>>
>>>>>> I've put the output of a run with --debug on Google Drive, you can
>>>>>> access it via:
>>>>>> https://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8
>>>>>> /view?usp=sharing
>>>>>>
>>>>>> The "offending" statement in the job script is:
>>>>>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" singularity --debug exec
>>>>>> -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img bash
>>>>>> -c 'cd /wrf/run/wrfwkdir; real.exe'
>>>>>>
>>>>>> So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH
>>>>>> and SIMDIR refer to a GPFS file system, and that is also where the working
>>>>>> directory for the job is on.
>>>>>>
>>>>>> Singularity's config file is unaltered from default.
>>>>>>
>>>>>> Thanks in advance, -gjb-
>>>>>>
>>>>>> On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. Kurtzer
>>>>>> wrote:
>>>>>>>
>>>>>>> Hi Geert,
>>>>>>>
>>>>>>> The directory is only supposed to be removed when the flock()s have
>>>>>>> all been released. I just did some fixups for this in one of the
>>>>>>> development branches, but 2.2* should be working in the same way. Is `/tmp`
>>>>>>> a local directory, or is it on some form of shared NFS? Does it have any
>>>>>>> mount options to disable locking?
>>>>>>>
>>>>>>> Can you also try with --debug enabled and send all of the output?
>>>>>>>
>>>>>>> Thanks!
>>>>>>>
>>>>>>> On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <
>>>>>>> gee...@uhasselt.be> wrote:
>>>>>>>
>>>>>>>> Dear,
>>>>>>>>
>>>>>>>> I've run into a problem with singularity, and out of ideas by now.
>>>>>>>>
>>>>>>>> When running a job that involves multiple calls to singularity
>>>>>>>> (same image each time), some of them serial, some MPI, the MPI ones
>>>>>>>> intermittently fail with:
>>>>>>>> ERROR  : Could not remove session directory
>>>>>>>> /tmp/.singularity-session-2530140.42.465829344: Device or resource
>>>>>>>> busy
>>>>>>>>
>>>>>>>> This is for OpenMPI 1.10.2 on host and in image.  The host OS is
>>>>>>>> CentOS 7, the image OS Ubuntu 16.04,  Singularity 2.2.  The job runs on 4
>>>>>>>> nodes, 27 processes each.
>>>>>>>>
>>>>>>>> I've tried several approaches (sleep between calls, setting a
>>>>>>>> session directory per singularity call, ...).
>>>>>>>>
>>>>>>>> Any ideas?
>>>>>>>>
>>>>>>>> Thanks in advance, Geert Jan Bex
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Gregory M. Kurtzer
>>>>>>> HPC Systems Architect and Technology Developer
>>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>>> University of California Berkeley Research IT
>>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>>>>> er.com/gmkurtzer
>>>>>>>
>>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> HPC Systems Architect and Technology Developer
>>>> Lawrence Berkeley National Laboratory HPCS
>>>> University of California Berkeley Research IT
>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>> er.com/gmkurtzer
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--001a114f853a32d190054a2e5c03
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Geert,<div><br></div><div>It seems there are many Singu=
larity processes running, but initially I am seeing various warnings/errors=
 about not being able to bind mount /user within the container. Is that whe=
re your home directory is located? What happens if you create &quot;/user&q=
uot; within the container?</div><div><br></div><div>Thanks!</div></div><div=
 class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Mar 7, 2017 a=
t 6:38 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a href=3D"mailto:geert...@u=
hasselt.be" target=3D"_blank">geert...@uhasselt.be</a>&gt;</span> wrote:<br=
><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1=
px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div><br></di=
v><div>Sorry this took a while, I first wanted to get the user in productio=
n with a standard build.</div><div><br></div><div>You&#39;ll find the debug=
 output on <a href=3D"https://drive.google.com/file/d/0BxlSeep1Z7B-aFdYMXZu=
d2lQREE/view?usp=3Dsharing" target=3D"_blank">Google Drive</a>.</div><div><=
br></div><div><span class=3D"">Thanks in advance, best regards, -gjb-<br><b=
r></span><span class=3D"">On Friday, March 3, 2017 at 4:10:22 AM UTC+1, Gre=
gory M. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" style=3D"mar=
gin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><span =
class=3D""><div dir=3D"ltr">Heya, I don&#39;t know off the top of my head. =
Can you provide some debugging output (both from the MPI program and Singul=
arity on the errors you are getting)?<div><br></div><div>Thanks!</div></div=
></span><div><div class=3D"h5"><div><br><div class=3D"gmail_quote">On Fri, =
Feb 24, 2017 at 5:52 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a rel=3D"nofo=
llow">gee...@uhasselt.be</a>&gt;</span> wrote:<br><blockquote class=3D"gmai=
l_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left=
:1ex"><div dir=3D"ltr">Dear Greg,<div><br></div><div>Thanks for the suggest=
ion.</div><div><br></div><div>Some update: my initial impression that this =
problem had to do with multiple singularity calls was wrong.=C2=A0 I&#39;ve=
 reproduced this in isolation as well.</div><div><br></div><div>When I try =
your suggestion with Singularity 2.2, I get the following error:</div><div>=
<div>Primary job =C2=A0terminated normally, but 1 process returned</div><di=
v>a non-zero exit code.. Per user-direction, the job has been aborted.</div=
><div>------------------------------<wbr>-------------------------</div><di=
v>------------------------------<wbr>------------------------------<wbr>---=
-----------</div><div>A system call failed during shared memory initializat=
ion that should</div><div>not have.=C2=A0 It is likely that your MPI job wi=
ll now either abort or</div><div>experience performance degradation.</div><=
div><br></div><div>=C2=A0 Local host: =C2=A0r04i15n1</div><div>=C2=A0 Syste=
m call: open(2)=C2=A0</div><div>=C2=A0 Error: =C2=A0 =C2=A0 =C2=A0 No such =
file or directory (errno 2)</div><div><div><br></div><div>[r04i15n1:29725] =
9 more processes have sent help message help-opal-shmem-mmap.txt / sys call=
 fail</div></div><div><br></div><div>This is the same type of error I get w=
ith Singularity 2.2.1, without setting=C2=A0<span style=3D"font-size:small"=
>SINGULARITY_</span><span style=3D"font-size:small">NOSESSIONC<wbr>LEANUP..=
=C2=A0 That would indeed imply that the session directory cleanup is fixed =
in 2.2.1, but now there seems to be an issue with the shared memory file of=
 OpenMPI.</span></div><div><span style=3D"font-size:small"><br></span></div=
><div><span style=3D"font-size:small">Any ideas?</span></div><div><span sty=
le=3D"font-size:small"><br></span></div><div><span style=3D"font-size:small=
">Thanks in advance, best regards, Geert Jan Bex</span></div><span><div><sp=
an style=3D"font-size:small"><br></span></div><div><span style=3D"font-size=
:small"><br></span></div><br>On Thursday, February 23, 2017 at 4:23:08 PM U=
TC+1, Gregory M. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1e=
x"><span><div dir=3D"ltr"><div>While I think you are hitting some locking b=
ug in Singularity, that I think/hope is already fixed in one of our develop=
ment branches, I do think there is a workaround you can use. Modify your MP=
I line to include:</div><div><br></div><div>SINGULARITY_NOSESSIONCLEANUP=3D=
1</div><div><br></div><div>I think you can do it like:</div><div><br></div>=
<div><div style=3D"font-size:12.8px">mpirun -mca orte_tmpdir_base &quot;${O=
RTE_TMP}&quot;=C2=A0<span style=3D"font-size:small">SINGULARITY_NOSE<wbr>SS=
IONCLEANUP=3D1=C2=A0</span><span style=3D"font-size:12.8px">singularity --d=
ebug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/</span><span style=
=3D"font-size:12.8px">wrfrun<wbr>-latest.img bash -c &#39;cd /wrf/run/wrfwk=
dir; real.exe&#39;</span></div></div><div><br></div><div>Or you can make su=
re that the variable is set on every node running the process. A better fix=
 is included (or rather will be included) in the latest version of Open MPI=
 that will automatically set that variable, and allow Open MPI to clean up =
the Singularity sessionfiles directly after the job runs. Presently those o=
ptimizations exist in Open MPI&#39;s master branch. So if you can build tha=
t yourself and test, that is also an option.</div><div><br></div>Sorry for =
the delay, and hope that helps!<div><br></div><div>Greg</div></div></span><=
div><div><div><br><div class=3D"gmail_quote">On Thu, Feb 23, 2017 at 5:11 A=
M, Geert Jan BEX <span dir=3D"ltr">&lt;<a rel=3D"nofollow">gee...@uhasselt.=
be</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"marg=
in:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
>Dear Gregory,<div><br></div><div>Any feedback you can give me?</div><div><=
br></div><div>Thanks in advance, best regards, -gjb-</div><div><div><div><b=
r><br>On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrot=
e:<blockquote class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;bor=
der-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<di=
v><br></div><div>Thanks for looking into this.</div><div><br></div><div>The=
 /tmp is on the local disks of the compute nodes, so no NFS mounts involved=
.</div><div><br></div><div>The /tmp is also used for the MCA orte_tmpdir_ba=
se for mpirun.</div><div><br></div><div>I&#39;ve put the output of a run wi=
th --debug on Google Drive, you can access it via:</div><div><a href=3D"htt=
ps://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=3Dsharin=
g" rel=3D"nofollow" target=3D"_blank">https://drive.google.com/file/<wbr>d/=
0BxlSeep1Z7B-MzRsd2pwV2dUTG8<wbr>/view?usp=3Dsharing</a></div><div><br></di=
v><div>The &quot;offending&quot; statement in the job script is:</div><div>=
<div>mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP}&quot; singularity --deb=
ug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun<wbr>-latest.im=
g bash -c &#39;cd /wrf/run/wrfwkdir; real.exe&#39;</div></div><div><br></di=
v><div>So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH =
and SIMDIR refer to a GPFS file system, and that is also where the working =
directory for the job is on.</div><div><br></div><div>Singularity&#39;s con=
fig file is unaltered from default.</div><div><br></div><div>Thanks in adva=
nce, -gjb-<br><br>On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory=
 M. Kurtzer wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0;margi=
n-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">=
Hi Geert,<div><br></div><div>The directory is only supposed to be removed w=
hen the flock()s have all been released. I just did some fixups for this in=
 one of the development branches, but 2.2* should be working in the same wa=
y. Is `/tmp` a local directory, or is it on some form of shared NFS? Does i=
t have any mount options to disable locking?</div><div><br></div><div>Can y=
ou also try with --debug enabled and send all of the output?=C2=A0</div><di=
v><br></div><div>Thanks!</div></div><div><br><div class=3D"gmail_quote">On =
Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a rel=3D=
"nofollow">gee...@uhasselt.be</a>&gt;</span> wrote:<br><blockquote class=3D=
"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding=
-left:1ex"><div dir=3D"ltr">Dear,<div><br></div><div>I&#39;ve run into a pr=
oblem with singularity, and out of ideas by now.</div><div><br></div><div>W=
hen running a job that involves multiple calls to singularity (same image e=
ach time), some of them serial, some MPI, the MPI ones intermittently fail =
with:</div><div><div>ERROR =C2=A0: Could not remove session directory /tmp/=
.singularity-session-2530<wbr>140.42.465829344: Device or resource busy</di=
v></div><div><br></div><div>This is for OpenMPI 1.10.2 on host and in image=
.=C2=A0 The host OS is CentOS 7, the image OS Ubuntu 16.04, =C2=A0Singulari=
ty 2.2.=C2=A0 The job runs on 4 nodes, 27 processes each.</div><div><br></d=
iv><div>I&#39;ve tried several approaches (sleep between calls, setting a s=
ession directory per singularity call, ...).</div><div><br></div><div>Any i=
deas?</div><div><br></div><div>Thanks in advance, Geert Jan Bex</div><span>=
<font color=3D"#888888"><div><br></div><div><br></div><div><br></div></font=
></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.go=
v/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warew=
ulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.g=
ov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" re=
l=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</blockquote></div></div></blockquote></div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.gov/=
</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewul=
f.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.gov=
/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=
=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</div></div></blockquote></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.gov/=
</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewul=
f.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.gov=
/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=
=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</div></div></blockquote></div></div><div class=3D"HOEnZb"><div class=3D"h5=
">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr">=
<div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Sys=
tems Architect and Technology Developer</div><div>Lawrence Berkeley Nationa=
l Laboratory HPCS<br>University of California Berkeley Research IT<br>Singu=
larity Linux Containers=C2=A0(<a href=3D"http://singularity.lbl.gov/" targe=
t=3D"_blank">http://singularity.lbl.gov/</a>)</div><div>Warewulf Cluster Ma=
nagement=C2=A0(<a href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http:=
//warewulf.lbl.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.c=
om/gmkurtzer" target=3D"_blank">https://github.com/gmkurtzer</a>,=C2=A0<spa=
n style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitte=
r.com/gmkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twitt=
er.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div><=
/div></div>
</div>

--001a114f853a32d190054a2e5c03--
