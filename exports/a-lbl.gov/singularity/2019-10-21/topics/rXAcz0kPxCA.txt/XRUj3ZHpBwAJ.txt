X-Received: by 10.157.27.226 with SMTP id v31mr66086586otv.4.1470407221980;
        Fri, 05 Aug 2016 07:27:01 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.34.81 with SMTP id o78ls1757816ito.6.gmail; Fri, 05 Aug
 2016 07:27:01 -0700 (PDT)
X-Received: by 10.66.48.133 with SMTP id l5mr135006576pan.151.1470407221557;
        Fri, 05 Aug 2016 07:27:01 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id hq8si20724725pac.110.2016.08.05.07.27.01
        for <singu...@lbl.gov>;
        Fri, 05 Aug 2016 07:27:01 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.69 as permitted sender) client-ip=74.125.82.69;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.69 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 1.6
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EwAQCtoaRXdEVSfUpchRcHtBSFB4F9hh0CgT0HOBQBAQEBAQEBAw8BCgsMCBcxhF8BBAESEVsLCws3AgIiEgEFARwGExoIiAcIBaB4gTI+MYs7kAQBCyUQiWSBA4dBgloFjhd1hGaFQwGPB0+Oc4wzgjgSHoEPHoRQHDKHZQEBAQ
X-IPAS-Result: A2EwAQCtoaRXdEVSfUpchRcHtBSFB4F9hh0CgT0HOBQBAQEBAQEBAw8BCgsMCBcxhF8BBAESEVsLCws3AgIiEgEFARwGExoIiAcIBaB4gTI+MYs7kAQBCyUQiWSBA4dBgloFjhd1hGaFQwGPB0+Oc4wzgjgSHoEPHoRQHDKHZQEBAQ
X-IronPort-AV: E=Sophos;i="5.28,474,1464678000"; 
   d="scan'208,217";a="31733764"
Received: from mail-wm0-f69.google.com ([74.125.82.69])
  by fe4.lbl.gov with ESMTP; 05 Aug 2016 07:27:00 -0700
Received: by mail-wm0-f69.google.com with SMTP id 1so18874777wmz.2
        for <singu...@lbl.gov>; Fri, 05 Aug 2016 07:27:00 -0700 (PDT)
X-Gm-Message-State: AEkoouuCg213Tx4OQTgm3YjeJ9uqnD5uOrPH7o/d3On4fwbE5wOZc9JkH/ffuFxgIUoOvDb48Jw5zcYFFstgwZNHE2ztrBc2JXgpPLRlQJf3faT0Kq7hof2fkbZd5VTA5sWF2YWI5UYq1jXwwx3rttB171g=
X-Received: by 10.25.219.198 with SMTP id t67mr21465787lfi.177.1470407219685;
        Fri, 05 Aug 2016 07:26:59 -0700 (PDT)
X-Received: by 10.25.219.198 with SMTP id t67mr21465781lfi.177.1470407219401;
 Fri, 05 Aug 2016 07:26:59 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.141 with HTTP; Fri, 5 Aug 2016 07:26:58 -0700 (PDT)
In-Reply-To: <5397fc9b-cc20-4813-8eaf-c806f8e27771@lbl.gov>
References: <5397fc9b-cc20-4813-8eaf-c806f8e27771@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Fri, 5 Aug 2016 07:26:58 -0700
Message-ID: <CAN7etTxYNne=gysZyGC535ygutWn_WBX_xqHf5RVE9kYN=mSNw@mail.gmail.com>
Subject: Re: [Singularity] Archiving research experiments using singularity
To: singularity <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=94eb2c0663b6fce605053953db76

--94eb2c0663b6fce605053953db76
Content-Type: text/plain; charset=UTF-8

Hi Steven,

Sorry for the delay, responses in-line.

On Tue, Aug 2, 2016 at 10:25 AM, 'Stefan Kombrink' via singularity <
singu...@lbl.gov> wrote:

> Hello there,
>
>  my name is Steven, and I am evaluating singularity in the context of the
> academic project "Citing&Archiving Research" (Citar).
> It's a federal project and amongst us (Ulm University, Germany) as of now
> two other HPC computing centres are involved.
> So far I succeeded in building and running the HPC example with OpenMPI
> trunk, and I was able to get gromacs working multi-node in singularity
> containers on our HPC cluster JUSTUS.
>

That is great news, glad to hear it is working for you!


>
> I'm glad I have found singularity it seems to meet our requirements better
> than docker - and would be glad to gather some more knowledge about it here.
>
> Now a few questions to you:
>
> -Does anyone of you use singularity for migration or archiving purposes
> already?
>

Yes and will be doing it more. We currently have one workflow running on a
15+ year old distribution of Linux inside of a Singularity container (RHL8
- before RHEL!).


> -Is trunk OpenMPI really a requirement or can I use older versions, too?
> What about IMPI?
>

Open MPI is not a requirement but it has been tuned for Singularity and
well tested. I can not speak definitively for IMPI, but people have told me
that it is working fine out of the box!


> -Can singularity be installed/run from inside docker containers?
>

I'm not sure... Perhaps if you run the container with the --privileged
option it will, but I can tell you without it does not. Someone here on
this list can probably test easily enough.


>
> that's for now,
> thanks and bye Steven
>

My pleasure.

-- 
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--94eb2c0663b6fce605053953db76
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Steven,<div><br></div><div>Sorry for the delay, respons=
es in-line.<br><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On=
 Tue, Aug 2, 2016 at 10:25 AM, &#39;Stefan Kombrink&#39; via singularity <s=
pan dir=3D"ltr">&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">s=
ingu...@lbl.gov</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" =
style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><di=
v dir=3D"ltr">Hello there,<br><br>=C2=A0my name is Steven, and I am evaluat=
ing singularity in the context of the academic project &quot;Citing&amp;Arc=
hiving Research&quot; (Citar).<br>It&#39;s a federal project and amongst us=
 (Ulm University, Germany) as of now two other HPC computing centres are in=
volved.<br>So far I succeeded in building and running the HPC example with =
OpenMPI trunk, and I was able to get gromacs working multi-node in singular=
ity containers on our HPC cluster JUSTUS.<br></div></blockquote><div><br></=
div><div>That is great news, glad to hear it is working for you!</div><div>=
=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>I&#39;m gla=
d I have found singularity it seems to meet our requirements better than do=
cker - and would be glad to gather some more knowledge about it here.<br><b=
r>Now a few questions to you: <br><br>-Does anyone of you use singularity f=
or migration or archiving purposes already?<br></div></blockquote><div><br>=
</div><div>Yes and will be doing it more. We currently have one workflow ru=
nning on a 15+ year old distribution of Linux inside of a Singularity conta=
iner (RHL8 - before RHEL!).</div><div>=C2=A0</div><blockquote class=3D"gmai=
l_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left=
:1ex"><div dir=3D"ltr">-Is trunk OpenMPI really a requirement or can I use =
older versions, too? What about IMPI?<br></div></blockquote><div><br></div>=
<div>Open MPI is not a requirement but it has been tuned for Singularity an=
d well tested. I can not speak definitively for IMPI, but people have told =
me that it is working fine out of the box!</div><div>=C2=A0</div><blockquot=
e class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc sol=
id;padding-left:1ex"><div dir=3D"ltr">-Can singularity be installed/run fro=
m inside docker containers?<br></div></blockquote><div><br></div><div>I&#39=
;m not sure... Perhaps if you run the container with the --privileged optio=
n it will, but I can tell you without it does not. Someone here on this lis=
t can probably test easily enough.</div><div>=C2=A0</div><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div dir=3D"ltr"><br>that&#39;s for now,<br>thanks and bye St=
even<br></div></blockquote><div><br></div><div>My pleasure.=C2=A0</div></di=
v><div><br></div>-- <br><div class=3D"gmail_signature" data-smartmail=3D"gm=
ail_signature"><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance=
 Computing Services (HPCS)<br>University of California<br>Lawrence Berkeley=
 National Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div><=
/div>
</div></div></div>

--94eb2c0663b6fce605053953db76--
