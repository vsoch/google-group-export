X-Received: by 2002:a65:4388:: with SMTP id m8-v6mr867482pgp.110.1527106139250;
        Wed, 23 May 2018 13:08:59 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:2903:: with SMTP id g3-v6ls7747867plb.9.gmail; Wed,
 23 May 2018 13:08:58 -0700 (PDT)
X-Received: by 2002:a17:902:684c:: with SMTP id f12-v6mr4317661pln.139.1527106138253;
        Wed, 23 May 2018 13:08:58 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1527106138; cv=none;
        d=google.com; s=arc-20160816;
        b=nNpp6k/FuQtTaqFFNuR6cW65JB3ZXZKOP+OofDGftShM6m1ue1lXuBRt0v/dE4vcqP
         RT4AF+yeaNAQu6eXFyZmm4wVFI5ee6OfexwaeUR+yTrTOi+YU5AAJkCXT/iSR66c5TcK
         6jRoxht00/ujRns8kj9QjwrJOO7VsseF1xzFure7TdmaAs1hDg34wnCegFVVEnZUYoJw
         XjwebL7m5eQkG0vIbVgKV+CHNvNO7vIyivOY7Hv0sgBZSr4zdwGPDxFE9Skt4G8aCZUo
         TfUrAP702qmIJCaBmshN+iJuNIiqI4camNNVyYMJZXd96e++nGSvo2QTzr9Ad9D06yl2
         A44Q==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature:arc-authentication-results;
        bh=r27yUaPrTZ5nibH/Tsk7cXOw//S5aS1bv3adDtxulkM=;
        b=Vurgc3GU+4VBZQBodnnLvFc1EeWPQPQ7TQF/NbxrUiRth8lsBf3Hv0Vb5VrS0I2Tyj
         iNTVtZqNZgWm5REoxJzu7DRJEkdSid5fEsOkcRdsUxsAIeqWmq1079DOItQqvrFtXU7d
         Z4HYyVlSpq9Kc7mjWVLYHPsPcbpubTg24s2s+xNbZdji2EIa4oHF3s/acwYwGtSfg1cU
         4DypHatPNt7WTgpAmcRDFrG5MRDiQuzE0jNyvPAIo2Ur5E1teUciF6IN8bgrohVQaHuF
         G+zpOGVRq/phD9cDDqPHqzGhdB9wI+6QtN7uUZXGIePE/+niZ2x61IYMcscUa1juUYp1
         nUfg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=BwrRsUlo;
       spf=pass (google.com: domain of georg...@gmail.com designates 209.85.216.173 as permitted sender) smtp.mailfrom=georg...@gmail.com
Return-Path: <georg...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id o78-v6si19715719pfa.54.2018.05.23.13.08.58
        for <singu...@lbl.gov>;
        Wed, 23 May 2018 13:08:58 -0700 (PDT)
Received-SPF: pass (google.com: domain of georg...@gmail.com designates 209.85.216.173 as permitted sender) client-ip=209.85.216.173;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=BwrRsUlo;
       spf=pass (google.com: domain of georg...@gmail.com designates 209.85.216.173 as permitted sender) smtp.mailfrom=georg...@gmail.com
X-Ironport-SBRS: 3.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2ELAwC9yQVbh63YVdFdHAEBAQQBAQoBA?=
 =?us-ascii?q?YMZgQ19KINxBoEdglCEF4xkgXmBD5M3FIEOAxgXIQMjAQwJgUmCL0YCgighNBg?=
 =?us-ascii?q?BAgEBAQEBAQIBAQIQAQEBCA0JCCgjDII1BQIDAh4FBEsrATABAQEBAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEUAgwBIEMBAQEDARoBCB0BDQ4EGgMBCwYDAgsGAwEBAQEVCwE?=
 =?us-ascii?q?JAgIhAQEOAwEFAQsJCA4HBAEHEwIEgwECKIE+AQMNCAUKj2KQAjyLBYF/BQEXg?=
 =?us-ascii?q?nAFg0oKGSYNVVeBdAIBBRKIJIFUP4EPgl8ugk9CAQECgRkKCAESAT8NEoJBglQ?=
 =?us-ascii?q?Ch2eJOIZ4FywJhWqFcIJ/gTmDb4dZiWVKhAmCODCBBBxsLnEzGiNQMYISghQMD?=
 =?us-ascii?q?gmDRYpuIzABD4s5R4FwAQE?=
X-IronPort-AV: E=Sophos;i="5.49,434,1520924400"; 
   d="scan'208,217";a="24164240"
Received: from mail-qt0-f173.google.com ([209.85.216.173])
  by fe4.lbl.gov with ESMTP; 23 May 2018 13:08:55 -0700
Received: by mail-qt0-f173.google.com with SMTP id m5-v6so29808008qti.1
        for <singu...@lbl.gov>; Wed, 23 May 2018 13:08:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=r27yUaPrTZ5nibH/Tsk7cXOw//S5aS1bv3adDtxulkM=;
        b=BwrRsUlo322tBzov38HGJAp7FXZAybjv5Iced63z5poRN/aTsqujiPoGvg4HNIqSAp
         fi4uS+PwjC1lztn2dkzuuVg/avrUVCurE0nMSgNDG/nqjl4g7qOSPwZwG7zFhImKw/xf
         Vj6Qh7PjFCzDsjMq5Lxv74wYe0cuVEe2BNMgIASwNDyHnPs/viOYTkRtcpS3LaHb1/r+
         I3iZHTRGCQYF7eGwdcFxcnb9BBWNmgoOEwW82hZh25++7SsfmK7eMMqdD2iGRChRHsnE
         UqfXJccYR21zbzzpcWpcSXQu1jzgp9rpqUP6yYRCmFMUOwwraK+v/1zHgQSiICKssqhB
         uXTA==
X-Gm-Message-State: ALKqPweC4PqxW5RDfdlJqi6mQ4eL/bwNBPpu6qpy8kq1QVIlHrKLwl/F
	D7JOdI0D39o78G3tkz7oyO0o22n+/8hGfeyG8zc=
X-Received: by 2002:a0c:e211:: with SMTP id q17-v6mr546687qvl.240.1527106134565;
 Wed, 23 May 2018 13:08:54 -0700 (PDT)
MIME-Version: 1.0
References: <167671ff-657d-4560-a35f-87091223fe29@lbl.gov> <B58197C146EC324AA00A2A07DC082602C2CBA0C3@XMAIL-MBX-BT1.AD.UCSD.EDU>
 <CAB6eJZ+6PDjs3POSQPNoLdaMys5d7iYDYcUeQFpMyzJ6DEP31w@mail.gmail.com>
 <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com>
 <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com>
 <CAB6eJZL+F7DwMcebA6DC+QWHqqsnnqEfJ2uqR2D-6bas6DmnYw@mail.gmail.com>
 <CA+Wz_FznUJ9kWSy7qneZsVEYqemqnm6eq0ASo+AoD49VNqU=zw@mail.gmail.com>
 <CAB6eJZLupe=P+5awRJjV0=V4uNWuPcwTevLELZpMsRR5uJVStA@mail.gmail.com> <CA+Wz_FyZdYe65GfUp2qApunch=dvV9HzAohStJPqAi4+1j2nUQ@mail.gmail.com>
In-Reply-To: <CA+Wz_FyZdYe65GfUp2qApunch=dvV9HzAohStJPqAi4+1j2nUQ@mail.gmail.com>
From: George Zaki <georg...@gmail.com>
Date: Wed, 23 May 2018 16:08:42 -0400
Message-ID: <CAB6eJZ+szuxb+L8xtOEyuFKTYG8Qvbj111tKpsKSqg8tK187qQ@mail.gmail.com>
Subject: Re: [Singularity] Running an mpi program with mvapitch
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000af6b3e056ce51a11"

--000000000000af6b3e056ce51a11
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi Victor,

No I did not explicitly install slurm is not installed in the container.
Does the native installation of mvapitch requires slurm to be installed?


Thanks and regards,
George.

On Wed, May 23, 2018 at 2:57 AM victor sv <vict...@gmail.com> wrote:

> Hi George,
>
> it's strange. The file that is not finding is "srun" not "mpi-pi.o". Is
> this something related with Slurm workload manager? is srun installed
> inside the container?
>
> Best,
> V=C3=ADctor
>
> 2018-05-21 17:35 GMT+02:00 George Zaki <georg...@gmail.com>:
>
>> Thanks Victor,
>>
>> Here is the results with the path of the executable:
>>
>> Singularity
>> swift-hypervisor-horovod-mvapich.simg:~/mpi-examples/mpi-example> mpiexe=
c
>> -n 1 ./mpi-pi.o
>>
>> [mpiexec@cn3112] HYDU_create_process (utils/launch/launch.c:75): execvp
>> error on file srun (No such file or directory)
>>
>>
>>
>> On Mon, May 21, 2018 at 9:18 AM victor sv <vict...@gmail.com> wrote:
>>
>>> Hi George,
>>>
>>> please check that you are calling the right program. Is the executable
>>> path in the PATH environment variable? If not you have the prepend the =
path
>>> to call the executable.
>>>
>>> Take a look to this:
>>>
>>>
>>> https://stackoverflow.com/questions/47472153/mpi-mpirun-execvp-error-no=
-such-file-or-directory
>>>
>>> BR,
>>> V=C3=ADctor.
>>>
>>> 2018-05-21 14:58 GMT+02:00 George Zaki <georg...@gmail.com>:
>>>
>>>> Hi Jason,
>>>>
>>>> I was not able to run the program even without singularity when I use
>>>> mvapitch. I am in contact with our system admin
>>>>
>>>> Here is the  output I got when I run mpiexec within singularity:
>>>>
>>>> mpiexec -n 1 mpi-pi.o
>>>>
>>>> [mpiexec@cn3137] HYDU_create_process (utils/launch/launch.c:75):
>>>> execvp error on file srun (No such file or directory)
>>>>
>>>> Best regards,
>>>>
>>>> George.
>>>>
>>>> On Mon, May 21, 2018 at 3:04 AM victor sv <vict...@gmail.com> wrote:
>>>>
>>>>> Hi George,
>>>>>
>>>>> not any experience with mvapitch. I think the compiler version has no
>>>>> effect here.
>>>>>
>>>>> Jason solution and is a good starting point to check if the container
>>>>> MPI works in a single node (not in several nodes).
>>>>>
>>>>> To run the hybrid MPI approach you should take into account that both
>>>>> version and vendor of MPI and PMI must match. Can you check if PMI
>>>>> libraries match?
>>>>>
>>>>> BR,
>>>>> V=C3=ADctor.
>>>>>
>>>>> 2018-05-18 20:14 GMT+02:00 Jason Stover <jason...@gmail.com>:
>>>>>
>>>>>> Hi George,
>>>>>>
>>>>>>   Can you run it from inside the container? For example:
>>>>>>
>>>>>>     singularity exec
>>>>>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n=
 1
>>>>>> mpi-pi.o
>>>>>>
>>>>>> -J
>>>>>>
>>>>>>
>>>>>> On Fri, May 18, 2018 at 12:56 PM, George Zaki <georg...@gmail.com>
>>>>>> wrote:
>>>>>> > Thanks Marty
>>>>>> >
>>>>>> > Below are the values I got, any obvious mismatch? I git this
>>>>>> working fine
>>>>>> > with OpenMPI.
>>>>>> >
>>>>>> > Here is also what I try to run:
>>>>>> >
>>>>>> > singularity exec
>>>>>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg
>>>>>> > mpicc mpi-pi.c -o  mpi-pi.o
>>>>>> >
>>>>>> > [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec
>>>>>> > /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o
>>>>>> >
>>>>>> > Then I kill after no response:
>>>>>> >
>>>>>> > ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested
>>>>>> >
>>>>>> > [mpiexec@cn2360] Press Ctrl-C again to force abort
>>>>>> >
>>>>>> > [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write
>>>>>> error (Bad
>>>>>> > file descriptor)
>>>>>> >
>>>>>> > [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal
>>>>>> (pm/pmiserv/pmiserv_cb.c:169):
>>>>>> > unable to write data to proxy
>>>>>> >
>>>>>> > [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable
>>>>>> to send
>>>>>> > signal downstream
>>>>>> >
>>>>>> > [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event
>>>>>> > (tools/demux/demux_poll.c:76): callback returned error status
>>>>>> >
>>>>>> > [mpiexec@cn2360] HYD_pmci_wait_for_completion
>>>>>> > (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event
>>>>>> >
>>>>>> > [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager
>>>>>> error
>>>>>> > waiting for completion
>>>>>> >
>>>>>> >
>>>>>> > Now about the versions:
>>>>>> >
>>>>>> > Singularity: Invoking an interactive shell within container...
>>>>>> >
>>>>>> >
>>>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> mpiexec
>>>>>> --version
>>>>>> >
>>>>>> > HYDRA build details:
>>>>>> >
>>>>>> >     Version:                                 3.1.4
>>>>>> >
>>>>>> >     Release Date:                            Wed Sep  7 14:33:43
>>>>>> EDT 2016
>>>>>> >
>>>>>> >     CC:                              gcc
>>>>>> >
>>>>>> >     CXX:                             g++
>>>>>> >
>>>>>> >     F77:                             gfortran
>>>>>> >
>>>>>> >     F90:                             gfortran
>>>>>> >
>>>>>> >     Configure options:
>>>>>>  '--disable-option-checking'
>>>>>> > '--prefix=3DNONE' '--cache-file=3D/dev/null' '--srcdir=3D.' 'CC=3D=
gcc'
>>>>>> 'CFLAGS=3D
>>>>>> > -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib
>>>>>> -Wl,-rpath,/lib
>>>>>> > -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad -libumad
>>>>>> -libverbs -ldl
>>>>>> > -lrt -lm -lpthread ' 'CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/incl=
ude
>>>>>> > -I/tmp/mvapich2-2.2/src/mpl/include
>>>>>> -I/tmp/mvapich2-2.2/src/openpa/src
>>>>>> > -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>>>>> > -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include
>>>>>> -I/include
>>>>>> > -I/include'
>>>>>> >
>>>>>> >     Process Manager:                         pmi
>>>>>> >
>>>>>> >     Launchers available:                     ssh rsh fork slurm ll
>>>>>> lsf sge
>>>>>> > manual persist
>>>>>> >
>>>>>> >     Topology libraries available:            hwloc
>>>>>> >
>>>>>> >     Resource management kernels available:   user slurm ll lsf sge
>>>>>> pbs
>>>>>> > cobalt
>>>>>> >
>>>>>> >     Checkpointing libraries available:
>>>>>> >
>>>>>> >     Demux engines available:                 poll select
>>>>>> >
>>>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> gcc --version
>>>>>> >
>>>>>> > gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>>>>> >
>>>>>> > Copyright (C) 2015 Free Software Foundation, Inc.
>>>>>> >
>>>>>> > This is free software; see the source for copying conditions.
>>>>>> There is NO
>>>>>> >
>>>>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>>>> PURPOSE.
>>>>>> >
>>>>>> >
>>>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> exit
>>>>>> >
>>>>>> > exit
>>>>>> >
>>>>>> >
>>>>>> > [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0
>>>>>> >
>>>>>> > [+] Loading mvapich2 2.2 for GCC 5.3.0
>>>>>> >
>>>>>> > [zakigf@cn2360 ~]$ gcc --version
>>>>>> >
>>>>>> > gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
>>>>>> >
>>>>>> > Copyright (C) 2010 Free Software Foundation, Inc.
>>>>>> >
>>>>>> > This is free software; see the source for copying conditions.
>>>>>> There is NO
>>>>>> >
>>>>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>>>> PURPOSE.
>>>>>> >
>>>>>> >
>>>>>> > [zakigf@cn2360 ~]$ mpiexec --version
>>>>>> >
>>>>>> > HYDRA build details:
>>>>>> >
>>>>>> >     Version:                                 3.1.4
>>>>>> >
>>>>>> >     Release Date:                            Wed Sep  7 14:33:43
>>>>>> EDT 2016
>>>>>> >
>>>>>> >     CC:                              /usr/local/GCC/5.3.0/bin/gcc
>>>>>> >
>>>>>> >     CXX:                             /usr/local/GCC/5.3.0/bin/g++
>>>>>> >
>>>>>> >     F77:
>>>>>>  /usr/local/GCC/5.3.0/bin/gfortran
>>>>>> >
>>>>>> >     F90:
>>>>>>  /usr/local/GCC/5.3.0/bin/gfortran
>>>>>> >
>>>>>> >     Configure options:
>>>>>>  '--disable-option-checking'
>>>>>> > '--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0'
>>>>>> > '--with-slurm-lib=3D/usr/local/slurm/lib'
>>>>>> > '--with-slurm-include=3D/usr/local/slurm/include'
>>>>>> '--enable-debug=3Dnone'
>>>>>> > '--enable-timing=3Druntime' 'CC=3D/usr/local/GCC/5.3.0/bin/gcc'
>>>>>> > 'CXX=3D/usr/local/GCC/5.3.0/bin/g++'
>>>>>> 'FC=3D/usr/local/GCC/5.3.0/bin/gfortran'
>>>>>> > 'F77=3D/usr/local/GCC/5.3.0/bin/gfortran' '--cache-file=3D/dev/nul=
l'
>>>>>> > '--srcdir=3D.' 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/=
lib
>>>>>> -L/lib
>>>>>> > -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
>>>>>> 'LIBS=3D-libmad
>>>>>> > -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src
>>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include
>>>>>> -I/include
>>>>>> > -I/include -I/include -I/include'
>>>>>> >
>>>>>> >     Process Manager:                         pmi
>>>>>> >
>>>>>> >     Launchers available:                     ssh rsh fork slurm ll
>>>>>> lsf sge
>>>>>> > manual persist
>>>>>> >
>>>>>> >     Topology libraries available:            hwloc
>>>>>> >
>>>>>> >     Resource management kernels available:   user slurm ll lsf sge
>>>>>> pbs
>>>>>> > cobalt
>>>>>> >
>>>>>> >     Checkpointing libraries available:
>>>>>> >
>>>>>> >     Demux engines available:                 poll select
>>>>>> >
>>>>>> >
>>>>>> > On Fri, May 18, 2018 at 1:31 PM Kandes, Martin <mka...@sdsc.edu>
>>>>>> wrote:
>>>>>> >>
>>>>>> >> Hi George,
>>>>>> >>
>>>>>> >> I run with different gcc compiler versions inside and outside my
>>>>>> MPI
>>>>>> >> containers. So I would be surprised if that is the issue here. I'=
m
>>>>>> not sure
>>>>>> >> I have a good recommendation of where to start debugging your
>>>>>> problem. But I
>>>>>> >> might start by double checking the MPI versions match inside and
>>>>>> outside the
>>>>>> >> container. e.g. see [1].
>>>>>> >>
>>>>>> >> Marty
>>>>>> >>
>>>>>> >> [1]
>>>>>> >>
>>>>>> >> [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1
>>>>>> >> --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin=
/bash
>>>>>> >> srun: job 16364303 queued and waiting for resources
>>>>>> >> srun: job 16364303 has been allocated resources
>>>>>> >> [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/
>>>>>> >> [mkandes@comet-14-06 16364303]$ cp
>>>>>> >>
>>>>>> /oasis/scratch/comet/mkandes/temp_project/singularity/images/ubuntu-=
mvapich2.img
>>>>>> >> ./
>>>>>> >> [mkandes@comet-14-06 16364303]$ module purge
>>>>>> >> [mkandes@comet-14-06 16364303]$ module load gnu
>>>>>> >> [mkandes@comet-14-06 16364303]$ module load mvapich2_ib
>>>>>> >> [mkandes@comet-14-06 16364303]$ module list
>>>>>> >> Currently Loaded Modulefiles:
>>>>>> >>   1) gnu/4.9.2         2) mvapich2_ib/2.1
>>>>>> >> [mkandes@comet-14-06 16364303]$ module load singularity
>>>>>> >> [mkandes@comet-14-06 16364303]$ gcc --version
>>>>>> >> gcc (GCC) 4.9.2
>>>>>> >> Copyright (C) 2014 Free Software Foundation, Inc.
>>>>>> >> This is free software; see the source for copying conditions.
>>>>>> There is NO
>>>>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULA=
R
>>>>>> >> PURPOSE.
>>>>>> >>
>>>>>> >> [mkandes@comet-14-06 16364303]$ mpirun --version
>>>>>> >> HYDRA build details:
>>>>>> >>     Version:                                 3.1.4
>>>>>> >>     Release Date:                            Thu Apr  2 17:15:15
>>>>>> EDT 2015
>>>>>> >>     CC:                              gcc  -fPIC -O3
>>>>>> >>     CXX:                             g++  -fPIC -O3
>>>>>> >>     F77:                             gfortran -fPIC -O3
>>>>>> >>     F90:                             gfortran -fPIC -O3
>>>>>> >>     Configure options:
>>>>>>  '--disable-option-checking'
>>>>>> >> '--prefix=3D/opt/mvapich2/gnu/ib' '--enable-shared'
>>>>>> '--enable-sharedlibs=3Dgcc'
>>>>>> >> '--with-hwloc' '--enable-f77' '--enable-fc' '--enable-hybrid'
>>>>>> >> '--with-ib-include=3D/usr/include/infiniband'
>>>>>> '--with-ib-libpath=3D/usr/lib64'
>>>>>> >> '--enable-fast=3DO3'
>>>>>> >>
>>>>>> '--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_=
gnu_ib-2.1/../..//cache/build-limic'
>>>>>> >> '--with-slurm=3D/usr/lib64/slurm' '--with-file-system=3Dlustre'
>>>>>> 'CC=3Dgcc'
>>>>>> >> 'CFLAGS=3D-fPIC -O3 -O3' 'CXX=3Dg++' 'CXXFLAGS=3D-fPIC -O3 -O3'
>>>>>> 'FC=3Dgfortran'
>>>>>> >> 'FCFLAGS=3D-fPIC -O3 -O3' 'F77=3Dgfortran' 'FFLAGS=3D-L/usr/lib64=
 -L/lib
>>>>>> -L/lib
>>>>>> >> -fPIC -O3 -O3' '--cache-file=3D/dev/null' '--srcdir=3D.'
>>>>>> 'LDFLAGS=3D-L/usr/lib64
>>>>>> >> -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib
>>>>>> -L/usr/lib64
>>>>>> >>
>>>>>> -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/../.=
.//cache/build-limic/lib
>>>>>> >> -L/lib -L/lib' 'LIBS=3D-libmad -lrdmacm -libumad -libverbs -ldl -=
lrt
>>>>>> -llimic2
>>>>>> >> -lm -lpthread ' 'CPPFLAGS=3D-I/usr/include/infiniband
>>>>>> >>
>>>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvap=
ich2-2.1/src/mpl/include
>>>>>> >>
>>>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvap=
ich2-2.1/src/mpl/include
>>>>>> >>
>>>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvap=
ich2-2.1/src/openpa/src
>>>>>> >>
>>>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvap=
ich2-2.1/src/openpa/src
>>>>>> >> -D_REENTRANT
>>>>>> >>
>>>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mvap=
ich2-2.1/src/mpi/romio/include
>>>>>> >> -I/include -I/include -I/usr/include/infiniband
>>>>>> >>
>>>>>> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/../.=
.//cache/build-limic/include
>>>>>> >> -I/include -I/include'
>>>>>> >>     Process Manager:                         pmi
>>>>>> >>     Launchers available:                     ssh rsh fork slurm l=
l
>>>>>> lsf sge
>>>>>> >> manual persist
>>>>>> >>     Topology libraries available:            hwloc
>>>>>> >>     Resource management kernels available:   user slurm ll lsf sg=
e
>>>>>> pbs
>>>>>> >> cobalt
>>>>>> >>     Checkpointing libraries available:
>>>>>> >>     Demux engines available:                 poll select
>>>>>> >> [mkandes@comet-14-06 16364303]$ singularity shell
>>>>>> ubuntu-mvapich2.img
>>>>>> >> Singularity: Invoking an interactive shell within container...
>>>>>> >>
>>>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> gcc
>>>>>> --version
>>>>>> >> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>>>>> >> Copyright (C) 2015 Free Software Foundation, Inc.
>>>>>> >> This is free software; see the source for copying conditions.
>>>>>> There is NO
>>>>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULA=
R
>>>>>> >> PURPOSE.
>>>>>> >>
>>>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> mpirun
>>>>>> >> --version
>>>>>> >> HYDRA build details:
>>>>>> >>     Version:                                 3.1.4
>>>>>> >>     Release Date:                            Thu Apr  2 17:15:15
>>>>>> EDT 2015
>>>>>> >>     CC:                              gcc
>>>>>> >>     CXX:                             g++
>>>>>> >>     F77:                             gfortran
>>>>>> >>     F90:                             gfortran
>>>>>> >>     Configure options:
>>>>>>  '--disable-option-checking'
>>>>>> >> '--prefix=3D/opt/mvapich2' '--cache-file=3D/dev/null' '--srcdir=
=3D.'
>>>>>> 'CC=3Dgcc'
>>>>>> >> 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/=
lib
>>>>>> >> -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
>>>>>> 'LIBS=3D-libmad -lrdmacm
>>>>>> >> -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>>>>> >> -I/tmp/mvapich2-2.1/src/mpl/include
>>>>>> -I/tmp/mvapich2-2.1/src/mpl/include
>>>>>> >> -I/tmp/mvapich2-2.1/src/openpa/src
>>>>>> -I/tmp/mvapich2-2.1/src/openpa/src
>>>>>> >> -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include
>>>>>> -I/include
>>>>>> >> -I/include -I/include'
>>>>>> >>     Process Manager:                         pmi
>>>>>> >>     Launchers available:                     ssh rsh fork slurm l=
l
>>>>>> lsf sge
>>>>>> >> manual persist
>>>>>> >>     Topology libraries available:            hwloc
>>>>>> >>     Resource management kernels available:   user slurm ll lsf sg=
e
>>>>>> pbs
>>>>>> >> cobalt
>>>>>> >>     Checkpointing libraries available:
>>>>>> >>     Demux engines available:                 poll select
>>>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303>
>>>>>> >>
>>>>>> >>
>>>>>> >> ________________________________
>>>>>> >> From: George Zaki [georg...@gmail.com]
>>>>>> >> Sent: Friday, May 18, 2018 6:48 AM
>>>>>> >> To: singularity
>>>>>> >> Subject: [Singularity] Running an mpi program with mvapitch
>>>>>> >>
>>>>>> >> Hi singularity team,
>>>>>> >>
>>>>>> >>
>>>>>> >> I would like to run an MPI program in a singularity container. Th=
e
>>>>>> program
>>>>>> >> is compiled using mvapicth2.2 using a gcc version 5.4.
>>>>>> >>
>>>>>> >>
>>>>>> >> I can see that my cluster has a compiled version of mvapitch2.2
>>>>>> with gcc
>>>>>> >> 5.3
>>>>>> >>
>>>>>> >>
>>>>>> >> When I run:
>>>>>> >>
>>>>>> >>
>>>>>> >> mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o
>>>>>> >>
>>>>>> >> the call does not return.
>>>>>> >>
>>>>>> >>
>>>>>> >>
>>>>>> >> Does the gcc version has to be exactly the same? I tried the
>>>>>> switch the
>>>>>> >> compiler in this image:
>>>>>> >>
>>>>>> >>
>>>>>> >> BootStrap: docker
>>>>>> >> From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04
>>>>>> >>
>>>>>> >>
>>>>>> >> However when gcc 5.3 is used the mvapitch does not build correctl=
y.
>>>>>> >>
>>>>>> >>
>>>>>> >> If that's the problem, Is there a preferred method of switching g=
cc
>>>>>> >> version in this container singularity container?
>>>>>> >>
>>>>>> >>
>>>>>> >> Thanks,
>>>>>> >> George
>>>>>> >>
>>>>>> >> --
>>>>>> >> You received this message because you are subscribed to the Googl=
e
>>>>>> Groups
>>>>>> >> "singularity" group.
>>>>>> >> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an
>>>>>> >> email to singu...@lbl.gov.
>>>>>> >>
>>>>>> >> --
>>>>>> >> You received this message because you are subscribed to a topic i=
n
>>>>>> the
>>>>>> >> Google Groups "singularity" group.
>>>>>> >> To unsubscribe from this topic, visit
>>>>>> >>
>>>>>> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/=
unsubscribe
>>>>>> .
>>>>>> >> To unsubscribe from this group and all its topics, send an email =
to
>>>>>> >> singu...@lbl.gov.
>>>>>> >
>>>>>> > --
>>>>>> > You received this message because you are subscribed to the Google
>>>>>> Groups
>>>>>> > "singularity" group.
>>>>>> > To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an
>>>>>> > email to singu...@lbl.gov.
>>>>>>
>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to a topic in th=
e
>>>>> Google Groups "singularity" group.
>>>>> To unsubscribe from this topic, visit
>>>>> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/u=
nsubscribe
>>>>> .
>>>>> To unsubscribe from this group and all its topics, send an email to
>>>>> singu...@lbl.gov.
>>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>> --
>>> You received this message because you are subscribed to a topic in the
>>> Google Groups "singularity" group.
>>> To unsubscribe from this topic, visit
>>> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/uns=
ubscribe
>>> .
>>> To unsubscribe from this group and all its topics, send an email to
>>> singu...@lbl.gov.
>>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
> --
> You received this message because you are subscribed to a topic in the
> Google Groups "singularity" group.
> To unsubscribe from this topic, visit
> https://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsub=
scribe
> .
> To unsubscribe from this group and all its topics, send an email to
> singu...@lbl.gov.
>

--000000000000af6b3e056ce51a11
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Victor,<div><br></div><div>No I did not explicitly inst=
all slurm is not installed in the container. Does the native installation o=
f mvapitch requires slurm to be installed?</div><div><br></div><div><br></d=
iv><div>Thanks and regards,</div><div>George.</div><div dir=3D"ltr"><br><di=
v class=3D"gmail_quote"><div dir=3D"ltr">On Wed, May 23, 2018 at 2:57 AM vi=
ctor sv &lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@=
gmail.com</a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" style=
=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr">Hi George,<div><br></div><div>it&#39;s strange. The file that is n=
ot finding is &quot;srun&quot; not &quot;mpi-pi.o&quot;. Is this something =
related with Slurm workload manager? is srun installed inside the container=
?</div><div><br></div><div>Best,</div><div>V=C3=ADctor</div></div><div clas=
s=3D"gmail_extra"><br><div class=3D"gmail_quote">2018-05-21 17:35 GMT+02:00=
 George Zaki <span dir=3D"ltr">&lt;<a href=3D"mailto:georg...@gmail.com" ta=
rget=3D"_blank">georg...@gmail.com</a>&gt;</span>:<br><blockquote class=3D"=
gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-=
left:1ex"><div dir=3D"ltr">Thanks Victor,=C2=A0<div><br></div><div>Here is =
the results with the path of the executable:</div><div><br></div><div>





<p class=3D"m_6833311993410565273m_4731593095917203810m_6837501669649334982=
m_351347644437966094inbox-inbox-p1"><span class=3D"m_6833311993410565273m_4=
731593095917203810m_6837501669649334982m_351347644437966094inbox-inbox-s1">=
Singularity swift-hypervisor-horovod-mvapich.simg:~/mpi-examples/mpi-exampl=
e&gt; mpiexec -n 1 ./mpi-pi.o<span class=3D"m_6833311993410565273m_47315930=
95917203810m_6837501669649334982m_351347644437966094inbox-inbox-Apple-conve=
rted-space">=C2=A0</span></span></p>
<p class=3D"m_6833311993410565273m_4731593095917203810m_6837501669649334982=
m_351347644437966094inbox-inbox-p1"><span class=3D"m_6833311993410565273m_4=
731593095917203810m_6837501669649334982m_351347644437966094inbox-inbox-s1">=
[mpiexec@cn3112] HYDU_create_process (utils/launch/launch.c:75): execvp err=
or on file srun (No such file or directory)</span></p><p class=3D"m_6833311=
993410565273m_4731593095917203810m_6837501669649334982m_351347644437966094i=
nbox-inbox-p1"><span class=3D"m_6833311993410565273m_4731593095917203810m_6=
837501669649334982m_351347644437966094inbox-inbox-s1"><br></span></p></div>=
</div><div class=3D"m_6833311993410565273m_4731593095917203810m_68375016696=
49334982HOEnZb"><div class=3D"m_6833311993410565273m_4731593095917203810m_6=
837501669649334982h5"><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Mo=
n, May 21, 2018 at 9:18 AM victor sv &lt;<a href=3D"mailto:vict...@gmail.co=
m" target=3D"_blank">vict...@gmail.com</a>&gt; wrote:<br></div><blockquote =
class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid=
;padding-left:1ex"><div dir=3D"ltr">Hi George,=C2=A0<div><br></div><div>ple=
ase check that you are calling the right program. Is the executable path in=
 the PATH environment variable? If not you have the prepend the path to cal=
l the executable.</div><div><br></div><div>Take a look to this:</div><div><=
br></div><div><a href=3D"https://stackoverflow.com/questions/47472153/mpi-m=
pirun-execvp-error-no-such-file-or-directory" target=3D"_blank">https://sta=
ckoverflow.com/questions/47472153/mpi-mpirun-execvp-error-no-such-file-or-d=
irectory</a><br></div><div><br></div><div>BR,</div><div>V=C3=ADctor.</div><=
/div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">2018-05-21 1=
4:58 GMT+02:00 George Zaki <span dir=3D"ltr">&lt;<a href=3D"mailto:georg...=
@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt;</span>:<br><blockq=
uote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc =
solid;padding-left:1ex"><div dir=3D"ltr">Hi Jason,=C2=A0<div><br></div><div=
>I was not able to run the program even without singularity when I use mvap=
itch. I am in contact with our system admin</div><div><br></div><div>Here i=
s the=C2=A0 output I got when I run mpiexec within singularity:=C2=A0</div>=
<div><br></div><div>mpiexec -n 1 mpi-pi.o<span class=3D"m_68333119934105652=
73m_4731593095917203810m_6837501669649334982m_351347644437966094m_754195818=
1832643763m_-6547978355576408365inbox-inbox-Apple-converted-space">=C2=A0</=
span><br></div><div>
<p class=3D"m_6833311993410565273m_4731593095917203810m_6837501669649334982=
m_351347644437966094m_7541958181832643763m_-6547978355576408365inbox-inbox-=
p1"><span class=3D"m_6833311993410565273m_4731593095917203810m_683750166964=
9334982m_351347644437966094m_7541958181832643763m_-6547978355576408365inbox=
-inbox-s1">[mpiexec@cn3137] HYDU_create_process (utils/launch/launch.c:75):=
 execvp error on file srun (No such file or directory)</span></p><p class=
=3D"m_6833311993410565273m_4731593095917203810m_6837501669649334982m_351347=
644437966094m_7541958181832643763m_-6547978355576408365inbox-inbox-p1">Best=
 regards,</p><p class=3D"m_6833311993410565273m_4731593095917203810m_683750=
1669649334982m_351347644437966094m_7541958181832643763m_-654797835557640836=
5inbox-inbox-p1">George.</p></div></div><div class=3D"m_6833311993410565273=
m_4731593095917203810m_6837501669649334982m_351347644437966094m_75419581818=
32643763HOEnZb"><div class=3D"m_6833311993410565273m_4731593095917203810m_6=
837501669649334982m_351347644437966094m_7541958181832643763h5"><br><div cla=
ss=3D"gmail_quote"><div dir=3D"ltr">On Mon, May 21, 2018 at 3:04 AM victor =
sv &lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@gmail=
.com</a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" style=3D"mar=
gin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr=
"><div><div><div><div>Hi George,<br><br></div>not any experience with mvapi=
tch. I think the compiler version has no effect here. <br><br>Jason solutio=
n and is a good starting point to check if the container MPI works in a sin=
gle node (not in several nodes).<br><br></div>To run the hybrid MPI approac=
h you should take into account that both version and vendor of MPI and PMI =
must match. Can you check if PMI libraries match?<br><br></div>BR,<br></div=
>V=C3=ADctor.<br></div><div class=3D"gmail_extra"><br><div class=3D"gmail_q=
uote">2018-05-18 20:14 GMT+02:00 Jason Stover <span dir=3D"ltr">&lt;<a href=
=3D"mailto:jason...@gmail.com" target=3D"_blank">jason...@gmail.com</a>&gt;=
</span>:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex">Hi George,<br>
<br>
=C2=A0 Can you run it from inside the container? For example:<br>
<br>
=C2=A0 =C2=A0 singularity exec<br>
/data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n 1<br>
mpi-pi.o<br>
<span class=3D"m_6833311993410565273m_4731593095917203810m_6837501669649334=
982m_351347644437966094m_7541958181832643763m_-6547978355576408365m_-659149=
3537311087044HOEnZb"><font color=3D"#888888"><br>
-J<br>
</font></span><div class=3D"m_6833311993410565273m_4731593095917203810m_683=
7501669649334982m_351347644437966094m_7541958181832643763m_-654797835557640=
8365m_-6591493537311087044HOEnZb"><div class=3D"m_6833311993410565273m_4731=
593095917203810m_6837501669649334982m_351347644437966094m_75419581818326437=
63m_-6547978355576408365m_-6591493537311087044h5"><br>
<br>
On Fri, May 18, 2018 at 12:56 PM, George Zaki &lt;<a href=3D"mailto:georg..=
.@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt; wrote:<br>
&gt; Thanks Marty<br>
&gt;<br>
&gt; Below are the values I got, any obvious mismatch? I git this working f=
ine<br>
&gt; with OpenMPI.<br>
&gt;<br>
&gt; Here is also what I try to run:<br>
&gt;<br>
&gt; singularity exec /data/zakigf/candle/swift-hypervisor-horovod-mvapich.=
simg<br>
&gt; mpicc mpi-pi.c -o=C2=A0 mpi-pi.o<br>
&gt;<br>
&gt; [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec<br>
&gt; /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o<br>
&gt;<br>
&gt; Then I kill after no response:<br>
&gt;<br>
&gt; ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested<br>
&gt;<br>
&gt; [mpiexec@cn2360] Press Ctrl-C again to force abort<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error =
(Bad<br>
&gt; file descriptor)<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal (pm/pmiserv/pmiserv_cb.c=
:169):<br>
&gt; unable to write data to proxy<br>
&gt;<br>
&gt; [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable to s=
end<br>
&gt; signal downstream<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event<br>
&gt; (tools/demux/demux_poll.c:76): callback returned error status<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmci_wait_for_completion<br>
&gt; (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event<br>
&gt;<br>
&gt; [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error<=
br>
&gt; waiting for completion<br>
&gt;<br>
&gt;<br>
&gt; Now about the versions:<br>
&gt;<br>
&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; mpiexec --vers=
ion<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3DNONE&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--sr=
cdir=3D.&#39; &#39;CC=3Dgcc&#39; &#39;CFLAGS=3D<br>
&gt; -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,=
-rpath,/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -libumad=
 -libverbs -ldl<br>
&gt; -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/=
include<br>
&gt; -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/openpa/src=
<br>
&gt; -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT<br>
&gt; -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include -I/inc=
lude<br>
&gt; -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; gcc --version<=
br>
&gt;<br>
&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;<br>
&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-mvapich.simg:~&gt; exit<br>
&gt;<br>
&gt; exit<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0<br>
&gt;<br>
&gt; [+] Loading mvapich2 2.2 for GCC 5.3.0<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ gcc --version<br>
&gt;<br>
&gt; gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)<br>
&gt;<br>
&gt; Copyright (C) 2010 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ mpiexec --version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 /usr/local/GCC/5.3=
.0/bin/gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0&#39;<br>
&gt; &#39;--with-slurm-lib=3D/usr/local/slurm/lib&#39;<br>
&gt; &#39;--with-slurm-include=3D/usr/local/slurm/include&#39; &#39;--enabl=
e-debug=3Dnone&#39;<br>
&gt; &#39;--enable-timing=3Druntime&#39; &#39;CC=3D/usr/local/GCC/5.3.0/bin=
/gcc&#39;<br>
&gt; &#39;CXX=3D/usr/local/GCC/5.3.0/bin/g++&#39; &#39;FC=3D/usr/local/GCC/=
5.3.0/bin/gfortran&#39;<br>
&gt; &#39;F77=3D/usr/local/GCC/5.3.0/bin/gfortran&#39; &#39;--cache-file=3D=
/dev/null&#39;<br>
&gt; &#39;--srcdir=3D.&#39; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#=
39;LDFLAGS=3D-L/lib -L/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;=
LIBS=3D-libmad<br>
&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT<br>
&gt; -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include -I/includ=
e<br>
&gt; -I/include -I/include -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt;<br>
&gt; On Fri, May 18, 2018 at 1:31 PM Kandes, Martin &lt;<a href=3D"mailto:m=
ka...@sdsc.edu" target=3D"_blank">mka...@sdsc.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi George,<br>
&gt;&gt;<br>
&gt;&gt; I run with different gcc compiler versions inside and outside my M=
PI<br>
&gt;&gt; containers. So I would be surprised if that is the issue here. I&#=
39;m not sure<br>
&gt;&gt; I have a good recommendation of where to start debugging your prob=
lem. But I<br>
&gt;&gt; might start by double checking the MPI versions match inside and o=
utside the<br>
&gt;&gt; container. e.g. see [1].<br>
&gt;&gt;<br>
&gt;&gt; Marty<br>
&gt;&gt;<br>
&gt;&gt; [1]<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1<=
br>
&gt;&gt; --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash<br>
&gt;&gt; srun: job 16364303 queued and waiting for resources<br>
&gt;&gt; srun: job 16364303 has been allocated resources<br>
&gt;&gt; [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ cp<br>
&gt;&gt; /oasis/scratch/comet/mkandes/temp_project/singularity/images/ubunt=
u-mvapich2.img<br>
&gt;&gt; ./<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module purge<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load gnu<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load mvapich2_ib<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module list<br>
&gt;&gt; Currently Loaded Modulefiles:<br>
&gt;&gt;=C2=A0 =C2=A01) gnu/4.9.2=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A02) mvapi=
ch2_ib/2.1<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load singularity<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ gcc --version<br>
&gt;&gt; gcc (GCC) 4.9.2<br>
&gt;&gt; Copyright (C) 2014 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ mpirun --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc=C2=A0 -f=
PIC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++=C2=A0 -fP=
IC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2/gnu/ib&#39; &#39;--enable-shared&#39=
; &#39;--enable-sharedlibs=3Dgcc&#39;<br>
&gt;&gt; &#39;--with-hwloc&#39; &#39;--enable-f77&#39; &#39;--enable-fc&#39=
; &#39;--enable-hybrid&#39;<br>
&gt;&gt; &#39;--with-ib-include=3D/usr/include/infiniband&#39; &#39;--with-=
ib-libpath=3D/usr/lib64&#39;<br>
&gt;&gt; &#39;--enable-fast=3DO3&#39;<br>
&gt;&gt; &#39;--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/sdsc-mva=
pich2_gnu_ib-2.1/../..//cache/build-limic&#39;<br>
&gt;&gt; &#39;--with-slurm=3D/usr/lib64/slurm&#39; &#39;--with-file-system=
=3Dlustre&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D-fPIC -O3 -O3&#39; &#39;CXX=3Dg++&#39; &#39;CXXFLAGS=
=3D-fPIC -O3 -O3&#39; &#39;FC=3Dgfortran&#39;<br>
&gt;&gt; &#39;FCFLAGS=3D-fPIC -O3 -O3&#39; &#39;F77=3Dgfortran&#39; &#39;FF=
LAGS=3D-L/usr/lib64 -L/lib -L/lib<br>
&gt;&gt; -fPIC -O3 -O3&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--srcdi=
r=3D.&#39; &#39;LDFLAGS=3D-L/usr/lib64<br>
&gt;&gt; -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr=
/lib64<br>
&gt;&gt; -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/..=
/..//cache/build-limic/lib<br>
&gt;&gt; -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -lrdmacm -libumad -libverbs=
 -ldl -lrt -llimic2<br>
&gt;&gt; -lm -lpthread &#39; &#39;CPPFLAGS=3D-I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpl/include<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpl/include<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/openpa/src<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/openpa/src<br>
&gt;&gt; -D_REENTRANT<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/mv=
apich2-2.1/src/mpi/romio/include<br>
&gt;&gt; -I/include -I/include -I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_ib-2.1/..=
/..//cache/build-limic/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.=
img<br>
&gt;&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt; gcc =
--version<br>
&gt;&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt; mpir=
un<br>
&gt;&gt; --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2&#39; &#39;--cache-file=3D/dev/null&#=
39; &#39;--srcdir=3D.&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib=
 -L/lib -L/lib<br>
&gt;&gt; -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIB=
S=3D-libmad -lrdmacm<br>
&gt;&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<=
br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/in=
clude<br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/openpa/=
src<br>
&gt;&gt; -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include =
-I/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ________________________________<br>
&gt;&gt; From: George Zaki [<a href=3D"mailto:georg...@gmail.com" target=3D=
"_blank">georg...@gmail.com</a>]<br>
&gt;&gt; Sent: Friday, May 18, 2018 6:48 AM<br>
&gt;&gt; To: singularity<br>
&gt;&gt; Subject: [Singularity] Running an mpi program with mvapitch<br>
&gt;&gt;<br>
&gt;&gt; Hi singularity team,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I would like to run an MPI program in a singularity container. The=
 program<br>
&gt;&gt; is compiled using mvapicth2.2 using a gcc version 5.4.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I can see that my cluster has a compiled version of mvapitch2.2 wi=
th gcc<br>
&gt;&gt; 5.3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; When I run:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o<br>
&gt;&gt;<br>
&gt;&gt; the call does not return.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Does the gcc version has to be exactly the same? I tried the switc=
h the<br>
&gt;&gt; compiler in this image:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; BootStrap: docker<br>
&gt;&gt; From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However when gcc 5.3 is used the mvapitch does not build correctly=
.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; If that&#39;s the problem, Is there a preferred method of switchin=
g gcc<br>
&gt;&gt; version in this container singularity container?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; George<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singu...@lbl.gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to a topic in=
 the<br>
&gt;&gt; Google Groups &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this topic, visit<br>
&gt;&gt; <a href=3D"https://groups.google.com/a/lbl.gov/d/topic/singularity=
/A6I5mZxnmFU/unsubscribe" rel=3D"noreferrer" target=3D"_blank">https://grou=
ps.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe</a>.<br=
>
&gt;&gt; To unsubscribe from this group and all its topics, send an email t=
o<br>
&gt;&gt; <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu=
...@lbl.gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singu...@lbl.gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu.=
..@lbl.gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscri=
be</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscri=
be</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscri=
be</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov</a>.<br>
</blockquote></div></div></div>

--000000000000af6b3e056ce51a11--
