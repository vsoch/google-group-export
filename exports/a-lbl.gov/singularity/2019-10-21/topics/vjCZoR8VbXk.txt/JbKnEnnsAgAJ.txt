X-Received: by 10.31.65.208 with SMTP id o199mr383603vka.6.1476283992348;
        Wed, 12 Oct 2016 07:53:12 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.141.15 with SMTP id p15ls1429109iod.36.gmail; Wed, 12 Oct
 2016 07:53:12 -0700 (PDT)
X-Received: by 10.98.150.79 with SMTP id c76mr2327156pfe.154.1476283991867;
        Wed, 12 Oct 2016 07:53:11 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id n11si6237606pgd.282.2016.10.12.07.53.11
        for <singu...@lbl.gov>;
        Wed, 12 Oct 2016 07:53:11 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.71 as permitted sender) client-ip=209.85.215.71;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 209.85.215.71 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
IronPort-PHdr: 9a23:8G1sghU/2XwvC5Iqzo86NcyqIpnV8LGtZVwlr6E/grcLSJyIuqrYZxCHt8tkgFKBZ4jH8fUM07OQ6PG6HzBaqsbY+Fk5M7V0HycfjssXmwFySOWkMmbcaMDQUiohAc5ZX0Vk9XzoeWJcGcL5ekGA6ibqtW1aJBzzOEJPK/jvHcaK1oLshrr0p8eYM1wArQH+SIs6FA+xowTVu5teqqpZAYF19CH0pGBVcf9d32JiKAHbtR/94sCt4MwrqHwI6Loc7coIbYHWN+R9E/0LRAkgKH0/sc/iqxDYRhGO/mdOZWhWgQ8MTAPb6RW1Gtjqsy31q+50wiiGLI7rS6spUy+p9aZhRTfsgiNBODknvyCE2fB32ehAoRSuuhh22Y/IcamRPv44caTDN5tOXmtHUdtVXjZAHpKUa4ELSeUGI7ALgZP6og5EggGzCg62FavKjHdrm2Xqz6Agmax1HgTFwRY8Dd8UmHDeqJP6M7lEArP997XB0TiWN6Ae4jz68oWdN04s
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2FuAADuTf5XhkfXVdFcGgEBAQECAQEBAQgBAQEBFQEBAQECAQEBAQgBAQEBglw1AQEBAQF0bQ8HgziJdJcFglaFBYdFhRSBShsnGwEGgXSCMIFaAoIFBzgUAQEBAQEBAQEBAQECEAEBAQgLCwkZJAuCMgQCAQIRBQQBOQoxAQEBAQEBAQEBAQEBAQEBARoCCAUiDwMPAhkBAQEDARIRKzALCwsNKgICIQEPAwEFAQsRBggHBAEcBAGHSkoDDwgFqGqBMj4yi0KJCQ2DbAEBAQEGAQEBAQEBAQEcBAsFiwKCR4FSEQGDIIJbBYkghhWEc4UlNQGGJoZLgwuBbk6EGYkgiGWEFII9Ex6BEQ8PW4JhOxELgXMeNAeGPUcxgSgBAQE
X-IronPort-AV: E=Sophos;i="5.31,482,1473145200"; 
   d="scan'208,217";a="43546929"
Received: from mail-lf0-f71.google.com ([209.85.215.71])
  by fe3.lbl.gov with ESMTP; 12 Oct 2016 07:52:52 -0700
Received: by mail-lf0-f71.google.com with SMTP id x79so29172297lff.2
        for <singu...@lbl.gov>; Wed, 12 Oct 2016 07:52:52 -0700 (PDT)
X-Gm-Message-State: AA6/9RksS4WzxbSFmMlfnv5evt4iJu92IyNSeyk6yw2jwTYdLAXAFBJx3XFqiUBtzQuAOb9rYdtPavdbjRzDJCl7nzu1f6Xbyd7ZKOc/lPOfvxP4T43sJHBYaVsO9v+lvaN/JrZxfTWVujNNr3t/xjhp16M=
X-Received: by 10.25.16.210 with SMTP id 79mr1882401lfq.4.1476283971921;
        Wed, 12 Oct 2016 07:52:51 -0700 (PDT)
X-Received: by 10.25.16.210 with SMTP id 79mr1882362lfq.4.1476283971632; Wed,
 12 Oct 2016 07:52:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.167.67 with HTTP; Wed, 12 Oct 2016 07:52:51 -0700 (PDT)
In-Reply-To: <6e2a6338-f64e-4f2a-894b-d40f1f646113@lbl.gov>
References: <04cccf38-72fb-49ef-b010-15fb80e71e8e@lbl.gov> <CAN7etTyY_+ytGXpY4Te-=xED4zHNzz4Pg7xkB4ULj2sZ=gk2WA@mail.gmail.com>
 <6e2a6338-f64e-4f2a-894b-d40f1f646113@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Wed, 12 Oct 2016 07:52:51 -0700
Message-ID: <CAN7etTzk47mf9bCo2S6Wp-9sYpS6u3JfwVgN2koRpKcamDtRWw@mail.gmail.com>
Subject: Re: [Singularity] Using singularity with MPI jobs
To: "singu...@lbl.gov" <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a113fb462b790c9053eac25f6

--001a113fb462b790c9053eac25f6
Content-Type: text/plain; charset=UTF-8

Can you replicate the problem with a -np 1? If so can you strace it from
within the container:

mpirun -np 1 singularity exec container.img strace -ff /path/to/mpi.exe
(opts)

Yes you can try Singularity 2.2. Please install it to a different path so
we can test side by side if you don't mind (if really like to debug this).

Thanks!



On Wednesday, October 12, 2016, Steve Mehlberg <sgmeh...@gmail.com> wrote:

> Greg,
>
> I put a bind to /opt in the singularity.conf file so that /opt/intel is
> available in the container.
>
> All the tasks (16) immediately exit code 6.  The job exits after about 4
> seconds.  It normally takes about 16 minutes to run with the configuration
> I'm using and I do see the start of some output.
>
> I am using openmpi 2.0.0.
>
> I tried an "export SINGULARITY_NO_NAMESPACE_PID=1" in the bash script
> that runs all of this for each process and I still get the problem.
>
> [node12:9779] *** An error occurred in MPI_Isend
> [node12:9779] *** reported by process [3025076225,0]
> [node12:9779] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
> [node12:9779] *** MPI_ERR_RANK: invalid rank
> [node12:9779] *** MPI_ERRORS_ARE_FATAL (processes in this communicator
> will now abort,
> [node12:9779] ***    and potentially your MPI job)
>
> I can try 2.2 - do you think it might behave differently?
>
> Thanks for the ideas and help.
>
> Regards,
>
> Steve
>
> On Tuesday, October 11, 2016 at 8:19:47 PM UTC-6, Gregory M. Kurtzer wrote:
>>
>> Hi Steve,
>>
>> I'm not sure at first glance, but just to touch on the basics... Is
>> /opt/intel available from within the container? Do all tasks exit code 6,
>> or just some of them?
>>
>> What version of OMPI are you using?
>>
>> I wonder if the PID namespace is causing a problem here... I'm not sure
>> it gets effectively disabled when running via srun and pmi. Can you export
>> the environment variable "SINGULARITY_NO_NAMESPACE_PID=1" in a place
>> where Singularity will pick it up definitively by all ranks? That will
>> ensure that the PID namespace is not being exported.
>>
>> Additionally, you could try version 2.2. I just released it, and by
>> default it does not unshare() out the PID namespace. But... It is the first
>> release in the 2.2 series so it may bring with it other issues that still
>> need resolving.... But we should debug those too! :)
>>
>> Greg
>>
>>
>>
>> On Tue, Oct 11, 2016 at 2:40 PM, Steve Mehlberg <sg...@gmail.com>
>> wrote:
>>
>>> Does singularity support MPI PMI-2 jobs?  I've had mixed success testing
>>> benchmark applications using a singularity container.
>>>
>>> Currently I'm struggling to get the NEMO benchmark to run using slurm
>>> 16.05 and pmi2.  I can run the exact same executable on bare metal with the
>>> same slurm, but I get Rank errors when I run using "srun --mpi=pmi2
>>> singularity...".  The application aborts with an exit code 6.
>>>
>>> I tried pmix too, but that gets mpi aborts for both bare metal and
>>> singularity.
>>>
>>> The only way I could get the NEMO application to compile was to use the
>>> intel compilers and mpi:
>>>
>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/compilervars.sh
>>> intel64
>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/ifortvars.sh
>>> intel64
>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/iccvars.sh
>>> intel64
>>> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>>>
>>> It runs fine when I use mpirun with or without singularity.
>>>
>>> Example run/error:
>>>
>>> sbatch ...
>>> srun --mpi=pmi2 -n16 singularity exec c7.img run.it > out_now
>>>
>>> .......
>>> srun: error: node11: tasks 0-7: Exited with exit code 6
>>> srun: error: node12: tasks 8-15: Exited with exit code 6
>>>
>>> $ cat run.it
>>> #!/bin/sh
>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/compilervars.sh
>>> intel64
>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/ifortvars.sh
>>> intel64
>>> source /opt/intel/compilers_and_libraries_2016.3.210/linux/bin/iccvars.sh
>>> intel64
>>> source /opt/mpi/openmpi-icc/2.0.0/bin/mpivars.sh
>>> source env_bench
>>> export PATH=/opt/mpi/openmpi-icc/2.0.0/bin:/opt/pmix/1.1.5/bin:$PATH
>>> export LD_LIBRARY_PATH=/opt/openmpi-icc/2.0.0/lib:/opt/pmix/1.1.5/l
>>> ib:$LD_LIBRARY_PATH
>>> export OMPI_MCA_btl=self,sm,openib
>>>
>>> ./opa_8_2 namelist >out_now
>>>
>>> $ cat out_now
>>> [node12:29725] *** An error occurred in MPI_Isend
>>> [node12:29725] *** reported by process [3865116673,0]
>>> [node12:29725] *** on communicator MPI COMMUNICATOR 3 DUP FROM 0
>>> [node12:29725] *** MPI_ERR_RANK: invalid rank
>>> [node12:29725] *** MPI_ERRORS_ARE_FATAL (processes in this communicator
>>> will now abort,
>>> [node12:29725] ***    and potentially your MPI job)
>>>
>>> I am running singularity 2.1 - any ideas?
>>>
>>> -Steve
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>


-- 
Gregory M. Kurtzer
HPC Systems Architect and Technology Developer
Lawrence Berkeley National Laboratory HPCS
University of California Berkeley Research IT
Singularity Linux Containers (http://singularity.lbl.gov/)
Warewulf Cluster Management (http://warewulf.lbl.gov/)
GitHub: https://github.com/gmkurtzer, Twitter: https://twitter.com/gmkurtzer

--001a113fb462b790c9053eac25f6
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Can you replicate the problem with a -np 1? If so can you strace it from wi=
thin the container:<div><br></div><div>mpirun -np 1 singularity exec contai=
ner.img strace -ff /path/to/mpi.exe (opts)<span></span></div><div><br></div=
><div>Yes you can try Singularity 2.2. Please install it to a different pat=
h so we can test side by side if you don&#39;t mind (if really like to debu=
g this).=C2=A0</div><div><br></div><div>Thanks!</div><div><br></div><div><b=
r><br>On Wednesday, October 12, 2016, Steve Mehlberg &lt;<a href=3D"mailto:=
sgmeh...@gmail.com">sgmeh...@gmail.com</a>&gt; wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div dir=3D"ltr">Greg,<br><br>I put a bind to /opt in the sin=
gularity.conf file so that /opt/intel is available in the container.<br><br=
>All the tasks (16) immediately exit code 6.=C2=A0 The job exits after abou=
t 4 seconds.=C2=A0 It normally takes about 16 minutes to run with the confi=
guration I&#39;m using and I do see the start of some output.<br><br>I am u=
sing openmpi 2.0.0.<br><br>I tried an &quot;export <span>SINGULARITY_NO_NAM=
ESPACE_</span><span>PID=3D1<wbr>&quot; in the bash script that runs all of =
this for each process and I still get the problem.</span><br><br>[node12:97=
79] *** An error occurred in MPI_Isend<br>[node12:9779] *** reported by pro=
cess [3025076225,0]<br>[node12:9779] *** on communicator MPI COMMUNICATOR 3=
 DUP FROM 0<br>[node12:9779] *** MPI_ERR_RANK: invalid rank<br>[node12:9779=
] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,<=
br>[node12:9779] ***=C2=A0=C2=A0=C2=A0 and potentially your MPI job)<br><br=
>I can try 2.2 - do you think it might behave differently?<br><br>Thanks fo=
r the ideas and help.<br><br>Regards,<br><br>Steve<br><br>On Tuesday, Octob=
er 11, 2016 at 8:19:47 PM UTC-6, Gregory M. Kurtzer wrote:<blockquote class=
=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div dir=3D"ltr">Hi Steve,<div><br></div><div>I&#39;=
m not sure at first glance, but just to touch on the basics... Is /opt/inte=
l available from within the container? Do all tasks exit code 6, or just so=
me of them?</div><div><br></div><div>What version of OMPI are you using?</d=
iv><div><br></div><div>I wonder if the PID namespace is causing a problem h=
ere... I&#39;m not sure it gets effectively disabled when running via srun =
and pmi. Can you export the environment variable &quot;<span>SINGULARITY_NO=
_NAMESPACE_</span><span>PID=3D<wbr>1&quot; in a place where Singularity wil=
l pick it up definitively by all ranks? That will ensure that the PID names=
pace is not being exported.</span></div><div><span><br></span></div><div><s=
pan>Additionally, you could try version 2.2. I just released it, and by def=
ault it does not unshare() out the PID namespace. But... It is the first re=
lease in the 2.2 series so it may bring with it other issues that still nee=
d resolving.... But we should debug those too! :)</span></div><div><span><b=
r></span></div><div>Greg</div><div><br></div><div><br></div>







</div><div><br><div class=3D"gmail_quote">On Tue, Oct 11, 2016 at 2:40 PM, =
Steve Mehlberg <span dir=3D"ltr">&lt;<a rel=3D"nofollow">sg...@gmail.com</a=
>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 =
0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Does=
 singularity support MPI PMI-2 jobs?=C2=A0 I&#39;ve had mixed success testi=
ng benchmark applications using a singularity container.=C2=A0 <br><br>Curr=
ently I&#39;m struggling to get the NEMO benchmark to run using slurm 16.05=
 and pmi2.=C2=A0 I can run the exact same executable on bare metal with the=
 same slurm, but I get Rank errors when I run using &quot;srun --mpi=3Dpmi2=
 singularity...&quot;.=C2=A0 The application aborts with an exit code 6.<br=
><br>I tried pmix too, but that gets mpi aborts for both bare metal and sin=
gularity.<br><br>The only way I could get the NEMO application to compile w=
as to use the intel compilers and mpi:<br><br><span style=3D"font-size:12.0=
pt;font-family:&quot;Times New Roman&quot;,&quot;serif&quot;;color:black">s=
ource
/opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>compilerv=
ars.sh intel64<br>
source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>if=
ortvars.sh
intel64<br>
source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>ic=
cvars.sh
intel64<br>
source /opt/mpi/openmpi-icc/2.0.0/bin<wbr>/mpivars.sh<br><br></span>It runs=
 fine when I use mpirun with or without singularity.<br><br>Example run/err=
or:<br><br>sbatch ...<br>srun --mpi=3Dpmi2 -n16 singularity exec c7.img <a =
href=3D"http://run.it" rel=3D"nofollow" target=3D"_blank">run.it</a> &gt; o=
ut_now<br><br>.......<br>srun: error: node11: tasks 0-7: Exited with exit c=
ode 6<br>srun: error: node12: tasks 8-15: Exited with exit code 6<br><br>$ =
cat <a href=3D"http://run.it" rel=3D"nofollow" target=3D"_blank">run.it</a>=
<br>#!/bin/sh<br>source /opt/intel/compilers_and_libra<wbr>ries_2016.3.210/=
linux/bin/<wbr>compilervars.sh intel64<br>source /opt/intel/compilers_and_l=
ibra<wbr>ries_2016.3.210/linux/bin/<wbr>ifortvars.sh intel64<br>source /opt=
/intel/compilers_and_libra<wbr>ries_2016.3.210/linux/bin/<wbr>iccvars.sh in=
tel64<br>source /opt/mpi/openmpi-icc/2.0.0/bin<wbr>/mpivars.sh<br>source en=
v_bench<br>export PATH=3D/opt/mpi/openmpi-icc/2.0.<wbr>0/bin:/opt/pmix/1.1.=
5/bin:$PAT<wbr>H<br>export LD_LIBRARY_PATH=3D/opt/openmpi-i<wbr>cc/2.0.0/li=
b:/opt/pmix/1.1.5/l<wbr>ib:$LD_LIBRARY_PATH<br>export OMPI_MCA_btl=3Dself,s=
m,openib<br><br>./opa_8_2 namelist &gt;out_now<br><br>$ cat out_now<br>[nod=
e12:29725] *** An error occurred in MPI_Isend<br>[node12:29725] *** reporte=
d by process <a value=3D"+13865116673">[3865116673</a>,0]<br>[node12:29725]=
 *** on communicator MPI COMMUNICATOR 3 DUP FROM 0<br>[node12:29725] *** MP=
I_ERR_RANK: invalid rank<br>[node12:29725] *** MPI_ERRORS_ARE_FATAL (proces=
ses in this communicator will now abort,<br>[node12:29725] ***=C2=A0=C2=A0=
=C2=A0 and potentially your MPI job)<br><br>I am running singularity 2.1 - =
any ideas?<span><font color=3D"#888888"><br><br>-Steve<br><br></font></span=
></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.lbl.go=
v/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warew=
ulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<wbr>l.g=
ov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" re=
l=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=
=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https:/=
/twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=
=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></div><=
/div></div></div></div></div></div></div>
</div>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singularity+unsubscribe@lbl.<wbr>gov</a>.=
<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><div><div dir=3D"ltr"><d=
iv><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"=
><div>Gregory M. Kurtzer</div><div>HPC Systems Architect and Technology Dev=
eloper</div><div>Lawrence Berkeley National Laboratory HPCS<br>University o=
f California Berkeley Research IT<br>Singularity Linux Containers=C2=A0(<a =
href=3D"http://singularity.lbl.gov/" target=3D"_blank">http://singularity.l=
bl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://=
warewulf.lbl.gov/" target=3D"_blank">http://warewulf.lbl.gov/</a>)</div><di=
v>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" target=3D"_blank">h=
ttps://github.com/gmkurtzer</a>,=C2=A0<span style=3D"font-size:12.8px">Twit=
ter:=C2=A0</span><a href=3D"https://twitter.com/gmkurtzer" style=3D"font-si=
ze:12.8px" target=3D"_blank">https://twitter.com/gmkurtzer</a></div></div><=
/div></div></div></div></div></div></div></div><br>

--001a113fb462b790c9053eac25f6--
