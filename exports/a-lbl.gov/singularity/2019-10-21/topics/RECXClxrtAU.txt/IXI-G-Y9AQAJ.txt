X-Received: by 2002:a62:b405:: with SMTP id h5-v6mr1856412pfn.59.1530181881621;
        Thu, 28 Jun 2018 03:31:21 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a62:fb01:: with SMTP id x1-v6ls1754103pfm.0.gmail; Thu, 28
 Jun 2018 03:31:20 -0700 (PDT)
X-Received: by 2002:a62:6147:: with SMTP id v68-v6mr935140pfb.115.1530181880461;
        Thu, 28 Jun 2018 03:31:20 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1530181880; cv=none;
        d=google.com; s=arc-20160816;
        b=xsaw8Nz7Yjde9JHzcV2nlYVykLKK3w1lrMtt08Er2DBK4TYuRdgFksfYHpeWFrV3Xq
         mUHfm211Wu4f2z3abevMhV+2GaoZ4Mctxb9fLjq8hqw2hgcU+vaUx7ZiSTW329Cu/LFE
         O4txT8jVJIpFMQLGzcbNOGJ53djKRKqWkPCCgEb6BPTLebYx3wLk0jUCZjvvioDKdvty
         ib+epwGJxw/I58Jjx/+uLJeqzCNaMlX/kKfjo1Q9NgwPyZQo6YUPafjkgaIaUqTItSPw
         xxWeHql9+u0gbVMuoFbSt6QyE6Dup5pP+Pqu4X1JgYTH6Z4qnzYuK88ENk41LYDc6eJA
         niEw==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=joZFIjoxk0VfZcA6XVxI7wqnkerQQOW2vUOlm3EbYo8=;
        b=hE4xPwWV5fQTUNzCsrglZ1pDYkQHYt65mNqh0jtianSNVxa54tj6DqBDZ6w5hW7rHO
         isAyWuZi6SLecETFHMUfsf1xIurZf4vX9lFVJ6XKvyPQSGEsMcTF5tiJF/eeCfJ4BQJJ
         KkHvSMyk2y9q+nh5uJXojl9RF8CambLaxb47aNJEyP9JNp5xAEaWLWFHnpjhDsFKUwma
         IWeMCYCrwsHTs+zV/kij/8YOaCoejZquU8oKZlLN11BEiLnXRTaSLgDoY3Yu+VL95UlU
         uR5ltkXk1Yg7I4WL32bebVUJswLrCGYQ3nTBEFwzvk+PEhUiBQu6Q/3Qcv3ldIOZ6H9K
         YDsg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b="n/m1STWq";
       spf=pass (google.com: domain of maxime...@gmail.com designates 209.85.214.42 as permitted sender) smtp.mailfrom=maxime...@gmail.com
Return-Path: <maxime...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id q74-v6si1710344pfa.272.2018.06.28.03.31.20
        for <singu...@lbl.gov>;
        Thu, 28 Jun 2018 03:31:20 -0700 (PDT)
Received-SPF: pass (google.com: domain of maxime...@gmail.com designates 209.85.214.42 as permitted sender) client-ip=209.85.214.42;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b="n/m1STWq";
       spf=pass (google.com: domain of maxime...@gmail.com designates 209.85.214.42 as permitted sender) smtp.mailfrom=maxime...@gmail.com
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2CHBACltzRbgCrWVdFQBAUDg1CBDH8og?=
 =?us-ascii?q?3MGg22QVIIHApUvgWYjD4FQgnECAgKDGSE3FQECAQECAQECARMBAQkNCQgnJQy?=
 =?us-ascii?q?CNQyCdQEBAQMBHQYEGQENDgkVAwwGBQsNAgImAgIhAQEOAwEFARwOBwQBFAYCB?=
 =?us-ascii?q?II0TCiBPgEDDQgFCqEGPIlXEYEggWkWBQEXgnIFaoJtChkmDVdXgRQCBhJ5h2K?=
 =?us-ascii?q?BVj+BD4IRfoJWNwsBAQEBgR8MAQ0FAQMGAjUmgjqCVQKHUBEIFYExgxlpi1srB?=
 =?us-ascii?q?wKBbYQThgqDCYFAQoNFgmqFGYonToZxMIE2VS5dDAgzGiOBAYI4CRaCBBeDRYJ?=
 =?us-ascii?q?kgjCCZoJXDjIwAQsEjg4BAQ0XBAOCFgUBAQ?=
X-IronPort-AV: E=Sophos;i="5.51,283,1526367600"; 
   d="scan'208";a="27850725"
Received: from mail-it0-f42.google.com ([209.85.214.42])
  by fe4.lbl.gov with ESMTP; 28 Jun 2018 03:31:18 -0700
Received: by mail-it0-f42.google.com with SMTP id u4-v6so12002587itg.0
        for <singu...@lbl.gov>; Thu, 28 Jun 2018 03:31:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=joZFIjoxk0VfZcA6XVxI7wqnkerQQOW2vUOlm3EbYo8=;
        b=n/m1STWqnZRkA16Dw6DlW1yp+z2PYQXtZfwrU7WmZFXkuCt1I8VjYZTovX+pfat/cy
         hPOvEXnLtYD6+0RK5yEFYnOFmIHTSgoRGeypqDnZjS9cu4f7zzeb6LbwrtmZQvcBKkZT
         xadgiQZ5mtc8k+N+AoWahm4aux0sreOzYIWX6Irzy7rGaIivjEHL4sNn3fMbNZyNAJuT
         fKKu2a1P8REkl8Kb7PO9FPXhFPVm98y0l2vC3Pbd1n76QDIsOKDuRxtJ4LriZUFAmPml
         U6QMXjSrutQaPjNRrGd+lqf6+I107d9Z9DbzjpsHVFy1Kj1FSKPvF+oetK8CoE3mqJKX
         GlRQ==
X-Gm-Message-State: APt69E1OVugxb/tMFm+2vD+PmuISnhK4h9gP7sQ2/ejzdDebzEzL4g0J
	3srz6x/0ADSJCIEuDJKKvutdHyE1pTIN9/V/9u+X/Q==
X-Received: by 2002:a02:9914:: with SMTP id r20-v6mr7833992jaj.144.1530181877897;
 Thu, 28 Jun 2018 03:31:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 2002:a02:2b0e:0:0:0:0:0 with HTTP; Thu, 28 Jun 2018 03:31:17
 -0700 (PDT)
In-Reply-To: <CAM=pu++GTFfSA5riN7F7KmLzGR=U-TdbX1C9hJzD9x=-h-M_cw@mail.gmail.com>
References: <4d550130-4d54-4d4f-bf9f-a46f34132e96@lbl.gov> <CAJZ53CkoPup6sXotzVLO_toCu2c+rwK-A6Y0+TU277Y9km8N9w@mail.gmail.com>
 <CAM=pu+Jq2HcgVEBoYJ-USVQomnT6=pmd_16UdEpz2nniPQewdw@mail.gmail.com>
 <CADf5cTEmD1BEEg=jQHf2rrK1h+cwPYYqSD3Tz0UKS-RpumNNRA@mail.gmail.com>
 <CAMsq4T1c4vA5Yv6_bWJhpNC=UCze1reP2yPQa2YUtutJRPzQbA@mail.gmail.com>
 <CAM=pu+L-DXgQJ4eSUqSEnriDX9U2+dwOU-e2P2gWFxW3jsPcWQ@mail.gmail.com>
 <CAPqNE2VEL6oOB2NMP89=BxHqQj5k=Mbg2C=7O=oZuRvN5Q-pnQ@mail.gmail.com>
 <CAPqNE2UYU6+k3k6eef49t1gUegbDRSAD5XyDAf28bQWEBKjCqg@mail.gmail.com> <CAM=pu++GTFfSA5riN7F7KmLzGR=U-TdbX1C9hJzD9x=-h-M_cw@mail.gmail.com>
From: Maxime Hebrard <maxime...@gmail.com>
Date: Thu, 28 Jun 2018 18:31:17 +0800
Message-ID: <CAMsq4T2BrK5ro0wuLdELmOTOeEgafW_OZ7c=s7QERA4K5_atwg@mail.gmail.com>
Subject: Re: [Singularity] Research data and containers
To: singularity@lbl.gov
Content-Type: text/plain; charset="UTF-8"

but that is the point ...
to say "yes, I ran the pipeline with your data, and can verify what
you claimed in the paper, I have replicated the result."
we need a way to get the hand on the original dataset

well my personal problem is not really on "dataset" per se but more
with references data
like reference genome / indexes / annotation files
that are Huge and hard to manage :-/

I could point to public repo for that, but still it is a pain for the
user to download (or let the container download for him) this huge
files :-/

looking for a "better solution" ...



On Thu, Jun 28, 2018 at 6:08 PM, v <vso...@gmail.com> wrote:
> All the cool kids are signing things these days!
> https://docs.docker.com/engine/security/trust/content_trust/#image-tags-and-content-trust
> :P
>
> Dataset signing, especially for these fantastic "actually large" datasets,
> seems to be taken with the same importance for reproducibility as the tools
> that go into the anayses. To play devil's advocate, given that we want to
> prove some phenomena, wouldn't we want to be able to show the same (or
> comparable within some range of confidence) result across different
> datasets? It doesn't strengthen the hypothsis to do the same thing again on
> the exact same data, it strengthens the confidence in the reproducibility of
> the pipeline (two separate things). So perhaps a small bit of data could be
> stored with the container to validate the pipeline, but then new samples
> encouraged to show if the hypothsis holds?
>
> And I'm probably off here, but I think the sharing of datasets is less about
> "run it again on exactly the same data" but more about "share the data so we
> can all learn from it and increase overall N for better power." A culture of
> data sharing, along with validation of method + published result, also
> allows for checks and balances ("yes, I ran the pipeline with your data, and
> can verify what you claimed in the paper, I have replicated the result."
>
> On Thu, Jun 28, 2018 at 12:20 AM 'John Hearns' via singularity
> <singu...@lbl.gov> wrote:
>>
>> Chris Hines mentions signing of tool packages.  In danger of crossing the
>> streams, I am an enthusiast for the Julia language.
>> the new Pkg3 infrastructure for Julia, which is the default package
>> manager now, implements UUIDs for packages
>> https://github.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries
>>
>> On 28 June 2018 at 09:15, John Hearns <hea...@googlemail.com> wrote:
>>>
>>> I think this a very, very interesting topic.  Please allow me to explore
>>> and hopefully bring out some responses?
>>>
>>> Are we talking about large public datasets here - for instance I worked
>>> on a CERN experiment (ahem) many years ago.
>>> One hopes that huge, public datasets do not change. For instance if I
>>> went back to analyze what was tagged at the time with a run number and an
>>> event number I hope I would get the same data.
>>> OK, it is clear that some large institutiosn worldwide will store that
>>> data. And that bitrot or the scarcity of the tape drives to read the data
>>> has not rendered it beyond recovery.
>>>
>>> I also worked with the JASMIN project in the UK, where several institutes
>>> have banded together to create a central data store for climate modelling.
>>> Again large public datasets.
>>>
>>> I also worked in Formula 1 with CFD engineers, and managed over a
>>> petabyte of data from CFD studies. Those studies were catalogued by year and
>>> then case number.
>>> Again, we assume that the data is held on tape somewhere and if you ind
>>> the correctly named directory you will get the same data back.
>>>
>>> I guess here we are talking about reproducibility. So we have a
>>> Singularity container, which is signed, and captures the packages and
>>> libraries needed to run that analysis.
>>> We also have to have the same data available to run on.  Do we need to
>>> physically keep that data 'beside' the container? Maybe, for certain data
>>> and certain values of 'not too large'.
>>> but how about analyses of (say) CERN or JASMIN data? Is it enough to make
>>> a pointer towards those datasets somehow?
>>> I would say URL's or URIs here, but we have seen how the Web is dynamic
>>> and sites disappear and links change.
>>>
>>>
>>> Its worth saying that in the UK there is a lot of effort in this space.
>>> The Research Councils there mandated that if they fund research the data
>>> must be retained for N years (I forget N)
>>> So many Universities now have specific services for research data, for
>>> instance:
>>> http://www.ucl.ac.uk/research-it-services/research-data-service
>>> https://www.ukdataservice.ac.uk/
>>>
>>> Arkivum https://arkivum.com/ works closely with the UK academic network
>>> to provide large data storage. I just found this blog on their website -
>>> well worth reading.
>>>
>>> https://arkivum.com/articles/blog-series-part-1-four-economic-factors-make-cost-nothing-expensive-something/
>>>
>>> https://arkivum.com/pharma/blog-series-part-4-final-economic-fallacy-long-term-data-temporarily-dynamic-path-dependent/
>>>
>>>
>>> To end up, I guess what we are aiming towards is an S3 compatible storage
>>> bucket, whether this is hosted in-house, by Amazon or by a compatible
>>> provider.
>>> Such a bucket would have to be locked, and signed.  Ideas??
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On 28 June 2018 at 03:44, v <vso...@gmail.com> wrote:
>>>>
>>>> Your description of the different components is on point.
>>>>
>>>> Finding hosting for large data is, and will continue to be, always one
>>>> of the biggest problems. It's expensive. Someone has to pay for it. Nobody
>>>> will do it for you for free. We are lucky to have things like Github repos,
>>>> Google Drive, Dropbox, that gives some (mediocre) access to small data. For
>>>> large data, everyone is pretty much on their own. Most academics have a
>>>> university that provides some sort of web space / library data archive, and
>>>> if you are publishing, some journals will support it too -->
>>>> https://www.nature.com/sdata/policies/repositories. I try to figure out
>>>> hacks to get everything hosted, for free, in version control, but that
>>>> breaks when anything gets large.
>>>>
>>>> The entity that manages a data respository, if this is a thing, must
>>>> have incentive to do it. The datasets have to bring in money in some way, or
>>>> someone has to value it enough to pay for it. That's sort of what most
>>>> things come down to, unfortunately.
>>>>
>>>>
>>>> On Wed, Jun 27, 2018 at 6:38 PM Maxime Hebrard
>>>> <maxime...@gmail.com> wrote:
>>>>>
>>>>> I start to use singularity / containers and obviously I ran into the
>>>>> same question ...
>>>>>
>>>>> a half solution that I found by exploring existing solution (thanks to
>>>>> nf-core and SciLifeLab to share their workflow / containers) is ( as
>>>>> far I understand ):
>>>>>
>>>>> the container encapsulate only the sorftware.
>>>>> the data are store in external repositories.
>>>>>
>>>>> the user receive the container.
>>>>>
>>>>> case 1:
>>>>> the user download the data by himself and point his files to the
>>>>> container
>>>>> the container use local files as resources
>>>>> pros:
>>>>>  * the container is slim
>>>>>  * user can store the data on machine A (data server) and run the
>>>>> software on machine B (analyse server)
>>>>> cons:
>>>>> * user is involved in downloading the right data
>>>>>
>>>>> case 2:
>>>>> the user do not have the data
>>>>> the container download the data from the external repository
>>>>> the container use the data it just download
>>>>> pro:
>>>>> * the container manage everything
>>>>> cons:
>>>>> * the step of downloading the data require bandwith + disk space =
>>>>> time and resources
>>>>>
>>>>> a general worry for both scenarii is:
>>>>> who manage the data repository ? can we ensure the version of the data
>>>>> package will be maintain ?
>>>>>
>>>>> Waiting for your feedback ;)
>>>>> Maxime
>>>>>
>>>>> On Thu, Jun 28, 2018 at 8:15 AM, 'Chris Hines' via singularity
>>>>> <singu...@lbl.gov> wrote:
>>>>> > To quote 90's band "The Offspring" IMHO "You gotta keep 'em
>>>>> > separated"
>>>>> >
>>>>> > Personally I think this is a larger question: How do we (as research
>>>>> > software engineers, or whatever you would like to call us today)
>>>>> > catalog and
>>>>> > attach references (DOI's I guess) to larger data sets. Does it make
>>>>> > sense to
>>>>> > mksquashfs or tar.bz2? What if they are already in a single hdf5
>>>>> > file? And
>>>>> > what about the output data set? we obviously want that to land
>>>>> > outside the
>>>>> > computational container, but how should we package the output so it
>>>>> > can be
>>>>> > referenced directly? How do we make that whole thing discoverable and
>>>>> > encourage researchers to attach sufficient metadata to make it
>>>>> > searchable?
>>>>> >
>>>>> > I'm wondering if part of the solution is some sort of "meta
>>>>> > container" that
>>>>> > downloads a set of versioned data with specific checksums, and a
>>>>> > versioned
>>>>> > tool container with a specific checksum, and attaches them in a
>>>>> > reproducible
>>>>> > way.
>>>>> >
>>>>> > To address Dominque's question directly: I'd keep the
>>>>> > "semi-separated".
>>>>> > Whatever your researchers use for archiving/cataloging/managing their
>>>>> > data
>>>>> > sets, keep that in place and add the singularity container to the
>>>>> > data set
>>>>> > (rather than adding the data set to the singularity container).
>>>>> >
>>>>> > Regards,
>>>>> > --
>>>>> > Chris.
>>>>> > Monash eResearch Centre, Monash University, Australia
>>>>> >
>>>>> > On Thu, 28 Jun 2018 at 01:19, v <vso...@gmail.com> wrote:
>>>>> >>
>>>>> >> If you are making containers with singularity (and using Squashfs
>>>>> >> anyway)
>>>>> >> it wouldn't be so nutty to just compress with mksquashfs and then
>>>>> >> mount
>>>>> >> where needed - I did this with a huge dataset and it worked pretty
>>>>> >> nicely
>>>>> >> :_) It relies on FUSE then and all the issues around that, but it's
>>>>> >> an
>>>>> >> option!
>>>>> >>
>>>>> >> This is good showing of how use use mksquashfs (it's really
>>>>> >> simple,actually!) -->
>>>>> >> http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.html
>>>>> >> And then mount -->
>>>>> >> https://vsoch.github.io/datasets/2018/zenodo/#mount-with-sudo
>>>>> >>
>>>>> >> On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker
>>>>> >> <brando...@cornell.edu> wrote:
>>>>> >>>
>>>>> >>> I seem to recall code ocean may have explored this idea. Sorry I
>>>>> >>> can't
>>>>> >>> say more, at the moment.
>>>>> >>>
>>>>> >>> On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen
>>>>> >>> <dominiqu...@gmail.com> wrote:
>>>>> >>>>
>>>>> >>>> Hello everyone,
>>>>> >>>>
>>>>> >>>> I am seeking for tips and experience on how to handle research
>>>>> >>>> data,
>>>>> >>>> especially bigger sets of data (inside the GB range), in
>>>>> >>>> combination with
>>>>> >>>> containers.
>>>>> >>>>
>>>>> >>>> I am a student assistant working in the IT department of a
>>>>> >>>> research
>>>>> >>>> institute. And I am involved in building the infrastructure for
>>>>> >>>> singularity
>>>>> >>>> containerization and introducing researchers to the technology. We
>>>>> >>>> have
>>>>> >>>> already build a few base images containing frequently used tools.
>>>>> >>>> Recently a
>>>>> >>>> research group approached us, wishing to integrate data, they
>>>>> >>>> created for
>>>>> >>>> one tool, into a container with the tool. The data takes up
>>>>> >>>> several GB of
>>>>> >>>> disk space and we are unsure how to handle this and similar future
>>>>> >>>> use
>>>>> >>>> cases.
>>>>> >>>>
>>>>> >>>> Does anyone have a set of best practices or is there an intended
>>>>> >>>> way to
>>>>> >>>> use singularity with big research data?
>>>>> >>>>
>>>>> >>>> The options we considered are:
>>>>> >>>>
>>>>> >>>> Moving the data into the container at build time.
>>>>> >>>>
>>>>> >>>> Pro:
>>>>> >>>>
>>>>> >>>> Keeps the whole thing mobile
>>>>> >>>> Keeps work away from the researchers
>>>>> >>>> Can be referenced as a single entity in a publication.
>>>>> >>>>
>>>>> >>>> Con:
>>>>> >>>>
>>>>> >>>> Where would you store such big containers, especially several of
>>>>> >>>> them?
>>>>> >>>> What would happen, if separate versions of the tool are needed?
>>>>> >>>> Keep the
>>>>> >>>> old and accumulate redundant data? Delete the old and risk
>>>>> >>>> reproducibility
>>>>> >>>> problems? (Maybe container Apps can be used to minimize this.)
>>>>> >>>>
>>>>> >>>> Let the researchers mount the data into the container at startup.
>>>>> >>>>
>>>>> >>>> Pro:
>>>>> >>>>
>>>>> >>>> Keeps the containers slimmer, the tools more modular.
>>>>> >>>>
>>>>> >>>> Con:
>>>>> >>>>
>>>>> >>>> Adds to the workload and the things the researchers have to keep
>>>>> >>>> track
>>>>> >>>> of.
>>>>> >>>> Spreads the parts needed for reproduction over at least two
>>>>> >>>> points.
>>>>> >>>> Hampers mobility.
>>>>> >>>>
>>>>> >>>> Mounting during early phases and publish with a container,
>>>>> >>>> containing
>>>>> >>>> the data.
>>>>> >>>>
>>>>> >>>> Pro:
>>>>> >>>>
>>>>> >>>> Reduces amount of containers with redundant and deprecated  data.
>>>>> >>>> Makes reproduction of results easier after publication.
>>>>> >>>>
>>>>> >>>> Con:
>>>>> >>>>
>>>>> >>>> Ads to the workload.
>>>>> >>>> When is the point when its time to "freeze" the data inside the
>>>>> >>>> container?
>>>>> >>>> Storage of the container is still problematic.
>>>>> >>>> Might introduce reproducibility problems, since you change the
>>>>> >>>> original
>>>>> >>>> container to put the data into it.
>>>>> >>>>
>>>>> >>>> Are there any recommendations from experience?
>>>>> >>>>
>>>>> >>>> Thank you and best regards,
>>>>> >>>> Dominique
>>>>> >>>>
>>>>> >>>> --
>>>>> >>>> You received this message because you are subscribed to the Google
>>>>> >>>> Groups "singularity" group.
>>>>> >>>> To unsubscribe from this group and stop receiving emails from it,
>>>>> >>>> send
>>>>> >>>> an email to singu...@lbl.gov.
>>>>> >>>
>>>>> >>> --
>>>>> >>> You received this message because you are subscribed to the Google
>>>>> >>> Groups
>>>>> >>> "singularity" group.
>>>>> >>> To unsubscribe from this group and stop receiving emails from it,
>>>>> >>> send an
>>>>> >>> email to singu...@lbl.gov.
>>>>> >>>
>>>>> >>> --
>>>>> >>> Vanessa Villamia Sochat
>>>>> >>> Stanford University '16
>>>>> >>> (603) 321-0676
>>>>> >>
>>>>> >> --
>>>>> >> You received this message because you are subscribed to the Google
>>>>> >> Groups
>>>>> >> "singularity" group.
>>>>> >> To unsubscribe from this group and stop receiving emails from it,
>>>>> >> send an
>>>>> >> email to singu...@lbl.gov.
>>>>> >
>>>>> > --
>>>>> > You received this message because you are subscribed to the Google
>>>>> > Groups
>>>>> > "singularity" group.
>>>>> > To unsubscribe from this group and stop receiving emails from it,
>>>>> > send an
>>>>> > email to singu...@lbl.gov.
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>>> an email to singu...@lbl.gov.
>>>>
>>>>
>>>>
>>>> --
>>>> Vanessa Villamia Sochat
>>>> Stanford University '16
>>>> (603) 321-0676
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>
>>>
>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
>>
>> --
>> Vanessa Villamia Sochat
>> Stanford University '16
>> (603) 321-0676
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
