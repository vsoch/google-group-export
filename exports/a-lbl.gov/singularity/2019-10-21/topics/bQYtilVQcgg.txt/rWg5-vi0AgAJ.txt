Date: Tue, 7 Mar 2017 06:38:13 -0800 (PST)
From: Geert Jan BEX <geert...@uhasselt.be>
To: singularity <singu...@lbl.gov>
Message-Id: <57a275c4-71cc-4dba-8c10-027862b820b7@lbl.gov>
In-Reply-To: <CAN7etTx65za_GXuaYMvY44iYQc3erzYQWgYqSNAR_ZDiaCRqfA@mail.gmail.com>
References: <4a675b1d-5116-4702-a9e4-cc0024dcb436@lbl.gov> <CAN7etTzUMksoSB_H1ZgA3+-fFvM2pO+XzeD3E-pLqLiiL69c4w@mail.gmail.com>
 <5c39ff15-22d6-4b4f-8566-a670d2910e6a@lbl.gov> <2e6d299f-12d6-42ea-aacb-55f0d8924e36@lbl.gov>
 <CAN7etTyX+_RYBb8m4eNFORisfa848Bksm6s5vWZZjMgsdD1WHg@mail.gmail.com> <104158ea-3718-400a-a536-b0c64ccad1fa@lbl.gov>
 <CAN7etTx65za_GXuaYMvY44iYQc3erzYQWgYqSNAR_ZDiaCRqfA@mail.gmail.com>
Subject: Re: [Singularity] job with multiple singularity calls (mixed
 MPI/serial) fails
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_1949_535978342.1488897493833"

------=_Part_1949_535978342.1488897493833
Content-Type: multipart/alternative; 
	boundary="----=_Part_1950_1519043551.1488897493834"

------=_Part_1950_1519043551.1488897493834
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

Dear Gregory,

Sorry this took a while, I first wanted to get the user in production with 
a standard build.

You'll find the debug output on Google Drive 
<https://drive.google.com/file/d/0BxlSeep1Z7B-aFdYMXZud2lQREE/view?usp=sharing>
.

Thanks in advance, best regards, -gjb-

On Friday, March 3, 2017 at 4:10:22 AM UTC+1, Gregory M. Kurtzer wrote:
>
> Heya, I don't know off the top of my head. Can you provide some debugging 
> output (both from the MPI program and Singularity on the errors you are 
> getting)?
>
> Thanks!
>
> On Fri, Feb 24, 2017 at 5:52 AM, Geert Jan BEX <gee...@uhasselt.be 
> <javascript:>> wrote:
>
>> Dear Greg,
>>
>> Thanks for the suggestion.
>>
>> Some update: my initial impression that this problem had to do with 
>> multiple singularity calls was wrong.  I've reproduced this in isolation as 
>> well.
>>
>> When I try your suggestion with Singularity 2.2, I get the following 
>> error:
>> Primary job  terminated normally, but 1 process returned
>> a non-zero exit code.. Per user-direction, the job has been aborted.
>> -------------------------------------------------------
>> --------------------------------------------------------------------------
>> A system call failed during shared memory initialization that should
>> not have.  It is likely that your MPI job will now either abort or
>> experience performance degradation.
>>
>>   Local host:  r04i15n1
>>   System call: open(2) 
>>   Error:       No such file or directory (errno 2)
>>
>> [r04i15n1:29725] 9 more processes have sent help message 
>> help-opal-shmem-mmap.txt / sys call fail
>>
>> This is the same type of error I get with Singularity 2.2.1, without 
>> setting SINGULARITY_NOSESSIONCLEANUP..  That would indeed imply that the 
>> session directory cleanup is fixed in 2.2.1, but now there seems to be an 
>> issue with the shared memory file of OpenMPI.
>>
>> Any ideas?
>>
>> Thanks in advance, best regards, Geert Jan Bex
>>
>>
>>
>> On Thursday, February 23, 2017 at 4:23:08 PM UTC+1, Gregory M. Kurtzer 
>> wrote:
>>>
>>> While I think you are hitting some locking bug in Singularity, that I 
>>> think/hope is already fixed in one of our development branches, I do think 
>>> there is a workaround you can use. Modify your MPI line to include:
>>>
>>> SINGULARITY_NOSESSIONCLEANUP=1
>>>
>>> I think you can do it like:
>>>
>>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" 
>>> SINGULARITY_NOSESSIONCLEANUP=1 singularity --debug exec -B 
>>> ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img bash -c 
>>> 'cd /wrf/run/wrfwkdir; real.exe'
>>>
>>> Or you can make sure that the variable is set on every node running the 
>>> process. A better fix is included (or rather will be included) in the 
>>> latest version of Open MPI that will automatically set that variable, and 
>>> allow Open MPI to clean up the Singularity sessionfiles directly after the 
>>> job runs. Presently those optimizations exist in Open MPI's master branch. 
>>> So if you can build that yourself and test, that is also an option.
>>>
>>> Sorry for the delay, and hope that helps!
>>>
>>> Greg
>>>
>>> On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BEX <gee...@uhasselt.be> 
>>> wrote:
>>>
>>>> Dear Gregory,
>>>>
>>>> Any feedback you can give me?
>>>>
>>>> Thanks in advance, best regards, -gjb-
>>>>
>>>>
>>>> On Tuesday, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:
>>>>>
>>>>> Dear Gregory,
>>>>>
>>>>> Thanks for looking into this.
>>>>>
>>>>> The /tmp is on the local disks of the compute nodes, so no NFS mounts 
>>>>> involved.
>>>>>
>>>>> The /tmp is also used for the MCA orte_tmpdir_base for mpirun.
>>>>>
>>>>> I've put the output of a run with --debug on Google Drive, you can 
>>>>> access it via:
>>>>>
>>>>> https://drive.google.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=sharing
>>>>>
>>>>> The "offending" statement in the job script is:
>>>>> mpirun -mca orte_tmpdir_base "${ORTE_TMP}" singularity --debug exec -B 
>>>>> ${SIMDIR}:/wrf/run $VSC_SCRATCH/containers/wrfrun-latest.img bash -c 'cd 
>>>>> /wrf/run/wrfwkdir; real.exe'
>>>>>
>>>>> So ORTE_TMP points to a directory in /tmp (local disk), VSC_SCRATCH 
>>>>> and SIMDIR refer to a GPFS file system, and that is also where the working 
>>>>> directory for the job is on.
>>>>>
>>>>> Singularity's config file is unaltered from default.
>>>>>
>>>>> Thanks in advance, -gjb-
>>>>>
>>>>> On Monday, February 20, 2017 at 5:56:56 PM UTC+1, Gregory M. Kurtzer 
>>>>> wrote:
>>>>>>
>>>>>> Hi Geert,
>>>>>>
>>>>>> The directory is only supposed to be removed when the flock()s have 
>>>>>> all been released. I just did some fixups for this in one of the 
>>>>>> development branches, but 2.2* should be working in the same way. Is `/tmp` 
>>>>>> a local directory, or is it on some form of shared NFS? Does it have any 
>>>>>> mount options to disable locking?
>>>>>>
>>>>>> Can you also try with --debug enabled and send all of the output? 
>>>>>>
>>>>>> Thanks!
>>>>>>
>>>>>> On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <gee...@uhasselt.be
>>>>>> > wrote:
>>>>>>
>>>>>>> Dear,
>>>>>>>
>>>>>>> I've run into a problem with singularity, and out of ideas by now.
>>>>>>>
>>>>>>> When running a job that involves multiple calls to singularity (same 
>>>>>>> image each time), some of them serial, some MPI, the MPI ones 
>>>>>>> intermittently fail with:
>>>>>>> ERROR  : Could not remove session directory 
>>>>>>> /tmp/.singularity-session-2530140.42.465829344: Device or resource busy
>>>>>>>
>>>>>>> This is for OpenMPI 1.10.2 on host and in image.  The host OS is 
>>>>>>> CentOS 7, the image OS Ubuntu 16.04,  Singularity 2.2.  The job runs on 4 
>>>>>>> nodes, 27 processes each.
>>>>>>>
>>>>>>> I've tried several approaches (sleep between calls, setting a 
>>>>>>> session directory per singularity call, ...).
>>>>>>>
>>>>>>> Any ideas?
>>>>>>>
>>>>>>> Thanks in advance, Geert Jan Bex
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> -- 
>>>>>>> You received this message because you are subscribed to the Google 
>>>>>>> Groups "singularity" group.
>>>>>>> To unsubscribe from this group and stop receiving emails from it, 
>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> -- 
>>>>>> Gregory M. Kurtzer
>>>>>> HPC Systems Architect and Technology Developer
>>>>>> Lawrence Berkeley National Laboratory HPCS
>>>>>> University of California Berkeley Research IT
>>>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>>>> GitHub: https://github.com/gmkurtzer, Twitter: 
>>>>>> https://twitter.com/gmkurtzer
>>>>>>
>>>>> -- 
>>>> You received this message because you are subscribed to the Google 
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send 
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> -- 
>>> Gregory M. Kurtzer
>>> HPC Systems Architect and Technology Developer
>>> Lawrence Berkeley National Laboratory HPCS
>>> University of California Berkeley Research IT
>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>> GitHub: https://github.com/gmkurtzer, Twitter: 
>>> https://twitter.com/gmkurtzer
>>>
>> -- 
>> You received this message because you are subscribed to the Google Groups 
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an 
>> email to singu...@lbl.gov <javascript:>.
>>
>
>
>
> -- 
> Gregory M. Kurtzer
> HPC Systems Architect and Technology Developer
> Lawrence Berkeley National Laboratory HPCS
> University of California Berkeley Research IT
> Singularity Linux Containers (http://singularity.lbl.gov/)
> Warewulf Cluster Management (http://warewulf.lbl.gov/)
> GitHub: https://github.com/gmkurtzer, Twitter: 
> https://twitter.com/gmkurtzer
>

------=_Part_1950_1519043551.1488897493834
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Dear Gregory,<div><br></div><div>Sorry this took a while, =
I first wanted to get the user in production with a standard build.</div><d=
iv><br></div><div>You&#39;ll find the debug output on <a href=3D"https://dr=
ive.google.com/file/d/0BxlSeep1Z7B-aFdYMXZud2lQREE/view?usp=3Dsharing">Goog=
le Drive</a>.</div><div><br></div><div>Thanks in advance, best regards, -gj=
b-<br><br>On Friday, March 3, 2017 at 4:10:22 AM UTC+1, Gregory M. Kurtzer =
wrote:<blockquote class=3D"gmail_quote" style=3D"margin: 0;margin-left: 0.8=
ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=3D"ltr">Heya, I=
 don&#39;t know off the top of my head. Can you provide some debugging outp=
ut (both from the MPI program and Singularity on the errors you are getting=
)?<div><br></div><div>Thanks!</div></div><div><br><div class=3D"gmail_quote=
">On Fri, Feb 24, 2017 at 5:52 AM, Geert Jan BEX <span dir=3D"ltr">&lt;<a h=
ref=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"oxTpax1VAQAJ=
" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;return =
true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">gee...@uh=
asselt.be</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=
=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=
=3D"ltr">Dear Greg,<div><br></div><div>Thanks for the suggestion.</div><div=
><br></div><div>Some update: my initial impression that this problem had to=
 do with multiple singularity calls was wrong.=C2=A0 I&#39;ve reproduced th=
is in isolation as well.</div><div><br></div><div>When I try your suggestio=
n with Singularity 2.2, I get the following error:</div><div><div>Primary j=
ob =C2=A0terminated normally, but 1 process returned</div><div>a non-zero e=
xit code.. Per user-direction, the job has been aborted.</div><div>--------=
----------------------<wbr>-------------------------</div><div>------------=
------------------<wbr>------------------------------<wbr>--------------</d=
iv><div>A system call failed during shared memory initialization that shoul=
d</div><div>not have.=C2=A0 It is likely that your MPI job will now either =
abort or</div><div>experience performance degradation.</div><div><br></div>=
<div>=C2=A0 Local host: =C2=A0r04i15n1</div><div>=C2=A0 System call: open(2=
)=C2=A0</div><div>=C2=A0 Error: =C2=A0 =C2=A0 =C2=A0 No such file or direct=
ory (errno 2)</div><div><div><br></div><div>[r04i15n1:29725] 9 more process=
es have sent help message help-opal-shmem-mmap.txt / sys call fail</div></d=
iv><div><br></div><div>This is the same type of error I get with Singularit=
y 2.2.1, without setting=C2=A0<span style=3D"font-size:small">SINGULARITY_<=
/span><span style=3D"font-size:small">NOSESSIONC<wbr>LEANUP..=C2=A0 That wo=
uld indeed imply that the session directory cleanup is fixed in 2.2.1, but =
now there seems to be an issue with the shared memory file of OpenMPI.</spa=
n></div><div><span style=3D"font-size:small"><br></span></div><div><span st=
yle=3D"font-size:small">Any ideas?</span></div><div><span style=3D"font-siz=
e:small"><br></span></div><div><span style=3D"font-size:small">Thanks in ad=
vance, best regards, Geert Jan Bex</span></div><span><div><span style=3D"fo=
nt-size:small"><br></span></div><div><span style=3D"font-size:small"><br></=
span></div><br>On Thursday, February 23, 2017 at 4:23:08 PM UTC+1, Gregory =
M. Kurtzer wrote:</span><blockquote class=3D"gmail_quote" style=3D"margin:0=
;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><span><div =
dir=3D"ltr"><div>While I think you are hitting some locking bug in Singular=
ity, that I think/hope is already fixed in one of our development branches,=
 I do think there is a workaround you can use. Modify your MPI line to incl=
ude:</div><div><br></div><div>SINGULARITY_NOSESSIONCLEANUP=3D1</div><div><b=
r></div><div>I think you can do it like:</div><div><br></div><div><div styl=
e=3D"font-size:12.8px">mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP}&quot;=
=C2=A0<span style=3D"font-size:small">SINGULARITY_<wbr>NOSESSIONCLEANUP=3D1=
=C2=A0</span><span style=3D"font-size:12.8px">singularity --debug exec -B $=
{SIMDIR}:/wrf/run $VSC_SCRATCH/containers/</span><span style=3D"font-size:1=
2.8px">wrfrun<wbr>-latest.img bash -c &#39;cd /wrf/run/wrfwkdir; real.exe&#=
39;</span></div></div><div><br></div><div>Or you can make sure that the var=
iable is set on every node running the process. A better fix is included (o=
r rather will be included) in the latest version of Open MPI that will auto=
matically set that variable, and allow Open MPI to clean up the Singularity=
 sessionfiles directly after the job runs. Presently those optimizations ex=
ist in Open MPI&#39;s master branch. So if you can build that yourself and =
test, that is also an option.</div><div><br></div>Sorry for the delay, and =
hope that helps!<div><br></div><div>Greg</div></div></span><div><div><div><=
br><div class=3D"gmail_quote">On Thu, Feb 23, 2017 at 5:11 AM, Geert Jan BE=
X <span dir=3D"ltr">&lt;<a rel=3D"nofollow">gee...@uhasselt.be</a>&gt;</spa=
n> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<=
div><br></div><div>Any feedback you can give me?</div><div><br></div><div>T=
hanks in advance, best regards, -gjb-</div><div><div><div><br><br>On Tuesda=
y, February 21, 2017 at 5:52:48 PM UTC+1, Geert Jan BEX wrote:<blockquote c=
lass=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #c=
cc solid;padding-left:1ex"><div dir=3D"ltr">Dear Gregory,<div><br></div><di=
v>Thanks for looking into this.</div><div><br></div><div>The /tmp is on the=
 local disks of the compute nodes, so no NFS mounts involved.</div><div><br=
></div><div>The /tmp is also used for the MCA orte_tmpdir_base for mpirun.<=
/div><div><br></div><div>I&#39;ve put the output of a run with --debug on G=
oogle Drive, you can access it via:</div><div><a href=3D"https://drive.goog=
le.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp=3Dsharing" rel=3D"nofol=
low" target=3D"_blank" onmousedown=3D"this.href=3D&#39;https://drive.google=
.com/file/d/0BxlSeep1Z7B-MzRsd2pwV2dUTG8/view?usp\x3dsharing&#39;;return tr=
ue;" onclick=3D"this.href=3D&#39;https://drive.google.com/file/d/0BxlSeep1Z=
7B-MzRsd2pwV2dUTG8/view?usp\x3dsharing&#39;;return true;">https://drive.goo=
gle.com/file/<wbr>d/0BxlSeep1Z7B-<wbr>MzRsd2pwV2dUTG8/view?usp=3D<wbr>shari=
ng</a></div><div><br></div><div>The &quot;offending&quot; statement in the =
job script is:</div><div><div>mpirun -mca orte_tmpdir_base &quot;${ORTE_TMP=
}&quot; singularity --debug exec -B ${SIMDIR}:/wrf/run $VSC_SCRATCH/contain=
ers/<wbr>wrfrun-latest.img bash -c &#39;cd /wrf/run/wrfwkdir; real.exe&#39;=
</div></div><div><br></div><div>So ORTE_TMP points to a directory in /tmp (=
local disk), VSC_SCRATCH and SIMDIR refer to a GPFS file system, and that i=
s also where the working directory for the job is on.</div><div><br></div><=
div>Singularity&#39;s config file is unaltered from default.</div><div><br>=
</div><div>Thanks in advance, -gjb-<br><br>On Monday, February 20, 2017 at =
5:56:56 PM UTC+1, Gregory M. Kurtzer wrote:<blockquote class=3D"gmail_quote=
" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-le=
ft:1ex"><div dir=3D"ltr">Hi Geert,<div><br></div><div>The directory is only=
 supposed to be removed when the flock()s have all been released. I just di=
d some fixups for this in one of the development branches, but 2.2* should =
be working in the same way. Is `/tmp` a local directory, or is it on some f=
orm of shared NFS? Does it have any mount options to disable locking?</div>=
<div><br></div><div>Can you also try with --debug enabled and send all of t=
he output?=C2=A0</div><div><br></div><div>Thanks!</div></div><div><br><div =
class=3D"gmail_quote">On Mon, Feb 20, 2017 at 6:25 AM, Geert Jan BEX <span =
dir=3D"ltr">&lt;<a rel=3D"nofollow">gee...@uhasselt.be</a>&gt;</span> wrote=
:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Dear,<div><br></div><d=
iv>I&#39;ve run into a problem with singularity, and out of ideas by now.</=
div><div><br></div><div>When running a job that involves multiple calls to =
singularity (same image each time), some of them serial, some MPI, the MPI =
ones intermittently fail with:</div><div><div>ERROR =C2=A0: Could not remov=
e session directory /tmp/.singularity-session-<wbr>2530140.42.465829344: De=
vice or resource busy</div></div><div><br></div><div>This is for OpenMPI 1.=
10.2 on host and in image.=C2=A0 The host OS is CentOS 7, the image OS Ubun=
tu 16.04, =C2=A0Singularity 2.2.=C2=A0 The job runs on 4 nodes, 27 processe=
s each.</div><div><br></div><div>I&#39;ve tried several approaches (sleep b=
etween calls, setting a session directory per singularity call, ...).</div>=
<div><br></div><div>Any ideas?</div><div><br></div><div>Thanks in advance, =
Geert Jan Bex</div><span><font color=3D"#888888"><div><br></div><div><br></=
div><div><br></div></font></span></div><span><font color=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div>-- <b=
r><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><d=
iv dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</d=
iv><div>HPC Systems Architect and Technology Developer</div><div>Lawrence B=
erkeley National Laboratory HPCS<br>University of California Berkeley Resea=
rch IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.=
lbl.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#3=
9;http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\=
x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return =
true;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2=
F%2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjd=
e-iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a=
>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.l=
bl.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39=
;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD=
\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true=
;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2F=
warewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BK=
cVgBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><d=
iv>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"nofollow" t=
arget=3D"_blank" onmousedown=3D"this.href=3D&#39;https://www.google.com/url=
?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x=
3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.hre=
f=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtz=
er\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39=
;;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"=
font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkur=
tzer" style=3D"font-size:12.8px" rel=3D"nofollow" target=3D"_blank" onmouse=
down=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwit=
ter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsK=
sNsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.go=
ogle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\=
x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:=
//<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div><=
/div></div></div></div>
</div>
</blockquote></div></div></blockquote></div></div></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;=
http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\x3=
dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return tr=
ue;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%=
2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-=
iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a>)=
</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lbl=
.gov/" rel=3D"nofollow" target=3D"_blank" onmousedown=3D"this.href=3D&#39;h=
ttp://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD\x=
26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true;"=
 onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwa=
rewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcV=
gBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><div=
>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" rel=3D"nofollow" tar=
get=3D"_blank" onmousedown=3D"this.href=3D&#39;https://www.google.com/url?q=
\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3d=
AFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.href=
=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtze=
r\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;=
;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"f=
ont-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurt=
zer" style=3D"font-size:12.8px" rel=3D"nofollow" target=3D"_blank" onmoused=
own=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwitt=
er.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKs=
NsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.goo=
gle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x=
3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:/=
/<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div></=
div></div></div></div>
</div>
</div></div></blockquote></div></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"=
oxTpax1VAQAJ" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&=
#39;;return true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true=
;">singularity...@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div=
 dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurtzer</div=
><div>HPC Systems Architect and Technology Developer</div><div>Lawrence Ber=
keley National Laboratory HPCS<br>University of California Berkeley Researc=
h IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singularity.lb=
l.gov/" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;=
http://www.google.com/url?q\x3dhttp%3A%2F%2Fsingularity.lbl.gov%2F\x26sa\x3=
dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-iKsg1vSOOrRt58XtEQ&#39;;return tr=
ue;" onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%=
2Fsingularity.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHITKHVjde-=
iKsg1vSOOrRt58XtEQ&#39;;return true;">http://<wbr>singularity.lbl.gov/</a>)=
</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http://warewulf.lbl=
.gov/" target=3D"_blank" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;h=
ttp://www.google.com/url?q\x3dhttp%3A%2F%2Fwarewulf.lbl.gov%2F\x26sa\x3dD\x=
26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcVgBhWc77Jxww&#39;;return true;"=
 onclick=3D"this.href=3D&#39;http://www.google.com/url?q\x3dhttp%3A%2F%2Fwa=
rewulf.lbl.gov%2F\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNFPtSL1wiDx3C_BKcV=
gBhWc77Jxww&#39;;return true;">http://warewulf.<wbr>lbl.gov/</a>)</div><div=
>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer" target=3D"_blank" re=
l=3D"nofollow" onmousedown=3D"this.href=3D&#39;https://www.google.com/url?q=
\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3d=
AFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;;return true;" onclick=3D"this.href=
=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Fgithub.com%2Fgmkurtze=
r\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNHgwLrV-1wbChhxINJY_U3Xyjg2uw&#39;=
;return true;">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=3D"f=
ont-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/gmkurt=
zer" style=3D"font-size:12.8px" target=3D"_blank" rel=3D"nofollow" onmoused=
own=3D"this.href=3D&#39;https://www.google.com/url?q\x3dhttps%3A%2F%2Ftwitt=
er.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKs=
NsH_Zw5B_gRA&#39;;return true;" onclick=3D"this.href=3D&#39;https://www.goo=
gle.com/url?q\x3dhttps%3A%2F%2Ftwitter.com%2Fgmkurtzer\x26sa\x3dD\x26sntz\x=
3d1\x26usg\x3dAFQjCNGiphjH-YHVVhLsKsNsH_Zw5B_gRA&#39;;return true;">https:/=
/<wbr>twitter.com/gmkurtzer</a></div></div></div></div></div></div></div></=
div></div></div></div>
</div>
</blockquote></div></div>
------=_Part_1950_1519043551.1488897493834--

------=_Part_1949_535978342.1488897493833--
