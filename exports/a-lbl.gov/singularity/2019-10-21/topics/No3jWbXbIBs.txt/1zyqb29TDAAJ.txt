X-Received: by 10.157.16.10 with SMTP id h10mr11516472ote.95.1487839894389;
        Thu, 23 Feb 2017 00:51:34 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.3.1 with SMTP id e1ls971813ite.16.gmail; Thu, 23 Feb 2017
 00:51:33 -0800 (PST)
X-Received: by 10.98.113.9 with SMTP id m9mr44438123pfc.121.1487839893538;
        Thu, 23 Feb 2017 00:51:33 -0800 (PST)
Return-Path: <rem...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id o6si3742051pfb.173.2017.02.23.00.51.33
        for <singu...@lbl.gov>;
        Thu, 23 Feb 2017 00:51:33 -0800 (PST)
Received-SPF: pass (google.com: domain of rem...@gmail.com designates 209.85.218.44 as permitted sender) client-ip=209.85.218.44;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of rem...@gmail.com designates 209.85.218.44 as permitted sender) smtp.mailfrom=rem...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2H4AgBcoa5YhizaVdFdHAEBBAEBCgEBFwEBBAEBCgEBgkOBQ4EJB4NMCJtigmSPcYJfgUobKB8BBoFzQxCBXIFaAoMOB0AXAQEBAQEBAQEBAQECEAEBAQgLCwodL4IqCQQCAxkFBAQ9CgMDAQEBAQEBAQEBAgEBAQEBAQEBAQEBAQIBAQECAQEBAgEBAQEBAwEBAQEBAQEBAQEBAQEBAQEaAg0eBA8DKQEBAQMBIx0BDQ4eAwELBgMCCw0qAgIhAQEOAwEFAQsRDgcEAQcMBwIEh2cDgQhJAQMNCAWQV5EWP4wDggQFARyDCQWDYgoZJw1VgykBCgEBAQEBAQEYAgYSiymCUYFVEAIBSIJZgl8FgSsBh21hi26FcTgCgTqFOocQhB+CToEHjTuKQocdFB6BFQ8RAXMNLwg1H1MXTYMuKgwDEQyBYj81B4lYgWcBAQE
X-IronPort-AV: E=Sophos;i="5.35,197,1484035200"; 
   d="scan'208,217";a="65612870"
Received: from mail-oi0-f44.google.com ([209.85.218.44])
  by fe3.lbl.gov with ESMTP; 23 Feb 2017 00:51:31 -0800
Received: by mail-oi0-f44.google.com with SMTP id 65so14393611oig.1
        for <singu...@lbl.gov>; Thu, 23 Feb 2017 00:51:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=rjoIWR5bLAbOEu5MQE2xPPMQY3uWFkX/dh99UXm+MKc=;
        b=JiIk4LTt6eL1IgAaEV3n7ssycs/ZzYNcO2QGxkyjamGb/SVVAXBY78PfDswKivvcNy
         CEVPv31nCeT27j+tkm74n+vkCQHMujyB1J3fo/jM10onuczwsGIJxm0jqdW76E/CwQch
         l/bDBpQKm/bt2+I+WkLy2Ss9dbTUT+hJlrhWosfeRiho233nD2mpGDNDuW8ThCAdONnW
         k6QRluuWKKoIvqfFbSszr4R+oneI9Iaxxe6ynmWXotq4AE7Y/plZ6JDXurSC1TiQFUux
         2i4mEC7oMO1tKxE+ygSNSG68bqK2ZQlik7rezwvmXW16QgGxCDl4DRgTssJqJwfAYEkQ
         dKIw==
X-Gm-Message-State: AMke39miTEQJydfiDdNK4H79umRreLvLde2dirV1rC+sfCE4uarvl5b3zUcFYGkx0WXnOCv8ef3WrkqCtHf4KQ==
X-Received: by 10.202.3.197 with SMTP id 188mr19992124oid.31.1487839890373;
 Thu, 23 Feb 2017 00:51:30 -0800 (PST)
MIME-Version: 1.0
Received: by 10.183.1.3 with HTTP; Thu, 23 Feb 2017 00:51:29 -0800 (PST)
In-Reply-To: <CABWwhHr9Z_h+nQM2SFq8Mhq=S1x+K9zCB74iPAn9tGdZKBUHNQ@mail.gmail.com>
References: <d0a10fdc-f912-4e9c-8681-a54f5d53fd72@lbl.gov> <CAN7etTxxeYYxY7aB93H7E686y-8Qru-c_H3t1ANNQ_4oE1C-aA@mail.gmail.com>
 <887cfc8c-48ed-4720-8040-989e407f4203@lbl.gov> <CAN7etTwgGhbKjQw3EpWXMQN7jcQURt2tTHhWZn3FgZjij_=GDA@mail.gmail.com>
 <CABWwhHr9Z_h+nQM2SFq8Mhq=S1x+K9zCB74iPAn9tGdZKBUHNQ@mail.gmail.com>
From: =?UTF-8?B?UsOpbXkgRGVybmF0?= <rem...@gmail.com>
Date: Thu, 23 Feb 2017 09:51:29 +0100
Message-ID: <CAA6Bz=fgxCcGtxQz7wVTTqgq_nyuLjNeuu2fOKMhJ3HTR6k5_g@mail.gmail.com>
Subject: Re: [Singularity] Performance impact: My experience
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a113b720626038f05492eb8b0

--001a113b720626038f05492eb8b0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

You can try to copy your libraries directly from the host to the container
instead of a classical install ? I do not know what is the best way... But
if you check woth solutions we will have an answer.
BTW, I am very interesting by your containers. Could you share it (through
singularity-hub ?) with your command line to run the benchmark in the
"singularity" file inside the container ? Indeed, I will do some
performance tests also...

Best regards,
R=C3=A9my

2017-02-23 0:58 GMT+01:00 Chihsong <chihs...@gmail.com>:

> Hi Greg,
>
> The problem is that I don't feel I have IB libraries inside my container.
> How can I check that? Or did you simply install the ofed into the
> container?
>
> Chiu-Song
>
> On Thursday, February 23, 2017, Gregory M. Kurtzer <gmku...@lbl.gov>
> wrote:
>
>> Hi Chih-Song,
>>
>> Haha, every now and then I get lucky when a new mail comes in and is at
>> the top of my mbox and I have a moment.
>>
>> In summary, yes. The MPI inside the container must also link against the
>> IB libraries (also within the container).
>>
>> Hopefully that helps!
>>
>> On Wed, Feb 22, 2017 at 10:58 AM, Chih-Song Kuo <chihs...@gmail.com>
>> wrote:
>>
>>> Hi Greg,
>>>
>>> That reply was very prompt! Anyways, my answer follows.
>>>
>>> > * Make sure the MPI inside the container is properly linking against
>>> the IB libraries
>>> Did that mean that I need to install IB libraries (like Mellanox OFED)
>>> into the container? I guess not?
>>> But apparently openib is missing in the container.
>>> [me@cn03 ompi-rhel6-host]$ ompi_info | grep openib
>>>                  MCA btl: openib (MCA v2.1.0, API v3.0.0, Component
>>> v3.0.0)
>>>
>>> Singularity.container-centos6-demo.img> ompi_info | grep openib
>>> Singularity.container-centos6-demo.img>
>>> Singularity.container-centos6-demo.img> ls /etc/infiniband/openib.conf
>>> ls: cannot access /etc/infiniband/openib.conf: No such file or director=
y
>>>
>>> > * Make sure that the MPI configuration is configured to use MPI
>>> I think you meant "to use IB" instead.
>>> But still, did you mean that OpenMPI should be "configured"
>>> "--with-verbs"? (Did you do so or you never had my problem?)
>>> I did not use this flag when compiling Open MPI either on the host or i=
n
>>> the container.
>>> > * Make sure you have Singularity configured properly to share the
>>> devices properly, tmp, and you are *NOT* using the IPC or PID namespace=
s
>>> Can you provide more hint on how that can be done?
>>>
>>> Thank you once again for your time.
>>> Chih-Song
>>>
>>> On Wednesday, February 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer
>>> wrote:
>>>>
>>>> There are various things that *could* go wrong, and usage of container=
s
>>>> (any of the technologies) actually introduce complexities in kernel an=
d
>>>> user space alignment that normally we don't consider. For that reason,=
 my
>>>> first suspicion is that your IB fabric is not being properly utilized =
by
>>>> the MPI within the container. That could be due to anything from build
>>>> errors within the container to IB library/kmod API misalignment.
>>>>
>>>> Things I would look at and check:
>>>>
>>>> * Make sure the MPI inside the container is properly linking against
>>>> the IB libraries
>>>> * Make sure that the IB libraries inside the container are compatible
>>>> with the host kernel
>>>> * Make sure that the MPI configuration is configured to use MPI
>>>> * Make sure you have Singularity configured properly to share the
>>>> devices properly, tmp, and you are *NOT* using the IPC or PID namespac=
es
>>>>
>>>> Hope that helps.
>>>>
>>>> Greg
>>>>
>>>> On Wed, Feb 22, 2017 at 10:19 AM, Chih-Song Kuo <chi...@gmail.com>
>>>> wrote:
>>>>
>>>>> Hello,
>>>>>
>>>>> This is again Chih-Song from Fujitsu. I decided to make another post
>>>>> to share my experience of performance impact with two kernel benchmar=
ks:
>>>>> High Performance Linpack (HPL) and the OSU MPI benchmark suit.
>>>>>
>>>>> Overall, there was no noticeable performance difference for benchmark=
s
>>>>> running on a single node. But for benchmarks running across nodes, I =
did
>>>>> observe some difference, which was against to the claim of Singularit=
y.
>>>>> Have anybody done any similar exercise? What are your findings? Can y=
ou
>>>>> suspect whether I was doing anything wrong?
>>>>>
>>>>> Host configuration:
>>>>> 2x Intel E5-2680v2 (Ivybridge)
>>>>> 64GB memory
>>>>> Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switc=
h)
>>>>> RHEL 6.7
>>>>> OpenMPI development master branch (8.2.17)
>>>>> Intel MKL 2017.0 community edition
>>>>> gcc 4.4
>>>>>
>>>>> Container:
>>>>> Centos 6 and 7 both tested without noticeable performance difference
>>>>> OpenMPI development master branch (8.2.17)
>>>>> Intel MKL 2017.0 community edition
>>>>> gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)
>>>>>
>>>>> Benchmarks:
>>>>> 1. LINPACK 2.2
>>>>> 2. OSU 5.3.2
>>>>>
>>>>> <LINPACK>
>>>>> Single node. N=3D40000, P=3D5, Q=3D4
>>>>> Container: 368 GFlops
>>>>> Host: 368 GFLOPS
>>>>> #A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x 8
>>>>> * 2.8 =3D 448 GFlops
>>>>> Efficiency =3D 368 / 448 =3D 82%. Not bad (given the small N (matrix =
size)
>>>>> and that gcc instead of icc was used, and the executable was dynamica=
lly
>>>>> linked -- by purpose)
>>>>>
>>>>> Dual-node, N=3D60000, P=3D8, Q=3D5
>>>>> Container: 702 GFLOPS
>>>>> Host:737 GFLOPS
>>>>> There is roughly 5% of performance degradation with the container.
>>>>>
>>>>> <OSU-P2P-Bandwidth>
>>>>> The container only saw 50-65% of the total bandwidth.
>>>>>
>>>>> Container:
>>>>> Msg size(bytes) BW (MB/s)
>>>>> 65536                2142.28
>>>>> 131072               2363.45
>>>>> 262144               1705.79
>>>>> 524288               1592.56
>>>>> 1048576              1721.88
>>>>> 2097152              1557.42
>>>>> 4194304              1655.90
>>>>>
>>>>> Host:
>>>>> Msg size(bytes) BW (MB/s)
>>>>> 65536                3722.32
>>>>> 131072               3751.33
>>>>> 262144               3771.13
>>>>> 524288               3774.33
>>>>> 1048576              3781.43
>>>>> 2097152              3775.00
>>>>> 4194304              3773.68
>>>>>
>>>>> <OSU-P2P-Latency>
>>>>> Here the container was significantly slower.
>>>>>
>>>>> Container:
>>>>> Msg size(bytes)  Latency (us)
>>>>> 0                      31.59
>>>>> 1                      31.86
>>>>> 2                      31.83
>>>>>
>>>>> Host:
>>>>> Msg size(bytes)  Latency (us)
>>>>> 0                       1.55
>>>>> 1                       1.63
>>>>> 2                       1.63
>>>>>
>>>>> Note 1: Run-to-run variation of performance was much smaller than the
>>>>> difference on the host and in the container.
>>>>> Note 2: When Singularity was used, I could not instruct mpirun to use
>>>>> the ofed by specifying "--mca btl openib,self,vader" in the mpirun
>>>>> parameter list. Doing so would give me an error message stating that =
the
>>>>> openib component is missing. However, from the bandwidth measured abo=
ve,
>>>>> the container did seem to be able to use InfiniBand, otherwise the
>>>>> bandwidth would not be so high (the nodes only had InfiniBand and 1G
>>>>> Ethernet). Maybe container was using IPoIB? I did not check that yet.
>>>>>
>>>>> Reference: How the benchmarks were executed:
>>>>> mpirun -n 20 -hostfile hostfile singularity exec
>>>>> /home/chih/containers/container-centos6-demo.img xhpl
>>>>> mpirun -n 2 -hostfile hostfile singularity exec
>>>>> /home/chih/containers/container-centos6-demo.img osu_bw
>>>>> mpirun -n 2 -hostfile hostfile singularity exec
>>>>> /home/chih/containers/container-centos6-demo.img osu_latency
>>>>>
>>>>> Chih-Song
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gregory M. Kurtzer
>>>> HPC Systems Architect and Technology Developer
>>>> Lawrence Berkeley National Laboratory HPCS
>>>> University of California Berkeley Research IT
>>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>>> er.com/gmkurtzer
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>>
>> --
>> Gregory M. Kurtzer
>> HPC Systems Architect and Technology Developer
>> Lawrence Berkeley National Laboratory HPCS
>> University of California Berkeley Research IT
>> Singularity Linux Containers (http://singularity.lbl.gov/)
>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>> er.com/gmkurtzer
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
> --
> Chih-Song Kuo =E9=83=AD=E7=9F=A5=E9=A0=8C
> Senior Sales Consultant - HPC Benchmark Specialist at Fujitsu
> M.Sc. RWTH with distinction in Software Systems Engineering with HPC focu=
s
> B.Sc. NTHU in Computer Science, B.S.M. NTHU in Quantitative Finance
> Tel:  +49-177-88949-28 <+49%20177%208894928>; +49-241-88949-155
> <+49%20241%2088949155>; +886-2-26629518 <+886%202%202662%209518>
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a113b720626038f05492eb8b0
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi,<div><br></div><div>You can try to copy your libraries=
=C2=A0directly=C2=A0from the host to the container instead of a classical i=
nstall ? I do not know what is the best way... But if you check woth soluti=
ons we will have an answer.</div><div>BTW, I am very interesting by your co=
ntainers. Could you share it (through singularity-hub ?) with your command =
line to run the benchmark in the &quot;singularity&quot; file inside the co=
ntainer ? Indeed, I will do some performance tests also...</div><div><br></=
div><div>Best regards,</div><div>R=C3=A9my</div></div><div class=3D"gmail_e=
xtra"><br><div class=3D"gmail_quote">2017-02-23 0:58 GMT+01:00 Chihsong <sp=
an dir=3D"ltr">&lt;<a href=3D"mailto:chihs...@gmail.com" target=3D"_blank">=
chihs...@gmail.com</a>&gt;</span>:<br><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">Hi Gre=
g,<div><br></div><div>The problem is that I=C2=A0don&#39;t feel I=C2=A0have=
 IB libraries inside my container. How can I=C2=A0check that? Or did you si=
mply=C2=A0install the ofed into the container?=C2=A0</div><div><br>Chiu-Son=
g</div><div class=3D"HOEnZb"><div class=3D"h5"><div><br>On Thursday, Februa=
ry 23, 2017, Gregory M. Kurtzer &lt;<a href=3D"mailto:gmku...@lbl.gov" targ=
et=3D"_blank">gmku...@lbl.gov</a>&gt; wrote:<br><blockquote class=3D"gmail_=
quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1=
ex"><div dir=3D"ltr">Hi Chih-Song,<div><br></div><div>Haha, every now and t=
hen I get lucky when a new mail comes in and is at the top of my mbox and I=
 have a moment.</div><div><br></div><div>In summary, yes. The MPI inside th=
e container must also link against the IB libraries (also within the contai=
ner).</div><div><br></div><div>Hopefully that helps!</div></div><div class=
=3D"gmail_extra"><br><div class=3D"gmail_quote">On Wed, Feb 22, 2017 at 10:=
58 AM, Chih-Song Kuo <span dir=3D"ltr">&lt;<a>chihs...@gmail.com</a>&gt;</s=
pan> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex=
;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Greg,<br>=
<br>That reply was very prompt! Anyways, my answer follows.<span><br><br>&g=
t; * Make sure the MPI inside the container is properly linking against the=
 IB libraries<br></span>Did that mean that I need to install IB libraries (=
like Mellanox OFED) into the container? I guess not?<br>But apparently open=
ib is missing in the container.<br>[me@cn03 ompi-rhel6-host]$ ompi_info | g=
rep openib<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 MCA btl: openib (MCA v2.1.0, API v3.0.=
0, Component v3.0.0)<br><br>Singularity.container-centos6-<wbr>demo.img&gt;=
 ompi_info | grep openib<br>Singularity.container-centos6-<wbr>demo.img&gt;=
<br>Singularity.container-centos6-<wbr>demo.img&gt; ls /etc/infiniband/open=
ib.conf<br>ls: cannot access /etc/infiniband/openib.conf: No such file or d=
irectory<span><br><br>&gt; * Make sure that the MPI configuration is config=
ured to use MPI<br></span>I think you meant &quot;to use IB&quot; instead. =
<br>But still, did you mean that OpenMPI should be &quot;configured&quot; &=
quot;--with-verbs&quot;? (Did you do so or you never had my problem?)<br>I =
did not use this flag when compiling Open MPI either on the host or in the =
container.<span><br>&gt; * Make sure you have Singularity configured proper=
ly to share the=20
devices properly, tmp, and you are *NOT* using the IPC or PID namespaces<br=
></span>Can you provide more hint on how that can be done?<br><br>Thank you=
 once again for your time.<br>Chih-Song<span><br><br>On Wednesday, February=
 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer wrote:</span><blockquote =
class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #=
ccc solid;padding-left:1ex"><span><div dir=3D"ltr">There are various things=
 that *could* go wrong, and usage of containers (any of the technologies) a=
ctually introduce complexities in kernel and user space alignment that norm=
ally we don&#39;t consider. For that reason, my first suspicion is that you=
r IB fabric is not being properly utilized by the MPI within the container.=
 That could be due to anything from build errors within the container to IB=
 library/kmod API misalignment.<div><br></div><div>Things I would look at a=
nd check:</div><div><br></div><div>* Make sure the MPI inside the container=
 is properly linking against the IB libraries</div><div>* Make sure that th=
e IB libraries inside the container are compatible with the host kernel</di=
v><div>* Make sure that the MPI configuration is configured to use MPI</div=
><div>* Make sure you have Singularity configured properly to share the dev=
ices properly, tmp, and you are *NOT* using the IPC or PID namespaces</div>=
<div><br></div><div>Hope that helps.</div><div><br></div><div>Greg</div></d=
iv></span><div><br><div class=3D"gmail_quote"><div><div>On Wed, Feb 22, 201=
7 at 10:19 AM, Chih-Song Kuo <span dir=3D"ltr">&lt;<a rel=3D"nofollow">chi.=
..@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class=3D"gmai=
l_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left=
:1ex"><div><div><div dir=3D"ltr">Hello,<br><br>This is again Chih-Song from=
 Fujitsu. I decided to make another post to share my experience of performa=
nce impact with two kernel benchmarks: High Performance Linpack (HPL) and t=
he OSU MPI benchmark suit.<br><br>Overall, there was no noticeable performa=
nce difference for benchmarks running on a single node. But for benchmarks =
running across nodes, I did observe some difference, which was against to t=
he claim of Singularity. Have anybody done any similar exercise? What are y=
our findings? Can you suspect whether I was doing anything wrong?<br><br>Ho=
st configuration:<br>2x Intel E5-2680v2 (Ivybridge)<br>64GB memory<br>Mella=
nox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)<br>RHEL =
6.7<br>OpenMPI development master branch (8.2.17)<br>Intel MKL 2017.0 commu=
nity edition<br>gcc 4.4<br><br>Container:<br>Centos 6 and 7 both tested wit=
hout noticeable performance difference<br>OpenMPI development master branch=
 (8.2.17)<br>Intel MKL 2017.0 community edition<br>gcc 4.4.7 (Centos-6), gc=
c 4.8.5 (Centos-7)<br><br>Benchmarks:<br>1. LINPACK 2.2<br>2. OSU 5.3.2<br>=
<br>&lt;LINPACK&gt;<br>Single node. N=3D40000, P=3D5, Q=3D4<br>Container: 3=
68 GFlops<br>Host: 368 GFLOPS<br>#A single node has 2x Intel E5-2680v2. So =
we are expecting 2 x 10 x 8 * 2.8 =3D 448 GFlops<br>Efficiency =3D 368 / 44=
8 =3D 82%. Not bad (given the small N (matrix size) and that gcc instead of=
 icc was used, and the executable was dynamically linked -- by purpose)<br>=
<br>Dual-node, N=3D60000, P=3D8, Q=3D5<br>Container: 702 GFLOPS<br>Host:737=
 GFLOPS<br>There is roughly 5% of performance degradation with the containe=
r.<br><br>&lt;OSU-P2P-Bandwidth&gt;<br>The container only saw 50-65% of the=
 total bandwidth.<br><br>Container: <br>Msg size(bytes) BW (MB/s)<br>65536=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 2142.28<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2363.45<br>262144=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1705.=
79<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 1592.56<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1721.88<br>2097152=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1557.=
42<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1655.90<br><br>Host:<br>Msg size(bytes) BW (MB/s)<br>655=
36=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 3722.32<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
71.13<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 3774.33<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
75.00<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 3773.68<br><br>&lt;OSU-P2P-Latency&gt;<br>Here the co=
ntainer was significantly slower.<br><br>Container: <br>Msg size(bytes)=C2=
=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31=
.59<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.86<br>2=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.83<br><br>Host:<br>Msg =
size(bytes)=C2=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1.55<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 1.63<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 1.63<br><br>Note 1: Run-to-run variation of performance was much sma=
ller than the difference on the host and in the container.<br>Note 2: When =
Singularity was used, I could not instruct mpirun to use the ofed by specif=
ying &quot;--mca btl openib,self,vader&quot; in the mpirun parameter list. =
Doing so would give me an error message stating that the openib component i=
s missing. However, from the bandwidth measured above, the container did se=
em to be able to use InfiniBand, otherwise the bandwidth would not be so hi=
gh (the nodes only had InfiniBand and 1G Ethernet). Maybe container was usi=
ng IPoIB? I did not check that yet. <br><br>Reference: How the benchmarks w=
ere executed:<br>mpirun -n 20 -hostfile hostfile singularity exec /home/chi=
h/containers/containe<wbr>r-centos6-demo.img xhpl<br>mpirun -n 2 -hostfile =
hostfile singularity exec /home/chih/containers/containe<wbr>r-centos6-demo=
.img osu_bw<br>mpirun -n 2 -hostfile hostfile singularity exec /home/chih/c=
ontainers/containe<wbr>r-centos6-demo.img osu_latency<span><font color=3D"#=
888888"><br><br>Chih-Song<br><br></font></span></div></div></div><span><fon=
t color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><=
div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurt=
zer</div><div>HPC Systems Architect and Technology Developer</div><div>Lawr=
ence Berkeley National Laboratory HPCS<br>University of California Berkeley=
 Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singul=
arity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.=
lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http:/=
/warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<w=
br>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtz=
er" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a=
>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"ht=
tps://twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" ta=
rget=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></d=
iv></div></div></div></div></div></div></div>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D"l=
tr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Systems Architect and Techno=
logy Developer</div><div>Lawrence Berkeley National Laboratory HPCS<br>Univ=
ersity of California Berkeley Research IT<br>Singularity Linux Containers=
=C2=A0(<a href=3D"http://singularity.lbl.gov/" target=3D"_blank">http://sin=
gularity<wbr>.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a =
href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http://warewulf.lb<wbr>=
l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer"=
 target=3D"_blank">https://github.com/gmk<wbr>urtzer</a>,=C2=A0<span style=
=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/g=
mkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://twitt<wbr>er=
.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div></d=
iv></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a>singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
</blockquote></div><br><br></div></div><span class=3D"HOEnZb"><font color=
=3D"#888888">-- <br><div dir=3D"ltr"><span><div><div dir=3D"ltr"><div>Chih-=
Song Kuo =E9=83=AD=E7=9F=A5=E9=A0=8C<br>Senior Sales Consultant - HPC Bench=
mark Specialist at Fujitsu<br>M.Sc. RWTH with distinction in Software Syste=
ms Engineering with HPC focus<br>B.Sc. NTHU in Computer Science, B.S.M. NTH=
U in Quantitative Finance<br>Tel: =C2=A0<a href=3D"tel:+49%20177%208894928"=
 value=3D"+491778894928" target=3D"_blank">+49-177-88949-28</a>; <a href=3D=
"tel:+49%20241%2088949155" value=3D"+4924188949155" target=3D"_blank">+49-2=
41-88949-155</a>; <a href=3D"tel:+886%202%202662%209518" value=3D"+88622662=
9518" target=3D"_blank">+886-2-26629518</a></div></div></div></span></div><=
/font></span><div class=3D"HOEnZb"><div class=3D"h5"><br>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--001a113b720626038f05492eb8b0--
