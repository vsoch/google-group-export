Date: Fri, 20 Jan 2017 00:29:57 -0800 (PST)
From: Stefan Kombrink <stefan....@googlemail.com>
To: singularity <singu...@lbl.gov>
Message-Id: <2ef68eab-3140-4008-95e9-3ebd8cbab9af@lbl.gov>
In-Reply-To: <CAN7etTxw0f+oX1nNOQPD20Dzi-hpM1gLgfujm29d+CABhqBV-w@mail.gmail.com>
References: <8a569f24-bd38-4bf0-b025-5843a1525141@lbl.gov> <CAN7etTz5bYr-TTXJibNH+5qii9z_7pKpU_HXyR-SyNLiLRQk-A@mail.gmail.com>
 <3589df38-4328-41e6-9c55-ad51fbc8271d@lbl.gov>
 <CAN7etTxw0f+oX1nNOQPD20Dzi-hpM1gLgfujm29d+CABhqBV-w@mail.gmail.com>
Subject: Re: [Singularity] Survey: MPI Hybrid mode vs MPI native?
MIME-Version: 1.0
Content-Type: multipart/mixed; 
	boundary="----=_Part_428_1107049693.1484900997164"

------=_Part_428_1107049693.1484900997164
Content-Type: multipart/alternative; 
	boundary="----=_Part_429_1918141684.1484900997164"

------=_Part_429_1918141684.1484900997164
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit



Am Mittwoch, 18. Januar 2017 17:55:01 UTC+1 schrieb Gregory M. Kurtzer:
>
>
>
> On Wed, Jan 18, 2017 at 3:00 AM, 'Stefan Kombrink' via singularity <
> si...@lbl.gov <javascript:>> wrote:
>
>> Hi Greg,
>>
>>  thanks for your reply.
>>
>> Am Mittwoch, 18. Januar 2017 04:51:46 UTC+1 schrieb Gregory M. Kurtzer:
>>>
>>>
>>>
>>> On Tue, Jan 10, 2017 at 12:25 AM, 'Stefan Kombrink' via singularity <
>>> si...@lbl.gov> wrote:
>>>
>>>> Happy new year, dear Singularity-community,
>>>>
>>>>  I am asking those of you using the MPI Hybrid mode with OpenMPI 2 for 
>>>> sharing your experience.
>>>> What does the hybrid approach do for you that cannot be done by having 
>>>> MPI within the container?
>>>>
>>>
>>> A few that come up quick for me are:
>>>
>>> * if MPI is inside container jobs are typically limited to single node 
>>> runs
>>> * resource managers can not communicate reasonably to MPI inside 
>>> container (e.g. no coupling of the MPI and RM)
>>> * In this model, MPI would try to interface with ssh within the 
>>> container, and thus expect sshd running within the other containers (which 
>>> is opening pandora's box on traditional HPC)
>>>
>>
>> That is how I currently run my experimental container jobs. I contain the 
>> originally installed MPI and bind mount /var/lib/torque.
>> The most tricky part was to replace ssh inside the container with a 
>> wrapper which launches the passed command inside another instance of the 
>> origin container.
>>
>
> I am very curious what you did here. Can you elaborate on this?
>


I've created containers based on our well-established module software stack 
and selected two popular chemistry simulation packages:

1) VASP + Intel MPI + Intel MKL
2) DaCapo + ASE + OpenMPI 1.8 build with Intel 2014

My intent was to containerize these apps in such way that these containers 
would behave and run equally on our HPC cluster while still being able to 
operate in an OpenStack VM.

The procedure looks as follows:
1) Create OS clone container: using singularity I bootstrap from 
docker://centos:7.2 a container with the necessary bind mount points and a 
duplicated package lists of the original RHEL system. 
   I rip it off most of the vendor-specific software and take only selected 
configs which are site-specific.
2) Pack the necessary directory tree of the required SW modules into a tar 
ball, run singularity import
3) mv /bin/ssh /bin/ssh-orig ; replace /bin/ssh by a wrapper which 
effectively calls:

/usr/bin/ssh_orig $_PARAM $_HOST /usr/bin/singularity exec -B /etc/ssh -B 
/scratch -B /var/spool/torque $SINGULARITY_IMAGE $_COMMAND

I found that idea for docker containers on 
https://github.com/ambu50/docker-ib-mpi a while ago, and it seems to work 
just fine for these two containers.
99% of our multinode jobs invoke their hostlist via mpirun calls and I've 
heard that there exists other ways to fork jobs on other nodes, so it might 
well fail in those cases!

I'd be interested to hear how you and others think of the stability of this 
approach as I am just testing in an artificially small subset on our 
production nodes.

greets 
Stefan 

------=_Part_429_1918141684.1484900997164
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><br><br>Am Mittwoch, 18. Januar 2017 17:55:01 UTC+1 schrie=
b Gregory M. Kurtzer:<blockquote class=3D"gmail_quote" style=3D"margin: 0;m=
argin-left: 0.8ex;border-left: 1px #ccc solid;padding-left: 1ex;"><div dir=
=3D"ltr"><br><div><br><div class=3D"gmail_quote">On Wed, Jan 18, 2017 at 3:=
00 AM, &#39;Stefan Kombrink&#39; via singularity <span dir=3D"ltr">&lt;<a h=
ref=3D"javascript:" target=3D"_blank" gdf-obfuscated-mailto=3D"CYTp7yfECAAJ=
" rel=3D"nofollow" onmousedown=3D"this.href=3D&#39;javascript:&#39;;return =
true;" onclick=3D"this.href=3D&#39;javascript:&#39;;return true;">si...@lbl=
.gov</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"lt=
r">Hi Greg,<br><br>=C2=A0thanks for your reply.<br><br>Am Mittwoch, 18. Jan=
uar 2017 04:51:46 UTC+1 schrieb Gregory M. Kurtzer:<span><blockquote class=
=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div dir=3D"ltr"><br><div><br><div class=3D"gmail_qu=
ote">On Tue, Jan 10, 2017 at 12:25 AM, &#39;Stefan Kombrink&#39; via singul=
arity <span dir=3D"ltr">&lt;<a rel=3D"nofollow">si...@lbl.gov</a>&gt;</span=
> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bo=
rder-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Happy new year,=
 dear Singularity-community,<br><br>=C2=A0I am asking those of you using th=
e MPI Hybrid mode with OpenMPI 2 for sharing your experience.<br>What does =
the hybrid approach do for you that cannot be done by having MPI within the=
 container?<br></div></blockquote><div><br></div><div>A few that come up qu=
ick for me are:</div><div><br></div><div>* if MPI is inside container jobs =
are typically limited to single node runs</div><div>* resource managers can=
 not communicate reasonably to MPI inside container (e.g. no coupling of th=
e MPI and RM)</div><div>* In this model, MPI would try to interface with ss=
h within the container, and thus expect sshd running within the other conta=
iners (which is opening pandora&#39;s box on traditional HPC)</div></div></=
div></div></blockquote></span><div><br>That is how I currently run my exper=
imental container jobs. I contain the originally installed MPI and bind mou=
nt /var/lib/torque.<br>The most tricky part was to replace ssh inside the c=
ontainer with a wrapper which launches the passed command inside another in=
stance of the origin container.<br></div></div></blockquote><div><br></div>=
<div>I am very curious what you did here. Can you elaborate on this?</div><=
/div></div></div></blockquote><div><br><br>I&#39;ve created containers base=
d on our well-established module software stack and selected two popular ch=
emistry simulation packages:<br><br>1) VASP + Intel MPI + Intel MKL<br>2) D=
aCapo + ASE + OpenMPI 1.8 build with Intel 2014<br><br>My intent was to con=
tainerize these apps in such way that these containers would behave and run=
 equally on our HPC cluster while still being able to operate in an OpenSta=
ck VM.<br><br>The procedure looks as follows:<br>1) Create OS clone contain=
er: using singularity I bootstrap from docker://centos:7.2 a container with=
 the necessary bind mount points and a duplicated package lists of the orig=
inal RHEL system. <br>=C2=A0=C2=A0 I rip it off most of the vendor-specific=
 software and take only selected configs which are site-specific.<br>2) Pac=
k the necessary directory tree of the required SW modules into a tar ball, =
run singularity import<br>3) mv /bin/ssh /bin/ssh-orig ; replace /bin/ssh b=
y a wrapper which effectively calls:<br><br>/usr/bin/ssh_orig $_PARAM $_HOS=
T /usr/bin/singularity exec -B /etc/ssh -B /scratch -B /var/spool/torque $S=
INGULARITY_IMAGE $_COMMAND<br><br>I found that idea for docker containers o=
n https://github.com/ambu50/docker-ib-mpi a while ago, and it seems to work=
 just fine for these two containers.<br>99% of our multinode jobs invoke th=
eir hostlist via mpirun calls and I&#39;ve heard that there exists other wa=
ys to fork jobs on other nodes, so it might well fail in those cases!<br><b=
r>I&#39;d be interested to hear how you and others think of the stability o=
f this approach as I am just testing in an artificially small subset on our=
 production nodes.<br><br>greets <br>Stefan <br></div></div>
------=_Part_429_1918141684.1484900997164--

------=_Part_428_1107049693.1484900997164--
