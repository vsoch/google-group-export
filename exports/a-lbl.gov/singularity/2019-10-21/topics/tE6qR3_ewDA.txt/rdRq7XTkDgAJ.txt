X-Received: by 10.66.144.138 with SMTP id sm10mr62152391pab.4.1451967012169;
        Mon, 04 Jan 2016 20:10:12 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.107.62.86 with SMTP id l83ls3642108ioa.54.gmail; Mon, 04 Jan
 2016 20:10:11 -0800 (PST)
X-Received: by 10.66.55.66 with SMTP id q2mr64850208pap.120.1451967011521;
        Mon, 04 Jan 2016 20:10:11 -0800 (PST)
Return-Path: <rhc.o...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id w17si52939371pfa.96.2016.01.04.20.10.11
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 04 Jan 2016 20:10:11 -0800 (PST)
Received-SPF: pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.51 as permitted sender) client-ip=209.85.220.51;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of rhc.o...@gmail.com designates 209.85.220.51 as permitted sender) smtp.mailfrom=rhc.o...@gmail.com;
       dkim=pass head...@gmail.com
X-Ironport-SBRS: 4.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EYAgDrQItWlDPcVdFUCoR5gyiFMaBQAQaVExsBhxU7EQEBAQEBAQEDDgEBAQEHCwsJHzCCYjwBAQEBAQEjAitAAQEBAwESCAkdAQ0eAggEAwELAQUFGCAHAwICMQMBBQELAw4OAgUEARoCBAGHeAMKCAWiU4ExPjGLSIRjiG8nDYMAGwEFDoYABgWCTAiCaIQmBgsBBoM1LoEbBY4whFqDfIgyhnyHPA6FVIpLgic1gRc4ZoEcOxyBflEHgz4JFwSBJwEBAQ
X-IronPort-AV: E=Sophos;i="5.20,523,1444719600"; 
   d="scan'208,217";a="10066692"
Received: from mail-pa0-f51.google.com ([209.85.220.51])
  by fe3.lbl.gov with ESMTP; 04 Jan 2016 20:10:09 -0800
Received: by mail-pa0-f51.google.com with SMTP id uo6so188414828pac.1
        for <singu...@lbl.gov>; Mon, 04 Jan 2016 20:10:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=sender:from:content-type:message-id:mime-version:subject:date
         :references:to:in-reply-to;
        bh=dmXTuPYpqTKduwrv0Ax1zOVAyN5N4ieT4l4AXAlKlzo=;
        b=n7Y5NoDjcP9DWBEGaXj3Aq7LWZsYaJqTt4pMpm2BZncs5oldxuKKfoUYMyAToCSFLW
         MrfcZY+R4qmXFCJ+CMIgQpdbHSC495wubsC6MoQuastPkFOy/zuuA7Wg5/tNNwpyx6M0
         yd+I8lMESxdFfCKkAwMgdmkx2oG3RPU0mdBAJgFrbgtz+pri93nwGPpOBTuL75lG2YDj
         1HD9fuY7TQK7zg0eqSC1JjsOJhdMXRsmyZz1N2tTLCrqP9apdu74w5Hk0zZbefItEXna
         +7UCUVRmQF/GzecWgvXrUHLlWnFxEp3df2VIFwPxe+NET/HlpxGMyUZDxBUWRKP2AW5D
         K8ow==
X-Received: by 10.66.190.66 with SMTP id go2mr133238412pac.114.1451967009535;
        Mon, 04 Jan 2016 20:10:09 -0800 (PST)
Return-Path: <rhc.o...@gmail.com>
Received: from [192.168.0.8] ([208.100.172.177])
        by smtp.gmail.com with ESMTPSA id v11sm76359607pfa.81.2016.01.04.20.10.07
        for <singu...@lbl.gov>
        (version=TLSv1/SSLv3 cipher=OTHER);
        Mon, 04 Jan 2016 20:10:08 -0800 (PST)
Sender: Ralph Castain <rhc.o...@gmail.com>
From: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary="Apple-Mail=_431BCFC6-1364-4B10-924A-444A186E976E"
Message-Id: <310A83CD-CF76-4575-8D69-883088D8CD89@open-mpi.org>
Mime-Version: 1.0 (Mac OS X Mail 9.2 \(3112\))
Subject: Re: [Singularity] Singularity and HPC support
Date: Mon, 4 Jan 2016 20:10:07 -0800
References: <937ff7ed-30b6-4143-849e-5e1a234ff89b@lbl.gov> <4D239990-0E7E-4F15-B5CB-09C8465562F0@lbl.gov> <2778d375-2145-4c15-81d5-0c961734993f@lbl.gov> <CAN7etTz8O5Vt4x+=oTp9+7MXEGxr8NT3K2u+BLxB5s=a9MKn4A@mail.gmail.com> <CAN7etTyafQgmAwAH94z1-3of=NNj9QQmLzJUzwn=mO8VRjJAFQ@mail.gmail.com> <D29DD7ED.89FB1%Grigory.Shamov@ad.umanitoba.ca> <D33AC0B6-5024-4234-BD25-7878E8E7C417@lbl.gov> <0C2E7133-169B-4546-9A8E-DF261094EF6F@open-mpi.org> <CAN7etTx+WFj6tQ=6=woiTJCta2z7P9m+hqG-TA8teAyqGXiijA@mail.gmail.com>
To: singularity@lbl.gov
In-Reply-To: <CAN7etTx+WFj6tQ=6=woiTJCta2z7P9m+hqG-TA8teAyqGXiijA@mail.gmail.com>
X-Mailer: Apple Mail (2.3112)

--Apple-Mail=_431BCFC6-1364-4B10-924A-444A186E976E
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8


> On Jan 4, 2016, at 3:52 PM, Gregory M. Kurtzer <gmku...@lbl.gov> wrote:
>=20
> Hi Ralph,
>=20
> Sorry for the delay, the holidays got me a bit behind!

No issues - glad you got a chance to get away for a bit

>=20
>=20
>=20
> On Sat, Jan 2, 2016 at 9:45 AM, Ralph Castain <r...@open-mpi.org <mailto:=
r...@open-mpi.org>> wrote:
> Hello all
>=20
> I=E2=80=99ve been monitoring this mailing list since it began, but have b=
een swamped by too many things to respond. I=E2=80=99d like to specifically=
 address one of Greg=E2=80=99s early requests for input from some MPI folks=
. As some of you may know, I=E2=80=99ve been a lead developer of OpenMPI=E2=
=80=99s runtime since the project=E2=80=99s inception some 12 years ago, an=
d have spent a lot of time interfacing it to various environments. So I=E2=
=80=99d like to try and provide some perspective in the hopes that Singular=
ity may be able to better support HPC operations.
>=20
> Excellent, and exactly what I was hoping for! I will answer as much as I =
can, but I feel we may want to have a discussion in realtime to get more de=
tails as it maybe faster to get on the same page initially (or that I may c=
onfuse you with my answers! LOL). Perhaps a google hangout, skype, or other=
 means (e.g. we can go old fashioned and use a phone too!) would do well.

I agree - let=E2=80=99s up the bandwidth with a skype or call. I confess th=
at I don=E2=80=99t really know much about Singularity, having only tried to=
 follow what was said here, and so I may well be misunderstanding how it wo=
rks. I was expecting that the containers provide some degree of isolation b=
etween each other when sharing a node, and thus some entity (e.g., the host=
 resource manager) is responsible for assigning resources to each container=
.

Once the container is instantiated, it has to stay within those applied res=
ource constraints. At that point, we have two options - the applications ex=
ecuting within each container can either:

(a) see themselves as being on the same physical node, which corresponds to=
 being under the same OS instance. In this case, the container is basically=
 just an application delivery vehicle and has no =E2=80=9Csubstance=E2=80=
=9D. It isn=E2=80=99t quite clear to me what advantage this provides over a=
ny other binary prelocation mechanism such as those we have in today=E2=80=
=99s RMs as those already position both binaries and any associated/require=
d libraries. However, I agree that there is no networking confusion in this=
 case.

(b) see themselves as being under different OS instances - i.e., the contai=
ner provides some level of environment abstraction (e.g., I can replace the=
 OS or package app-specific library environments, while maintaining cross-c=
ontainer isolation). Here we do have some networking confusion as the proce=
sses don=E2=80=99t realize where they are relative to each other when commu=
nicating to the local NIC.

Your answers below imply that Singularity is focused on (a) - is that corre=
ct? If so, then it probably doesn=E2=80=99t really address the use-cases we=
 are facing, though I=E2=80=99m sure it provides some functionality that me=
ets your needs. On the plus side, I don=E2=80=99t see any reason for that u=
se-case to have an issue running MPI applications today.

Ralph

> =20
>=20
> First, let me clarify my expected use-case. I=E2=80=99m assuming that a u=
ser requests an allocation of  some number of containers for running a mult=
i-process application. The user may intend to run one or more application p=
rocesses in each container. When allocated, the containers will belong to t=
hat one user, and will be for their exclusive use. The containers will be r=
eturned to the scheduler upon completion of the application.
>=20
> Singularity really fogs up the idea of containers in terms of allocations=
, resources, and contentions because the container runs like a standard tra=
ditional application... Because it is not like other container platforms yo=
u maybe using as a baseline. There are two ways to run Singularity apps:
>=20
> The first is directly executing the container archive file (it is created=
 with +x, and via the magic of interpretation Singularity can run it seamle=
ssly). When the container is run in this manner, it extracts its contents t=
o a temporary directory and will only run in a single usage context.
>=20
> The second manner of running Singularity containers is more inline with w=
hat you have described. Using the Singularity command, you install the sing=
ularity container into a user specific cache, and from that cached containe=
r, you can invoke as many instances of the program(s)/workflows within the =
container at once as you like. If that cache directory is shared, you can r=
un processes from within that container on any number of nodes (a kin to ru=
nning an application from an NFS volume).
>=20
> Again, I want to stress that Singularity containers all run within the co=
ntext of the user. There is no centralized management facility for any of t=
he Singularity containers (with the exception the application stack of Sing=
ularity itself).
> =20
>=20
> So when I talk about =E2=80=9Cmultiple containers=E2=80=9D below, I=E2=80=
=99m talking strictly about the above use-case. It is quite possible that a=
 user could have multiple allocations running in parallel - the containers =
for each allocation would be distinct and have no knowledge of the containe=
rs in another allocation, even though they belong to the same user. Allocat=
ions given to different users should have no knowledge of each other.
>=20
> Understood and agreed.
> =20
>=20
> I expect that containers from different users will share a physical node =
(i.e., that cloud will typically be a multi-tenant environment), which impl=
ies there will be some resource contention between them. Thus, we will requ=
ire some kind of QoS control over the networks. Remember, these will be OS-=
bypass networks, and so the OS in each container will have no control or ev=
en visibility over them.
>=20
> This is where Singularity diverges considerably from other container solu=
tions. Singularity containers (or more specifically the applications within=
 the containers) are intended to run on the same networks, file systems, an=
d environment that the user already has access to. There is no mechanism fo=
r the user to gain access or change users thus there is no need to isolate =
the network stack into a virtualized namespace. Additionally, while that ma=
ybe beneficially for containers that are mimicking virtual machines, there =
is no need or benefit (that I see) to separate the network namespace. Soooo=
... Long story short, there is no OS-bypass network or additional QOSs that=
 must be managed for Singularity.
> =20
>=20
> Within that context, the container must be given the following knowledge =
at startup if we really want to provide performance comparable to what is a=
chieved today:
>=20
> * the ID of all other containers in this allocation - a unique =E2=80=9Ch=
ostname=E2=80=9D or its equivalent. Must include the IP address assigned to=
 each container
>=20
> If the container cache is located in a shared location, this should be ea=
sy to do either within the job script or beforehand by the user. The IP add=
ress would be the same as the host compute node running the job.
> =20
>=20
> * the =E2=80=9Ctrue=E2=80=9D location of each container in the allocation=
 - i.e., the physical node where the container is executing (so we can iden=
tify which processes are =E2=80=9Csharing=E2=80=9D physical resources) and =
where it is bound (which physical cpus; what network, memory, and other res=
ources it has been given; etc.)
>=20
> I am hoping Singularity will make this easier then other container soluti=
ons.
> =20
>=20
> * if application processes were started as part of setting up the contain=
ers, then which processes (i.e., which ranks) were put on each container, a=
long with what network endpoints were assigned to them and where they are b=
ound
>=20
> It would be very convenient if the ranks, network ports, and everything w=
orks just as it work natively from the MPI front. Perhaps I am overthinking=
 this, but because the Singularity containers exist on the same physical ne=
twork as would be expected, things should be easier in this context.
> =20
>=20
> * the network topology between the containers - alternatively, you can pr=
ovide the topology between the involved physical nodes and we can use the a=
bove location info to get what we need
>=20
> Same as above.
> =20
>=20
> * a method by which application procs can request any or all of that info=
 as they need it in order to properly setup their communication infrastruct=
ure.
>=20
> Hopefully environment variables will suffice (they pass through to the Si=
ngularity container).
> =20
>=20
> * an enhancement would include a shared memory region on each physical no=
de that hosts multiple containers so we could use shared memory communicati=
on in that situation. Obviously, there would need to be a dedicated region =
for each allocation to ensure separation of applications.
>=20
> Shared memory (should) already be managed properly (in theory), but I hav=
e not tested this yet.
> =20
>=20
> * a =E2=80=9Cglobal=E2=80=9D key-value exchange server that will allow us=
ers to =E2=80=9Cdiscover=E2=80=9D applications and their allocations, subje=
ct to access controls. Thus, an ocean simulator in one allocation could =E2=
=80=9Cdiscover=E2=80=9D and connect to an atmospheric model in another allo=
cation so they can share data. The HPC world knows how to build this now - =
just a case of ensuring adequate knowledge transfer.
>=20
> I'd be very interested in hearing more about what you are thinking here.
> =20
>=20
> I hope that helps provide some direction. I=E2=80=99d be happy to help co=
nstruct this capability, and interface it to OpenMPI.
>=20
> Yep, totally!
>=20
> Additionally, I have a major concern which has to do with environment por=
tability in matching the kernel and container API versions (e.g. OFED). Wha=
t I would love to see is that part of the MPI stack that interfaces with th=
e host environment/kernel gets called directly on the host, but the process=
es itself still runs within the container context (possibly with a containe=
r installed runtime component of the MPI stack). I have no idea how this co=
uld work, but perhaps you would have some ideas on this or just tell me it =
is impossible and kick me.
>=20
> Thank you and let me know if I have indeed made this more complicated or =
confusing and that we should discuss in real time!
>=20
> Greg
>=20
>=20
> --=20
> Gregory M. Kurtzer
> Technical Lead and HPC Systems Architect
> High Performance Computing Services (HPCS)
> University of California
> Lawrence Berkeley National Laboratory
> One Cyclotron Road, Berkeley, CA 94720
>=20
> --=20
> You received this message because you are subscribed to the Google Groups=
 "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an=
 email to singu...@lbl.gov <mailto:singu...@lbl.gov>.


--Apple-Mail=_431BCFC6-1364-4B10-924A-444A186E976E
Content-Transfer-Encoding: quoted-printable
Content-Type: text/html;
	charset=utf-8

<html><head><meta http-equiv=3D"Content-Type" content=3D"text/html charset=
=3Dutf-8"></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space;" class=3D""><br class=3D""><di=
v><blockquote type=3D"cite" class=3D""><div class=3D"">On Jan 4, 2016, at 3=
:52 PM, Gregory M. Kurtzer &lt;<a href=3D"mailto:gmku...@lbl.gov" class=3D"=
">gmku...@lbl.gov</a>&gt; wrote:</div><br class=3D"Apple-interchange-newlin=
e"><div class=3D""><div dir=3D"ltr" class=3D"">Hi Ralph,<div class=3D""><br=
 class=3D""></div><div class=3D"">Sorry for the delay, the holidays got me =
a bit behind!</div></div></div></blockquote><div><br class=3D""></div>No is=
sues - glad you got a chance to get away for a bit</div><div><br class=3D""=
><blockquote type=3D"cite" class=3D""><div class=3D""><div dir=3D"ltr" clas=
s=3D""><div class=3D""><br class=3D""></div><div class=3D""><br class=3D"">=
<div class=3D"gmail_extra"><br class=3D""><div class=3D"gmail_quote">On Sat=
, Jan 2, 2016 at 9:45 AM, Ralph Castain <span dir=3D"ltr" class=3D"">&lt;<a=
 href=3D"mailto:r...@open-mpi.org" target=3D"_blank" class=3D"">r...@open-m=
pi.org</a>&gt;</span> wrote:<br class=3D""><blockquote class=3D"gmail_quote=
" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><=
div style=3D"word-wrap:break-word" class=3D"">Hello all<div class=3D""><br =
class=3D""></div><div class=3D"">I=E2=80=99ve been monitoring this mailing =
list since it began, but have been swamped by too many things to respond. I=
=E2=80=99d like to specifically address one of Greg=E2=80=99s early request=
s for input from some MPI folks. As some of you may know, I=E2=80=99ve been=
 a lead developer of OpenMPI=E2=80=99s runtime since the project=E2=80=99s =
inception some 12 years ago, and have spent a lot of time interfacing it to=
 various environments. So I=E2=80=99d like to try and provide some perspect=
ive in the hopes that Singularity may be able to better support HPC operati=
ons.</div></div></blockquote><div class=3D""><br class=3D""></div><div clas=
s=3D"">Excellent, and exactly what I was hoping for! I will answer as much =
as I can, but I feel we may want to have a discussion in realtime to get mo=
re details as it maybe faster to get on the same page initially (or that I =
may confuse you with my answers! LOL). Perhaps a google hangout, skype, or =
other means (e.g. we can go old fashioned and use a phone too!) would do we=
ll.</div></div></div></div></div></div></blockquote><div><br class=3D""></d=
iv>I agree - let=E2=80=99s up the bandwidth with a skype or call. I confess=
 that I don=E2=80=99t really know much about Singularity, having only tried=
 to follow what was said here, and so I may well be misunderstanding how it=
 works. I was expecting that the containers provide some degree of isolatio=
n between each other when sharing a node, and thus some entity (e.g., the h=
ost resource manager) is responsible for assigning resources to each contai=
ner.</div><div><br class=3D""></div><div>Once the container is instantiated=
, it has to stay within those applied resource constraints. At that point, =
we have two options - the applications executing within each container can =
either:</div><div><br class=3D""></div><div>(a) see themselves as being on =
the same physical node, which corresponds to being under the same OS instan=
ce. In this case, the container is basically just an application delivery v=
ehicle and has no =E2=80=9Csubstance=E2=80=9D. It isn=E2=80=99t quite clear=
 to me what advantage this provides over any other binary prelocation mecha=
nism such as those we have in today=E2=80=99s RMs as those already position=
 both binaries and any associated/required libraries. However, I agree that=
 there is no networking confusion in this case.</div><div><br class=3D""></=
div><div>(b) see themselves as being under different OS instances - i.e., t=
he container provides some level of environment abstraction (e.g., I can re=
place the OS or package app-specific library environments, while maintainin=
g cross-container isolation). Here we do have some networking confusion as =
the processes don=E2=80=99t realize where they are relative to each other w=
hen communicating to the local NIC.</div><div><br class=3D""></div><div>You=
r answers below imply that Singularity is focused on (a) - is that correct?=
 If so, then it probably doesn=E2=80=99t really address the use-cases we ar=
e facing, though I=E2=80=99m sure it provides some functionality that meets=
 your needs. On the plus side, I don=E2=80=99t see any reason for that use-=
case to have an issue running MPI applications today.</div><div><br class=
=3D""></div><div>Ralph</div><div><br class=3D""></div><div><blockquote type=
=3D"cite" class=3D""><div class=3D""><div dir=3D"ltr" class=3D""><div class=
=3D""><div class=3D"gmail_extra"><div class=3D"gmail_quote"><div class=3D""=
>&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-w=
ord" class=3D""><div class=3D""><br class=3D""></div><div class=3D""><div c=
lass=3D"">First, let me clarify my expected use-case. I=E2=80=99m assuming =
that a user requests an allocation of &nbsp;some number of containers for r=
unning a multi-process application. The user may intend to run one or more =
application processes in each container. When allocated, the containers wil=
l belong to that one user, and will be for their exclusive use. The contain=
ers will be returned to the scheduler upon completion of the application.</=
div></div></div></blockquote><div class=3D""><br class=3D""></div><div clas=
s=3D"">Singularity really fogs up the idea of containers in terms of alloca=
tions, resources, and contentions because the container runs like a standar=
d traditional application... Because it is not like other container platfor=
ms you maybe using as a baseline. There are two ways to run Singularity app=
s:</div><div class=3D""><br class=3D""></div><div class=3D"">The first is d=
irectly executing the container archive file (it is created with +x, and vi=
a the magic of interpretation Singularity can run it seamlessly). When the =
container is run in this manner, it extracts its contents to a temporary di=
rectory and will only run in a single usage context.</div><div class=3D""><=
br class=3D""></div><div class=3D"">The second manner of running Singularit=
y containers is more inline with what you have described. Using the Singula=
rity command, you install the singularity container into a user specific ca=
che, and from that cached container, you can invoke as many instances of th=
e program(s)/workflows within the container at once as you like. If that ca=
che directory is shared, you can run processes from within that container o=
n any number of nodes (a kin to running an application from an NFS volume).=
</div><div class=3D""><br class=3D""></div><div class=3D"">Again, I want to=
 stress that Singularity containers all run within the context of the user.=
 There is no centralized management facility for any of the Singularity con=
tainers (with the exception the application stack of Singularity itself).</=
div><div class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"=
margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word" class=3D""><div class=3D""><div class=3D""><br cl=
ass=3D""></div><div class=3D"">So when I talk about =E2=80=9Cmultiple conta=
iners=E2=80=9D below, I=E2=80=99m talking strictly about the above use-case=
. It is quite possible that a user could have multiple allocations running =
in parallel - the containers for each allocation would be distinct and have=
 no knowledge of the containers in another allocation, even though they bel=
ong to the same user. Allocations given to different users should have no k=
nowledge of each other.</div></div></div></blockquote><div class=3D""><br c=
lass=3D""></div><div class=3D"">Understood and agreed.</div><div class=3D""=
>&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-w=
ord" class=3D""><div class=3D""><div class=3D""><br class=3D""></div><div c=
lass=3D"">I expect that containers from different users will share a physic=
al node (i.e., that cloud will typically be a multi-tenant environment), wh=
ich implies there will be some resource contention between them. Thus, we w=
ill require some kind of QoS control over the networks. Remember, these wil=
l be OS-bypass networks, and so the OS in each container will have no contr=
ol or even visibility over them.</div></div></div></blockquote><div class=
=3D""><br class=3D""></div><div class=3D"">This is where Singularity diverg=
es considerably from other container solutions. Singularity containers (or =
more specifically the applications within the containers) are intended to r=
un on the same networks, file systems, and environment that the user alread=
y has access to. There is no mechanism for the user to gain access or chang=
e users thus there is no need to isolate the network stack into a virtualiz=
ed namespace. Additionally, while that maybe beneficially for containers th=
at are mimicking virtual machines, there is no need or benefit (that I see)=
 to separate the network namespace. Soooo... Long story short, there is no =
OS-bypass network or additional QOSs that must be managed for Singularity.<=
/div><div class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" style=3D=
"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word" class=3D""><div class=3D""><div class=3D""><br cl=
ass=3D""></div><div class=3D"">Within that context, the container must be g=
iven the following knowledge at startup if we really want to provide perfor=
mance comparable to what is achieved today:</div><div class=3D""><br class=
=3D""></div><div class=3D"">* the ID of all other containers in this alloca=
tion - a unique =E2=80=9Chostname=E2=80=9D or its equivalent. Must include =
the IP address assigned to each container</div></div></div></blockquote><di=
v class=3D""><br class=3D""></div><div class=3D"">If the container cache is=
 located in a shared location, this should be easy to do either within the =
job script or beforehand by the user. The IP address would be the same as t=
he host compute node running the job.</div><div class=3D"">&nbsp;</div><blo=
ckquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #c=
cc solid;padding-left:1ex"><div style=3D"word-wrap:break-word" class=3D""><=
div class=3D""><div class=3D""><br class=3D""></div><div class=3D"">* the =
=E2=80=9Ctrue=E2=80=9D location of each container in the allocation - i.e.,=
 the physical node where the container is executing (so we can identify whi=
ch processes are =E2=80=9Csharing=E2=80=9D physical resources) and where it=
 is bound (which physical cpus; what network, memory, and other resources i=
t has been given; etc.)</div></div></div></blockquote><div class=3D""><br c=
lass=3D""></div><div class=3D"">I am hoping Singularity will make this easi=
er then other container solutions.</div><div class=3D"">&nbsp;</div><blockq=
uote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc =
solid;padding-left:1ex"><div style=3D"word-wrap:break-word" class=3D""><div=
 class=3D""><div class=3D""><br class=3D""></div><div class=3D"">* if appli=
cation processes were started as part of setting up the containers, then wh=
ich processes (i.e., which ranks) were put on each container, along with wh=
at network endpoints were assigned to them and where they are bound</div></=
div></div></blockquote><div class=3D""><br class=3D""></div><div class=3D""=
>It would be very convenient if the ranks, network ports, and everything wo=
rks just as it work natively from the MPI front. Perhaps I am overthinking =
this, but because the Singularity containers exist on the same physical net=
work as would be expected, things should be easier in this context.</div><d=
iv class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"margin=
:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word=
-wrap:break-word" class=3D""><div class=3D""><div class=3D""><br class=3D""=
></div><div class=3D"">* the network topology between the containers - alte=
rnatively, you can provide the topology between the involved physical nodes=
 and we can use the above location info to get what we need</div></div></di=
v></blockquote><div class=3D""><br class=3D""></div><div class=3D"">Same as=
 above.</div><div class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" =
style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><di=
v style=3D"word-wrap:break-word" class=3D""><div class=3D""><div class=3D""=
><br class=3D""></div><div class=3D"">* a method by which application procs=
 can request any or all of that info as they need it in order to properly s=
etup their communication infrastructure.</div></div></div></blockquote><div=
 class=3D""><br class=3D""></div><div class=3D"">Hopefully environment vari=
ables will suffice (they pass through to the Singularity container).</div><=
div class=3D"">&nbsp;</div><blockquote class=3D"gmail_quote" style=3D"margi=
n:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"wor=
d-wrap:break-word" class=3D""><div class=3D""><div class=3D""><br class=3D"=
"></div><div class=3D"">* an enhancement would include a shared memory regi=
on on each physical node that hosts multiple containers so we could use sha=
red memory communication in that situation. Obviously, there would need to =
be a dedicated region for each allocation to ensure separation of applicati=
ons.</div></div></div></blockquote><div class=3D""><br class=3D""></div><di=
v class=3D"">Shared memory (should) already be managed properly (in theory)=
, but I have not tested this yet.</div><div class=3D"">&nbsp;</div><blockqu=
ote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc s=
olid;padding-left:1ex"><div style=3D"word-wrap:break-word" class=3D""><div =
class=3D""><div class=3D""><br class=3D""></div><div class=3D"">* a =E2=80=
=9Cglobal=E2=80=9D key-value exchange server that will allow users to =E2=
=80=9Cdiscover=E2=80=9D applications and their allocations, subject to acce=
ss controls. Thus, an ocean simulator in one allocation could =E2=80=9Cdisc=
over=E2=80=9D and connect to an atmospheric model in another allocation so =
they can share data. The HPC world knows how to build this now - just a cas=
e of ensuring adequate knowledge transfer.</div></div></div></blockquote><d=
iv class=3D""><br class=3D""></div><div class=3D"">I'd be very interested i=
n hearing more about what you are thinking here.</div><div class=3D"">&nbsp=
;</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-=
left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word" c=
lass=3D""><div class=3D""><br class=3D""></div><div class=3D"">I hope that =
helps provide some direction. I=E2=80=99d be happy to help construct this c=
apability, and interface it to OpenMPI.</div></div></blockquote><div class=
=3D""><br class=3D""></div><div class=3D"">Yep, totally!</div><div class=3D=
""><br class=3D""></div><div class=3D"">Additionally, I have a major concer=
n which has to do with environment portability in matching the kernel and c=
ontainer API versions (e.g. OFED). What I would love to see is that part of=
 the MPI stack that interfaces with the host environment/kernel gets called=
 directly on the host, but the processes itself still runs within the conta=
iner context (possibly with a container installed runtime component of the =
MPI stack). I have no idea how this could work, but perhaps you would have =
some ideas on this or just tell me it is impossible and kick me.</div><div =
class=3D""><br class=3D""></div><div class=3D"">Thank you and let me know i=
f I have indeed made this more complicated or confusing and that we should =
discuss in real time!</div><div class=3D""><br class=3D""></div><div class=
=3D"">Greg</div></div><br clear=3D"all" class=3D""><div class=3D""><br clas=
s=3D""></div>-- <br class=3D""><div class=3D"">Gregory M. Kurtzer<br class=
=3D"">Technical Lead and HPC Systems Architect<br class=3D"">High Performan=
ce Computing Services (HPCS)<br class=3D"">University of California<br clas=
s=3D"">Lawrence Berkeley National Laboratory<br class=3D"">One Cyclotron Ro=
ad, Berkeley, CA 94720</div>
</div></div></div><div class=3D""><br class=3D"webkit-block-placeholder"></=
div>

-- <br class=3D"">
You received this message because you are subscribed to the Google Groups "=
singularity" group.<br class=3D"">
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" class=3D"">singu...@lbl.gov</a>=
.<br class=3D"">
</div></blockquote></div><br class=3D""></body></html>
--Apple-Mail=_431BCFC6-1364-4B10-924A-444A186E976E--
