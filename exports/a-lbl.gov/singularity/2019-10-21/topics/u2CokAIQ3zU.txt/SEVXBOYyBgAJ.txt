X-Received: by 10.101.92.73 with SMTP id v9mr453861pgr.11.1509466472692;
        Tue, 31 Oct 2017 09:14:32 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.101.71.200 with SMTP id f8ls5100728pgs.7.gmail; Tue, 31 Oct
 2017 09:14:31 -0700 (PDT)
X-Received: by 10.99.173.74 with SMTP id y10mr2258382pgo.107.1509466471546;
        Tue, 31 Oct 2017 09:14:31 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1509466471; cv=none;
        d=google.com; s=arc-20160816;
        b=DxcYOH3QE28OrqdrPo1UgWO/t0r4Xk7LzPH+Kdz6myH6oQWhfyd6Y4n9gybpM4XCA5
         7YFV134PyIQvc60SFT/foJgalkvBRsWVlBHpMcOIUFewPuPCYjvUhGOFBgY6ZHLBN0mV
         1KWr0qpwhkC61TeMMBXf6tJKX1lnWxafa1HrfkHyQ772/ZRMYbTAZ0U9mBD7bv0cFlQx
         sW5Ux92rR7djgsA6Mphy09uE/lWIFXchTj3TB7WpDrBt29Nn1ncthKOoIgh2BZH/93Tt
         lp2WV59EK2/+ro/7c0Qv+Yq0ca/z6f8QA/q7y0UjmZLoldImpvtg2cf9YIyQ5AoDPd7A
         F+mA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=F9tSRgTzIsUTaaeev4UIik2mwe+LJD7aOx8AsbAKFAo=;
        b=IjXHXlHprmgetvhNLsoEaDZmRUVy4tuARRdd5opLyDD7pqxjwwW6miiPiRKlbbujY2
         QayCazTBtBWw91FHsOFiatV06hSgUejMN1mxP2bH6U8sAUi2JKZz6CgT9Ef5ZFZSsGf7
         5g2O2vilUBivupTgmFtPqdTGxnvDwl0A+MAYsJ+71sUvV49CHhYK88ybZaMyrPl+Kvzu
         BfVFz3PcS8/s01VzKcXXwvKt1FuVwOHdMFcPwGcw+2n3/YIxgIO3NtAGqCmgz7VhlbDI
         QHINTQT/Vra9k2AZDO+tJ218v4w11Jt8U8PypkefHVdH0LGVP2pSH/lqI//Co3eySmnx
         DF9w==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=eALucm2D;
       spf=pass (google.com: domain of ebagr...@gmail.com designates 209.85.223.177 as permitted sender) smtp.mailfrom=ebagr...@gmail.com
Return-Path: <ebagr...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id x10si1916657plm.259.2017.10.31.09.14.31
        for <singu...@lbl.gov>;
        Tue, 31 Oct 2017 09:14:31 -0700 (PDT)
Received-SPF: pass (google.com: domain of ebagr...@gmail.com designates 209.85.223.177 as permitted sender) client-ip=209.85.223.177;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=eALucm2D;
       spf=pass (google.com: domain of ebagr...@gmail.com designates 209.85.223.177 as permitted sender) smtp.mailfrom=ebagr...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2GnAAD5oPhZhrHfVdFdHgYMGQYMgwYCU?=
 =?us-ascii?q?YEtJweDbQiBNpd/PAcBAQaBLAWQfYVFEIE+QyIBhSIChGwHPxgBAQEBAQEBAQE?=
 =?us-ascii?q?BAQIQAQEBCAsLCCgvgjgFAgMYCAhGKS8BAQEBAQEBAQEBHwIrJRsBBAEjHQEND?=
 =?us-ascii?q?h4DAQsGBQs3AgIhAQEOAwEFARwOBwQBHASIMYE4AQMIBQgFnA1AjAyCBQUBHIM?=
 =?us-ascii?q?JBYNiChknDViCcAEBAQEGAQEBAQEBGgIGEoMcggeBDIIwgyqCaoF0AQwGAQuDK?=
 =?us-ascii?q?YJhBZJuhhWIRzyQA4R5ky6NGIhmGR+BFR+BCDRoNCElgROCL4JNDxAMggUjNgi?=
 =?us-ascii?q?JCw4YMIFtAQEB?=
X-IronPort-AV: E=Sophos;i="5.44,324,1505804400"; 
   d="scan'208,217";a="2512118"
Received: from mail-io0-f177.google.com ([209.85.223.177])
  by fe4.lbl.gov with ESMTP; 31 Oct 2017 09:14:07 -0700
Received: by mail-io0-f177.google.com with SMTP id 101so36311442ioj.3
        for <singu...@lbl.gov>; Tue, 31 Oct 2017 09:14:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=F9tSRgTzIsUTaaeev4UIik2mwe+LJD7aOx8AsbAKFAo=;
        b=eALucm2D3Kq75u2iKURhycuU05V0lvkQ2hsf9KfUbef/hjZy4SxADGUpSBpZ7eHVVY
         xsivmS7XBUdrxap3AQ0CCD3LF2SLMjDrwn/2gICOEY/dCc/sbNL1j4+aHMIZz6hxiAcI
         I7NieXauGUPkGSrpMYvXw/w8qQl/VIGh1+5nx5p2nxGWXFxiqapsaqkqY2ve4iOTwB8a
         wshrvfh41kSFDzhx5qWOQoNFamoSsHM35YUb6w//q/Y+0m4q4B9k/aBzqFszb7IAr5de
         nDRJyeieKR6pWgaZibtk4N8WrqbRfV+t9wkD89gVvtn4mnBzR72F6kS2voATGzBnie7z
         HLfA==
X-Gm-Message-State: AMCzsaWPENXYHbS4xpwJF9B+DLCwd5T0fBpO32UsS3OjDYKDHnzSttSg
	YsufbJgEeP4ukfQYtiwRuAt9+kS9Xv8+6alBIr5N1w==
X-Received: by 10.107.166.16 with SMTP id p16mr3175162ioe.53.1509466446529;
 Tue, 31 Oct 2017 09:14:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.53.194 with HTTP; Tue, 31 Oct 2017 09:14:05 -0700 (PDT)
In-Reply-To: <dc630f79-37fc-4c83-a358-b9e8e9b39d1f@lbl.gov>
References: <dc630f79-37fc-4c83-a358-b9e8e9b39d1f@lbl.gov>
From: Gabe Turner <ebagr...@gmail.com>
Date: Tue, 31 Oct 2017 11:14:05 -0500
Message-ID: <CAEBt5ZK5xzzWcSt2WSd7G624kGvoMBN=KzGh=f=mx0cd_+i+mw@mail.gmail.com>
Subject: Re: [Singularity] Panasas on Singularity
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="001a1141f2085887d8055cda0b00"

--001a1141f2085887d8055cda0b00
Content-Type: text/plain; charset="UTF-8"

Nick:

I'm skeptical that the Panasas has anything to do with the problems you've
mentioned. I've used Singularity with user homes, scratches, and share
application spaces on Panasas. You will need to bind all of the volumes
that your users need to access, or bind the whole realm (which might be
your /panfs/pfs.local above), and ensure that any symlinks to anything in
the realm exist in your other bound filesystems or in the image itself
(whichever is applicable).

Lastly, disable overlay, as it's not support in RHEL 6.

Gabe

On Mon, Oct 30, 2017 at 1:16 PM, Nick Eggleston <nicke...@gmail.com>
wrote:

> Hey All,
>
> I'm one of the HPC Sysadmins for University of Kansas. We use Panasas for
> our cluster file storage, and I'm having a bit of an issue getting
> Singularity to play nice with some of our Panasas. As is standard with a
> lot of compute clusters, we give users a home, work, and scratch directory
> to play in. Home is accessible under /home/<username> as would be expected,
> and we do this with a symlink to the home volume under our Panasas mount.
> Like I said before we also give a work and scratch directory to every user
> and these directories have higher space limitations than the home directory
> so that's where most everyone puts their files. When I start a Singularity
> container, I don't have any issues with /home, that just seems to work.
> What doesn't seem to work is anything else that needs to reach out to the
> Panasas. Users all have a symlink in their homedir that points to their
> work and scratch and those links appear to be broken, and that Panasas
> doesn't show up at all, even though I've enabled what I think would be the
> relevant settings in my singularity.conf file (which I'll paste a condensed
> version of below in case you all see something I don't). Does anyone have
> any experience with using Panasas in this context with Singularity? So far
> the people I know that use both only use the homedir as the only link to
> the outside world so to speak, but that's not enough for our users. Also,
> just for a relevant info include, we run RHEL 6.4 with a 2.6 kernel.
>
> Thanks!
>
>
> # SINGULARITY.CONF
> # This is the global configuration file for Singularity. This file controls
> # what the container is allowed to do on a particular host, and as a result
> # this file must be owned by root.
>
> allow setuid = yes
>
> max loop devices = 256
>
> allow pid ns = yes
>
> config passwd = yes
>
> config group = yes
>
> config resolv_conf = yes
>
> mount proc = yes
>
> mount sys = yes
>
> mount dev = yes
>
> mount devpts = yes
>
> mount home = yes
>
> mount tmp = yes
>
> #mount hostfs = no
> mount hostfs = yes
>
> #bind path = /etc/singularity/default-nsswitch.conf:/etc/nsswitch.conf
> #bind path = /opt
> #bind path = /scratch
> #bind path = /etc/localtime
> bind path = /etc/hosts
> bind path = /panfs/pfs.local
>
> user bind control = yes
>
> #enable overlay = no
> enable overlay = yes
>
> sessiondir max size = 16
>
> #limit container owners = gmk, singularity, nobody
>
> #limit container paths = /scratch, /tmp, /global
>
> allow container squashfs = yes
> allow container extfs = yes
> allow container dir = yes
>
> #autofs bug path = /nfs
> #autofs bug path = /cifs-share
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--001a1141f2085887d8055cda0b00
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div>Nick:<br><br></div>I&#39;m skeptical that t=
he Panasas has anything to do with the problems you&#39;ve mentioned. I&#39=
;ve used Singularity with user homes, scratches, and share application spac=
es on Panasas. You will need to bind all of the volumes that your users nee=
d to access, or bind the whole realm (which might be your /panfs/pfs.local =
above), and ensure that any symlinks to anything in the realm exist in your=
 other bound filesystems or in the image itself (whichever is applicable).<=
br><br></div>Lastly, disable overlay, as it&#39;s not support in RHEL 6.<br=
><br></div>Gabe<br></div><div class=3D"gmail_extra"><br><div class=3D"gmail=
_quote">On Mon, Oct 30, 2017 at 1:16 PM, Nick Eggleston <span dir=3D"ltr">&=
lt;<a href=3D"mailto:nicke...@gmail.com" target=3D"_blank">nicke...@gmail.c=
om</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"marg=
in:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"=
>Hey All,<div><br></div><div>I&#39;m one of the HPC Sysadmins for Universit=
y of Kansas. We use Panasas for our cluster file storage, and I&#39;m havin=
g a bit of an issue getting Singularity to play nice with some of our Panas=
as. As is standard with a lot of compute clusters, we give users a home, wo=
rk, and scratch directory to play in. Home is accessible under /home/&lt;us=
ername&gt; as would be expected, and we do this with a symlink to the home =
volume under our Panasas mount. Like I said before we also give a work and =
scratch directory to every user and these directories have higher space lim=
itations than the home directory so that&#39;s where most everyone puts the=
ir files. When I start a Singularity container, I don&#39;t have any issues=
 with /home, that just seems to work. What doesn&#39;t seem to work is anyt=
hing else that needs to reach out to the Panasas. Users all have a symlink =
in their homedir that points to their work and scratch and those links appe=
ar to be broken, and that Panasas doesn&#39;t show up at all, even though I=
&#39;ve enabled what I think would be the relevant settings in my singulari=
ty.conf file (which I&#39;ll paste a condensed version of below in case you=
 all see something I don&#39;t). Does anyone have any experience with using=
 Panasas in this context with Singularity? So far the people I know that us=
e both only use the homedir as the only link to the outside world so to spe=
ak, but that&#39;s not enough for our users. Also, just for a relevant info=
 include, we run RHEL 6.4 with a 2.6 kernel.</div><div><br></div><div>Thank=
s!</div><div><br></div><div><br></div><div><div># SINGULARITY.CONF</div><di=
v># This is the global configuration file for Singularity. This file contro=
ls</div><div># what the container is allowed to do on a particular host, an=
d as a result</div><div># this file must be owned by root.</div><div><br></=
div><div>allow setuid =3D yes<br></div><div><br></div><div>max loop devices=
 =3D 256</div><div><br></div><div>allow pid ns =3D yes<br></div><div><br></=
div><div>config passwd =3D yes<br></div><div><br></div><div>config group =
=3D yes<br></div><div><br></div><div>config resolv_conf =3D yes<br></div><d=
iv><br></div><div>mount proc =3D yes<br></div><div><br></div><div>mount sys=
 =3D yes<br></div><div><br></div><div>mount dev =3D yes<br></div><div><br><=
/div><div>mount devpts =3D yes<br></div><div><br></div><div>mount home =3D =
yes</div><div><br></div><div>mount tmp =3D yes</div><div><br></div><div>#mo=
unt hostfs =3D no</div><div>mount hostfs =3D yes</div><div><br></div><div>#=
bind path =3D /etc/singularity/default-<wbr>nsswitch.conf:/etc/nsswitch.<wb=
r>conf<br></div><div>#bind path =3D /opt</div><div>#bind path =3D /scratch<=
/div><div>#bind path =3D /etc/localtime</div><div>bind path =3D /etc/hosts<=
/div><div>bind path =3D /panfs/pfs.local</div><div><br></div><div>user bind=
 control =3D yes</div><div><br></div><div>#enable overlay =3D no<br></div><=
div>enable overlay =3D yes</div><div><br></div><div>sessiondir max size =3D=
 16<br></div><div><br></div><div>#limit container owners =3D gmk, singulari=
ty, nobody</div><div><br></div><div>#limit container paths =3D /scratch, /t=
mp, /global</div><div><br></div><div>allow container squashfs =3D yes<br></=
div><div>allow container extfs =3D yes</div><div>allow container dir =3D ye=
s</div><div><br></div><div>#autofs bug path =3D /nfs</div><div>#autofs bug =
path =3D /cifs-share</div></div><span class=3D"HOEnZb"><font color=3D"#8888=
88"><div><br></div></font></span></div><span class=3D"HOEnZb"><font color=
=3D"#888888">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</font></span></blockquote></div><br></div>

--001a1141f2085887d8055cda0b00--
