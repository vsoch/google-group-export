X-Received: by 10.200.56.250 with SMTP id g55mr31336163qtc.18.1466593548062;
        Wed, 22 Jun 2016 04:05:48 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.33.1 with SMTP id e1ls660866ita.25.canary; Wed, 22 Jun 2016
 04:05:47 -0700 (PDT)
X-Received: by 10.107.132.28 with SMTP id g28mr42329912iod.34.1466593547580;
        Wed, 22 Jun 2016 04:05:47 -0700 (PDT)
Return-Path: <D....@liverpool.ac.uk>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id q66si46541330pfi.216.2016.06.22.04.05.47
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 22 Jun 2016 04:05:47 -0700 (PDT)
Received-SPF: pass (google.com: domain of D....@liverpool.ac.uk designates 138.253.100.117 as permitted sender) client-ip=138.253.100.117;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of D....@liverpool.ac.uk designates 138.253.100.117 as permitted sender) smtp.mailfrom=D....@liverpool.ac.uk
X-Ironport-SBRS: 3.5
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A0BdAgAccGpXh3Vk/YpeHQGDdn2ySYllHoV5AoErOxEBAQEBAQEBEgEBAQoLCQkhL4RNAQEBAgEnVwsLISUPAQQoIROIKAgFv2ODMQEBCAIlHoYJhE2FDIUPBYgNB4cWiVOGCIpcjGyPfDSCOYFYbYpxAQEB
X-IPAS-Result: A0BdAgAccGpXh3Vk/YpeHQGDdn2ySYllHoV5AoErOxEBAQEBAQEBEgEBAQoLCQkhL4RNAQEBAgEnVwsLISUPAQQoIROIKAgFv2ODMQEBCAIlHoYJhE2FDIUPBYgNB4cWiVOGCIpcjGyPfDSCOYFYbYpxAQEB
X-IronPort-AV: E=Sophos;i="5.26,509,1459839600"; 
   d="scan'208";a="27088498"
Received: from chsophx.liv.ac.uk ([138.253.100.117])
  by fe4.lbl.gov with ESMTP; 22 Jun 2016 04:05:46 -0700
Received: from chsophx.liv.ac.uk (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id E00443E0894
	for <singu...@lbl.gov>; Wed, 22 Jun 2016 12:05:45 +0100 (BST)
Received: from mxe.liv.ac.uk (mxe.liv.ac.uk [138.253.100.159])
	by chsophx.liv.ac.uk (Postfix) with ESMTP id D96B63E0884
	for <singu...@lbl.gov>; Wed, 22 Jun 2016 12:05:45 +0100 (BST)
Received: from mailhubd.liv.ac.uk ([138.253.100.81])
	by mxe.liv.ac.uk with esmtp (Exim 4.80.1)
	(envelope-from <D....@liverpool.ac.uk>)
	id 1bFfyb-0000rm-Qz
	for singu...@lbl.gov; Wed, 22 Jun 2016 12:05:45 +0100
Received: from localhost ([127.0.0.1] helo=mailhubd.liv.ac.uk)
	by mailhubd.liv.ac.uk with esmtp (Exim 4.80.1)
	(envelope-from <D....@liverpool.ac.uk>)
	id 1bFfyb-00065E-QK
	for singu...@lbl.gov; Wed, 22 Jun 2016 12:05:45 +0100
Received: from pc102091.liv.ac.uk ([138.253.102.91] helo=albion)
	by mailhubd.liv.ac.uk with esmtps (TLSv1.2:DHE-RSA-AES128-SHA:128)
	(Exim 4.84_2)
	(envelope-from <dl...@liverpool.ac.uk>)
	id 1bFfyb-00065A-Fp
	for singu...@lbl.gov; Wed, 22 Jun 2016 12:05:45 +0100
Received: from dlove by albion with local (Exim 4.82)
	(envelope-from <dl...@pc102091.liv.ac.uk>)
	id 1bFfyb-0001nJ-97
	for singu...@lbl.gov; Wed, 22 Jun 2016 12:05:45 +0100
From: Dave Love <d....@liverpool.ac.uk>
To: <singu...@lbl.gov>
Subject: Re: [Singularity] Communication between singularity containers
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov>
	<CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
X-Draft-From: ("singularity" 28)
Date: Wed, 22 Jun 2016 12:05:45 +0100
In-Reply-To: <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
	(Gregory M. Kurtzer's message of "Tue, 21 Jun 2016 07:50:35 -0700")
Message-ID: <87por9o54m.fsf@pc102091.liv.ac.uk>
User-Agent: Gnus/5.13 (Gnus v5.13) Emacs/24.3 (gnu/linux)
MIME-Version: 1.0
Content-Type: text/plain

"Gregory M. Kurtzer" <gmku...@lbl.gov> writes:

> [...] management via an existing scheduling system, and the list goes
> on and on.

In this connexion, the startup for more-or-less privileged users via a
privileged daemon (rather than the resource manager) seems the most
important -- at least to a resource manager maintainer.

[By the way, if you're interested in security aspects, there's
<https://www.nccgroup.trust/globalassets/our-research/us/whitepapers/2016/april/ncc_group_understanding_hardening_linux_containers-10pdf>,
for instance.]

> Think of it this way, Singularity does not do any of that... It runs a
> program within the container as if it were running on the host itself, so
> to communicate between containers is as easy as communicating between
> programs. So for MPI, it would happen with the MPI on the physical host
> (outside the container) invoking the container subsystem which then invokes
> the MPI programs within the container and the MPI programs within the
> container communicate back to the MPI (orted) outside the container on the
> host to get access to the host resources.

But if I run a normal program not built against the system MPI, I expect
it to fail (typically un-obviously).  You seem to be saying that's
entirely supported; could someone explain the magic that allows
incompatible OMPI components to communicate?  Also, how does it work if
I do

  mpirun --mca <parameter> <value> --<option> container

when <parameter> and <option> don't exist for the OMPI version inside
the container?

> In this model all available
> resources and infrastructure can be leveraged at full bandwidth by the
> contained processes and all of the aforementioned complexities akin to
> running on a virtualized mini-cluster are circumvented.

This should probably be another FAQ:  How do you make the resources
visible?  For instance, I can't see the Lustre, PVFS, or large NFS
filesystems' mounts inside the container or how to change that.

> There is additional information I have written at:
>
> http://singularity.lbl.gov/#hpc

If I follow those instructions (practicality aside) with singularity
v2.0, the ranks fail like this:

  --------------------------------------------------------------------------
  It looks like MPI_INIT failed for some reason; your parallel process is
  likely to abort.  There are many reasons that a parallel process can
  fail during MPI_INIT; some of which are due to configuration or environment
  problems.  This failure appears to be an internal failure; here's some
  additional information (which may only be relevant to an Open MPI
  developer):
  
    ompi_mpi_init: ompi_rte_init failed
    --> Returned "(null)" (-43) instead of "Success" (0)
  --------------------------------------------------------------------------

It's the same with the container as the mpirun <program> directly.
