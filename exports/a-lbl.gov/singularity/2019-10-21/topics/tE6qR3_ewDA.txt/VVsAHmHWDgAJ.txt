X-Received: by 10.13.197.193 with SMTP id h184mr77809232ywd.3.1451951533970;
        Mon, 04 Jan 2016 15:52:13 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.50.61.137 with SMTP id p9ls66364igr.30.gmail; Mon, 04 Jan 2016
 15:52:13 -0800 (PST)
X-Received: by 10.98.13.151 with SMTP id 23mr131030547pfn.48.1451951533488;
        Mon, 04 Jan 2016 15:52:13 -0800 (PST)
Return-Path: <gmku...@lbl.gov>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTPS id va5si53905235pac.165.2016.01.04.15.52.13
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 04 Jan 2016 15:52:13 -0800 (PST)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.50 as permitted sender) client-ip=74.125.82.50;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.50 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 4.9
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2HPAAD6BItWlTJSfUpUCoRqDwaIU7Vphg8CgRcHOxEBAQEBAQEBAw4BAQEBBw0JCR8whDUBAQMBEhFJAggICwsLNwICIhIBBQEODhAJGweIBQgFomuBMT4xi0iQfwELASAKhk2EfoQsEoM1gUkFjjCEWoN8AYgwhSCOe4xuESSBFziCUoIFHTSDRSSBJwEBAQ
X-IronPort-AV: E=Sophos;i="5.20,522,1444719600"; 
   d="scan'208";a="9355867"
Received: from mail-wm0-f50.google.com ([74.125.82.50])
  by fe4.lbl.gov with ESMTP; 04 Jan 2016 15:52:11 -0800
Received: by mail-wm0-f50.google.com with SMTP id f206so4357672wmf.0
        for <singu...@lbl.gov>; Mon, 04 Jan 2016 15:52:11 -0800 (PST)
X-Gm-Message-State: ALoCoQmWq10DBH0Z+nyimbGvj/YBhHSzUDNzeBriap9ILgSd1kksaBHrP7e1bUicQzdBSP8bjpDIdkfVnwp3CCUqcAzYQc26Pl4NRKkP7DzyhW+we0hX+kWadUBQuNkT0hfTg2FQL3HAISpNcFr7D8jfUlRqSepFABpkW8tg9tIF5/Mw5mpoeyA=
X-Received: by 10.28.51.135 with SMTP id z129mr905936wmz.19.1451951531037;
        Mon, 04 Jan 2016 15:52:11 -0800 (PST)
MIME-Version: 1.0
X-Received: by 10.28.51.135 with SMTP id z129mr905930wmz.19.1451951530906;
 Mon, 04 Jan 2016 15:52:10 -0800 (PST)
Received: by 10.28.218.71 with HTTP; Mon, 4 Jan 2016 15:52:10 -0800 (PST)
In-Reply-To: <0C2E7133-169B-4546-9A8E-DF261094EF6F@open-mpi.org>
References: <937ff7ed-30b6-4143-849e-5e1a234ff89b@lbl.gov>
	<4D239990-0E7E-4F15-B5CB-09C8465562F0@lbl.gov>
	<2778d375-2145-4c15-81d5-0c961734993f@lbl.gov>
	<CAN7etTz8O5Vt4x+=oTp9+7MXEGxr8NT3K2u+BLxB5s=a9MKn4A@mail.gmail.com>
	<CAN7etTyafQgmAwAH94z1-3of=NNj9QQmLzJUzwn=mO8VRjJAFQ@mail.gmail.com>
	<D29DD7ED.89FB1%Grigory.Shamov@ad.umanitoba.ca>
	<D33AC0B6-5024-4234-BD25-7878E8E7C417@lbl.gov>
	<0C2E7133-169B-4546-9A8E-DF261094EF6F@open-mpi.org>
Date: Mon, 4 Jan 2016 15:52:10 -0800
Message-ID: <CAN7etTx+WFj6tQ=6=woiTJCta2z7P9m+hqG-TA8teAyqGXiijA@mail.gmail.com>
Subject: Re: [Singularity] Singularity and HPC support
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary=001a1144424e3b1eb005288acf85

--001a1144424e3b1eb005288acf85
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Ralph,

Sorry for the delay, the holidays got me a bit behind!



On Sat, Jan 2, 2016 at 9:45 AM, Ralph Castain <r...@open-mpi.org> wrote:

> Hello all
>
> I=E2=80=99ve been monitoring this mailing list since it began, but have b=
een
> swamped by too many things to respond. I=E2=80=99d like to specifically a=
ddress one
> of Greg=E2=80=99s early requests for input from some MPI folks. As some o=
f you may
> know, I=E2=80=99ve been a lead developer of OpenMPI=E2=80=99s runtime sin=
ce the project=E2=80=99s
> inception some 12 years ago, and have spent a lot of time interfacing it =
to
> various environments. So I=E2=80=99d like to try and provide some perspec=
tive in
> the hopes that Singularity may be able to better support HPC operations.
>

Excellent, and exactly what I was hoping for! I will answer as much as I
can, but I feel we may want to have a discussion in realtime to get more
details as it maybe faster to get on the same page initially (or that I may
confuse you with my answers! LOL). Perhaps a google hangout, skype, or
other means (e.g. we can go old fashioned and use a phone too!) would do
well.


>
> First, let me clarify my expected use-case. I=E2=80=99m assuming that a u=
ser
> requests an allocation of  some number of containers for running a
> multi-process application. The user may intend to run one or more
> application processes in each container. When allocated, the containers
> will belong to that one user, and will be for their exclusive use. The
> containers will be returned to the scheduler upon completion of the
> application.
>

Singularity really fogs up the idea of containers in terms of allocations,
resources, and contentions because the container runs like a standard
traditional application... Because it is not like other container platforms
you maybe using as a baseline. There are two ways to run Singularity apps:

The first is directly executing the container archive file (it is created
with +x, and via the magic of interpretation Singularity can run it
seamlessly). When the container is run in this manner, it extracts its
contents to a temporary directory and will only run in a single usage
context.

The second manner of running Singularity containers is more inline with
what you have described. Using the Singularity command, you install the
singularity container into a user specific cache, and from that cached
container, you can invoke as many instances of the program(s)/workflows
within the container at once as you like. If that cache directory is
shared, you can run processes from within that container on any number of
nodes (a kin to running an application from an NFS volume).

Again, I want to stress that Singularity containers all run within the
context of the user. There is no centralized management facility for any of
the Singularity containers (with the exception the application stack of
Singularity itself).


>
> So when I talk about =E2=80=9Cmultiple containers=E2=80=9D below, I=E2=80=
=99m talking strictly
> about the above use-case. It is quite possible that a user could have
> multiple allocations running in parallel - the containers for each
> allocation would be distinct and have no knowledge of the containers in
> another allocation, even though they belong to the same user. Allocations
> given to different users should have no knowledge of each other.
>

Understood and agreed.


>
> I expect that containers from different users will share a physical node
> (i.e., that cloud will typically be a multi-tenant environment), which
> implies there will be some resource contention between them. Thus, we wil=
l
> require some kind of QoS control over the networks. Remember, these will =
be
> OS-bypass networks, and so the OS in each container will have no control =
or
> even visibility over them.
>

This is where Singularity diverges considerably from other container
solutions. Singularity containers (or more specifically the applications
within the containers) are intended to run on the same networks, file
systems, and environment that the user already has access to. There is no
mechanism for the user to gain access or change users thus there is no need
to isolate the network stack into a virtualized namespace. Additionally,
while that maybe beneficially for containers that are mimicking virtual
machines, there is no need or benefit (that I see) to separate the network
namespace. Soooo... Long story short, there is no OS-bypass network or
additional QOSs that must be managed for Singularity.


>
> Within that context, the container must be given the following knowledge
> at startup if we really want to provide performance comparable to what is
> achieved today:
>
> * the ID of all other containers in this allocation - a unique =E2=80=9Ch=
ostname=E2=80=9D
> or its equivalent. Must include the IP address assigned to each container
>

If the container cache is located in a shared location, this should be easy
to do either within the job script or beforehand by the user. The IP
address would be the same as the host compute node running the job.


>
> * the =E2=80=9Ctrue=E2=80=9D location of each container in the allocation=
 - i.e., the
> physical node where the container is executing (so we can identify which
> processes are =E2=80=9Csharing=E2=80=9D physical resources) and where it =
is bound (which
> physical cpus; what network, memory, and other resources it has been give=
n;
> etc.)
>

I am hoping Singularity will make this easier then other container
solutions.


>
> * if application processes were started as part of setting up the
> containers, then which processes (i.e., which ranks) were put on each
> container, along with what network endpoints were assigned to them and
> where they are bound
>

It would be very convenient if the ranks, network ports, and everything
works just as it work natively from the MPI front. Perhaps I am
overthinking this, but because the Singularity containers exist on the same
physical network as would be expected, things should be easier in this
context.


>
> * the network topology between the containers - alternatively, you can
> provide the topology between the involved physical nodes and we can use t=
he
> above location info to get what we need
>

Same as above.


>
> * a method by which application procs can request any or all of that info
> as they need it in order to properly setup their communication
> infrastructure.
>

Hopefully environment variables will suffice (they pass through to the
Singularity container).


>
> * an enhancement would include a shared memory region on each physical
> node that hosts multiple containers so we could use shared memory
> communication in that situation. Obviously, there would need to be a
> dedicated region for each allocation to ensure separation of applications=
.
>

Shared memory (should) already be managed properly (in theory), but I have
not tested this yet.


>
> * a =E2=80=9Cglobal=E2=80=9D key-value exchange server that will allow us=
ers to =E2=80=9Cdiscover=E2=80=9D
> applications and their allocations, subject to access controls. Thus, an
> ocean simulator in one allocation could =E2=80=9Cdiscover=E2=80=9D and co=
nnect to an
> atmospheric model in another allocation so they can share data. The HPC
> world knows how to build this now - just a case of ensuring adequate
> knowledge transfer.
>

I'd be very interested in hearing more about what you are thinking here.


>
> I hope that helps provide some direction. I=E2=80=99d be happy to help co=
nstruct
> this capability, and interface it to OpenMPI.
>

Yep, totally!

Additionally, I have a major concern which has to do with environment
portability in matching the kernel and container API versions (e.g. OFED).
What I would love to see is that part of the MPI stack that interfaces with
the host environment/kernel gets called directly on the host, but the
processes itself still runs within the container context (possibly with a
container installed runtime component of the MPI stack). I have no idea how
this could work, but perhaps you would have some ideas on this or just tell
me it is impossible and kick me.

Thank you and let me know if I have indeed made this more complicated or
confusing and that we should discuss in real time!

Greg


--=20
Gregory M. Kurtzer
Technical Lead and HPC Systems Architect
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a1144424e3b1eb005288acf85
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Ralph,<div><br></div><div>Sorry for the delay, the holi=
days got me a bit behind!</div><div><br></div><div><br><div class=3D"gmail_=
extra"><br><div class=3D"gmail_quote">On Sat, Jan 2, 2016 at 9:45 AM, Ralph=
 Castain <span dir=3D"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org" target=
=3D"_blank">r...@open-mpi.org</a>&gt;</span> wrote:<br><blockquote class=3D=
"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding=
-left:1ex"><div style=3D"word-wrap:break-word">Hello all<div><br></div><div=
>I=E2=80=99ve been monitoring this mailing list since it began, but have be=
en swamped by too many things to respond. I=E2=80=99d like to specifically =
address one of Greg=E2=80=99s early requests for input from some MPI folks.=
 As some of you may know, I=E2=80=99ve been a lead developer of OpenMPI=E2=
=80=99s runtime since the project=E2=80=99s inception some 12 years ago, an=
d have spent a lot of time interfacing it to various environments. So I=E2=
=80=99d like to try and provide some perspective in the hopes that Singular=
ity may be able to better support HPC operations.</div></div></blockquote><=
div><br></div><div>Excellent, and exactly what I was hoping for! I will ans=
wer as much as I can, but I feel we may want to have a discussion in realti=
me to get more details as it maybe faster to get on the same page initially=
 (or that I may confuse you with my answers! LOL). Perhaps a google hangout=
, skype, or other means (e.g. we can go old fashioned and use a phone too!)=
 would do well.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div s=
tyle=3D"word-wrap:break-word"><div><br></div><div><div>First, let me clarif=
y my expected use-case. I=E2=80=99m assuming that a user requests an alloca=
tion of =C2=A0some number of containers for running a multi-process applica=
tion. The user may intend to run one or more application processes in each =
container. When allocated, the containers will belong to that one user, and=
 will be for their exclusive use. The containers will be returned to the sc=
heduler upon completion of the application.</div></div></div></blockquote><=
div><br></div><div>Singularity really fogs up the idea of containers in ter=
ms of allocations, resources, and contentions because the container runs li=
ke a standard traditional application... Because it is not like other conta=
iner platforms you maybe using as a baseline. There are two ways to run Sin=
gularity apps:</div><div><br></div><div>The first is directly executing the=
 container archive file (it is created with +x, and via the magic of interp=
retation Singularity can run it seamlessly). When the container is run in t=
his manner, it extracts its contents to a temporary directory and will only=
 run in a single usage context.</div><div><br></div><div>The second manner =
of running Singularity containers is more inline with what you have describ=
ed. Using the Singularity command, you install the singularity container in=
to a user specific cache, and from that cached container, you can invoke as=
 many instances of the program(s)/workflows within the container at once as=
 you like. If that cache directory is shared, you can run processes from wi=
thin that container on any number of nodes (a kin to running an application=
 from an NFS volume).</div><div><br></div><div>Again, I want to stress that=
 Singularity containers all run within the context of the user. There is no=
 centralized management facility for any of the Singularity containers (wit=
h the exception the application stack of Singularity itself).</div><div>=C2=
=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-word"=
><div><div><br></div><div>So when I talk about =E2=80=9Cmultiple containers=
=E2=80=9D below, I=E2=80=99m talking strictly about the above use-case. It =
is quite possible that a user could have multiple allocations running in pa=
rallel - the containers for each allocation would be distinct and have no k=
nowledge of the containers in another allocation, even though they belong t=
o the same user. Allocations given to different users should have no knowle=
dge of each other.</div></div></div></blockquote><div><br></div><div>Unders=
tood and agreed.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" st=
yle=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div =
style=3D"word-wrap:break-word"><div><div><br></div><div>I expect that conta=
iners from different users will share a physical node (i.e., that cloud wil=
l typically be a multi-tenant environment), which implies there will be som=
e resource contention between them. Thus, we will require some kind of QoS =
control over the networks. Remember, these will be OS-bypass networks, and =
so the OS in each container will have no control or even visibility over th=
em.</div></div></div></blockquote><div><br></div><div>This is where Singula=
rity diverges considerably from other container solutions. Singularity cont=
ainers (or more specifically the applications within the containers) are in=
tended to run on the same networks, file systems, and environment that the =
user already has access to. There is no mechanism for the user to gain acce=
ss or change users thus there is no need to isolate the network stack into =
a virtualized namespace. Additionally, while that maybe beneficially for co=
ntainers that are mimicking virtual machines, there is no need or benefit (=
that I see) to separate the network namespace. Soooo... Long story short, t=
here is no OS-bypass network or additional QOSs that must be managed for Si=
ngularity.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D=
"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word"><div><div><br></div><div>Within that context, the=
 container must be given the following knowledge at startup if we really wa=
nt to provide performance comparable to what is achieved today:</div><div><=
br></div><div>* the ID of all other containers in this allocation - a uniqu=
e =E2=80=9Chostname=E2=80=9D or its equivalent. Must include the IP address=
 assigned to each container</div></div></div></blockquote><div><br></div><d=
iv>If the container cache is located in a shared location, this should be e=
asy to do either within the job script or beforehand by the user. The IP ad=
dress would be the same as the host compute node running the job.</div><div=
>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;b=
order-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wrap:break-w=
ord"><div><div><br></div><div>* the =E2=80=9Ctrue=E2=80=9D location of each=
 container in the allocation - i.e., the physical node where the container =
is executing (so we can identify which processes are =E2=80=9Csharing=E2=80=
=9D physical resources) and where it is bound (which physical cpus; what ne=
twork, memory, and other resources it has been given; etc.)</div></div></di=
v></blockquote><div><br></div><div>I am hoping Singularity will make this e=
asier then other container solutions.</div><div>=C2=A0</div><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pa=
dding-left:1ex"><div style=3D"word-wrap:break-word"><div><div><br></div><di=
v>* if application processes were started as part of setting up the contain=
ers, then which processes (i.e., which ranks) were put on each container, a=
long with what network endpoints were assigned to them and where they are b=
ound</div></div></div></blockquote><div><br></div><div>It would be very con=
venient if the ranks, network ports, and everything works just as it work n=
atively from the MPI front. Perhaps I am overthinking this, but because the=
 Singularity containers exist on the same physical network as would be expe=
cted, things should be easier in this context.</div><div>=C2=A0</div><block=
quote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc=
 solid;padding-left:1ex"><div style=3D"word-wrap:break-word"><div><div><br>=
</div><div>* the network topology between the containers - alternatively, y=
ou can provide the topology between the involved physical nodes and we can =
use the above location info to get what we need</div></div></div></blockquo=
te><div><br></div><div>Same as above.</div><div>=C2=A0</div><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pa=
dding-left:1ex"><div style=3D"word-wrap:break-word"><div><div><br></div><di=
v>* a method by which application procs can request any or all of that info=
 as they need it in order to properly setup their communication infrastruct=
ure.</div></div></div></blockquote><div><br></div><div>Hopefully environmen=
t variables will suffice (they pass through to the Singularity container).<=
/div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"margin:0 0=
 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"word-wra=
p:break-word"><div><div><br></div><div>* an enhancement would include a sha=
red memory region on each physical node that hosts multiple containers so w=
e could use shared memory communication in that situation. Obviously, there=
 would need to be a dedicated region for each allocation to ensure separati=
on of applications.</div></div></div></blockquote><div><br></div><div>Share=
d memory (should) already be managed properly (in theory), but I have not t=
ested this yet.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div s=
tyle=3D"word-wrap:break-word"><div><div><br></div><div>* a =E2=80=9Cglobal=
=E2=80=9D key-value exchange server that will allow users to =E2=80=9Cdisco=
ver=E2=80=9D applications and their allocations, subject to access controls=
. Thus, an ocean simulator in one allocation could =E2=80=9Cdiscover=E2=80=
=9D and connect to an atmospheric model in another allocation so they can s=
hare data. The HPC world knows how to build this now - just a case of ensur=
ing adequate knowledge transfer.</div></div></div></blockquote><div><br></d=
iv><div>I&#39;d be very interested in hearing more about what you are think=
ing here.</div><div>=C2=A0</div><blockquote class=3D"gmail_quote" style=3D"=
margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=
=3D"word-wrap:break-word"><div><br></div><div>I hope that helps provide som=
e direction. I=E2=80=99d be happy to help construct this capability, and in=
terface it to OpenMPI.</div></div></blockquote><div><br></div><div>Yep, tot=
ally!</div><div><br></div><div>Additionally, I have a major concern which h=
as to do with environment portability in matching the kernel and container =
API versions (e.g. OFED). What I would love to see is that part of the MPI =
stack that interfaces with the host environment/kernel gets called directly=
 on the host, but the processes itself still runs within the container cont=
ext (possibly with a container installed runtime component of the MPI stack=
). I have no idea how this could work, but perhaps you would have some idea=
s on this or just tell me it is impossible and kick me.</div><div><br></div=
><div>Thank you and let me know if I have indeed made this more complicated=
 or confusing and that we should discuss in real time!</div><div><br></div>=
<div>Greg</div></div><br clear=3D"all"><div><br></div>-- <br><div>Gregory M=
. Kurtzer<br>Technical Lead and HPC Systems Architect<br>High Performance C=
omputing Services (HPCS)<br>University of California<br>Lawrence Berkeley N=
ational Laboratory<br>One Cyclotron Road, Berkeley, CA 94720</div>
</div></div></div>

--001a1144424e3b1eb005288acf85--
