X-Received: by 2002:a17:902:20ec:: with SMTP id v41mr822538plg.103.1553799748900;
        Thu, 28 Mar 2019 12:02:28 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:683:: with SMTP id 3ls1526070plh.1.gmail; Thu, 28
 Mar 2019 12:02:27 -0700 (PDT)
X-Received: by 2002:a17:902:7e05:: with SMTP id b5mr6559490plm.127.1553799746868;
        Thu, 28 Mar 2019 12:02:26 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1553799746; cv=none;
        d=google.com; s=arc-20160816;
        b=wQZpzy6tH5nmFYOfrytSpMXivBShiHv10PJvY79A0xuO1J0tQpVpkpFLMjwIyC3YIi
         RSMAfg1uUAkrKC08JGnnvnPKShH3w/nFJoR2pbCsWXCZIQuBWrEA9NXnEYtHH+UV+xQc
         rAstJJnPWIAmQ3nsN7wCyrX8EJoLXlgStflOfxnuKp9YbejLuHmKlicVP2pdcdE6hbMh
         5K/boF8FNqK6L47pBsYxw9U746nbfFg66OxC9cdh5VqulskDqu7vm4cZhbmpADDY2Sjh
         4XXXhs4CoyueXyLMbj9ZbC9QNxtzRzQSUf6qt+Ue31ZrP+jCElRY6NXqPJN1aMiVKx6L
         j/EQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:in-reply-to:references:mime-version
         :dkim-signature;
        bh=/yqBJ1Dquvnh1FKGyqkI/mFcKDzBG0KyWXKztZaD4yM=;
        b=oPZeFdhLmwvhGfvUEK7Ca+ueIRW0+ScYoq78ua0b7lgaReRc/QXP4Hyx/HXG5M801O
         Nnt/2Lupl76RsxnYaiFOUa2PVLsdmS2M+laVPHwav2z8sbR1fEas1z7WroWlDQ2D306V
         xi3zZTYP9WSO56rPZb7PEUJiTozSgCkkcgHVdh0BBzwWXFcSIBxSL11wIYJYyYIcQuLM
         6IWBHDyulQCWvYu/OpbPSM0ekG2xamdW1euQEjAPBeIU7AlI7gocdFw+eOmuC8Qv2O52
         HVovvAkOpYfczvfFs4h8xxQyBrDMj/OuYpqboiaht4O3synQ7fUViFLUL0KMQ8UwsGUH
         JAxA==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=uuwQJCB0;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.167.53 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [131.243.228.53])
        by mx.google.com with ESMTP id o6si20419132pgk.470.2019.03.28.12.02.26
        for <singu...@lbl.gov>;
        Thu, 28 Mar 2019 12:02:26 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 209.85.167.53 as permitted sender) client-ip=209.85.167.53;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=uuwQJCB0;
       spf=pass (google.com: domain of vict...@gmail.com designates 209.85.167.53 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2HFAACGGZ1chjWnVdFlHgEGBwaBVAYLA?=
 =?us-ascii?q?QGBZYESTDcnhAcHgR2CXo9Hgg2YUIErPCcBFYFJgnUChTUiNwYNAQEDAQEJAQM?=
 =?us-ascii?q?CAQECEAEBAQgLCwgpIwxCAQ4BgWgFAgMfBw5NOzABAQEBAQEBAQEBHwItKQEZA?=
 =?us-ascii?q?QEBAQIBGgkEGQENDg8PAwELBgULDRULAQkCAiEBAQ4DAQUBCxEOBwQBGgIEgwE?=
 =?us-ascii?q?BJgGBNQEDDQgFnwM8ix18FgUBF4J5BYQ9ChknDV+BOAIGEoEdAYRchlUXgUA/g?=
 =?us-ascii?q?RGCZC4+g38NBQERAgFdgkuCVwOKOgouA1OHWoVDjBA2CYJPiSqBc4VbGoIDiU+?=
 =?us-ascii?q?IOogxk3mCVTCBQksNMHFwFTsxgjsJggEMDgmBAAEJgkGKVEEwEIZLhyOBdwEB?=
X-IronPort-AV: E=Sophos;i="5.60,281,1549958400"; 
   d="scan'208,217";a="56768804"
Received: from mail-lf1-f53.google.com ([209.85.167.53])
  by fe4.lbl.gov with ESMTP; 28 Mar 2019 12:02:16 -0700
Received: by mail-lf1-f53.google.com with SMTP id g7so14779960lfh.10
        for <singu...@lbl.gov>; Thu, 28 Mar 2019 12:02:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to;
        bh=/yqBJ1Dquvnh1FKGyqkI/mFcKDzBG0KyWXKztZaD4yM=;
        b=uuwQJCB0EsdLQ9W3PRCPxl5n19Ag0kkQ1xfhacnc32Y5/KDi9ruU55P1YhoEzOCNSD
         OhwTck4lOIqxBwFg+FT1MWhRO3mR7J+mzotTG1NhYJj5ZurmUtZhSXY9d3bl8cBfU0DJ
         pFhr2ZVAaxMUsQ0HeLqVieP0Yp2hrmdwS6RNDkeFTxynJtrYlWPKDUVTOEKLmS0J0FqK
         EG19PK2ssi3ual5c4HLiVkus3J8id0WOg9p0h+HhWf8h2TF83bnUzrA43t+GERkvpTHi
         qttUathB/ydm1bPkThCnJaViDIK6I/gSDTtgBE/lekCfYDnZGb+nsyOdf9Oy+zEaXESz
         +cXg==
X-Gm-Message-State: APjAAAW1MLnKGIzKqHFe3qeuRSRk+dEvyaudcucZ1383Ws16XtUKicgi
	Xb6/5ldWiYmZBnKGs8ADX+ofDhKLRe3RtG7N8gEvgg==
X-Received: by 2002:a19:5201:: with SMTP id m1mr2313847lfb.68.1553799733651;
 Thu, 28 Mar 2019 12:02:13 -0700 (PDT)
MIME-Version: 1.0
References: <71fec772-2642-4b6e-98a3-8801b2ae0cf7@lbl.gov> <59709108-447E-491F-BB5A-C04D3F5AB654@gmail.com>
 <7A55B410-CD7D-4360-A1F3-44E1EF1ECA1B@gmail.com> <cbd878fc-f1a6-4548-8dd2-3ac380da2ecd@lbl.gov>
 <CAJk3+YVedLNPT6=hxrQtAR6xJG3NtERLRFRBCO3M4JL6YXS9Yg@mail.gmail.com>
 <2d3a29da-6518-4643-aef6-6664a21b2dc3@lbl.gov> <8a6e555b-8763-469f-aa4d-6f1b07521f93@lbl.gov>
 <CAJk3+YV6_KmOJoo+7gJ50izAxe_zAYQWhvrWxR8+5GfhGskcNw@mail.gmail.com>
 <bb9e3d9f-c17b-4a07-8733-3b5dc4a5ecd3@lbl.gov> <5ca92c63-5ef0-4074-b14c-7dba51726cce@lbl.gov>
 <042011a5-2246-4da6-a47c-3473fdae0dbe@lbl.gov>
In-Reply-To: <042011a5-2246-4da6-a47c-3473fdae0dbe@lbl.gov>
From: victor sv <vict...@gmail.com>
Date: Thu, 28 Mar 2019 20:02:02 +0100
Message-ID: <CA+Wz_Fy_RnHFLC7bP7kZbGPBjjQqeMaC8aTDF2O-NoYkeKSUwA@mail.gmail.com>
Subject: Re: [Singularity] Error when running OpenFoam over Singularity using Slurm
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="0000000000002d2ee705852c31b5"

--0000000000002d2ee705852c31b5
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi,

I've not read carefully all the thread, but (for me) the issue seems to be
clear in the first mail:

 Some info about slurm:
>
> # srun --mpi=3Dlist
> srun: MPI types are...
> srun: pmi2
> srun: openmpi
> srun: none
>
>
You describe how you match OpenMPI/PMI(x) versions in the container and in
the host to make it work. You are doing this because you need some level of
compatibility between these libraries in order to perform process
communications inside and outside the container.

If you use Slurm (srun) as process manager, you are removing the OpenMPI
layer outside the container. This should be Ok, but you still have to match
the PMI(x) inside and outside the container. If the Slurm you are using
supports PMIx (it seems that is not supported), then things could work with
the proper versions inside and outsie.

The main issue (I think) is that it seems that the host Slurm is linked
with PMI2. As far as I know, PMI2 is vendor dependant. I mean, there is
default API, but there is not ABI compatibility between different PMI
vendors (Slurm, OpenMPI, MPICH, etc.). To be more explicit, the combination
PMI2(Slurm) outside the container and PMI2(OpenMPI) inside the container
does not work in parallel multinode jobs.

In my mind the alternatives are:
  - to install exactly the same PMI2 version (implies the same Slurm
version) inside the container
  - If the OpenMPI inside the container  is linked agains PMI2, to
bindmount (and replace or at least prepend in the library path) the PMI2
from the host inside the container. This solution is dirty,but could work.
  - Continue using OpenMPI mpirun as process manager.

Hope it helps!

Best regards,
V=C3=ADctor

El lun., 4 mar. 2019 a las 10:50, 'ccvera' via singularity (<
singu...@lbl.gov>) escribi=C3=B3:

> Hello again,
>
> After a long time, I think one of the possible causes is that OpenFOAM
> works by default with SGE and not with Slurm. Today, in the cluster, they
> have installed OF and I don't need to use a container anymore.
>
> Anyway, I will continue testing to see if that is the real reason for the
> failure with Slurm or not.
>
> Thank you very much for your help!! :)
>
> Carmen
>
> El lunes, 18 de febrero de 2019, 17:33:03 (UTC+1), Justin Cook escribi=C3=
=B3:
>>
>> Carmen,
>>
>> It looks like this has a history going now. Can we move this to a github
>> issue?
>>
>> Can you show us the definition of your hello world container that is not
>> working? Let's start there and see where we get.
>>
>> Thanks,
>>
>> Justin
>>
>> On Wednesday, February 13, 2019 at 2:16:36 AM UTC-6, ccvera wrote:
>>>
>>> I was doing more tests and nothings works, I'm a little desperate with
>>> this problem... :(
>>>
>>> Thanks Samy for your answer but my problem exists with on single node a=
s
>>> well as 4 nodes (64 cores). Running:
>>>
>>> singularity exec -B /home ../../of6/openfoam6.MPIx.img mpiexec -n 16
>>> simpleFoam -case /home/carmen/test_singularity/OpenFOAM/pruebaOF5_16cor=
es
>>>
>>>
>>> I can work on 16 cores correctly, but I understand that I am running th=
e
>>> container's OMPI, likewise, I want to launch it on 64 cores for perform=
ance
>>> and for that (I understand) that I should launch it like this (as I hav=
e
>>> done with SGE):
>>>
>>> mpirun -n $SLURM_NTASKS singularity exec ../../of6/openfoam6.x.img
>>> simpleFoam -parallel -case
>>> /home/carmen/test_singularity/OpenFOAM/pruebaOF5_16cores
>>>
>>>
>>> And it doesn't work.
>>>
>>> Fatih :)
>>>
>>> xx True
>>> xx Not at all. I can run basic programs in parallel using slurm without
>>> singularity
>>> xx True. Only fails with Slurm and Singularity
>>>
>>> I've tried so many combinations and cases, so I'm sure I'm forgetting
>>> some of them.
>>>
>>> Background:
>>> - I was working with SGE and everything worked perfectly (recently the
>>> cluster in which I work has migrated to Slurm, -is a supercomputing cen=
ter,
>>> not a particular mini-cluster-)
>>> - This same container worked with up to 64 cores without problems.
>>>
>>> Some tests:
>>> - Connecting directly to a node by ssh, OF works correctly with 16 core=
s
>>> (w/ 32 too, but having only 16 cores creates oversubscription)
>>> - Doing salloc -N1 -n16 (or salloc -N2 -n32), and then run the program,
>>> fails.
>>> - Both with 16 cores and with 32, executing my script (sbatch
>>> myscript.sh), fails.
>>> - A simple "hello world" with Slurm doesn't work either, e.g. running:
>>> mpirun -n $SLURM_NTASKS singularity exec -B /home container.img
>>> /home/carmen/test_singularity/mpi_slurm/hello_world
>>>
>>> I attach the recipe of the last container that I created.
>>>
>>> Thanx,
>>>
>>> Carmen
>>> ----
>>> El s=C3=A1bado, 9 de febrero de 2019, 5:19:16 (UTC+1), Fatih Ertinaz es=
cribi=C3=B3:
>>>>
>>>> Ok, this is really interesting to me. So as a summary -- if I'm not
>>>> mistaken:
>>>>
>>>> xx You can run parallel OF on a single node using OMPI through
>>>> Singularity without Slurm
>>>> xx You can run basic parallel MPI tasks using Slurm with and without
>>>> Singularity
>>>> xx You cannot run multi-node basic parallel jobs using Singularity -- =
I
>>>> don't know if you used Slurm, so maybe fails with both
>>>>
>>>> Should Slurm have a special configuration to run mpi programs in
>>>>> parallel with singularity, apart from OpenMPI and PMIx?
>>>>
>>>>
>>>> I don't think Slurm is the problematic part because of the item 2
>>>> above. For sure Slurm needs information about compute nodes and user
>>>> account, but if that was the issue you wouldn't be able to run any tas=
ks
>>>> even on a single node. With that being said, I never configured Slurm =
from
>>>> scratch, so I am not a Slurm expert.
>>>>
>>>> Also, I don't think this is an OF issue because of the item 1. However
>>>> you might want to make sure if OF is built with "SYSTEMOPENMPI" option=
. But
>>>> again, even though restricted to 1 node, you managed to run it without
>>>> Slurm so OF should be fine imho.
>>>>
>>>> If you cannot run multi-node jobs, I guess that's a clear indication o=
f
>>>> a potential OMPI problem. Check how OMPI is installed, which fabrics a=
re
>>>> being used etc. Additionally, you can also check if Slurm flag is
>>>> explicitly defined, sth. like "--with-slurm=3D/opt/slurm".
>>>>
>>>> Moreover can you give some information about the cluster you're workin=
g
>>>> on? I mean, is this a typical cluster with many users running their
>>>> simulations? If that's the case, then I think Slurm or OMPI should be =
quite
>>>> reliable. If this is a cloud cluster that you're experimenting, I bet =
it is
>>>> OMPI :)
>>>>
>>>> Hope this helps
>>>>
>>>> On Fri, Feb 8, 2019 at 2:51 PM Samy <sma...@gmail.com> wrote:
>>>>
>>>>> Can you try to run the same command on single node and see? like:
>>>>> mpirun -n *1* singularity exec .....
>>>>> Also, if you have access to interactive mode nodes, it would be a goo=
d
>>>>> test to run OF with mpirun interactively on 2 or more nodes. It sound=
s to
>>>>> me that it's an issue running your OF on multinode not a slurm proble=
m.
>>>>>
>>>>> Good luck,
>>>>>
>>>>>
>>>>> On Friday, February 8, 2019 at 12:09:26 AM UTC-8, ccvera wrote:
>>>>>>
>>>>>> Hi!
>>>>>>
>>>>>> I didn't say it in my first post (sorry) but, in case it serves as
>>>>>> information, the problem appears only when I execute OF in parallel =
(using
>>>>>> the -parallel option, that is what I need).
>>>>>>
>>>>>> Regarding the options you mention to me, Fatih:
>>>>>> xx I don`t have problems executing simple works (and even some more
>>>>>> complicated) e.g. variable printing and information (all without
>>>>>> singularity). Also, I run singularity basic programs and, normally, =
I use
>>>>>> then to train CNN (no need MPI) and all work fine.
>>>>>> xx I have replicated the OpenMPI and PMIx host installation in the
>>>>>> container, so they have the same versions and libraries were copied.
>>>>>>
>>>>>> In the logs, both slurmd and slurmctl or the nodes logs I'm not
>>>>>> seeing nothing that gives me light.
>>>>>> I think you're right when you tell me that it can be an openmpi
>>>>>> problem. I'm trying again to execute a "hello world" on singularity =
and
>>>>>> when requesting several nodes I have the same problems :(
>>>>>>
>>>>>> Should Slurm have a special configuration to run mpi programs in
>>>>>> parallel with singularity, apart from OpenMPI and PMIx? Also, should=
 I
>>>>>> include other configuration in my container?
>>>>>>
>>>>>> Thax for your help.
>>>>>>
>>>>>> Carmen.
>>>>>>
>>>>>> El mi=C3=A9rcoles, 6 de febrero de 2019, 15:33:36 (UTC+1), Fatih Ert=
inaz
>>>>>> escribi=C3=B3:
>>>>>>>
>>>>>>> Hello Carmen
>>>>>>>
>>>>>>> To me this looks like an OpenMPI & Slurm issue rather than an OF &
>>>>>>> Slurm problem.
>>>>>>>
>>>>>>> Few things you can check;
>>>>>>> xx Try to execute simple jobs using Slurm, e.g. printing hostnames
>>>>>>> or mpi ping-pong stuff.
>>>>>>> xx Do you know how OpenMPI is installed in the host? Maybe it is
>>>>>>> built with some other underlying libraries regarding IB that you do=
n't have
>>>>>>> in your container.
>>>>>>>
>>>>>>> I'd say if the first one works with hostnames then I'd say focus on
>>>>>>> the OpenMPI installation.
>>>>>>>
>>>>>>> On Wed, Feb 6, 2019 at 4:44 AM 'ccvera' via singularity <
>>>>>>> si...@lbl.gov> wrote:
>>>>>>>
>>>>>>>> Thanks a lot for your quickly reply :)
>>>>>>>>
>>>>>>>> This solution doesn't work for me. I tried to unset all SLURM
>>>>>>>> environment variables (first SLURM_JOBID, then SLURM_NODELIST and =
finally
>>>>>>>> all as you told me) and i obtain the same MPI error.
>>>>>>>>
>>>>>>>> Carmen
>>>>>>>>
>>>>>>>> El martes, 5 de febrero de 2019, 17:56:27 (UTC+1), Shenglong Wang
>>>>>>>> escribi=C3=B3:
>>>>>>>>>
>>>>>>>>> It seems
>>>>>>>>>
>>>>>>>>> unset SLURM_JOBID
>>>>>>>>>
>>>>>>>>> is enough to cheat mpiexec
>>>>>>>>>
>>>>>>>>> Shenglong
>>>>>>>>>
>>>>>>>>> On Feb 5, 2019, at 11:37 AM, Shenglong Wang <wa...@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>> Can you try to unset all SLURM environment variables?
>>>>>>>>>
>>>>>>>>> for e in $(env | egrep ^SLURM_ | cut -d=3D -f1); do unset $e; don=
e
>>>>>>>>>
>>>>>>>>> or
>>>>>>>>>
>>>>>>>>> unset SLURM_NODELIST
>>>>>>>>>
>>>>>>>>> But you=E2=80=99ll have to manually generate host file.
>>>>>>>>>
>>>>>>>>> Best,
>>>>>>>>> Shenglong
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Feb 5, 2019, at 11:30 AM, 'ccvera' via singularity <
>>>>>>>>> si...@lbl.gov> wrote:
>>>>>>>>>
>>>>>>>>> Dear all,
>>>>>>>>>
>>>>>>>>> I'm experiencing some issues running OpenFOAM over singularity
>>>>>>>>> with slurm.
>>>>>>>>>
>>>>>>>>> I've several images based on Ubuntu and within several versions o=
f
>>>>>>>>> OpenMPI and PMIx and i'm able to run OpenFOAM correctly without u=
se slurm
>>>>>>>>> (directly on the node) using next command:
>>>>>>>>>
>>>>>>>>> $ mpirun -n 16 singularity exec -B /home ../../of6/openfoam6.x.im=
g
>>>>>>>>> simpleFoam -parallel -case
>>>>>>>>> /home/carmen/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/pr=
uebaOF6_16cores
>>>>>>>>>
>>>>>>>>> My problem comes when I run my program with slurm. Whether I make
>>>>>>>>> salloc or execute a script with sbatch, it shows me the following=
 error:
>>>>>>>>>
>>>>>>>>> It looks like MPI_INIT failed for some reason; your parallel
>>>>>>>>> process is
>>>>>>>>>
>>>>>>>>> likely to abort.  There are many reasons that a parallel process
>>>>>>>>> can
>>>>>>>>>
>>>>>>>>> fail during MPI_INIT; some of which are due to configuration or
>>>>>>>>> environment
>>>>>>>>>
>>>>>>>>> problems.  This failure appears to be an internal failure; here's
>>>>>>>>> some
>>>>>>>>>
>>>>>>>>> additional information (which may only be relevant to an Open MPI
>>>>>>>>>
>>>>>>>>> developer):
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>   ompi_mpi_init: ompi_rte_init failed
>>>>>>>>>
>>>>>>>>>   --> Returned "(null)" (-43) instead of "Success" (0)
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> and
>>>>>>>>>
>>>>>>>>> *** An error occurred in MPI_Init_thread
>>>>>>>>>
>>>>>>>>> *** on a NULL communicator
>>>>>>>>>
>>>>>>>>> *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now
>>>>>>>>> abort,
>>>>>>>>>
>>>>>>>>> ***    and potentially your MPI job)
>>>>>>>>>
>>>>>>>>> [cn3045:369] Local abort before MPI_INIT completed completed
>>>>>>>>> successfully, but am not able to aggregate error messages, and no=
t able to
>>>>>>>>> guarantee that all other processes were killed!
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> I know I must have the same openMPI versions on both (host and
>>>>>>>>> container) and I have also tried other versions of OpenMPI (2.X.X=
) and in
>>>>>>>>> all cases OpenFOAM works correctly, but at the moment I want to r=
un it with
>>>>>>>>> slurm it show me the errors.
>>>>>>>>>
>>>>>>>>> I have also tried other ways to run the program with srun using
>>>>>>>>> the option --mpi=3Dpmi2 (among others) but I always find the same=
 problem.
>>>>>>>>>
>>>>>>>>> I use the following script to run OpenFoam:
>>>>>>>>>
>>>>>>>>> ----------------------------
>>>>>>>>>
>>>>>>>>> #!/bin/bash
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> #SBATCH -N 1
>>>>>>>>>
>>>>>>>>> #SBATCH -p haswell
>>>>>>>>>
>>>>>>>>> #SBATCH -J test_OpenFOAM
>>>>>>>>>
>>>>>>>>> #SBATCH --output=3D"singularity.%j.o"
>>>>>>>>>
>>>>>>>>> #SBATCH --error=3D"singularity.%j.e"
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> module load haswell/singularity_2.6.0
>>>>>>>>>
>>>>>>>>> module load haswell/openmpi_3.1.2_gcc8.2.0_pmix
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ulimit -s unlimited
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> mpirun -n 16 singularity exec ../../of6/openfoam6.x.img simpleFoa=
m
>>>>>>>>> -parallel -case
>>>>>>>>> /home/software/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/=
pruebaOF6_16cores
>>>>>>>>>
>>>>>>>>> ----------------------------
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> The versions that I'm using are:
>>>>>>>>>
>>>>>>>>> *Host: *
>>>>>>>>>
>>>>>>>>> OS: CentOS7.5
>>>>>>>>>
>>>>>>>>> OpenMPI: 3.1.2
>>>>>>>>>
>>>>>>>>> PMIx: 2.1.4
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> *Container:*
>>>>>>>>>
>>>>>>>>> OS: Ubuntu16.04
>>>>>>>>>
>>>>>>>>> OpenMPI: 3.1.2
>>>>>>>>>
>>>>>>>>> PMIx: 2.1.4
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Can it be a configuration problem of SLURM? Is there any
>>>>>>>>> limitation of SLURM that is affecting OpenFOAM?
>>>>>>>>>
>>>>>>>>> Some info about slurm:
>>>>>>>>>
>>>>>>>>> # srun --version
>>>>>>>>> slurm 18.08.3
>>>>>>>>> # srun --mpi=3Dlist
>>>>>>>>> srun: MPI types are...
>>>>>>>>> srun: pmi2
>>>>>>>>> srun: openmpi
>>>>>>>>> srun: none
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> I'm a little bit lost with this issue :(
>>>>>>>>> Can someone give me some lights?
>>>>>>>>>
>>>>>>>>> Thanks a lot in advance,
>>>>>>>>> Carmen
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> You received this message because you are subscribed to the Googl=
e
>>>>>>>>> Groups "singularity" group.
>>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--0000000000002d2ee705852c31b5
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>Hi, <br></div><div><br></div><div>I&#39;ve not read c=
arefully all the thread, but (for me) the issue seems to be clear in the fi=
rst mail:</div><div><br></div><blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:=
1ex"><div><div><div>=C2=A0Some info about slurm:<br></div></div><blockquote=
 style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><div=
># srun --mpi=3Dlist</div></div><div><div>srun: MPI types are...</div></div=
><div><div>srun: pmi2</div></div><div><div>srun: openmpi</div></div><div><d=
iv>srun: none</div></div></blockquote></div></blockquote><div>=C2=A0</div><=
div>You describe how you match OpenMPI/PMI(x) versions in the container and=
 in the host to make it work. You are doing this because you need some leve=
l of compatibility between these libraries in order to perform process comm=
unications inside and outside the container.</div><div><br></div><div>If yo=
u use Slurm (srun) as process manager, you are removing the OpenMPI layer o=
utside the container. This should be Ok, but you still have to match the PM=
I(x) inside and outside the container. If the Slurm you are using supports =
PMIx (it seems that is not supported), then things could work with the prop=
er versions inside and outsie. <br></div><div><br></div><div>The main issue=
 (I think) is that it seems that the host Slurm is linked with PMI2. As far=
 as I know, PMI2 is vendor dependant. I mean, there is default API, but the=
re is not ABI compatibility between different PMI vendors (Slurm, OpenMPI, =
MPICH, etc.). To be more explicit, the combination PMI2(Slurm) outside the =
container and PMI2(OpenMPI) inside the container does not work in parallel =
multinode jobs.</div><div><br></div><div>In my mind the alternatives are:</=
div><div>=C2=A0 - to install exactly the same PMI2 version (implies the sam=
e Slurm version) inside the container</div><div>=C2=A0 - If the OpenMPI ins=
ide the container=C2=A0 is linked agains PMI2, to bindmount (and replace or=
 at least prepend in the library path) the PMI2 from the host inside the co=
ntainer. This solution is dirty,but could work.</div><div>=C2=A0 - Continue=
 using OpenMPI mpirun as process manager.</div><div><br></div><div>Hope it =
helps!</div><div><br></div><div>Best regards,<br></div><div>V=C3=ADctor <br=
></div></div><br><div class=3D"gmail_quote"><div dir=3D"ltr" class=3D"gmail=
_attr">El lun., 4 mar. 2019 a las 10:50, &#39;ccvera&#39; via singularity (=
&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov<=
/a>&gt;) escribi=C3=B3:<br></div><blockquote class=3D"gmail_quote" style=3D=
"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-le=
ft:1ex"><div dir=3D"ltr"><div>Hello again,</div><div><br></div><div>After a=
 long time, I think one of the possible causes is that OpenFOAM works by de=
fault with SGE and not with Slurm. Today, in the cluster, they have install=
ed OF and I don&#39;t need to use a container anymore.=C2=A0</div><div><br>=
</div><div>Anyway, I will continue testing to see if that is the real reaso=
n for the failure with Slurm or not.</div><div><br></div><div>Thank you ver=
y much for your help!! :)</div><div><br></div><div>Carmen</div><br>El lunes=
, 18 de febrero de 2019, 17:33:03 (UTC+1), Justin Cook  escribi=C3=B3:<bloc=
kquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:=
1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><div>Carmen,<=
/div><div><br></div><div>It looks like this has a history going now. Can we=
 move this to a github issue?</div><div><br></div><div>Can you show us the =
definition of your hello world container that is not working? Let&#39;s sta=
rt there and see where we get.<br></div><div><br></div><div>Thanks,</div><d=
iv><br></div><div>Justin<br></div><br>On Wednesday, February 13, 2019 at 2:=
16:36 AM UTC-6, ccvera wrote:<blockquote class=3D"gmail_quote" style=3D"mar=
gin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1=
ex"><div dir=3D"ltr"><div>I was doing more tests and nothings works, I&#39;=
m a little desperate with this problem... :(</div><div><br></div><div>Thank=
s Samy for your answer but my problem exists with on single node as well as=
 4 nodes (64 cores). Running:</div><div><br></div><blockquote style=3D"marg=
in:0px 0px 0px 40px;border:medium none;padding:0px"><div>singularity exec -=
B /home ../../of6/openfoam6.MPIx.img mpiexec -n 16 simpleFoam -case /home/c=
armen/test_singularity/OpenFOAM/pruebaOF5_16cores</div></blockquote><div><b=
r></div><div>I can work on 16 cores correctly, but I understand that I am r=
unning the container&#39;s OMPI, likewise, I want to launch it on 64 cores =
for performance and for that (I understand) that I should launch it like th=
is (as I have done with SGE):</div><div><br></div><blockquote style=3D"marg=
in:0px 0px 0px 40px;border:medium none;padding:0px"><div>mpirun -n $SLURM_N=
TASKS singularity exec ../../of6/openfoam6.x.img simpleFoam -parallel -case=
 /home/carmen/test_singularity/OpenFOAM/pruebaOF5_16cores</div></blockquote=
><div><br></div><div>And it doesn&#39;t work.</div><div><br></div><div>Fati=
h :)</div><div><br></div><div>xx True</div><div>xx Not at all. I can run ba=
sic programs in parallel using slurm without singularity</div><div>xx True.=
 Only fails with Slurm and Singularity</div><div><br></div><div>I&#39;ve tr=
ied so many combinations and cases, so I&#39;m sure I&#39;m forgetting some=
 of them.</div><div><br></div><div>Background:</div><div>- I was working wi=
th SGE and everything worked perfectly (recently the cluster in which I wor=
k has migrated to Slurm, -is a supercomputing center, not a particular mini=
-cluster-)</div><div>- This same container worked with up to 64 cores witho=
ut problems.</div><div><br></div><div>Some tests:</div><div>- Connecting di=
rectly to a node by ssh, OF works correctly with 16 cores (w/ 32 too, but h=
aving only 16 cores creates oversubscription)</div><div>- Doing salloc -N1 =
-n16 (or salloc -N2 -n32), and then run the program, fails.</div><div>- Bot=
h with 16 cores and with 32, executing my script (sbatch myscript.sh), fail=
s.</div><div>- A simple &quot;hello world&quot; with Slurm doesn&#39;t work=
 either, e.g. running:</div><div>mpirun -n $SLURM_NTASKS singularity exec -=
B /home container.img /home/carmen/test_singularity/mpi_slurm/hello_world</=
div><div><br></div><div>I attach the recipe of the last container that I cr=
eated.</div><div><br></div><div>Thanx,</div><div><br></div><div>Carmen</div=
>----<br>El s=C3=A1bado, 9 de febrero de 2019, 5:19:16 (UTC+1), Fatih Ertin=
az  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px=
 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex"><div di=
r=3D"ltr"><div dir=3D"ltr">Ok, this is really interesting to me. So as a su=
mmary -- if I&#39;m not mistaken:<div><br><div>xx You can run parallel OF o=
n a single node using OMPI through Singularity without Slurm=C2=A0</div><di=
v>xx You can run basic parallel MPI tasks using Slurm with and without Sing=
ularity</div><div>xx You cannot run multi-node basic parallel jobs using Si=
ngularity -- I don&#39;t know if you used Slurm, so maybe fails with both</=
div><div><div><br></div></div><blockquote class=3D"gmail_quote" style=3D"ma=
rgin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding-left:=
1ex">Should Slurm have a special configuration to run mpi programs in paral=
lel with singularity, apart from OpenMPI and PMIx?</blockquote><div><br></d=
iv><div>I don&#39;t think Slurm is the problematic part because of the item=
 2 above. For sure Slurm needs information about compute nodes and user acc=
ount, but if that was the issue you wouldn&#39;t be able to run any tasks e=
ven on a single node. With that being said, I never configured Slurm from s=
cratch, so I am not a Slurm expert.</div><div><br></div><div>Also, I don&#3=
9;t think this is an OF issue because of the item 1. However you might want=
 to make sure if OF is built with &quot;SYSTEMOPENMPI&quot; option. But aga=
in, even though restricted to 1 node, you managed to run it without Slurm s=
o OF should be fine imho.=C2=A0</div><div><br></div><div>If you cannot run =
multi-node jobs, I guess that&#39;s a clear indication of a potential OMPI =
problem. Check how OMPI is installed, which fabrics are being used etc. Add=
itionally, you can also check if Slurm flag is explicitly defined, sth. lik=
e &quot;--with-slurm=3D/opt/slurm&quot;.=C2=A0</div><div><br></div><div>Mor=
eover can you give some information about the cluster you&#39;re working on=
? I mean, is this a typical cluster with many users running their simulatio=
ns? If that&#39;s the case, then I think Slurm or OMPI should be quite reli=
able. If this is a cloud cluster that you&#39;re experimenting, I bet it is=
 OMPI :)</div><div><br></div><div>Hope this helps=C2=A0</div></div></div></=
div><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Fri, Feb 8, 2019 at =
2:51 PM Samy &lt;<a rel=3D"nofollow">sma...@gmail.com</a>&gt; wrote:<br></d=
iv><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;bord=
er-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr">Can y=
ou try to run the same command on single node and see? like: mpirun -n <b>1=
</b> singularity exec .....<div>Also, if you have access to interactive mod=
e nodes, it would be a good test to run OF with mpirun interactively on 2 o=
r more nodes. It sounds to me that it&#39;s an issue running your OF on mul=
tinode not a slurm problem.</div><div><br></div><div>Good luck,</div><div><=
br></div><div><br>On Friday, February 8, 2019 at 12:09:26 AM UTC-8, ccvera =
wrote:<blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;b=
order-left:1px solid rgb(204,204,204);padding-left:1ex"><div dir=3D"ltr"><d=
iv>Hi!</div><div><br></div><div>I didn&#39;t say it in my first post (sorry=
) but, in case it serves as information, the problem appears only when I ex=
ecute OF in parallel (using the -parallel option, that is what I need).</di=
v><div><br></div><div>Regarding the options you mention to me, Fatih:</div>=
<div>xx I don`t have problems executing simple works (and even some more co=
mplicated) e.g. variable printing and information (all without singularity)=
. Also, I run singularity basic programs and, normally, I use then to train=
 CNN (no need MPI) and all work fine.</div><div>xx I have replicated the Op=
enMPI and PMIx host installation in the container, so they have the same ve=
rsions and libraries were copied.=C2=A0</div><div><br></div><div>In the log=
s, both slurmd and slurmctl or the nodes logs I&#39;m not seeing nothing th=
at gives me light.=C2=A0</div><div>I think you&#39;re right when you tell m=
e that it can be an openmpi problem. I&#39;m trying again to execute a &quo=
t;hello world&quot; on singularity and when requesting several nodes I have=
 the same problems :(</div><div><br></div><div>Should Slurm have a special =
configuration to run mpi programs in parallel with singularity, apart from =
OpenMPI and PMIx? Also, should I include other configuration in my containe=
r?=C2=A0</div><div><br></div><div>Thax for your help.</div><div><br></div><=
div>Carmen.</div><br>El mi=C3=A9rcoles, 6 de febrero de 2019, 15:33:36 (UTC=
+1), Fatih Ertinaz  escribi=C3=B3:<blockquote class=3D"gmail_quote" style=
=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,204,204);padding=
-left:1ex"><div dir=3D"ltr">Hello Carmen<div><br></div><div>To me this look=
s like an OpenMPI &amp; Slurm issue rather than an OF &amp; Slurm problem.=
=C2=A0</div><div><br></div><div>Few things you can check;</div><div>xx Try =
to execute simple jobs using Slurm, e.g. printing hostnames or mpi ping-pon=
g stuff.=C2=A0</div><div>xx Do you know how OpenMPI is installed in the hos=
t? Maybe it is built with some other underlying libraries regarding IB that=
 you don&#39;t have in your container.</div><div><br></div><div>I&#39;d say=
 if the first one works with hostnames then I&#39;d say focus on the OpenMP=
I installation.</div></div><br><div class=3D"gmail_quote"><div dir=3D"ltr">=
On Wed, Feb 6, 2019 at 4:44 AM &#39;ccvera&#39; via singularity &lt;<a rel=
=3D"nofollow">si...@lbl.gov</a>&gt; wrote:<br></div><blockquote class=3D"gm=
ail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid rgb(204,=
204,204);padding-left:1ex"><div dir=3D"ltr"><div>Thanks a lot for your quic=
kly reply :)</div><div><br></div><div>This solution doesn&#39;t work for me=
. I tried to unset all SLURM environment variables (first SLURM_JOBID, then=
 SLURM_NODELIST and finally all as you told me) and i obtain the same MPI e=
rror.=C2=A0</div><div><br></div><div>Carmen</div><br>El martes, 5 de febrer=
o de 2019, 17:56:27 (UTC+1), Shenglong Wang  escribi=C3=B3:<blockquote clas=
s=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px solid r=
gb(204,204,204);padding-left:1ex"><div>It seems=C2=A0<div><br></div><div>un=
set SLURM_JOBID</div><div><br></div><div>is enough to cheat mpiexec</div><d=
iv><br></div><div>Shenglong<br><div><br><blockquote type=3D"cite"><div>On F=
eb 5, 2019, at 11:37 AM, Shenglong Wang &lt;<a rel=3D"nofollow">wa...@gmail=
.com</a>&gt; wrote:</div><br><div><div>Can you try to unset all SLURM envir=
onment variables?<div><br></div><div><div>for e in $(env | egrep ^SLURM_ | =
cut -d=3D -f1); do unset $e; done</div><div><br></div><div>or</div><div><br=
></div><div>unset SLURM_NODELIST</div><div><br></div><div>But you=E2=80=99l=
l have to manually generate host file.</div><div><br></div><div>Best,</div>=
<div>Shenglong</div><div><br><div><br><blockquote type=3D"cite"><div>On Feb=
 5, 2019, at 11:30 AM, &#39;ccvera&#39; via singularity &lt;<a rel=3D"nofol=
low">si...@lbl.gov</a>&gt; wrote:</div><br><div><div dir=3D"ltr"><div><div>=
Dear all,</div><div><br></div><div>I&#39;m experiencing some issues running=
 OpenFOAM over singularity with slurm.=C2=A0</div><div><br></div><div>I&#39=
;ve several images based on Ubuntu and within several versions of OpenMPI a=
nd PMIx and i&#39;m able to run OpenFOAM correctly without use slurm (direc=
tly on the node) using next command:</div><div><br></div><div>$ mpirun -n 1=
6 singularity exec -B /home ../../of6/openfoam6.x.img simpleFoam -parallel =
-case /home/carmen/test_singularity/OpenFOAM/pruebaOF6_16cores_SLURM/prueba=
OF6_16cores</div><div><br></div><div>My problem comes when I run my program=
 with slurm. Whether I make salloc or execute a script with sbatch, it show=
s me the following error:</div><div><br></div></div><blockquote style=3D"ma=
rgin:0px 0px 0px 40px;border:medium none;padding:0px"><blockquote style=3D"=
margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><div><font col=
or=3D"#666666">It looks like MPI_INIT failed for some reason; your parallel=
 process is</font></div></div></blockquote><blockquote style=3D"margin:0px =
0px 0px 40px;border:medium none;padding:0px"><div><div><font color=3D"#6666=
66">likely to abort.=C2=A0 There are many reasons that a parallel process c=
an</font></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 4=
0px;border:medium none;padding:0px"><div><div><font color=3D"#666666">fail =
during MPI_INIT; some of which are due to configuration or environment</fon=
t></div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;bor=
der:medium none;padding:0px"><div><div><font color=3D"#666666">problems.=C2=
=A0 This failure appears to be an internal failure; here&#39;s some</font><=
/div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border=
:medium none;padding:0px"><div><div><font color=3D"#666666">additional info=
rmation (which may only be relevant to an Open MPI</font></div></div></bloc=
kquote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;padd=
ing:0px"><div><div><font color=3D"#666666">developer):</font></div></div></=
blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;=
padding:0px"><div><div><font color=3D"#666666"><br></font></div></div></blo=
ckquote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;pad=
ding:0px"><div><div><font color=3D"#666666">=C2=A0 ompi_mpi_init: ompi_rte_=
init failed</font></div></div></blockquote><blockquote style=3D"margin:0px =
0px 0px 40px;border:medium none;padding:0px"><div><div><font color=3D"#6666=
66">=C2=A0 --&gt; Returned &quot;(null)&quot; (-43) instead of &quot;Succes=
s&quot; (0)</font></div></div></blockquote></blockquote><div><div><br></div=
><div>and</div><div><br></div></div><blockquote style=3D"margin:0px 0px 0px=
 40px;border:medium none;padding:0px"><blockquote style=3D"margin:0px 0px 0=
px 40px;border:medium none;padding:0px"><div><div><font color=3D"#666666">*=
** An error occurred in MPI_Init_thread</font></div></div></blockquote><blo=
ckquote style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><d=
iv><div><font color=3D"#666666">*** on a NULL communicator</font></div></di=
v></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium n=
one;padding:0px"><div><div><font color=3D"#666666">*** MPI_ERRORS_ARE_FATAL=
 (processes in this communicator will now abort,</font></div></div></blockq=
uote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;paddin=
g:0px"><div><div><font color=3D"#666666">***=C2=A0 =C2=A0 and potentially y=
our MPI job)</font></div></div></blockquote><blockquote style=3D"margin:0px=
 0px 0px 40px;border:medium none;padding:0px"><div><div><font color=3D"#666=
666">[cn3045:369] Local abort before MPI_INIT completed completed successfu=
lly, but am not able to aggregate error messages, and not able to guarantee=
 that all other processes were killed!</font></div></div></blockquote></blo=
ckquote><div><div><br></div><div>I know I must have the same openMPI versio=
ns on both (host and container) and I have also tried other versions of Ope=
nMPI (2.X.X) and in all cases OpenFOAM works correctly, but at the moment I=
 want to run it with slurm it show me the errors.</div><div><br></div><div>=
I have also tried other ways to run the program with srun using the option =
--mpi=3Dpmi2 (among others) but I always find the same problem.</div><div><=
br></div><div>I use the following script to run OpenFoam:</div></div><block=
quote style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div=
><div>----------------------------</div></div></blockquote><blockquote styl=
e=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><blockquote st=
yle=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><div>#!=
/bin/bash</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px 4=
0px;border:medium none;padding:0px"><div><div><br></div></div></blockquote>=
<blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px=
"><div><div>#SBATCH -N 1</div></div></blockquote><blockquote style=3D"margi=
n:0px 0px 0px 40px;border:medium none;padding:0px"><div><div>#SBATCH -p has=
well=C2=A0</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px =
40px;border:medium none;padding:0px"><div><div>#SBATCH -J test_OpenFOAM</di=
v></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:me=
dium none;padding:0px"><div><div>#SBATCH --output=3D&quot;singularity.%j.o&=
quot;=C2=A0</div></div></blockquote><blockquote style=3D"margin:0px 0px 0px=
 40px;border:medium none;padding:0px"><div><div>#SBATCH --error=3D&quot;sin=
gularity.%j.e&quot;</div></div></blockquote><blockquote style=3D"margin:0px=
 0px 0px 40px;border:medium none;padding:0px"><div><div><br></div></div></b=
lockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;p=
adding:0px"><div><div>module load haswell/singularity_2.6.0</div></div></bl=
ockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;pa=
dding:0px"><div><div>module load haswell/openmpi_3.1.2_gcc8.2.0_pmix</div><=
/div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:mediu=
m none;padding:0px"><div><div><br></div></div></blockquote><blockquote styl=
e=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><div>ulim=
it -s unlimited</div></div></blockquote><blockquote style=3D"margin:0px 0px=
 0px 40px;border:medium none;padding:0px"><div><div><br></div></div></block=
quote><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;paddi=
ng:0px"><div><div>mpirun -n 16=C2=A0singularity exec ../../of6/openfoam6.x.=
img simpleFoam -parallel -case /home/software/test_singularity/OpenFOAM/pru=
ebaOF6_16cores_SLURM/pruebaOF6_16cores</div></div></blockquote></blockquote=
><blockquote style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0p=
x"><div><div>----------------------------</div></div></blockquote><div><div=
><br></div><div>The versions that I&#39;m using are:</div></div><blockquote=
 style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><div=
><b>Host:=C2=A0</b></div></div></blockquote><blockquote style=3D"margin:0px=
 0px 0px 40px;border:medium none;padding:0px"><blockquote style=3D"margin:0=
px 0px 0px 40px;border:medium none;padding:0px"><div><div>OS: CentOS7.5</di=
v></div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:me=
dium none;padding:0px"><div><div>OpenMPI: 3.1.2</div></div></blockquote><bl=
ockquote style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><=
div><div>PMIx: 2.1.4</div></div></blockquote></blockquote><blockquote style=
=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><div><br><=
/div></div><div><div><b>Container:</b></div></div></blockquote><blockquote =
style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><blockquot=
e style=3D"margin:0px 0px 0px 40px;border:medium none;padding:0px"><div><di=
v>OS: Ubuntu16.04</div></div></blockquote><blockquote style=3D"margin:0px 0=
px 0px 40px;border:medium none;padding:0px"><div><div>OpenMPI: 3.1.2</div><=
/div></blockquote><blockquote style=3D"margin:0px 0px 0px 40px;border:mediu=
m none;padding:0px"><div><div>PMIx: 2.1.4</div></div></blockquote></blockqu=
ote><div><div><br></div><div>Can it be a configuration problem of SLURM? Is=
 there any limitation of SLURM that is affecting OpenFOAM?</div><div><br></=
div><div>Some info about slurm:</div></div><blockquote style=3D"margin:0px =
0px 0px 40px;border:medium none;padding:0px"><div><div># srun --version</di=
v></div><div><div>slurm 18.08.3</div></div><div><div># srun --mpi=3Dlist</d=
iv></div><div><div>srun: MPI types are...</div></div><div><div>srun: pmi2</=
div></div><div><div>srun: openmpi</div></div><div><div>srun: none</div></di=
v></blockquote><div><div><br></div><div>I&#39;m a little bit lost with this=
 issue :(</div><div>Can someone give me some lights?</div><div><br></div><d=
iv>Thanks a lot in advance,</div><div>Carmen</div></div><div><br></div></di=
v><div><br></div>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</div></blockquote></div><br></div></div></div></div></blockquote></div><br=
></div></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</blockquote></div>
</blockquote></div></blockquote></div></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</blockquote></div>
</blockquote></div></blockquote></div></blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</blockquote></div>

--0000000000002d2ee705852c31b5--
