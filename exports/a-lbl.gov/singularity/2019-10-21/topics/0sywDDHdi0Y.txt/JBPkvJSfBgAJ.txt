X-Received: by 10.157.43.165 with SMTP id u34mr20087412ota.6.1467735708732;
        Tue, 05 Jul 2016 09:21:48 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.78.21 with SMTP id r21ls1318518ita.23.canary; Tue, 05 Jul
 2016 09:21:48 -0700 (PDT)
X-Received: by 10.98.59.150 with SMTP id w22mr34057732pfj.4.1467735708144;
        Tue, 05 Jul 2016 09:21:48 -0700 (PDT)
Return-Path: <gmku...@lbl.gov>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTPS id f6si3227842pas.132.2016.07.05.09.21.47
        for <singu...@lbl.gov>
        (version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 05 Jul 2016 09:21:48 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) client-ip=74.125.82.72;
Authentication-Results: mx.google.com;
       spf=pass (google.com: domain of gmku...@lbl.gov designates 74.125.82.72 as permitted sender) smtp.mailfrom=gmku...@lbl.gov
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2DOAABY3XtXf0hSfUpcgnCBJHwGgzaBDKN7hHOMEIF3HoV6AoEtBzoSAQEBAQEBARIBAQkLCwkfMYRMAQEEARIICSsZEgULCwsNIAoCAiEBDwMBBQELEQYIBwQBBxMCBAGHdAMPCAWddYExPjGLO4stDYQlAQEBAQYBAQEBAQEBAR8QimSCQ4FPEQEGgxeCWgWIeYV/hGGFBjQBhgiGLoIQgWpOhAiIaogXhjQSHoEPDxUBhD8cMgeHGIE1AQEB
X-IronPort-AV: E=Sophos;i="5.28,580,1464678000"; 
   d="scan'208,217";a="29354295"
Received: from mail-wm0-f72.google.com ([74.125.82.72])
  by fe3.lbl.gov with ESMTP; 05 Jul 2016 09:21:45 -0700
Received: by mail-wm0-f72.google.com with SMTP id a66so92567681wme.1
        for <singu...@lbl.gov>; Tue, 05 Jul 2016 09:21:45 -0700 (PDT)
X-Gm-Message-State: ALyK8tK2zsr3NXc22Nj8Gb52OCmWrHzvJGdzRjp8mqcQo0A0pK6Q5NrJHKbCJDyahSRIYNByCJ+weuSpVv+uLDbpKbqbcfBxdwh2N8Cbe+i2OtwlnMPqBI3JkJc4CY7POCM/xaQFc513MSW4LqpfjfRY+UY=
X-Received: by 10.25.156.198 with SMTP id f189mr5641557lfe.51.1467735704987;
        Tue, 05 Jul 2016 09:21:44 -0700 (PDT)
X-Received: by 10.25.156.198 with SMTP id f189mr5641551lfe.51.1467735704778;
 Tue, 05 Jul 2016 09:21:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.214.158 with HTTP; Tue, 5 Jul 2016 09:21:43 -0700 (PDT)
In-Reply-To: <007b47f4-0aea-42dc-b871-d653bb7a67a1@lbl.gov>
References: <6247ec64-6881-4978-82a0-f7d6e24039e9@lbl.gov> <CAN7etTx+2ETq_aarfJfez_p4YzoNJ49zwO2CQe=KRwSSZkuR5Q@mail.gmail.com>
 <3998ac67-7f95-475d-ac75-ceb562e19e3b@lbl.gov> <CAN7etTwNG_1G9YuuTQZWSE3SKZjXqNjt8bsZFrVQBJC8_1-mAw@mail.gmail.com>
 <1403bcbe-c615-4417-a629-f95568b75ee7@lbl.gov> <CAN7etTwnpqqbiF=PAqZKDY0yDtyqJGVg2N3x2_-RHFvd6+Qh8Q@mail.gmail.com>
 <054d2758-0acd-48f0-a9bd-b0d52ce02f38@lbl.gov> <613ECCC0-A9C9-42D0-9C26-36695C612DA4@open-mpi.org>
 <007b47f4-0aea-42dc-b871-d653bb7a67a1@lbl.gov>
From: "Gregory M. Kurtzer" <gmku...@lbl.gov>
Date: Tue, 5 Jul 2016 09:21:43 -0700
Message-ID: <CAN7etTxOGqMfyg_C2AWisRWCCs2RKkM91s6SbWTcjYb3X5_Aew@mail.gmail.com>
Subject: Re: [Singularity] Communication between singularity containers
To: singularity <singu...@lbl.gov>
Cc: Ralph Castain <r...@open-mpi.org>
Content-Type: multipart/alternative; boundary=001a114114c24ed0090536e5d9c1

--001a114114c24ed0090536e5d9c1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Raimon,

I am confused as to what the issue is that you are having. Singularity
supports running both across nodes as well as multiple processes per node
in any number of containers. Can you paste your command and the error you
are getting, maybe that will help.

Thanks!



On Tue, Jul 5, 2016 at 8:25 AM, Raimon Bosch <raimo...@gmail.com> wrote:

>
> That solution does not work with nas/mpi benchmark. That's because bt.C.1=
6
> expects 16 processes. When you split processes it throws an exception
> because number of processes is lower than 16.
>
> I am still trying to figure out how to do this. Let me know if you have
> any suggestion.
>
> Cheers,
>
> El jueves, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Castain escribi=
=C3=B3:
>>
>> I think you are misunderstanding the basic nature of the Singularity
>> =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99s just a file system overlay. So=
 =E2=80=9Csharing=E2=80=9D a container is
>> no different than running on a node where the procs all see the same fil=
e
>> system. Thus, having multiple containers that are identical makes no sen=
se
>> - it=E2=80=99s all the same file system.
>>
>> Now if you want to run different containers (e.g., with different
>> libraries or OS in them), then you would use mpirun=E2=80=99s MPMD synta=
x - for
>> example:
>>
>> mpirun -n 1 <container1> : -n 1 <container2>
>>
>> HTH
>> Ralph
>>
>> On Jun 23, 2016, at 1:53 AM, Raimon Bosch <rai...@gmail.com> wrote:
>>
>>
>> One last question: What if I want to execute more than one container in
>> the same host? With this technique I am bounded always to the same
>> container. One of our experiments was based in measuring performance of
>> several containers working in parallel in the same node. Also we had
>> experiments with N containers per host in a multihost environment.
>>
>> El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Gregory M. Kur=
tzer
>> escribi=C3=B3:
>>>
>>> Hi Raimon,
>>>
>>> Sorry I wasn't clear. I am not yet at my computer and thinking while
>>> typing on an iPhone hinders my mental processes. Lol
>>>
>>> If I understand your example properly, you have a docker or VM
>>> infrastructure already set up and you are invoking the mpirun commands =
from
>>> within the virtual environment. Singularity works on a very different
>>> premis because integrating a virtual cluster into an existing cluster a=
nd
>>> scheduling system is a mess.
>>>
>>> So starting from the physical nodes, which already have access to all
>>> other nodes in the cluster, and already scheduled properly, and in dire=
ct
>>> access to optimized hardware and file systems.... You call mpirun.
>>>
>>> The mpirun command will take the standard format as you illustrated wit=
h
>>> the following change to call Singularity inline:
>>>
>>> $ mpirun -np 4 --hostfile hosts.txt singularity exec ~/container.img
>>> trace.sh bt.C.4
>>>
>>> This assumes the following:
>>>
>>> 1. The container image which contains the program's you want to run is
>>> at ~/container.img and accessible at this path on all nodes referenced =
in
>>> hosts.txt
>>> 2. The hosts.txt references other physical nodes you want to run on
>>> 3. The executables trace.sh and bt.C.4 are both inside the container an=
d
>>> in a standard path
>>>
>>> In this case we are performing one execution and MPI + singularity is
>>> managing all of the communication between processes, nodes and containe=
rs.
>>> Also it is now using any optimized hardware (eg. Infiniband) and existi=
ng
>>> high performance file systems (which should not be accessible via a
>>> virtualized or Docker'ized cluster for security reasons).
>>>
>>> This way is actually MUCH simpler then what you are proposing because
>>> there is no need to manage any virtual nodes, virtual networks, or reso=
urce
>>> manager hacks. It really is as easy as just running any other MPI proce=
ss
>>> on an existing cluster.
>>>
>>> Hope that helps better!
>>>
>>>
>>>
>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>>
>>>>
>>>>
>>>> Hi Gregory,
>>>>
>>>> I'm not sure if I would achieve the same with your commands. In an
>>>> environment based on dockers or virtual machines we would do something=
 like
>>>> this [non applicable to Singularity]:
>>>>
>>>> > cd $OPEN_MPI/bin && mpirun -np 4 --hostfile hosts.txt ./trace.sh
>>>> ./bt.C.4
>>>>
>>>> where hosts.txt* is:
>>>>
>>>> >vm-ip-01-on-*host01* slots=3D2
>>>> >vm-ip-01-on-*host02* slots=3D2
>>>>
>>>> * vm-ip-XX-on-hostXX are IPs i.e. 172.100.60.XX
>>>>
>>>> and trace.sh is:
>>>>
>>>> >#!/bin/bash
>>>> >
>>>> >export EXTRAE_HOME=3D/opt/extrae/
>>>> >export EXTRAE_CONFIG_FILE=3D/extrae.xml
>>>> >export LD_PRELOAD=3D${EXTRAE_HOME}/lib/libmpitrace.so
>>>> >
>>>> >## Run the desired program
>>>> >$*
>>>>
>>>> As you see we only perform one execution and OpenMPI transparently
>>>> manages communication between containers or virtual machines. This com=
mand
>>>> would work well rather VMs are on the same host or not.
>>>>
>>>> What I understand from your response is that now we should execute
>>>> OpenMPI on each host and then merge results manually. I don't know yet=
 how
>>>> to do this merge step or if it is any way to centralize everything lik=
e I
>>>> would do with VMs.
>>>>
>>>> Thanks in advance,
>>>>
>>>> El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (UTC+2), Gregory M. K=
urtzer
>>>> escribi=C3=B3:
>>>>>
>>>>> Hi Raimon,
>>>>>
>>>>> The quick answer is you have mpirun handle that as you would normally
>>>>> where the container file lives on a shared file system:
>>>>>
>>>>> $ mpirun singularity exec ~/container.img mpi_prog_in_container
>>>>>
>>>>> Let the MPI outside the container launch the singularity container on
>>>>> each host as it would normally launch any MPI program. Then it will c=
all
>>>>> Singulairty and Singularity will launch the MPI program inside the
>>>>> container on each of your hosts/servers.
>>>>>
>>>>> Hope that helps!
>>>>>
>>>>>
>>>>>
>>>>> On Wednesday, June 22, 2016, Raimon Bosch <rai...@gmail.com> wrote:
>>>>>
>>>>>>
>>>>>> Hi Gregory,
>>>>>>
>>>>>> Thank you for your answer. One of our experiments needs to run
>>>>>> OpenMPI among several servers. This means that we should put one of =
our
>>>>>> containers in host01, another in host02 and another in host03 and co=
llect
>>>>>> the results.
>>>>>>
>>>>>> How can I do this execution in parallel if I need to communicate wit=
h
>>>>>> more than one server?
>>>>>>
>>>>>> El martes, 21 de junio de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer
>>>>>> escribi=C3=B3:
>>>>>>>
>>>>>>> Hi Raimon,
>>>>>>>
>>>>>>> The communication model of a Singularity container is very differen=
t
>>>>>>> from that of a Docker implementation. This is because Docker for al=
l
>>>>>>> practical purposes emulates a virtual machine as each container has=
 it's
>>>>>>> own IP address and thus it's own ssh server. It also carries its ow=
n set of
>>>>>>> complexities, for example networks need to be segregated/VLan'ed, D=
NS/host
>>>>>>> resolution needs to be dynamic and passed down to the containers (s=
o they
>>>>>>> can reach each other), ssh daemons and other process running inside=
 the
>>>>>>> containers, management via an existing scheduling system, and the l=
ist goes
>>>>>>> on and on.
>>>>>>>
>>>>>>> Think of it this way, Singularity does not do any of that... It run=
s
>>>>>>> a program within the container as if it were running on the host it=
self, so
>>>>>>> to communicate between containers is as easy as communicating betwe=
en
>>>>>>> programs. So for MPI, it would happen with the MPI on the physical =
host
>>>>>>> (outside the container) invoking the container subsystem which then=
 invokes
>>>>>>> the MPI programs within the container and the MPI programs within t=
he
>>>>>>> container communicate back to the MPI (orted) outside the container=
 on the
>>>>>>> host to get access to the host resources. In this model all availab=
le
>>>>>>> resources and infrastructure can be leveraged at full bandwidth by =
the
>>>>>>> contained processes and all of the aforementioned complexities akin=
 to
>>>>>>> running on a virtualized mini-cluster are circumvented.
>>>>>>>
>>>>>>> There is additional information I have written at:
>>>>>>>
>>>>>>> http://singularity.lbl.gov/#hpc
>>>>>>>
>>>>>>> That page is still coming along, and needs more information still
>>>>>>> but if you have any questions, comments or change proposals please =
let us
>>>>>>> know!
>>>>>>>
>>>>>>> Thanks and hope that helps!
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch <rai...@gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> We are trying to run experiments using singularity containers. The
>>>>>>>> idea is to run OpenMPI among several containers and check performa=
nce
>>>>>>>> results.
>>>>>>>>
>>>>>>>> How can I communicate with another container? In docker this is
>>>>>>>> clear because every container gets an assigned IP and you can ping=
 there,
>>>>>>>> but what is the situation in the case of singularity? Is it possib=
le to
>>>>>>>> assign an IP to each container? Can I connect via ssh to them?
>>>>>>>>
>>>>>>>> Thanks in advance,
>>>>>>>>
>>>>>>>> --
>>>>>>>> You received this message because you are subscribed to the Google
>>>>>>>> Groups "singularity" group.
>>>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>>>> send an email to singu...@lbl.gov.
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> --
>>>>>>> Gregory M. Kurtzer
>>>>>>> High Performance Computing Services (HPCS)
>>>>>>> University of California
>>>>>>> Lawrence Berkeley National Laboratory
>>>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> You received this message because you are subscribed to the Google
>>>>>> Groups "singularity" group.
>>>>>> To unsubscribe from this group and stop receiving emails from it,
>>>>>> send an email to singu...@lbl.gov.
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gregory M. Kurtzer
>>>>> High Performance Computing Services (HPCS)
>>>>> University of California
>>>>> Lawrence Berkeley National Laboratory
>>>>> One Cyclotron Road, Berkeley, CA 94720
>>>>>
>>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> High Performance Computing Services (HPCS)
>>> University of California
>>> Lawrence Berkeley National Laboratory
>>> One Cyclotron Road, Berkeley, CA 94720
>>>
>>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>



--=20
Gregory M. Kurtzer
High Performance Computing Services (HPCS)
University of California
Lawrence Berkeley National Laboratory
One Cyclotron Road, Berkeley, CA 94720

--001a114114c24ed0090536e5d9c1
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi Raimon,<div><br></div><div>I am confused as to what the=
 issue is that you are having. Singularity supports running both across nod=
es as well as multiple processes per node in any number of containers. Can =
you paste your command and the error you are getting, maybe that will help.=
</div><div><br></div><div>Thanks!</div><div><br></div><div><br></div></div>=
<div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On Tue, Jul 5, 20=
16 at 8:25 AM, Raimon Bosch <span dir=3D"ltr">&lt;<a href=3D"mailto:raimo..=
.@gmail.com" target=3D"_blank">raimo...@gmail.com</a>&gt;</span> wrote:<br>=
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><br>That solution does not =
work with nas/mpi benchmark. That&#39;s because bt.C.16 expects 16 processe=
s. When you split processes it throws an exception because number of proces=
ses is lower than 16. <br><br>I am still trying to figure out how to do thi=
s. Let me know if you have any suggestion.<br><br>Cheers,<span class=3D""><=
br><br>El jueves, 23 de junio de 2016, 15:09:13 (UTC+2), Ralph Castain  esc=
ribi=C3=B3:</span><blockquote class=3D"gmail_quote" style=3D"margin:0;margi=
n-left:0.8ex;border-left:1px #ccc solid;padding-left:1ex"><div style=3D"wor=
d-wrap:break-word"><span class=3D""><div>I think you are misunderstanding t=
he basic nature of the Singularity =E2=80=9Ccontainer=E2=80=9D. It=E2=80=99=
s just a file system overlay. So =E2=80=9Csharing=E2=80=9D a container is n=
o different than running on a node where the procs all see the same file sy=
stem. Thus, having multiple containers that are identical makes no sense - =
it=E2=80=99s all the same file system.</div><div><br></div><div>Now if you =
want to run different containers (e.g., with different libraries or OS in t=
hem), then you would use mpirun=E2=80=99s MPMD syntax - for example:</div><=
div><br></div><div>mpirun -n 1 &lt;container1&gt; : -n 1 &lt;container2&gt;=
</div><div><br></div><div>HTH</div><div>Ralph</div></span><div><br><blockqu=
ote type=3D"cite"><div><div class=3D"h5"><div>On Jun 23, 2016, at 1:53 AM, =
Raimon Bosch &lt;<a rel=3D"nofollow">rai...@gmail.com</a>&gt; wrote:</div><=
br></div></div><div><div><div class=3D"h5"><div dir=3D"ltr" style=3D"font-f=
amily:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter-=
spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-s=
pace:normal;word-spacing:0px"><br>One last question: What if I want to exec=
ute more than one container in the same host? With this technique I am boun=
ded always to the same container. One of our experiments was based in measu=
ring performance of several containers working in parallel in the same node=
. Also we had experiments with N containers per host in a multihost environ=
ment.<br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 16:41:58 (UTC+2), Greg=
ory M. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" style=3D"mar=
gin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,2=
04);border-left-style:solid;padding-left:1ex">Hi Raimon,<div><br></div><div=
>Sorry I wasn&#39;t clear. I am not yet at my computer and thinking while t=
yping on an iPhone hinders my mental processes. Lol</div><div><br></div><di=
v>If I understand your example properly, you have a docker or VM infrastruc=
ture already set up and you are invoking the mpirun commands from within th=
e virtual environment. Singularity works on a very different premis because=
 integrating a virtual cluster into an existing cluster and scheduling syst=
em is a mess.</div><div><br></div><div>So starting from the physical nodes,=
 which already have access to all other nodes in the cluster, and already s=
cheduled properly, and in direct access to optimized hardware and file syst=
ems.... You call mpirun.=C2=A0</div><div><br></div><div>The mpirun command =
will take the standard format as you illustrated with the following change =
to call Singularity inline:</div><div><br></div><div>$ mpirun -np 4 --hostf=
ile hosts.txt singularity exec ~/container.img trace.sh bt.C.4</div><div><b=
r></div><div>This assumes the following:</div><div><br></div><div>1. The co=
ntainer image which contains the program&#39;s you want to run is at ~/cont=
ainer.img and accessible at this path=C2=A0on all nodes referenced in hosts=
.txt</div><div>2. The hosts.txt references other physical nodes you want to=
 run on</div><div>3. The executables trace.sh and bt.C.4 are both inside th=
e container and in a standard path</div><br><div>In this case we are perfor=
ming one execution and MPI + singularity is managing all of the communicati=
on between processes, nodes and containers. Also it is now using any optimi=
zed hardware (eg. Infiniband)=C2=A0and existing high performance=C2=A0file =
systems (which should not be accessible via a virtualized or Docker&#39;ize=
d=C2=A0cluster for security reasons).</div><div><br></div><div>This way is =
actually MUCH simpler then what you are proposing because there is no need =
to manage any virtual nodes, virtual networks, or resource manager hacks. I=
t really is as easy as just running any other MPI=C2=A0process on an existi=
ng cluster.=C2=A0</div><div><br></div><div>Hope that helps better!</div><di=
v><br></div><div><span></span><br><br>On Wednesday, June 22, 2016, Raimon B=
osch &lt;<a rel=3D"nofollow">raimon...@</a><a href=3D"http://gmail.com/" re=
l=3D"nofollow" target=3D"_blank">gmail.com</a>&gt; wrote:<br><blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px=
;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1e=
x"><div dir=3D"ltr"><br><br>Hi Gregory,<br><br>I&#39;m not sure if I would =
achieve the same with your commands. In an environment based on dockers or =
virtual machines we would do something like this [non applicable to Singula=
rity]:<br><br>&gt; cd $OPEN_MPI/bin &amp;&amp; mpirun -np 4 --hostfile host=
s.txt ./trace.sh ./bt.C.4<br><br>where hosts.txt* is:<br><br>&gt;vm-ip-01-o=
n-<b>host01</b><span>=C2=A0</span>slots=3D2<br>&gt;vm-ip-01-on-<b>host02</b=
><span>=C2=A0</span>slots=3D2<br><br>* vm-ip-XX-on-hostXX<b><span>=C2=A0</s=
pan></b>are IPs i.e. 172.100.60.XX<br><br>and trace.sh is:<br><br>&gt;#!/bi=
n/bash<br>&gt;<br>&gt;export EXTRAE_HOME=3D/opt/extrae/<br>&gt;export EXTRA=
E_CONFIG_FILE=3D/extrae.xml<br>&gt;export LD_PRELOAD=3D${EXTRAE_HOME}/lib/l=
ibmpitrace.so<br>&gt;<br>&gt;## Run the desired program<br>&gt;$*<br><br>As=
 you see we only perform one execution and OpenMPI transparently manages co=
mmunication between containers or virtual machines. This command would work=
 well rather VMs are on the same host or not.<br><br>What I understand from=
 your response is that now we should execute OpenMPI on each host and then =
merge results manually. I don&#39;t know yet how to do this merge step or i=
f it is any way to centralize everything like I would do with VMs.<br><br>T=
hanks in advance,<br><br>El mi=C3=A9rcoles, 22 de junio de 2016, 14:42:54 (=
UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote class=3D"gmail_quote" =
style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:r=
gb(204,204,204);border-left-style:solid;padding-left:1ex">Hi Raimon,<div><b=
r></div><div>The quick answer is you have mpirun handle that as you would n=
ormally where the container file lives on a shared file system:</div><div><=
br></div><div>$ mpirun singularity exec ~/container.img mpi_prog_in_contain=
er</div><div><br></div><div>Let the MPI outside the container launch the si=
ngularity container on each host as it would normally launch any MPI progra=
m. Then it will call Singulairty and Singularity will launch the MPI progra=
m inside the container on each of your hosts/servers.=C2=A0</div><div><br><=
/div><div>Hope that helps!</div><div><br></div><div><span></span><br><br>On=
 Wednesday, June 22, 2016, Raimon Bosch &lt;<a rel=3D"nofollow">rai...@gmai=
l.com</a>&gt; wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);b=
order-left-style:solid;padding-left:1ex"><div dir=3D"ltr"><br>Hi Gregory,<b=
r><br>Thank you for your answer. One of our experiments needs to run OpenMP=
I among several servers. This means that we should put one of our container=
s in host01, another in host02 and another in host03 and collect the result=
s.<span>=C2=A0</span><br><br>How can I do this execution in parallel if I n=
eed to communicate with more than one server?<br><br>El martes, 21 de junio=
 de 2016, 16:51:03 (UTC+2), Gregory M. Kurtzer escribi=C3=B3:<blockquote cl=
ass=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px=
;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1e=
x"><div dir=3D"ltr">Hi Raimon,<div><br></div><div>The communication model o=
f a Singularity container is very different from that of a Docker implement=
ation. This is because Docker for all practical purposes emulates a virtual=
 machine as each container has it&#39;s own IP address and thus it&#39;s ow=
n ssh server. It also carries its own set of complexities, for example netw=
orks need to be segregated/VLan&#39;ed, DNS/host resolution needs to be dyn=
amic and passed down to the containers (so they can reach each other), ssh =
daemons and other process running inside the containers, management via an =
existing scheduling system, and the list goes on and on.</div><div><br></di=
v><div>Think of it this way, Singularity does not do any of that... It runs=
 a program within the container as if it were running on the host itself, s=
o to communicate between containers is as easy as communicating between pro=
grams. So for MPI, it would happen with the MPI on the physical host (outsi=
de the container) invoking the container subsystem which then invokes the M=
PI programs within the container and the MPI programs within the container =
communicate back to the MPI (orted) outside the container on the host to ge=
t access to the host resources. In this model all available resources and i=
nfrastructure can be leveraged at full bandwidth by the contained processes=
 and all of the aforementioned complexities akin to running on a virtualize=
d mini-cluster are circumvented.</div><div><br></div><div>There is addition=
al information I have written at:</div><div><br></div><div><a href=3D"http:=
//singularity.lbl.gov/#hpc" rel=3D"nofollow" target=3D"_blank">http://singu=
larity.lbl.gov/#hpc</a><br></div><div><br></div><div>That page is still com=
ing along, and needs more information still but if you have any questions, =
comments or change proposals please let us know!</div><div><br></div><div>T=
hanks and hope that helps!</div><div><br></div><div><br></div></div><div><b=
r><div class=3D"gmail_quote">On Tue, Jun 21, 2016 at 7:37 AM, Raimon Bosch<=
span>=C2=A0</span><span dir=3D"ltr">&lt;<a rel=3D"nofollow">rai...@gmail.co=
m</a>&gt;</span><span>=C2=A0</span>wrote:<br><blockquote class=3D"gmail_quo=
te" style=3D"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-col=
or:rgb(204,204,204);border-left-style:solid;padding-left:1ex"><div dir=3D"l=
tr"><br><br>Hi,<br><br>We are trying to run experiments using singularity c=
ontainers. The idea is to run OpenMPI among several containers and check pe=
rformance results.<span>=C2=A0</span><br><br>How can I communicate with ano=
ther container? In docker this is clear because every container gets an ass=
igned IP and you can ping there, but what is the situation in the case of s=
ingularity? Is it possible to assign an IP to each container? Can I connect=
 via ssh to them?<br><br>Thanks in advance,<span><font color=3D"#888888"><b=
r></font></span></div><span><font color=3D"#888888"><div><br></div>--<span>=
=C2=A0</span><br>You received this message because you are subscribed to th=
e Google Groups &quot;singularity&quot; group.<br>To unsubscribe from this =
group and stop receiving emails from it, send an email to<span>=C2=A0</span=
><a rel=3D"nofollow">singu...@lbl.gov</a>.<br></font></span></blockquote></=
div><br><br clear=3D"all"><div><br></div>--<span>=C2=A0</span><br><div><div=
 dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services=
 (HPCS)<br>University of California<br>Lawrence Berkeley National Laborator=
y<br>One Cyclotron Road, Berkeley, CA 94720</div></div></div></div></blockq=
uote></div><div><br></div>--<span>=C2=A0</span><br>You received this messag=
e because you are subscribed to the Google Groups &quot;singularity&quot; g=
roup.<br>To unsubscribe from this group and stop receiving emails from it, =
send an email to<span>=C2=A0</span><a>singu...@lbl.gov</a>.<br></blockquote=
></div><br><br>--<span>=C2=A0</span><br><div dir=3D"ltr"><div>Gregory M. Ku=
rtzer<br>High Performance Computing Services (HPCS)<br>University of Califo=
rnia<br>Lawrence Berkeley National Laboratory<br>One Cyclotron Road, Berkel=
ey, CA 94720</div></div><br></blockquote></div><div><br></div>--<span>=C2=
=A0</span><br>You received this message because you are subscribed to the G=
oogle Groups &quot;singularity&quot; group.<br>To unsubscribe from this gro=
up and stop receiving emails from it, send an email to<span>=C2=A0</span><a=
>singu...@lbl.gov</a>.<br></blockquote></div><br><br>--<span>=C2=A0</span><=
br><div dir=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing S=
ervices (HPCS)<br>University of California<br>Lawrence Berkeley National La=
boratory<br>One Cyclotron Road, Berkeley, CA 94720</div></div><br></blockqu=
ote></div><div style=3D"font-family:Helvetica;font-size:12px;font-style:nor=
mal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0=
px;text-transform:none;white-space:normal;word-spacing:0px"><br></div><span=
 style=3D"font-family:Helvetica;font-size:12px;font-style:normal;font-weigh=
t:normal;letter-spacing:normal;text-align:start;text-indent:0px;text-transf=
orm:none;white-space:normal;word-spacing:0px;float:none;display:inline!impo=
rtant">--<span>=C2=A0</span></span><br style=3D"font-family:Helvetica;font-=
size:12px;font-style:normal;font-weight:normal;letter-spacing:normal;text-a=
lign:start;text-indent:0px;text-transform:none;white-space:normal;word-spac=
ing:0px"><span style=3D"font-family:Helvetica;font-size:12px;font-style:nor=
mal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:0=
px;text-transform:none;white-space:normal;word-spacing:0px;float:none;displ=
ay:inline!important">You received this message because you are subscribed t=
o the Google Groups &quot;singularity&quot; group.</span><br style=3D"font-=
family:Helvetica;font-size:12px;font-style:normal;font-weight:normal;letter=
-spacing:normal;text-align:start;text-indent:0px;text-transform:none;white-=
space:normal;word-spacing:0px"></div></div><span style=3D"font-family:Helve=
tica;font-size:12px;font-style:normal;font-weight:normal;letter-spacing:nor=
mal;text-align:start;text-indent:0px;text-transform:none;white-space:normal=
;word-spacing:0px;float:none;display:inline!important">To unsubscribe from =
this group and stop receiving emails from it, send an email to<span>=C2=A0<=
/span></span><a style=3D"font-family:Helvetica;font-size:12px;font-style:no=
rmal;font-weight:normal;letter-spacing:normal;text-align:start;text-indent:=
0px;text-transform:none;white-space:normal;word-spacing:0px" rel=3D"nofollo=
w">singu...@lbl.gov</a><span style=3D"font-family:Helvetica;font-size:12px;=
font-style:normal;font-weight:normal;letter-spacing:normal;text-align:start=
;text-indent:0px;text-transform:none;white-space:normal;word-spacing:0px;fl=
oat:none;display:inline!important">.</span></div></blockquote></div><br></d=
iv></blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.=
gov</a>.<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div class=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer<br>High Performance Computing Services (HP=
CS)<br>University of California<br>Lawrence Berkeley National Laboratory<br=
>One Cyclotron Road, Berkeley, CA 94720</div></div></div>
</div>

--001a114114c24ed0090536e5d9c1--
