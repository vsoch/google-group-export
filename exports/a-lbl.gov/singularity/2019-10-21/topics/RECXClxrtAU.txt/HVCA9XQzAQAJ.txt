X-Received: by 2002:a63:be0f:: with SMTP id l15-v6mr1772993pgf.83.1530170400601;
        Thu, 28 Jun 2018 00:20:00 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a63:24f:: with SMTP id 76-v6ls1286077pgc.0.gmail; Thu, 28
 Jun 2018 00:19:59 -0700 (PDT)
X-Received: by 2002:a65:508d:: with SMTP id r13-v6mr7953151pgp.143.1530170399465;
        Thu, 28 Jun 2018 00:19:59 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1530170399; cv=none;
        d=google.com; s=arc-20160816;
        b=CS3vchvmD11I+kH5mlH7ii3iH0l/7yr11VMrGcRBJq2m0UhEJkPgLgmDk/7RFNpgCY
         SAoZzJy6qrUcNDzsZvbm3xbeFAYb1/cR453yO2gI39vQFNMwjbZRlNjEx0563nex5Cgm
         AhUwklguVUg+jtFqv/sx+FAr/rMCmHkPhJbKK7uPs6IfDMcuFWocMZc/DOFR3K7/pWax
         eiNlqwlGCwx3/LbbInL4hlGzIoDJPHntA9k/ILKHR0EMOfpU9lCi665rAGkJsJy9EfBl
         Fpt8Zlhtriq5LTG6aB7VtLvyDQWPjr7ydL8Hjr+guyDbzEHZs0Bv0+uDjdpz3Ly5V9SC
         2QjQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=EsxQuviy8C1wZMnmF4gJSvJZXHMVrkU5mi/KRX5GFFQ=;
        b=uTWIjNYp2w3dyXJ/r0rnZfbreniAp2xIL3Sp7gJQFmv44RPgEFBTrImolw4IRp6Iv5
         CrTrso7lyh2sElD9E2AOcVjTkhTjOqni7A60aK3whxVE3K7KJRUk8WHNHhRb7niJD6+i
         fKGhKrh16aFLVRJ81jNOGakdkjAudZkKqrd7DAz+gFBbfuVF8JdmKXxNEmbi/fLlQdY0
         nEsDTT66WCliivueIkHvtKHPXkqp9uSV69u4kmN80P/JRybC9LZk+7ua8ztkRq8C6URF
         JZibqlLvjs+tG+kXeBC65X4ZKiVFVDxvJAyvsJroRAMnjsAarM/CCjjxfAHG3J/AqnDS
         Tn1Q==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@googlemail.com header.s=20161025 header.b=mjffRfmW;
       spf=pass (google.com: domain of hea...@googlemail.com designates 209.85.192.182 as permitted sender) smtp.mailfrom=hea...@googlemail.com
Return-Path: <hea...@googlemail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id p21-v6si6150729plq.94.2018.06.28.00.19.59
        for <singu...@lbl.gov>;
        Thu, 28 Jun 2018 00:19:59 -0700 (PDT)
Received-SPF: pass (google.com: domain of hea...@googlemail.com designates 209.85.192.182 as permitted sender) client-ip=209.85.192.182;
Authentication-Results: mx.google.com;
       dkim=pass head...@googlemail.com header.s=20161025 header.b=mjffRfmW;
       spf=pass (google.com: domain of hea...@googlemail.com designates 209.85.192.182 as permitted sender) smtp.mailfrom=hea...@googlemail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2CMBAAbizRbf7bAVdFQBAUDHgEGDIMfg?=
 =?us-ascii?q?Qx/KINzBoEdglCQVIIHAogrjQSBYwMjAQ6BUIE+gTcCgxkhNxUBAgEBAgEBAgE?=
 =?us-ascii?q?TAQEJCwsIJyUMgjUFAgMCHoJXAQEBAwEaAwYEGQENLAMBCwYFCw0gCgICIQEPA?=
 =?us-ascii?q?wEFARwOBwQBBxMCBII0TCiBPgEDDQgFCqELPIlXEYEggWkzgnIFaoJtCj8NgS6?=
 =?us-ascii?q?BFAIGEohbghWBD4MPglY3CwEBAQGBHwwBDQUBCQI1DBqCOoJVAodQEQiBRoMZh?=
 =?us-ascii?q?Q+HNSsJhgCGCoMJgUBCg0WCaoUZiidOhnEwgTZVLnEzARkIGxVsgjgJFoFUJAw?=
 =?us-ascii?q?XegEOgjyCZIIwgmaCVwM9MAEPjg4BDhcELIFtBQEB?=
X-IronPort-AV: E=Sophos;i="5.51,282,1526367600"; 
   d="scan'208,217";a="27842008"
Received: from mail-pf0-f182.google.com ([209.85.192.182])
  by fe4.lbl.gov with ESMTP; 28 Jun 2018 00:19:57 -0700
Received: by mail-pf0-f182.google.com with SMTP id h20-v6so257265pfn.4
        for <singu...@lbl.gov>; Thu, 28 Jun 2018 00:19:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=googlemail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=EsxQuviy8C1wZMnmF4gJSvJZXHMVrkU5mi/KRX5GFFQ=;
        b=mjffRfmW/4EMAirdHHE/fQ5YZVRFaC4NaR9iUP3LmCEd1KtZiUkCpg14osBwA/t0Sv
         YTw3fhRWu/6fmBhm5V9xMcMsZrs7yrT4KVqhsRr1zkPzkR+/v8TYc84dFbfleW9gxDma
         ob53jCbD8RN9D4mSkSa9gkOaIHR1i2p+rkwWMuPXCDTf/G0ZrBE/4U4/5TkqkvwtT+yn
         bICYN+L959BdHL8VqP9iqIdr22SqkxzvkP/iBPY3SDiLp+Qv7wuNQiO6cBMs1y5/XHpb
         r1t3o38wQeM/FZ4KkyElt+jsjK/Cs7lxPL8suc3c9n0yKPoUrtlenFDsVAb1EvELiCOD
         9uYg==
X-Gm-Message-State: APt69E0/SZetS4x0OjzXnROMRsiobbHZQIRecw388v/qE08/wsazB0xt
	MWSWt8XQW0WqTk0cnV6Q+mp242F+hegcd5WEpXVzyxeW
X-Received: by 2002:a63:7f46:: with SMTP id p6-v6mr5399647pgn.303.1530170396681;
 Thu, 28 Jun 2018 00:19:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 2002:a17:90a:2e83:0:0:0:0 with HTTP; Thu, 28 Jun 2018 00:19:26
 -0700 (PDT)
In-Reply-To: <CAPqNE2VEL6oOB2NMP89=BxHqQj5k=Mbg2C=7O=oZuRvN5Q-pnQ@mail.gmail.com>
References: <4d550130-4d54-4d4f-bf9f-a46f34132e96@lbl.gov> <CAJZ53CkoPup6sXotzVLO_toCu2c+rwK-A6Y0+TU277Y9km8N9w@mail.gmail.com>
 <CAM=pu+Jq2HcgVEBoYJ-USVQomnT6=pmd_16UdEpz2nniPQewdw@mail.gmail.com>
 <CADf5cTEmD1BEEg=jQHf2rrK1h+cwPYYqSD3Tz0UKS-RpumNNRA@mail.gmail.com>
 <CAMsq4T1c4vA5Yv6_bWJhpNC=UCze1reP2yPQa2YUtutJRPzQbA@mail.gmail.com>
 <CAM=pu+L-DXgQJ4eSUqSEnriDX9U2+dwOU-e2P2gWFxW3jsPcWQ@mail.gmail.com> <CAPqNE2VEL6oOB2NMP89=BxHqQj5k=Mbg2C=7O=oZuRvN5Q-pnQ@mail.gmail.com>
From: John Hearns <hea...@googlemail.com>
Date: Thu, 28 Jun 2018 09:19:26 +0200
Message-ID: <CAPqNE2UYU6+k3k6eef49t1gUegbDRSAD5XyDAf28bQWEBKjCqg@mail.gmail.com>
Subject: Re: [Singularity] Research data and containers
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000f0abfc056fae8e28"

--000000000000f0abfc056fae8e28
Content-Type: text/plain; charset="UTF-8"

Chris Hines mentions signing of tool packages.  In danger of crossing the
streams, I am an enthusiast for the Julia language.
the new Pkg3 infrastructure for Julia, which is the default package manager
now, implements UUIDs for packages
https://github.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries

On 28 June 2018 at 09:15, John Hearns <hea...@googlemail.com> wrote:

> I think this a very, very interesting topic.  Please allow me to explore
> and hopefully bring out some responses?
>
> Are we talking about large public datasets here - for instance I worked on
> a CERN experiment (ahem) many years ago.
> One hopes that huge, public datasets do not change. For instance if I went
> back to analyze what was tagged at the time with a run number and an event
> number I hope I would get the same data.
> OK, it is clear that some large institutiosn worldwide will store that
> data. And that bitrot or the scarcity of the tape drives to read the data
> has not rendered it beyond recovery.
>
> I also worked with the JASMIN project in the UK, where several institutes
> have banded together to create a central data store for climate modelling.
> Again large public datasets.
>
> I also worked in Formula 1 with CFD engineers, and managed over a petabyte
> of data from CFD studies. Those studies were catalogued by year and then
> case number.
> Again, we assume that the data is held on tape somewhere and if you ind
> the correctly named directory you will get the same data back.
>
> I guess here we are talking about reproducibility. So we have a
> Singularity container, which is signed, and captures the packages and
> libraries needed to run that analysis.
> We also have to have the same data available to run on.  Do we need to
> physically keep that data 'beside' the container? Maybe, for certain data
> and certain values of 'not too large'.
> but how about analyses of (say) CERN or JASMIN data? Is it enough to make
> a pointer towards those datasets somehow?
> I would say URL's or URIs here, but we have seen how the Web is dynamic
> and sites disappear and links change.
>
>
> Its worth saying that in the UK there is a lot of effort in this space.
> The Research Councils there mandated that if they fund research the data
> must be retained for N years (I forget N)
> So many Universities now have specific services for research data, for
> instance:
> http://www.ucl.ac.uk/research-it-services/research-data-service
> https://www.ukdataservice.ac.uk/
>
> Arkivum https://arkivum.com/ works closely with the UK academic network
> to provide large data storage. I just found this blog on their website -
> well worth reading.
> https://arkivum.com/articles/blog-series-part-1-four-
> economic-factors-make-cost-nothing-expensive-something/
> https://arkivum.com/pharma/blog-series-part-4-final-
> economic-fallacy-long-term-data-temporarily-dynamic-path-dependent/
>
>
> To end up, I guess what we are aiming towards is an S3 compatible storage
> bucket, whether this is hosted in-house, by Amazon or by a compatible
> provider.
> Such a bucket would have to be locked, and signed.  Ideas??
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> On 28 June 2018 at 03:44, v <vso...@gmail.com> wrote:
>
>> Your description of the different components is on point.
>>
>> Finding hosting for large data is, and will continue to be, always one of
>> the biggest problems. It's expensive. Someone has to pay for it. Nobody
>> will do it for you for free. We are lucky to have things like Github repos,
>> Google Drive, Dropbox, that gives some (mediocre) access to small data. For
>> large data, everyone is pretty much on their own. Most academics have a
>> university that provides some sort of web space / library data archive, and
>> if you are publishing, some journals will support it too -->
>> https://www.nature.com/sdata/policies/repositories. I try to figure out
>> hacks to get everything hosted, for free, in version control, but that
>> breaks when anything gets large.
>>
>> The entity that manages a data respository, if this is a thing, must have
>> incentive to do it. The datasets have to bring in money in some way, or
>> someone has to value it enough to pay for it. That's sort of what most
>> things come down to, unfortunately.
>>
>>
>> On Wed, Jun 27, 2018 at 6:38 PM Maxime Hebrard <maxime...@gmail.com>
>> wrote:
>>
>>> I start to use singularity / containers and obviously I ran into the
>>> same question ...
>>>
>>> a half solution that I found by exploring existing solution (thanks to
>>> nf-core and SciLifeLab to share their workflow / containers) is ( as
>>> far I understand ):
>>>
>>> the container encapsulate only the sorftware.
>>> the data are store in external repositories.
>>>
>>> the user receive the container.
>>>
>>> case 1:
>>> the user download the data by himself and point his files to the
>>> container
>>> the container use local files as resources
>>> pros:
>>>  * the container is slim
>>>  * user can store the data on machine A (data server) and run the
>>> software on machine B (analyse server)
>>> cons:
>>> * user is involved in downloading the right data
>>>
>>> case 2:
>>> the user do not have the data
>>> the container download the data from the external repository
>>> the container use the data it just download
>>> pro:
>>> * the container manage everything
>>> cons:
>>> * the step of downloading the data require bandwith + disk space =
>>> time and resources
>>>
>>> a general worry for both scenarii is:
>>> who manage the data repository ? can we ensure the version of the data
>>> package will be maintain ?
>>>
>>> Waiting for your feedback ;)
>>> Maxime
>>>
>>> On Thu, Jun 28, 2018 at 8:15 AM, 'Chris Hines' via singularity
>>> <singu...@lbl.gov> wrote:
>>> > To quote 90's band "The Offspring" IMHO "You gotta keep 'em separated"
>>> >
>>> > Personally I think this is a larger question: How do we (as research
>>> > software engineers, or whatever you would like to call us today)
>>> catalog and
>>> > attach references (DOI's I guess) to larger data sets. Does it make
>>> sense to
>>> > mksquashfs or tar.bz2? What if they are already in a single hdf5 file?
>>> And
>>> > what about the output data set? we obviously want that to land outside
>>> the
>>> > computational container, but how should we package the output so it
>>> can be
>>> > referenced directly? How do we make that whole thing discoverable and
>>> > encourage researchers to attach sufficient metadata to make it
>>> searchable?
>>> >
>>> > I'm wondering if part of the solution is some sort of "meta container"
>>> that
>>> > downloads a set of versioned data with specific checksums, and a
>>> versioned
>>> > tool container with a specific checksum, and attaches them in a
>>> reproducible
>>> > way.
>>> >
>>> > To address Dominque's question directly: I'd keep the "semi-separated".
>>> > Whatever your researchers use for archiving/cataloging/managing their
>>> data
>>> > sets, keep that in place and add the singularity container to the data
>>> set
>>> > (rather than adding the data set to the singularity container).
>>> >
>>> > Regards,
>>> > --
>>> > Chris.
>>> > Monash eResearch Centre, Monash University, Australia
>>> >
>>> > On Thu, 28 Jun 2018 at 01:19, v <vso...@gmail.com> wrote:
>>> >>
>>> >> If you are making containers with singularity (and using Squashfs
>>> anyway)
>>> >> it wouldn't be so nutty to just compress with mksquashfs and then
>>> mount
>>> >> where needed - I did this with a huge dataset and it worked pretty
>>> nicely
>>> >> :_) It relies on FUSE then and all the issues around that, but it's an
>>> >> option!
>>> >>
>>> >> This is good showing of how use use mksquashfs (it's really
>>> >> simple,actually!) -->
>>> >> http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.html
>>> >> And then mount -->
>>> >> https://vsoch.github.io/datasets/2018/zenodo/#mount-with-sudo
>>> >>
>>> >> On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker
>>> >> <brando...@cornell.edu> wrote:
>>> >>>
>>> >>> I seem to recall code ocean may have explored this idea. Sorry I
>>> can't
>>> >>> say more, at the moment.
>>> >>>
>>> >>> On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen
>>> >>> <dominiqu...@gmail.com> wrote:
>>> >>>>
>>> >>>> Hello everyone,
>>> >>>>
>>> >>>> I am seeking for tips and experience on how to handle research data,
>>> >>>> especially bigger sets of data (inside the GB range), in
>>> combination with
>>> >>>> containers.
>>> >>>>
>>> >>>> I am a student assistant working in the IT department of a research
>>> >>>> institute. And I am involved in building the infrastructure for
>>> singularity
>>> >>>> containerization and introducing researchers to the technology. We
>>> have
>>> >>>> already build a few base images containing frequently used tools.
>>> Recently a
>>> >>>> research group approached us, wishing to integrate data, they
>>> created for
>>> >>>> one tool, into a container with the tool. The data takes up several
>>> GB of
>>> >>>> disk space and we are unsure how to handle this and similar future
>>> use
>>> >>>> cases.
>>> >>>>
>>> >>>> Does anyone have a set of best practices or is there an intended
>>> way to
>>> >>>> use singularity with big research data?
>>> >>>>
>>> >>>> The options we considered are:
>>> >>>>
>>> >>>> Moving the data into the container at build time.
>>> >>>>
>>> >>>> Pro:
>>> >>>>
>>> >>>> Keeps the whole thing mobile
>>> >>>> Keeps work away from the researchers
>>> >>>> Can be referenced as a single entity in a publication.
>>> >>>>
>>> >>>> Con:
>>> >>>>
>>> >>>> Where would you store such big containers, especially several of
>>> them?
>>> >>>> What would happen, if separate versions of the tool are needed?
>>> Keep the
>>> >>>> old and accumulate redundant data? Delete the old and risk
>>> reproducibility
>>> >>>> problems? (Maybe container Apps can be used to minimize this.)
>>> >>>>
>>> >>>> Let the researchers mount the data into the container at startup.
>>> >>>>
>>> >>>> Pro:
>>> >>>>
>>> >>>> Keeps the containers slimmer, the tools more modular.
>>> >>>>
>>> >>>> Con:
>>> >>>>
>>> >>>> Adds to the workload and the things the researchers have to keep
>>> track
>>> >>>> of.
>>> >>>> Spreads the parts needed for reproduction over at least two points.
>>> >>>> Hampers mobility.
>>> >>>>
>>> >>>> Mounting during early phases and publish with a container,
>>> containing
>>> >>>> the data.
>>> >>>>
>>> >>>> Pro:
>>> >>>>
>>> >>>> Reduces amount of containers with redundant and deprecated  data.
>>> >>>> Makes reproduction of results easier after publication.
>>> >>>>
>>> >>>> Con:
>>> >>>>
>>> >>>> Ads to the workload.
>>> >>>> When is the point when its time to "freeze" the data inside the
>>> >>>> container?
>>> >>>> Storage of the container is still problematic.
>>> >>>> Might introduce reproducibility problems, since you change the
>>> original
>>> >>>> container to put the data into it.
>>> >>>>
>>> >>>> Are there any recommendations from experience?
>>> >>>>
>>> >>>> Thank you and best regards,
>>> >>>> Dominique
>>> >>>>
>>> >>>> --
>>> >>>> You received this message because you are subscribed to the Google
>>> >>>> Groups "singularity" group.
>>> >>>> To unsubscribe from this group and stop receiving emails from it,
>>> send
>>> >>>> an email to singu...@lbl.gov.
>>> >>>
>>> >>> --
>>> >>> You received this message because you are subscribed to the Google
>>> Groups
>>> >>> "singularity" group.
>>> >>> To unsubscribe from this group and stop receiving emails from it,
>>> send an
>>> >>> email to singu...@lbl.gov.
>>> >>>
>>> >>> --
>>> >>> Vanessa Villamia Sochat
>>> >>> Stanford University '16
>>> >>> (603) 321-0676
>>> >>
>>> >> --
>>> >> You received this message because you are subscribed to the Google
>>> Groups
>>> >> "singularity" group.
>>> >> To unsubscribe from this group and stop receiving emails from it,
>>> send an
>>> >> email to singu...@lbl.gov.
>>> >
>>> > --
>>> > You received this message because you are subscribed to the Google
>>> Groups
>>> > "singularity" group.
>>> > To unsubscribe from this group and stop receiving emails from it, send
>>> an
>>> > email to singu...@lbl.gov.
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>>
>> --
>> Vanessa Villamia Sochat
>> Stanford University '16
>> (603) 321-0676
>>
>> --
>> You received this message because you are subscribed to the Google Groups
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send an
>> email to singu...@lbl.gov.
>>
>
>

--000000000000f0abfc056fae8e28
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>Chris Hines mentions signing of tool packages.=C2=A0 =
In danger of crossing the streams, I am an enthusiast for the Julia languag=
e.</div><div>the new Pkg3 infrastructure for Julia, which is the default pa=
ckage manager now, implements UUIDs for packages</div><div><a href=3D"https=
://github.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries">https://gith=
ub.com/JuliaLang/Juleps/blob/master/Pkg3.md#registries</a><br></div></div><=
div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On 28 June 2018 at=
 09:15, John Hearns <span dir=3D"ltr">&lt;<a href=3D"mailto:hea...@googlema=
il.com" target=3D"_blank">hea...@googlemail.com</a>&gt;</span> wrote:<br><b=
lockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px =
#ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>I think this a very, ver=
y interesting topic.=C2=A0 Please allow me to explore and hopefully bring o=
ut some responses?</div><div><br></div><div>Are we talking about large publ=
ic datasets here - for instance I worked on a CERN experiment (ahem) many y=
ears ago.</div><div>One hopes that huge, public datasets do not change. For=
 instance if I went back to analyze what was tagged at the time with a run =
number and an event number I hope I would get the same data.</div><div>OK, =
it is clear that some large institutiosn worldwide will store that data. An=
d that bitrot or the scarcity of the tape drives to read the data has not r=
endered it beyond recovery.</div><div><br></div><div>I also worked with the=
 JASMIN project in the UK, where several institutes have banded together to=
 create a central data store for climate modelling. Again large public data=
sets.</div><div><br></div><div>I also worked in Formula 1 with CFD engineer=
s, and managed over a petabyte of data from CFD studies. Those studies were=
 catalogued by year and then case number.</div><div>Again, we assume that t=
he data is held on tape somewhere and if you ind the correctly named direct=
ory you will get the same data back.</div><div><br></div><div>I guess here =
we are talking about reproducibility. So we have a Singularity container, w=
hich is signed, and captures the packages and libraries needed to run that =
analysis.</div><div>We also have to have the same data available to run on.=
=C2=A0 Do we need to physically keep that data &#39;beside&#39; the contain=
er? Maybe, for certain data and certain values of &#39;not too large&#39;.<=
/div><div>but how about analyses of (say) CERN or JASMIN data? Is it enough=
 to make a pointer towards those datasets somehow?</div><div>I would say UR=
L&#39;s or URIs here, but we have seen how the Web is dynamic and sites dis=
appear and links change. <br></div><div><br></div><div><br></div><div>Its w=
orth saying that in the UK there is a lot of effort in this space. The Rese=
arch Councils there mandated that if they fund research the data must be re=
tained for N years (I forget N)</div><div>So many Universities now have spe=
cific services for research data, for instance:</div><div><a href=3D"http:/=
/www.ucl.ac.uk/research-it-services/research-data-service" target=3D"_blank=
">http://www.ucl.ac.uk/research-<wbr>it-services/research-data-<wbr>service=
</a></div><div><a href=3D"https://www.ukdataservice.ac.uk/" target=3D"_blan=
k">https://www.ukdataservice.ac.<wbr>uk/</a></div><div><br></div><div>Arkiv=
um <a href=3D"https://arkivum.com/" target=3D"_blank">https://arkivum.com/<=
/a> works closely with the UK academic network to provide large data storag=
e. I just found this blog on their website - well worth reading. <br></div>=
<div><a href=3D"https://arkivum.com/articles/blog-series-part-1-four-econom=
ic-factors-make-cost-nothing-expensive-something/" target=3D"_blank">https:=
//arkivum.com/articles/<wbr>blog-series-part-1-four-<wbr>economic-factors-m=
ake-cost-<wbr>nothing-expensive-something/</a></div><div><a href=3D"https:/=
/arkivum.com/pharma/blog-series-part-4-final-economic-fallacy-long-term-dat=
a-temporarily-dynamic-path-dependent/" target=3D"_blank">https://arkivum.co=
m/pharma/<wbr>blog-series-part-4-final-<wbr>economic-fallacy-long-term-<wbr=
>data-temporarily-dynamic-path-<wbr>dependent/</a></div><div><br></div><div=
><br></div><div>To end up, I guess what we are aiming towards is an S3 comp=
atible storage bucket, whether this is hosted in-house, by Amazon or by a c=
ompatible provider.</div><div>Such a bucket would have to be locked, and si=
gned.=C2=A0 Ideas??<br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div><div><br></div><=
div><br></div><div><br></div><div><br></div><div><br></div></div><div class=
=3D"HOEnZb"><div class=3D"h5"><div class=3D"gmail_extra"><br><div class=3D"=
gmail_quote">On 28 June 2018 at 03:44, v <span dir=3D"ltr">&lt;<a href=3D"m=
ailto:vso...@gmail.com" target=3D"_blank">vso...@gmail.com</a>&gt;</span> w=
rote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;borde=
r-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Your description o=
f the different components is on point.=C2=A0<div><br></div><div>Finding ho=
sting for large data is, and will continue to be, always one of the biggest=
 problems. It&#39;s expensive. Someone has to pay for it. Nobody will do it=
 for you for free. We are lucky to have things like Github repos, Google Dr=
ive, Dropbox, that gives some (mediocre) access to small data. For large da=
ta, everyone is pretty much on their own. Most academics have a university =
that provides some sort of web space / library data archive, and if you are=
 publishing, some journals will support it too --&gt;=C2=A0<a href=3D"https=
://www.nature.com/sdata/policies/repositories" target=3D"_blank">https://ww=
w.nature.com/sda<wbr>ta/policies/repositories</a>. I try to figure out hack=
s to get everything hosted, for free, in version control, but that breaks w=
hen anything gets large.</div><div><br></div><div>The entity that manages a=
 data respository, if this is a thing, must have incentive to do it. The da=
tasets have to bring in money in some way, or someone has to value it enoug=
h to pay for it. That&#39;s sort of what most things come down to, unfortun=
ately.</div><div><br></div></div><div class=3D"m_636930356989791511HOEnZb">=
<div class=3D"m_636930356989791511h5"><br><div class=3D"gmail_quote"><div d=
ir=3D"ltr">On Wed, Jun 27, 2018 at 6:38 PM Maxime Hebrard &lt;<a href=3D"ma=
ilto:maxime...@gmail.com" target=3D"_blank">maxime...@gmail.com</a>&gt; wro=
te:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0px 0px 0px =
0.8ex;border-left:1px solid rgb(204,204,204);padding-left:1ex">I start to u=
se singularity / containers and obviously I ran into the<br>
same question ...<br>
<br>
a half solution that I found by exploring existing solution (thanks to<br>
nf-core and SciLifeLab to share their workflow / containers) is ( as<br>
far I understand ):<br>
<br>
the container encapsulate only the sorftware.<br>
the data are store in external repositories.<br>
<br>
the user receive the container.<br>
<br>
case 1:<br>
the user download the data by himself and point his files to the container<=
br>
the container use local files as resources<br>
pros:<br>
=C2=A0* the container is slim<br>
=C2=A0* user can store the data on machine A (data server) and run the<br>
software on machine B (analyse server)<br>
cons:<br>
* user is involved in downloading the right data<br>
<br>
case 2:<br>
the user do not have the data<br>
the container download the data from the external repository<br>
the container use the data it just download<br>
pro:<br>
* the container manage everything<br>
cons:<br>
* the step of downloading the data require bandwith + disk space =3D<br>
time and resources<br>
<br>
a general worry for both scenarii is:<br>
who manage the data repository ? can we ensure the version of the data<br>
package will be maintain ?<br>
<br>
Waiting for your feedback ;)<br>
Maxime<br>
<br>
On Thu, Jun 28, 2018 at 8:15 AM, &#39;Chris Hines&#39; via singularity<br>
&lt;<a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singu...@lbl.gov<=
/a>&gt; wrote:<br>
&gt; To quote 90&#39;s band &quot;The Offspring&quot; IMHO &quot;You gotta =
keep &#39;em separated&quot;<br>
&gt;<br>
&gt; Personally I think this is a larger question: How do we (as research<b=
r>
&gt; software engineers, or whatever you would like to call us today) catal=
og and<br>
&gt; attach references (DOI&#39;s I guess) to larger data sets. Does it mak=
e sense to<br>
&gt; mksquashfs or tar.bz2? What if they are already in a single hdf5 file?=
 And<br>
&gt; what about the output data set? we obviously want that to land outside=
 the<br>
&gt; computational container, but how should we package the output so it ca=
n be<br>
&gt; referenced directly? How do we make that whole thing discoverable and<=
br>
&gt; encourage researchers to attach sufficient metadata to make it searcha=
ble?<br>
&gt;<br>
&gt; I&#39;m wondering if part of the solution is some sort of &quot;meta c=
ontainer&quot; that<br>
&gt; downloads a set of versioned data with specific checksums, and a versi=
oned<br>
&gt; tool container with a specific checksum, and attaches them in a reprod=
ucible<br>
&gt; way.<br>
&gt;<br>
&gt; To address Dominque&#39;s question directly: I&#39;d keep the &quot;se=
mi-separated&quot;.<br>
&gt; Whatever your researchers use for archiving/cataloging/managing their =
data<br>
&gt; sets, keep that in place and add the singularity container to the data=
 set<br>
&gt; (rather than adding the data set to the singularity container).<br>
&gt;<br>
&gt; Regards,<br>
&gt; --<br>
&gt; Chris.<br>
&gt; Monash eResearch Centre, Monash University, Australia<br>
&gt;<br>
&gt; On Thu, 28 Jun 2018 at 01:19, v &lt;<a href=3D"mailto:vso...@gmail.com=
" target=3D"_blank">vso...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; If you are making containers with singularity (and using Squashfs =
anyway)<br>
&gt;&gt; it wouldn&#39;t be so nutty to just compress with mksquashfs and t=
hen mount<br>
&gt;&gt; where needed - I did this with a huge dataset and it worked pretty=
 nicely<br>
&gt;&gt; :_) It relies on FUSE then and all the issues around that, but it&=
#39;s an<br>
&gt;&gt; option!<br>
&gt;&gt;<br>
&gt;&gt; This is good showing of how use use mksquashfs (it&#39;s really<br=
>
&gt;&gt; simple,actually!) --&gt;<br>
&gt;&gt; <a href=3D"http://tldp.org/HOWTO/SquashFS-HOWTO/creatingandusing.h=
tml" rel=3D"noreferrer" target=3D"_blank">http://tldp.org/HOWTO/SquashFS<wb=
r>-HOWTO/creatingandusing.html</a><br>
&gt;&gt; And then mount --&gt;<br>
&gt;&gt; <a href=3D"https://vsoch.github.io/datasets/2018/zenodo/#mount-wit=
h-sudo" rel=3D"noreferrer" target=3D"_blank">https://vsoch.github.io/datase=
<wbr>ts/2018/zenodo/#mount-with-<wbr>sudo</a><br>
&gt;&gt;<br>
&gt;&gt; On Wed, Jun 27, 2018 at 8:14 AM Brandon Barker<br>
&gt;&gt; &lt;<a href=3D"mailto:brando...@cornell.edu" target=3D"_blank">bra=
ndo...@cornell.edu</a>&gt; wrote:<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; I seem to recall code ocean may have explored this idea. Sorry=
 I can&#39;t<br>
&gt;&gt;&gt; say more, at the moment.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; On Wed, Jun 27, 2018, 8:50 AM Dominique Hansen<br>
&gt;&gt;&gt; &lt;<a href=3D"mailto:dominiqu...@gmail.com" target=3D"_blank"=
>dominiqu...@gmail.com</a>&gt; wrote:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Hello everyone,<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I am seeking for tips and experience on how to handle rese=
arch data,<br>
&gt;&gt;&gt;&gt; especially bigger sets of data (inside the GB range), in c=
ombination with<br>
&gt;&gt;&gt;&gt; containers.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; I am a student assistant working in the IT department of a=
 research<br>
&gt;&gt;&gt;&gt; institute. And I am involved in building the infrastructur=
e for singularity<br>
&gt;&gt;&gt;&gt; containerization and introducing researchers to the techno=
logy. We have<br>
&gt;&gt;&gt;&gt; already build a few base images containing frequently used=
 tools. Recently a<br>
&gt;&gt;&gt;&gt; research group approached us, wishing to integrate data, t=
hey created for<br>
&gt;&gt;&gt;&gt; one tool, into a container with the tool. The data takes u=
p several GB of<br>
&gt;&gt;&gt;&gt; disk space and we are unsure how to handle this and simila=
r future use<br>
&gt;&gt;&gt;&gt; cases.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Does anyone have a set of best practices or is there an in=
tended way to<br>
&gt;&gt;&gt;&gt; use singularity with big research data?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; The options we considered are:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Moving the data into the container at build time.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Keeps the whole thing mobile<br>
&gt;&gt;&gt;&gt; Keeps work away from the researchers<br>
&gt;&gt;&gt;&gt; Can be referenced as a single entity in a publication.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Where would you store such big containers, especially seve=
ral of them?<br>
&gt;&gt;&gt;&gt; What would happen, if separate versions of the tool are ne=
eded? Keep the<br>
&gt;&gt;&gt;&gt; old and accumulate redundant data? Delete the old and risk=
 reproducibility<br>
&gt;&gt;&gt;&gt; problems? (Maybe container Apps can be used to minimize th=
is.)<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Let the researchers mount the data into the container at s=
tartup.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Keeps the containers slimmer, the tools more modular.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Adds to the workload and the things the researchers have t=
o keep track<br>
&gt;&gt;&gt;&gt; of.<br>
&gt;&gt;&gt;&gt; Spreads the parts needed for reproduction over at least tw=
o points.<br>
&gt;&gt;&gt;&gt; Hampers mobility.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Mounting during early phases and publish with a container,=
 containing<br>
&gt;&gt;&gt;&gt; the data.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Pro:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Reduces amount of containers with redundant and deprecated=
=C2=A0 data.<br>
&gt;&gt;&gt;&gt; Makes reproduction of results easier after publication.<br=
>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Con:<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Ads to the workload.<br>
&gt;&gt;&gt;&gt; When is the point when its time to &quot;freeze&quot; the =
data inside the<br>
&gt;&gt;&gt;&gt; container?<br>
&gt;&gt;&gt;&gt; Storage of the container is still problematic.<br>
&gt;&gt;&gt;&gt; Might introduce reproducibility problems, since you change=
 the original<br>
&gt;&gt;&gt;&gt; container to put the data into it.<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Are there any recommendations from experience?<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; Thank you and best regards,<br>
&gt;&gt;&gt;&gt; Dominique<br>
&gt;&gt;&gt;&gt;<br>
&gt;&gt;&gt;&gt; --<br>
&gt;&gt;&gt;&gt; You received this message because you are subscribed to th=
e Google<br>
&gt;&gt;&gt;&gt; Groups &quot;singularity&quot; group.<br>
&gt;&gt;&gt;&gt; To unsubscribe from this group and stop receiving emails f=
rom it, send<br>
&gt;&gt;&gt;&gt; an email to <a href=3D"mailto:singularity%...@lbl.gov" tar=
get=3D"_blank">singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; You received this message because you are subscribed to the Go=
ogle Groups<br>
&gt;&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt;&gt; To unsubscribe from this group and stop receiving emails from =
it, send an<br>
&gt;&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"=
_blank">singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
&gt;&gt;&gt;<br>
&gt;&gt;&gt; --<br>
&gt;&gt;&gt; Vanessa Villamia Sochat<br>
&gt;&gt;&gt; Stanford University &#39;16<br>
&gt;&gt;&gt; (603) 321-0676<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singularity+unsubscribe@lbl.go<wbr>v</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singul=
arity+unsubscribe@lbl.go<wbr>v</a>.<br>
</blockquote></div><br clear=3D"all"><div><br></div>-- <br><div dir=3D"ltr"=
 class=3D"m_636930356989791511m_-8585967889488559609gmail_signature"><div c=
lass=3D"m_636930356989791511m_-8585967889488559609gmail_signature">Vanessa =
Villamia Sochat<br>Stanford University &#39;16<br><div><div><div>(603) 321-=
0676</div></div></div></div></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</div></div></blockquote></div><br></div>
</div></div></blockquote></div><br></div>

--000000000000f0abfc056fae8e28--
