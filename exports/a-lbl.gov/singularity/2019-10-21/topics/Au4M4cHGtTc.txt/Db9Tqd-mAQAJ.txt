X-Received: by 10.101.66.136 with SMTP id j8mr860083pgp.0.1507836719742;
        Thu, 12 Oct 2017 12:31:59 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.98.26.23 with SMTP id a23ls1315554pfa.12.gmail; Thu, 12 Oct
 2017 12:31:58 -0700 (PDT)
X-Received: by 10.98.9.27 with SMTP id e27mr2998941pfd.284.1507836718794;
        Thu, 12 Oct 2017 12:31:58 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1507836718; cv=none;
        d=google.com; s=arc-20160816;
        b=XNhmzilh3oJ3sY35yblBhCg3Ah48ipo3kzwPDoZgQ57oKbkkx2YIOQF34NfsT+S+0H
         fFif+5ho6smAvl9jDJqF2w1UPx+Lnaax9ECfMP+XaC4gWLcC9qggnacWqkdKlto4e9QM
         ruhn6UHmafVrUOuCLc5qMpsZeHaburCetGzqetvBEIW+L3zzq76qNzptdb/Vfd5/d34J
         Zkn5LeVAFE+d1QhaN6VC/+ySDJ4EyppFxd2bsUjEGksN1Hztogv3iIYQ8hEDBtbL01oq
         MaitkbfZQCG0hs0F4vry/rnStKZ/Jq/9dKoFuakXCUkX5JJuUtctcPdCqheqvfjmghxY
         +HwA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=mMCCdWYgLrPF1v4SMZXi9BxLWLf/TWpzI0Kf1i7NZ3U=;
        b=BQk+LREk5nEIuCF+Zc/telj00FwkUquRrg74fi32nvQA2hFNRVGJJ5CTQyb8aMCq/D
         ba4G7MPhncJtglL+QkxWwpbceqS5BQ9T3SpX2mrG7LxVnoqGTCHpagln7ahv+hh/zvdy
         IIJEsXbZ8w7Nx6BpNty63QIWAoL0epyjMRBmv6U92VFpq2NUm//KiMXMKXrb9imZRHtq
         63Jze/AuMEWKOMmJhKDljoJqQtQPVgu+alNMI0tk2n/0ikWVegkha9SV+kUnC1/JtvYi
         BGSQXBm0u+3f5oA1B61a1IaocjxVsPf44o1Id9OYnbEgMCZ6+nrxfa29PNnq0o30pCFR
         caxw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=C4biTsE8;
       spf=pass (google.com: domain of arok...@gmail.com designates 209.85.192.171 as permitted sender) smtp.mailfrom=arok...@gmail.com
Return-Path: <arok...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id q83si1625123pfj.99.2017.10.12.12.31.58
        for <singu...@lbl.gov>;
        Thu, 12 Oct 2017 12:31:58 -0700 (PDT)
Received-SPF: pass (google.com: domain of arok...@gmail.com designates 209.85.192.171 as permitted sender) client-ip=209.85.192.171;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=C4biTsE8;
       spf=pass (google.com: domain of arok...@gmail.com designates 209.85.192.171 as permitted sender) smtp.mailfrom=arok...@gmail.com
X-Ironport-SBRS: 2.7
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2F7AADawt9Zf6vAVdFdHAEBBAEBCgEBF?=
 =?us-ascii?q?wEBBAEBCgEBgwQCgRBuJweDawiBNohpjzGBdpBwhT8OgUFDIgGFIgKEOAc/GAE?=
 =?us-ascii?q?BAQEBAQEBAQEBAhABAQkLCwgmMYI4BQIDGgYIRikvAQEBAQEBAQEBAR8CKyUCG?=
 =?us-ascii?q?AEBAQMBIx0BDQ4eAwELBgMCCw0qAgIhAQEOAwEFAQsRDgcEARwEiC2BNwEDDQg?=
 =?us-ascii?q?FjgmRG0CMDIIFBQEcgwoFg2gKGScNWIMUAQEIAQEBAQEbAgYSgxuCB4FRhRSCX?=
 =?us-ascii?q?oF0ARIBgzKCYQEEihZ/lXM8j3GEeZMQjQCIVxkfgRUfgQg0CzIhCB0VSRqEcYI?=
 =?us-ascii?q?uJDYIiRpIgW0BAQE?=
X-IronPort-AV: E=Sophos;i="5.43,367,1503385200"; 
   d="scan'208,217";a="336461"
Received: from mail-pf0-f171.google.com ([209.85.192.171])
  by fe4.lbl.gov with ESMTP; 12 Oct 2017 12:31:40 -0700
Received: by mail-pf0-f171.google.com with SMTP id b85so6277658pfj.13
        for <singu...@lbl.gov>; Thu, 12 Oct 2017 12:31:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=mMCCdWYgLrPF1v4SMZXi9BxLWLf/TWpzI0Kf1i7NZ3U=;
        b=C4biTsE8YuQq33pwc81e+pKn1mlghugHVHKRPrQ6bB0k+6mdYbKRcYYwIkIdeps0VB
         dNTtjV8vNvAaNuvvO5cEWwdgvk1erCTLHIxXGyVnD0tR++/17msue+j3fBhkRU0amliP
         H4SNvFF5mfqTGBL7nia1XHeC6bdAPExj6gFh/hYvyOGSY0kJj79SoWT2dJqcofydWok8
         TIuhlgU5y0gTXs7yzASwzCT+1+2kWYu9MzxIDhjk/busSeX/bEIcYWeOWIs8C5rYO4n5
         Z0OrbKnc5lgjeTwKNo1jWVDMI1tgBYFRqdRBGvWpaIZW3mcK6njSuz1UVDKang6LIx2t
         2IuA==
X-Gm-Message-State: AMCzsaVeu3DebN0FwqMsouW0A6+1s72eO16rU3kaAANTquJxOe17+f+s
	fa5FJ/KZFEKi2BrcnIN/GCYw3HG//f/Fd48zhBlpDA==
X-Received: by 10.99.100.129 with SMTP id y123mr1017010pgb.267.1507836698406;
 Thu, 12 Oct 2017 12:31:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.100.137.22 with HTTP; Thu, 12 Oct 2017 12:31:37 -0700 (PDT)
In-Reply-To: <942d5c3f-57a6-4d81-a4d9-5755b03fdc60@lbl.gov>
References: <43e70f0f-e4c3-4876-aff5-ed8255ebbae4@lbl.gov> <CAHGBjRwXGr50wCTgknTD6TAhxoFc-hOvLr07qPw7YHeW3eqwdA@mail.gmail.com>
 <942d5c3f-57a6-4d81-a4d9-5755b03fdc60@lbl.gov>
From: Andrew Rokitka <arok...@gmail.com>
Date: Thu, 12 Oct 2017 15:31:37 -0400
Message-ID: <CAHGBjRxZ+D=Cbfr2OeaM3VFkDAUjKVY7xt4rw=bwZ-nBfUx1eg@mail.gmail.com>
Subject: Re: [Singularity] Beginner Singularity question (StarCCM++)
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="94eb2c11dc0ac9b200055b5e96c8"

--94eb2c11dc0ac9b200055b5e96c8
Content-Type: text/plain; charset="UTF-8"

I've done something similar.  Take advantage of the 'singularity run'
functionality to do this.  Executing *./starccm.img* is the same as running
"*singularity run starccm.img*".   If you *ln -s starccm.img mpirun* and
then execute ./mpirun, it too is the same as running "*singularity run
starccm.img*".   In your run script, you can write a case statement that
handles all of the commands you expect to consume and then execute them
within the container accordingly.  Then you softlink those command names to
starccm.img. For example (and this isn't meant to necessarily work in your
environment):

My image is: /misc/shared/images/analysis.img

In /misc/shared/bin, i have softlinks to /misc/shared/images/analysis.img
for all of my commands, including all of the mpich commands

I have my $PATH set with /misc/shared/bin first so that mpirun is
/misc/shared/bin/mpirun and not /usr/bin/mpirun

I have a runscript (I hope this formats correctly!) that allows you to run
the softlinked commands and any options within the container itself:

-------------------------------------------------------------
  EXEC_NAME=`basename $SINGULARITY_NAME || echo ""`

  case $EXEC_NAME in
    R)
      CMD="/usr/bin/${EXEC_NAME}"
      ;;
    starccm)
      CMD="/misc/analysisprogram/${EXEC_NAME}"
      ;;

bt2line|check_callstack|clog2_join|clog2_print|clog2_repair|mpiexec.hydra|parkill|rlog_check_timeorder|rlog_print)
      CMD="/usr/bin/${EXEC_NAME}"
      ;;

clog2print|clog2TOslog2|clogprint|clogTOslog2|hydra_nameserver|hydra_persist|hydra_pmi_proxy|jumpshot|logconvertor|mpic++|mpicc|mpich2version|mpicxx|mpiexec|mpif77|mpif90|mpirun)
      CMD="/usr/lib64/mpi-mpich2/bin/${EXEC_NAME}"
      ;;
    *)
      CMD="Command not found"
      exit 1
  esac

  cd $PWD
  $CMD $@

-------------------------------------------------------------

So what happens is if I run *R CMD INSTALL /tmp/boguspackage.tgz*, it's
actually running R within the container.  I suspect you could fairly easily
do something similar with starccm++.



On Thu, Oct 12, 2017 at 2:53 PM, Matt Kijowski <matthew...@gmail.com>
wrote:

> yes (ish).  I dont really care whether mpi runs inside the container or
> outside.  But given that starccm+ seems to only work with its own built in
> mpi I think I need it running within the container itself.
>
>
> On Thursday, October 12, 2017 at 2:48:04 PM UTC-4, Andrew Rokitka wrote:
>>
>> Just to clarify - you want mpi to run within the container itself?
>>
>> On Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski <mat...@gmail.com>
>> wrote:
>>
>>> Hello all, we're just starting to use Singularity on our HPC system in
>>> conjunction with Slurm and I am having some difficulties understanding how
>>> to set up one of our containers (but I might be setting things up wrong).
>>>
>>> So I understand why we would use mpirun outside of a container, but what
>>> is a good configuration for a program (StarCCM++) that wraps its own MPI
>>> and remote commands into its execution?
>>>
>>> Example.  I have a 64 node cluster and starccm++ installed into a
>>> container.  If I want to run my job inside the container I execute:
>>> *srun singularity exec starccm.img starccm+ options simfile*where srun
>>> requests a compute node from slurm and executes the rest of the command on
>>> that compute node.  The problem I have is with parallel jobs.  starccm+
>>> wants to start its processes on the compute nodes itself (I can specify
>>> what sort of remote shell to use and which compute nodes I have requested
>>> via a machine list file in the options) but starccm+ is not aware that it
>>> needs to launch a singularity container on the compute nodes first.
>>>
>>> My first thought is I need to launch the containers on the compute nodes
>>> before executing the starccm++ parallel job so that when it uses SSH to log
>>> in to the compute nodes the container is already running but I'm not sure
>>> on how to accomplish this.  Any thoughts?
>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--94eb2c11dc0ac9b200055b5e96c8
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">I&#39;ve done something similar.=C2=A0 Take advantage of t=
he &#39;singularity run&#39; functionality to do this.=C2=A0 Executing <i>.=
/starccm.img</i> is the same as running &quot;<i>singularity run starccm.im=
g</i>&quot;. =C2=A0 If you <i>ln -s starccm.img mpirun</i>=C2=A0and then ex=
ecute ./mpirun, it too is the same as running &quot;<i>singularity run star=
ccm.img</i>&quot;. =C2=A0 In your run script, you can write a case statemen=
t that handles all of the commands you expect to consume and then execute t=
hem within the container accordingly.=C2=A0 Then you softlink those command=
 names to starccm.img. For example (and this isn&#39;t meant to necessarily=
 work in your environment):<div><br></div><div>My image is: /misc/shared/im=
ages/analysis.img=C2=A0<br></div><div><br></div><div>In /misc/shared/bin, i=
 have softlinks to /misc/shared/images/analysis.img for all of my commands,=
 including all of the mpich commands</div><div><br></div><div>I have my $PA=
TH set with /misc/shared/bin first so that mpirun is /misc/shared/bin/mpiru=
n and not /usr/bin/mpirun</div><div><br></div><div>I have a runscript (I ho=
pe this formats correctly!) that allows you to run the softlinked commands =
and any options within the container itself:</div><div><font face=3D"monosp=
ace, monospace"><br></font></div><div><font face=3D"monospace, monospace">-=
------------------------------------------------------------</font></div><d=
iv><span style=3D"font-family:monospace,monospace">=C2=A0 EXEC_NAME=3D`base=
name $SINGULARITY_NAME || echo &quot;&quot;`</span><br></div><div><span sty=
le=3D"font-family:monospace,monospace"><br></span></div><div><div><font fac=
e=3D"monospace, monospace">=C2=A0 case $EXEC_NAME in</font></div><div><font=
 face=3D"monospace, monospace">=C2=A0 =C2=A0 R)</font></div><div><font face=
=3D"monospace, monospace">=C2=A0 =C2=A0 =C2=A0 CMD=3D&quot;/usr/bin/${EXEC_=
NAME}&quot;</font></div><div><span style=3D"font-family:monospace,monospace=
">=C2=A0 =C2=A0 =C2=A0</span><span style=3D"font-family:monospace,monospace=
">=C2=A0</span><font face=3D"monospace, monospace">;;</font></div><div><fon=
t face=3D"monospace, monospace">=C2=A0 =C2=A0 starccm)</font></div><div><sp=
an style=3D"font-family:monospace,monospace">=C2=A0 =C2=A0 =C2=A0</span><sp=
an style=3D"font-family:monospace,monospace">=C2=A0</span><font face=3D"mon=
ospace, monospace">CMD=3D&quot;/misc/analysisprogram/${EXEC_NAME}&quot;</fo=
nt></div><div><span style=3D"font-family:monospace,monospace">=C2=A0 =C2=A0=
 =C2=A0</span><span style=3D"font-family:monospace,monospace">=C2=A0</span>=
<font face=3D"monospace, monospace">;;</font></div><div><font face=3D"monos=
pace, monospace">=C2=A0 =C2=A0 bt2line|check_callstack|clog2_join|clog2_pri=
nt|clog2_repair|mpiexec.hydra|parkill|rlog_check_timeorder|rlog_print)</fon=
t></div><div><span style=3D"font-family:monospace,monospace">=C2=A0 =C2=A0 =
=C2=A0</span><span style=3D"font-family:monospace,monospace">=C2=A0</span><=
font face=3D"monospace, monospace">CMD=3D&quot;/usr/bin/${EXEC_NAME}&quot;<=
/font></div><div><span style=3D"font-family:monospace,monospace">=C2=A0 =C2=
=A0 =C2=A0</span><span style=3D"font-family:monospace,monospace">=C2=A0</sp=
an><font face=3D"monospace, monospace">;;</font></div><div><span style=3D"f=
ont-family:monospace,monospace">=C2=A0 =C2=A0 clog2print|clog2TOslog2|clogp=
rint|clogTOslog2|hydra_nameserver|hydra_persist|hydra_pmi_proxy|jumpshot|lo=
gconvertor|mpic++|mpicc|mpich2version|mpicxx|mpiexec|mpif77|mpif90|mpirun)<=
/span></div><div><span style=3D"font-family:monospace,monospace">=C2=A0 =C2=
=A0 =C2=A0</span><span style=3D"font-family:monospace,monospace">=C2=A0</sp=
an><font face=3D"monospace, monospace">CMD=3D&quot;/usr/lib64/mpi-mpich2/bi=
n/${EXEC_NAME}&quot;</font></div><div><span style=3D"font-family:monospace,=
monospace">=C2=A0 =C2=A0 =C2=A0</span><span style=3D"font-family:monospace,=
monospace">=C2=A0</span><font face=3D"monospace, monospace">;;</font></div>=
<div><font face=3D"monospace, monospace">=C2=A0 =C2=A0 *)</font></div><div>=
<span style=3D"font-family:monospace,monospace">=C2=A0 =C2=A0 =C2=A0</span>=
<span style=3D"font-family:monospace,monospace">=C2=A0</span><font face=3D"=
monospace, monospace">CMD=3D&quot;Command not found&quot;</font></div><div>=
<span style=3D"font-family:monospace,monospace">=C2=A0 =C2=A0 =C2=A0</span>=
<span style=3D"font-family:monospace,monospace">=C2=A0</span><font face=3D"=
monospace, monospace">exit 1</font></div><div><span style=3D"font-family:mo=
nospace,monospace">=C2=A0 esac</span><br></div><div><span style=3D"font-fam=
ily:monospace,monospace"><br></span></div><div><font face=3D"monospace, mon=
ospace">=C2=A0 cd $PWD</font></div><div><font face=3D"monospace, monospace"=
>=C2=A0 $CMD $@</font></div><div><font face=3D"monospace, monospace"><br></=
font></div><div><font face=3D"monospace, monospace">-----------------------=
--------------------------------------</font><br><div><div><br></div><div>S=
o what happens is if I run <i>R CMD INSTALL /tmp/boguspackage.tgz</i>, it&#=
39;s actually running R within the container.=C2=A0 I suspect you could fai=
rly easily do something similar with starccm++.</div><div><br><div><br></di=
v></div></div></div></div></div><div class=3D"gmail_extra"><br><div class=
=3D"gmail_quote">On Thu, Oct 12, 2017 at 2:53 PM, Matt Kijowski <span dir=
=3D"ltr">&lt;<a href=3D"mailto:matthew...@gmail.com" target=3D"_blank">matt=
hew...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote"=
 style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><d=
iv dir=3D"ltr">yes (ish).=C2=A0 I dont really care whether mpi runs inside =
the container or outside.=C2=A0 But given that starccm+ seems to only work =
with its own built in mpi I think I need it running within the container it=
self.<span class=3D""><br><br><br>On Thursday, October 12, 2017 at 2:48:04 =
PM UTC-4, Andrew Rokitka wrote:</span><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0;margin-left:0.8ex;border-left:1px #ccc solid;padding-left:1e=
x"><span class=3D""><div dir=3D"ltr">Just to clarify - you want mpi to run =
within the container itself?</div></span><div><br><div class=3D"gmail_quote=
"><span class=3D"">On Thu, Oct 12, 2017 at 2:41 PM, Matt Kijowski <span dir=
=3D"ltr">&lt;<a rel=3D"nofollow">mat...@gmail.com</a>&gt;</span> wrote:<br>=
</span><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-=
left:1px #ccc solid;padding-left:1ex"><span class=3D""><div dir=3D"ltr">Hel=
lo all, we&#39;re just starting to use Singularity on our HPC system in con=
junction with Slurm and I am having some difficulties understanding how to =
set up one of our containers (but I might be setting things up wrong).<br><=
br>So I understand why we would use mpirun outside of a container, but what=
 is a good configuration for a program (StarCCM++) that wraps its own MPI a=
nd remote commands into its execution?<br><br>Example.=C2=A0 I have a 64 no=
de cluster and starccm++ installed into a container.=C2=A0 If I want to run=
 my job inside the container I execute: <i>srun singularity exec starccm.im=
g starccm+ options simfile<br></i>where srun requests a compute node from s=
lurm and executes the rest of the command on that compute node.=C2=A0 The p=
roblem I have is with parallel jobs.=C2=A0 starccm+ wants to start its proc=
esses on the compute nodes itself (I can specify what sort of remote shell =
to use and which compute nodes I have requested via a machine list file in =
the options) but starccm+ is not aware that it needs to launch a singularit=
y container on the compute nodes first.<br><br>My first thought is I need t=
o launch the containers on the compute nodes before executing the starccm++=
 parallel job so that when it uses SSH to log in to the compute nodes the c=
ontainer is already running but I&#39;m not sure on how to accomplish this.=
=C2=A0 Any thoughts?<span><font color=3D"#888888"><br><i></i></font></span>=
</div></span><span><font color=3D"#888888"><span class=3D"">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></span>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><br></div>
</blockquote></div><div class=3D"HOEnZb"><div class=3D"h5">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--94eb2c11dc0ac9b200055b5e96c8--
