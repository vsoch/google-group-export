X-Received: by 10.129.82.133 with SMTP id g127mr10620345ywb.98.1499636732107;
        Sun, 09 Jul 2017 14:45:32 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.101.129 with SMTP id u123ls3980155itb.2.gmail; Sun, 09 Jul
 2017 14:45:31 -0700 (PDT)
X-Received: by 10.99.107.9 with SMTP id g9mr5016674pgc.147.1499636731037;
        Sun, 09 Jul 2017 14:45:31 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1499636731; cv=none;
        d=google.com; s=arc-20160816;
        b=AVnMOTrqYoWk4maGn98wzVh1NayPRkaOdXFGVq9fSRwT7s4H5n0/2Sjseh2JBhcv0L
         0s8cWYOLVjKcBsvcr3QlHlzDvA2LnirEtM4ik9Falcxgwffg8eoYj7S0NcmWhZ+XIS5o
         neWZOlky0fx/Ir9QnEy3w3YrEv38C5v8I2ZwAPa1mt+98fWgZFbngHbWLR5VZfYioxUg
         DafbH1wQHgSXA1uUElrNXER2jBqXtwjy1167jIOsGJiuIkQd38IiTzy/K+AbMw4leJQn
         gNUGz842zNqj3SphpjGTEYlUA06WJxiyHYWyqPxCxL7PsnccQ13exYgS4KSYlykPZ7bu
         dCqQ==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=cc:to:subject:message-id:date:from:references:in-reply-to
         :mime-version:dkim-signature:arc-authentication-results;
        bh=UJf055Gt64r0Y2/JMfNOTWYAXXx+68FZNISh3AI61Z4=;
        b=ieMWEe4q5355cu+sRFTDhXKv9Q9ZLujsw2elcgogvgNru7mQtKW3WCQ4Ay3joZSQKk
         AYEfX4wkJ4dh5p+NKFOFm8JHRF57tqHPm3jrvcUFHYJmy7Y35jt3yFkmNr3LrHw+Z2OE
         CMcXnSpB15kVbuXDXZe0m2dy5pVdVd3HifC83b98jzI88keT7Ic2GE9p/uT86JkxD5+q
         AUezWWPsKYVudNzLRwYy58fQV+Uf2JycG9ug+ql2mBRReO/eCwioW5o0EltSFTOjpoIJ
         yM5XUb44SzcKG/bKvdA3PRsUTyXgNO33NOaJXiVPitoYSpODv1rBSEHYdSUJxhiPcr2t
         obRw==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.b=OvDucsh6;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.214.49 as permitted sender) smtp.mailfrom=gmku...@gmail.com
Return-Path: <gmku...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id l185si6635813pgd.570.2017.07.09.14.45.30
        for <singu...@lbl.gov>;
        Sun, 09 Jul 2017 14:45:31 -0700 (PDT)
Received-SPF: pass (google.com: domain of gmku...@gmail.com designates 209.85.214.49 as permitted sender) client-ip=209.85.214.49;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.b=OvDucsh6;
       spf=pass (google.com: domain of gmku...@gmail.com designates 209.85.214.49 as permitted sender) smtp.mailfrom=gmku...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2FmAAClo2JZhjHWVdFbHAEBBAEBCgEBF?=
 =?us-ascii?q?wEBBAEBCgEBgkRAgQ+BFAeDYYE+iGORaYJsjWyFLIFOGyghAQyCM4FcgRBPAoN?=
 =?us-ascii?q?CBz8YAQEBAQEBAQEBAQECEAEBAQgLCwgoL4IzBQIDGgEFBARGJgMDAQEBAQEBA?=
 =?us-ascii?q?QEBIwEBAQEBAQEBAQEBAQEBARoCCAUeEgESAQEYAQEBAQIBIx0BDQ4SCwEDAQs?=
 =?us-ascii?q?GAwILCgMqAgIhAQEOAwEFARwOBwQBHAICiDyBOQEDDQgFC4xvkRo/jAqCBAUBH?=
 =?us-ascii?q?IMIBYNVChknDVaDKgEBAQEBAQQBAQEBAQEBAQEBARUCBhKDFoJSgluDJIJXgWQ?=
 =?us-ascii?q?SAUmCZoJhBZFYhV2HLjsCh0aDRYQPhG6CDFePP4k4gkGHfxQfgRUPEIEFMwt1X?=
 =?us-ascii?q?hqEQCoPEAyCByA2AQEBAQSGQ0eBaQEBAQ?=
X-IronPort-AV: E=Sophos;i="5.40,336,1496127600"; 
   d="scan'208,217";a="81583314"
Received: from mail-it0-f49.google.com ([209.85.214.49])
  by fe3.lbl.gov with ESMTP; 09 Jul 2017 14:45:28 -0700
Received: by mail-it0-f49.google.com with SMTP id k192so21565015ith.1
        for <singu...@lbl.gov>; Sun, 09 Jul 2017 14:45:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc;
        bh=UJf055Gt64r0Y2/JMfNOTWYAXXx+68FZNISh3AI61Z4=;
        b=OvDucsh6L/5EppNJ67SxB1C5iI9Ov9VNUBA4AGFHvUXT0OEEzS2F5qYUAAmtToMi+y
         pn3W+pALpm0Wf8lAHYFKvoMMyvcjQYGBhe7JBffLBDD9jclppOeVGS03wwdCwrrXp55D
         hDJ7naX4n7/CJaN08mGw7oKVlB0hPnoUbu6LsEW2xN/Hi7dJ/3zmnfUaY70aOb30+3X9
         lJwGjNsL1uk45V5+R66OYKykanzG1HoE3GLeilql4nLr3NMERX5wxhxveYj+yHfwJ/E4
         x/OOyk6JvQmsxQhHnPnNmbOEPcmaLPYAXT6+bNxnLIyi2vdydFqXx+XFrh9BF4pQFXk+
         ImRA==
X-Gm-Message-State: AIVw111QUC8DdI5X6H1AClwYYPtsLDQcxyqSSwWInYD1MOoAetnGwzok
	gwE5ga8LLce42OrQH/INMGYSfdVzamAnNF8=
X-Received: by 10.107.36.136 with SMTP id k130mr642777iok.7.1499636728311;
 Sun, 09 Jul 2017 14:45:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.23.132 with HTTP; Sun, 9 Jul 2017 14:45:27 -0700 (PDT)
In-Reply-To: <CAApQTTgQwZ3pu7Xa7RsbuHM7ndP05Kv5ciEBk_q5G-8P3xaUGw@mail.gmail.com>
References: <CA+Wz_FyhoZfQ7TBc1kvS0QRyZmQqaw89vBjvdW1tZCfzrKh4+w@mail.gmail.com>
 <CAApQTTgQwZ3pu7Xa7RsbuHM7ndP05Kv5ciEBk_q5G-8P3xaUGw@mail.gmail.com>
From: "Gregory M. Kurtzer" <gmku...@gmail.com>
Date: Sun, 9 Jul 2017 14:45:27 -0700
Message-ID: <CAApQTTg2vQY9wJg0g_Qp5n+558jYqb_ZX8-iezzwJf-6nM63QQ@mail.gmail.com>
Subject: Re: [Singularity] OpenMPI, Slurm & portability
To: singularity@lbl.gov
Cc: Ralph Castain <rcas...@gmail.com>
Content-Type: multipart/alternative; boundary="001a1140e3307bc60b0553e96245"

--001a1140e3307bc60b0553e96245
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hiya Victor, et al.,

I didn't realize this but Ralph had to drop off of the Singularity list.
Hopefully we will get him back again, as he is a fantastic resource for all
of the OMPI questions and always a great source of information and ideas
(poke, poke Ralph!). Ralph did send me this in response to the previous
email hoping it helps to explain things:


On Sun, Jul 9, 2017 at 2:22 PM, r...@open-mpi.org <r...@open-mpi.org> wrote=
:

> ...
You are welcome to forward the following to the list:

As Greg said, we have been concerned about this since we started looking at
Singularity support. Just for clarity, the version of PMI OMPI uses is PMIx
(https://pmix.github.io/pmix/). While our plan from the beginning was to
support cross-versions specifically to address this problem, we fell behind
on its implementation due to priorities. We just committed the code to the
PMIx repo in the last week, and it won=E2=80=99t be released into productio=
n for a
few months while we shake it down.

I fear it will be impossible to get the OMPI 1.10 series to work with
anything other than itself as it pre-dates PMIx.

The OMPI 2.0 and 2.1 series should work across each other as they both
include PMIx 1.x. However, you probably will need to configure the 2.1
series with --disable-pmix-dstore as there was an unintended compatibility
break there (the shared memory store was added during the PMIx 1.x series
and we didn=E2=80=99t catch the compatibility break it introduced).

Looking into the future, OMPI 3.0 is about to be released. It includes PMIx
2.0, which isn=E2=80=99t backwards compatible at this time, and so it won=
=E2=80=99t
cross-version with OMPI 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We haven=E2=
=80=99t tested this, but
one thing you could try is to build all three OMPI versions against the
same PMIx external library (you would probably have to experiment a bit
with PMIx versions to see which works across the different OMPI versions as
the glue between the two also changed a bit). This will ensure that the
shared memory store in PMIx is compatible across the versions, and things
should work since OMPI doesn=E2=80=99t care how the data is moved across th=
e
host-container boundary.

As I said, we will be adding cross-version support to the PMIx release
series soon, without changing the API, that will ensure support across all
PMIx versions starting with v1.2. Thus, you could (once that happens) build
OMPI 2.0, 2.1, and 3.0 against the new PMIx release (probably PMIx v2.1.0)
and the resulting containers would be future-proof as OMPI moves ahead. The
RMs plan to follow that path as well, so you should be in good shape once
this is done if you prefer to =E2=80=9Cdirect launch=E2=80=9D your containe=
rs (e.g., =E2=80=9Csrun
./mycontainer=E2=80=9D under SLURM).

Sorry if that is all confusing - we sometimes get lost in the numbering
schemes between OMPI and PMIx ourselves. Feel free to contact me directly,
or on the OMPI or PMIx mailing lists, if you have more questions or
encounter problems. We definitely want to make this work.

Ralph

On Sun, Jul 9, 2017 at 12:19 PM, Gregory M. Kurtzer <gmku...@gmail.com>
wrote:

> Hi Victor,
>
> Sorry for the latency, I'm on email overload.
>
> Open MPI uses PMI to communicate both inside and outside of the container=
.
> Ralph Castain (on this list, but possibly not monitoring actively) is
> leading the PMI effort and he is an active Open MPI developer. We have ha=
d
> several talks about how to achieve "hetero-versionistic" compatibility
> through the PMI handshake. I was under the impression that PMI now suppor=
ts
> that, as long as you are running equal or newer version on the host
> (outside the container). Also, I don't know what version of PMI this
> feature was introduced in, nor do I know what version of Open MPI include=
s
> that compatibility.
>
> I have CC'ed Ralph, and hopefully he will be able to offer some
> suggestions.
>
> Regarding your question about supporting the MPI libraries in the same
> manner that we are doing the Nvidia libraries, that would be hard. Nvidia
> specifically builds their libraries to be as generally compatible as
> possible (e.g. the same libraries/binaries work on a large array of Linux
> distributions). Most people do not build host libraries in a manner that
> would be generally compatible as Nvidia does.
>
> Hope that helps!
>
> Greg
>
>
>
> On Mon, Jul 3, 2017 at 2:07 AM, victor sv <vict...@gmail.com> wrote:
>
>> Dear Singularity team,
>>
>> first of all, thanks for the great work with Singularity. It looks
>> amazing!
>>
>> Sorry if this topic is duplicated and for the length of the email, but I
>> want to share my experience about Singularity and OpenMPI compatibility,
>> and also ask some questions.
>>
>> I've being reading a lot about OpenMPI and Singularity compatibility
>> because we are trying to find the generic way to run OpenMPI application=
s
>> within Singularity containers. It was not so clear (for me) in the
>> documentation, forums and mailing lists, and this is why we've performed=
 an
>> OpenMPI empiric compatibility study.
>>
>> We ran these comparisons in CESGA FinisTerrae II cluster (
>> https://www.cesga.es/en/infraestructuras/computacion/FinisTerrae2).
>>
>> We used several versions of OpenMPI. The chosen versions of OpenMPI were
>> the versions already installed in the cluster:
>>
>> - openmpi/1.10.2
>> - openmpi/2.0.0
>> - openmpi/2.0.1
>> - openmpi/2.0.2
>> - openmpi/2.1.1
>>
>> We have created Singularity images containing the same versions of
>> OpenMPI and with the basic OpenMPI ring example. I share the bootstrap
>> definition file template used below:
>>
>> ```
>> BootStrap: docker
>> From: ubuntu:16.04
>> IncludeCmd: yes
>>
>> %post
>>         sed -i 's/main/main restricted universe/g' /etc/apt/sources.list
>>         apt-get update
>>         apt-get install -y bash git wget build-essential gcc time
>> libc6-dev libgcc-5-dev
>>         apt-get install -y dapl2-utils libdapl-dev libdapl2 libibverbs1
>> librdmacm1 libcxgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 libne=
s1
>> libpmi0 libpmi0-dev libslurm29 libslurm-dev
>>
>>         ##Install OpenMPI
>>         cd /tmp
>>         wget 'https://www.open-mpi.org/software/ompi/vX.X/downloads/open=
m
>> pi-X.X.X.tar.gz' -O openmpi-X.X.X.tar.gz
>>         tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X
>>         mkdir -p /tmp/openmpi-X.X.X/build
>>         cd /tmp/openmpi-X.X.X/build
>>          ../configure --enable-shared --enable-mpi-thread-multiple
>> --with-verbs --enable-mpirun-prefix-by-default --with-hwloc
>> --disable-dlopen --with-pmi --prefix=3D/usr
>>         make all install
>>
>>         # Install ring
>>         cd /tmp
>>         wget https://raw.githubusercontent.com/open-mpi/ompi/master/exam=
p
>> les/ring_c.c
>>         mpicc ring_c.c -o /usr/bin/ring
>> ```
>>
>> Once the containers were created, we ran the ring app with mpirun using =
2
>> cores of 2 different nodes mixing all possible combinations of those
>> OpenMPI versions inside and outside the container.
>>
>> The obtained results shown that we need the same versions of OpenMPI
>> inside and outside the container to succesfully run the contained
>> application in parallel with mpirun.
>>
>> Is this the expected behaviour or am I missing something?
>>
>> Will be this the expected behaviour in the future (with future versions
>> of OpenMPI)?
>>
>> Currently, we have slurm 14.11.10-Bull.1.0 installed as job scheduler at
>> FinisTerrae II. We found the following tip/trick to use srun as process
>> manager:
>>
>> http://singularity.lbl.gov/tutorial-gpu-drivers-open-mpi-mtls
>>
>> In order to run whatever Singularity image containing OpenMPI
>> applications using Slurm, we've adapted it to our infrastructure and
>> checked the same test cases running them with srun. It seems that it's
>> working properly (no real world applications were tested yet).
>>
>> What do you think about this strategy?
>> Can you confirm that it provides portability of singularity images
>> containing OpenMPI applications?
>>
>> I think this strategy is similar to the one you are following with "--nv=
"
>> option  for NVidia drivers.
>>
>> Why not to do the same strategy with MPI, PMI, libibverbs, etc.?
>>
>> Thanks in advance and congrats again for your great work!
>>
>> V=C3=ADctor.
>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov.
>>
>
>
>
> --
> Gregory M. Kurtzer
> CEO, SingularityWare, LLC.
> Senior Architect, RStor
> Computational Science Advisor, Lawrence Berkeley National Laboratory
>



--=20
Gregory M. Kurtzer
CEO, SingularityWare, LLC.
Senior Architect, RStor
Computational Science Advisor, Lawrence Berkeley National Laboratory

--001a1140e3307bc60b0553e96245
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hiya Victor, et al.,=C2=A0<div><br></div><div>I didn&#39;t=
 realize this but Ralph had to drop off of the Singularity list. Hopefully =
we will get him back again, as he is a fantastic resource for all of the OM=
PI questions and always a great source of information and ideas (poke, poke=
 Ralph!). Ralph did send me this in response to the previous email hoping i=
t helps to explain things:</div><div><br></div><div><br></div><div>On Sun, =
Jul 9, 2017 at 2:22 PM, <a href=3D"mailto:r...@open-mpi.org">r...@open-mpi.=
org</a>=C2=A0<span dir=3D"ltr">&lt;<a href=3D"mailto:r...@open-mpi.org" tar=
get=3D"_blank">r...@open-mpi.org</a>&gt;</span>=C2=A0wrote:<br><blockquote =
class=3D"gmail_quote" style=3D"margin:0px 0px 0px 0.8ex;border-left:1px sol=
id rgb(204,204,204);padding-left:1ex"></blockquote></div><div><div style=3D=
"font-size:12.8px">...</div><div style=3D"font-size:12.8px">You are welcome=
 to forward the following to the list:</div><div style=3D"font-size:12.8px"=
><br></div><div style=3D"font-size:12.8px">As Greg said, we have been conce=
rned about this since we started looking at Singularity support. Just for c=
larity, the version of PMI OMPI uses is PMIx (<a href=3D"https://pmix.githu=
b.io/pmix/" target=3D"_blank">https://pmix.github.io/pmix/</a>)<wbr>. While=
 our plan from the beginning was to support cross-versions specifically to =
address this problem, we fell behind on its implementation due to prioritie=
s. We just committed the code to the PMIx repo in the last week, and it won=
=E2=80=99t be released into production for a few months while we shake it d=
own.</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-size=
:12.8px">I fear it will be impossible to get the OMPI 1.10 series to work w=
ith anything other than itself as it pre-dates PMIx.</div><div style=3D"fon=
t-size:12.8px"><br></div><div style=3D"font-size:12.8px">The OMPI 2.0 and 2=
.1 series should work across each other as they both include PMIx 1.x. Howe=
ver, you probably will need to configure the 2.1 series with --disable-pmix=
-dstore as there was an unintended compatibility break there (the shared me=
mory store was added during the PMIx 1.x series and we didn=E2=80=99t catch=
 the compatibility break it introduced).</div><div style=3D"font-size:12.8p=
x"><br></div><div style=3D"font-size:12.8px">Looking into the future, OMPI =
3.0 is about to be released. It includes PMIx 2.0, which isn=E2=80=99t back=
wards compatible at this time, and so it won=E2=80=99t cross-version with O=
MPI 2.x =E2=80=9Cout-of-the-box=E2=80=9D. We haven=E2=80=99t tested this, b=
ut one thing you could try is to build all three OMPI versions against the =
same PMIx external library (you would probably have to experiment a bit wit=
h PMIx versions to see which works across the different OMPI versions as th=
e glue between the two also changed a bit). This will ensure that the share=
d memory store in PMIx is compatible across the versions, and things should=
 work since OMPI doesn=E2=80=99t care how the data is moved across the host=
-container boundary.</div><div style=3D"font-size:12.8px"><br></div><div st=
yle=3D"font-size:12.8px">As I said, we will be adding cross-version support=
 to the PMIx release series soon, without changing the API, that will ensur=
e support across all PMIx versions starting with v1.2. Thus, you could (onc=
e that happens) build OMPI 2.0, 2.1, and 3.0 against the new PMIx release (=
probably PMIx v2.1.0) and the resulting containers would be future-proof as=
 OMPI moves ahead. The RMs plan to follow that path as well, so you should =
be in good shape once this is done if you prefer to =E2=80=9Cdirect launch=
=E2=80=9D your containers (e.g., =E2=80=9Csrun ./mycontainer=E2=80=9D under=
 SLURM).</div><div style=3D"font-size:12.8px"><br></div><div style=3D"font-=
size:12.8px">Sorry if that is all confusing - we sometimes get lost in the =
numbering schemes between OMPI and PMIx ourselves. Feel free to contact me =
directly, or on the OMPI or PMIx mailing lists, if you have more questions =
or encounter problems. We definitely want to make this work.</div><div styl=
e=3D"font-size:12.8px"><br></div><div style=3D"font-size:12.8px">Ralph</div=
></div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On S=
un, Jul 9, 2017 at 12:19 PM, Gregory M. Kurtzer <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:gmku...@gmail.com" target=3D"_blank">gmku...@gmail.com</a>&gt;=
</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .=
8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Victor=
,<div><br></div><div>Sorry for the latency, I&#39;m on email overload.</div=
><div><br></div><div>Open MPI uses PMI to communicate both inside and outsi=
de of the container. Ralph Castain (on this list, but possibly not monitori=
ng actively) is leading the PMI effort and he is an active Open MPI develop=
er. We have had several talks about how to achieve &quot;hetero-versionisti=
c&quot; compatibility through the PMI handshake. I was under the impression=
 that PMI now supports that, as long as you are running equal or newer vers=
ion on the host (outside the container). Also, I don&#39;t know what versio=
n of PMI this feature was introduced in, nor do I know what version of Open=
 MPI includes that compatibility.</div><div><br></div><div>I have CC&#39;ed=
 Ralph, and hopefully he will be able to offer some suggestions.</div><div>=
<br></div><div>Regarding your question about supporting the MPI libraries i=
n the same manner that we are doing the Nvidia libraries, that would be har=
d. Nvidia specifically builds their libraries to be as generally compatible=
 as possible (e.g. the same libraries/binaries work on a large array of Lin=
ux distributions). Most people do not build host libraries in a manner that=
 would be generally compatible as Nvidia does.</div><div><br></div><div>Hop=
e that helps!</div><div><br></div><div>Greg</div><div><br></div><div><br></=
div></div><div class=3D"gmail_extra"><div><div class=3D"h5"><br><div class=
=3D"gmail_quote">On Mon, Jul 3, 2017 at 2:07 AM, victor sv <span dir=3D"ltr=
">&lt;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@gmail.=
com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"mar=
gin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr=
">Dear Singularity team,<br><br>first of all, thanks for the great work wit=
h Singularity. It looks amazing!<br><br>Sorry if this topic is duplicated a=
nd for the length of the email, but I want to share my experience about Sin=
gularity and OpenMPI compatibility, and also ask some questions.<br><br>I&#=
39;ve being reading a lot about OpenMPI and Singularity compatibility becau=
se we are trying to find the generic way to run OpenMPI applications within=
 Singularity containers. It was not so clear (for me) in the documentation,=
 forums and mailing lists, and this is why we&#39;ve performed an OpenMPI e=
mpiric compatibility study.<br><br>We ran these comparisons in CESGA FinisT=
errae II cluster (<a href=3D"https://www.cesga.es/en/infraestructuras/compu=
tacion/FinisTerrae2" target=3D"_blank">https://www.cesga.es/en/infra<wbr>es=
tructuras/computacion/FinisT<wbr>errae2</a>).<br><br>We used several versio=
ns of OpenMPI. The chosen versions of OpenMPI were the versions already ins=
talled in the cluster:<br><br>- openmpi/1.10.2<br>- openmpi/2.0.0<br>- open=
mpi/2.0.1<br>- openmpi/2.0.2<br>- openmpi/2.1.1<br><br>We have created Sing=
ularity images containing the same versions of OpenMPI and with the basic O=
penMPI ring example. I share the bootstrap definition file template used be=
low:<br><br>```<br>BootStrap: docker<br>From: ubuntu:16.04<br>IncludeCmd: y=
es<br><br>%post<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 sed -i &#39;s=
/main/main restricted universe/g&#39; /etc/apt/sources.list<br>=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-get update<br>=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 apt-get install -y bash git wget build-essential gcc tim=
e libc6-dev libgcc-5-dev<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 apt-=
get install -y dapl2-utils libdapl-dev libdapl2 libibverbs1 librdmacm1 libc=
xgb3-1 libipathverbs1 libmlx4-1 libmlx5-1 libmthca1 libnes1 libpmi0 libpmi0=
-dev libslurm29 libslurm-dev<br><br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 ##Install OpenMPI<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp=
<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 wget &#39;<a href=3D"https:/=
/www.open-mpi.org/software/ompi/vX.X/downloads/openmpi-X.X.X.tar.gz" target=
=3D"_blank">https://www.open-mpi.org/soft<wbr>ware/ompi/vX.X/downloads/open=
m<wbr>pi-X.X.X.tar.gz</a>&#39; -O openmpi-X.X.X.tar.gz<br>=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0 tar -xzf openmpi-X.X.X.tar.gz -C openmpi-X.X.X<=
br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 mkdir -p /tmp/openmpi-X.X.X/b=
uild<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp/openmpi-X.X.X/bu=
ild<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 ../configure --enab=
le-shared --enable-mpi-thread-multiple --with-verbs --enable-mpirun-prefix-=
by-defa<wbr>ult --with-hwloc --disable-dlopen --with-pmi --prefix=3D/usr<br=
>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 make all install<br><br>=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 # Install ring<br>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 cd /tmp<br>=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 wget <a href=3D"https://raw.githubusercontent.com/open-mpi/ompi/master/=
examples/ring_c.c" target=3D"_blank">https://raw.githubusercontent.<wbr>com=
/open-mpi/ompi/master/examp<wbr>les/ring_c.c</a><br>=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 mpicc ring_c.c -o /usr/bin/ring<br>```<br><br>Once th=
e containers were created, we ran the ring app with mpirun using 2 cores of=
 2 different nodes mixing all possible combinations of those OpenMPI versio=
ns inside and outside the container.<br><br>The obtained results shown that=
 we need the same versions of OpenMPI inside and outside the container to s=
uccesfully run the contained application in parallel with mpirun.<br><br>Is=
 this the expected behaviour or am I missing something?<br><br>Will be this=
 the expected behaviour in the future (with future versions of OpenMPI)?<br=
><br>Currently, we have slurm 14.11.10-Bull.1.0 installed as job scheduler =
at FinisTerrae II. We found the following tip/trick to use srun as process =
manager:<br><br><a href=3D"http://singularity.lbl.gov/tutorial-gpu-drivers-=
open-mpi-mtls" target=3D"_blank">http://singularity.lbl.gov/tut<wbr>orial-g=
pu-drivers-open-mpi-mtl<wbr>s</a><br><br>In order to run whatever Singulari=
ty image containing OpenMPI applications using Slurm, we&#39;ve adapted it =
to our infrastructure and checked the same test cases running them with sru=
n. It seems that it&#39;s working properly (no real world applications were=
 tested yet).<br><br>What do you think about this strategy?<br>Can you conf=
irm that it provides portability of singularity images containing OpenMPI a=
pplications?<br><br>I think this strategy is similar to the one you are fol=
lowing with &quot;--nv&quot; option=C2=A0 for NVidia drivers.<br><br>Why no=
t to do the same strategy with MPI, PMI, libibverbs, etc.?<br><br>Thanks in=
 advance and congrats again for your great work!<br><br>V=C3=ADctor.<span c=
lass=3D"m_8312868002830329868HOEnZb"><font color=3D"#888888"><br></font></s=
pan></div><span class=3D"m_8312868002830329868HOEnZb"><font color=3D"#88888=
8">

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.go<wbr>v</a>.<br>
</font></span></blockquote></div><br><br clear=3D"all"><div><br></div></div=
></div><span class=3D"HOEnZb"><font color=3D"#888888">-- <br><div class=3D"=
m_8312868002830329868gmail_signature" data-smartmail=3D"gmail_signature"><d=
iv dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. =
Kurtzer</div><div>CEO, SingularityWare, LLC.</div><div>Senior Architect, RS=
tor</div><div><span style=3D"font-size:12.8px">Computational Science Adviso=
r, Lawrence Berkeley National Laboratory</span><br></div></div></div></div>=
</div></div></div>
</font></span></div>
</blockquote></div><br><br clear=3D"all"><div><br></div>-- <br><div class=
=3D"gmail_signature" data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><d=
iv><div dir=3D"ltr"><div><div dir=3D"ltr"><div>Gregory M. Kurtzer</div><div=
>CEO, SingularityWare, LLC.</div><div>Senior Architect, RStor</div><div><sp=
an style=3D"font-size:12.8px">Computational Science Advisor, Lawrence Berke=
ley National Laboratory</span><br></div></div></div></div></div></div></div=
>
</div>

--001a1140e3307bc60b0553e96245--
