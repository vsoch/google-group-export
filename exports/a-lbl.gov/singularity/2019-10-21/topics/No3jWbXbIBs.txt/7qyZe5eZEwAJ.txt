X-Received: by 10.157.29.194 with SMTP id w2mr4504125otw.29.1487807926136;
        Wed, 22 Feb 2017 15:58:46 -0800 (PST)
X-BeenThere: singularity@lbl.gov
Received: by 10.36.98.11 with SMTP id d11ls4044294itc.1.canary-gmail; Wed, 22
 Feb 2017 15:58:45 -0800 (PST)
X-Received: by 10.99.163.81 with SMTP id v17mr43941304pgn.39.1487807925437;
        Wed, 22 Feb 2017 15:58:45 -0800 (PST)
Return-Path: <chihs...@gmail.com>
Received: from fe4.lbl.gov (fe4.lbl.gov. [128.3.41.71])
        by mx.google.com with ESMTP id l21si2572069pgh.17.2017.02.22.15.58.45
        for <singu...@lbl.gov>;
        Wed, 22 Feb 2017 15:58:45 -0800 (PST)
Received-SPF: pass (google.com: domain of chihs...@gmail.com designates 209.85.214.44 as permitted sender) client-ip=209.85.214.44;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com;
       spf=pass (google.com: domain of chihs...@gmail.com designates 209.85.214.44 as permitted sender) smtp.mailfrom=chihs...@gmail.com
X-Ironport-SBRS: 3.4
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: A2EnAwCxJK5YhizWVdFdGwEBAQMBAQEJAQEBFwEBBAEBCgEBgkOBQ4EJB4NMCIoIkVqCZI0kgk0IgleBShsoHwEGgXNDEIFcgVoCgwYHPxgBAQEBAQEBAQEBAQIQAQEBCAsLCh0kC4IqCQQCAxkFBAQ9CgMDAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQECAQEBAgEBAQIBAQEBAQMBAQEBAQEBAQEBAQEBAQEBGgINHgQPAykBAQEDASMdAQ0OHgMBCwYDAgsNKgICIQEBDgMBBQELEQYIBwQBEwkEh2cDgQhJAQMNCAWRXJEWP4wDggQFARyDCQWDaQoZJw1VgykBAQEHAQEBAQEBAQEBFwICBA0FiymCUYFVEAIBSIJZgl8FiRlhi2yFcDqCW4QZhxCEH4JOgQIFjTuKQYcdFB6BFQ8QgQIvCDUfUxeDeyoMAxEMgWI/NQeIJoFnAQEB
X-IronPort-AV: E=Sophos;i="5.35,196,1484035200"; 
   d="scan'208,217";a="65031218"
Received: from mail-it0-f44.google.com ([209.85.214.44])
  by fe4.lbl.gov with ESMTP; 22 Feb 2017 15:58:36 -0800
Received: by mail-it0-f44.google.com with SMTP id d9so43154671itc.0
        for <singu...@lbl.gov>; Wed, 22 Feb 2017 15:58:36 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=tA1pz7XLiHQUJESrYI1B/p1x9ZqgXz1oCt+24Xmhbqo=;
        b=PVNT18C5xgTWuReY+mhHdbL+FZxjXsZYPqYLyllrODtBlFhTC8GI/nCCPdW1D2Q72P
         tdQ9a8tG8LTJYREElrWbG6GGy84q0V1waN1Y1JGlNVYVdybXiq6FLQG89/SWQhVfizer
         MkfZzeD+f9AaUMc8e63S4hbxXUZH7u/kyPGIHWlf+OMNix/0SX9nPOyLtU3sVmn3FTqz
         F256l7ik7GTTm0Iv66BreHopLe0TETfkWEmjnou43zfYm28/ew5yhkWStnmVYkme0BO3
         lbkX5hFoYzL/GaVHjBvPC3Vm35XtQVLcwmVac917NAE56vchSZ7hxJSNwPGuHT5BERXY
         vlkQ==
X-Gm-Message-State: AMke39kAYUIEVqYEDLcoktZVHbo6nOQIljFhLmZzmwezFZq2/hKhoHJtKvuVQNIbpz4rBZ2j0wxaa2kpF/Mi5Q==
X-Received: by 10.36.44.12 with SMTP id i12mr768118iti.92.1487807916238; Wed,
 22 Feb 2017 15:58:36 -0800 (PST)
MIME-Version: 1.0
Received: by 10.107.4.18 with HTTP; Wed, 22 Feb 2017 15:58:35 -0800 (PST)
In-Reply-To: <CAN7etTwgGhbKjQw3EpWXMQN7jcQURt2tTHhWZn3FgZjij_=GDA@mail.gmail.com>
References: <d0a10fdc-f912-4e9c-8681-a54f5d53fd72@lbl.gov> <CAN7etTxxeYYxY7aB93H7E686y-8Qru-c_H3t1ANNQ_4oE1C-aA@mail.gmail.com>
 <887cfc8c-48ed-4720-8040-989e407f4203@lbl.gov> <CAN7etTwgGhbKjQw3EpWXMQN7jcQURt2tTHhWZn3FgZjij_=GDA@mail.gmail.com>
From: Chihsong <chihs...@gmail.com>
Date: Thu, 23 Feb 2017 00:58:35 +0100
Message-ID: <CABWwhHr9Z_h+nQM2SFq8Mhq=S1x+K9zCB74iPAn9tGdZKBUHNQ@mail.gmail.com>
Subject: Re: [Singularity] Performance impact: My experience
To: "singu...@lbl.gov" <singu...@lbl.gov>
Content-Type: multipart/alternative; boundary=001a113f75f457714d0549274628

--001a113f75f457714d0549274628
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Greg,

The problem is that I don't feel I have IB libraries inside my container.
How can I check that? Or did you simply install the ofed into the
container?

Chiu-Song

On Thursday, February 23, 2017, Gregory M. Kurtzer <gmku...@lbl.gov>
wrote:

> Hi Chih-Song,
>
> Haha, every now and then I get lucky when a new mail comes in and is at
> the top of my mbox and I have a moment.
>
> In summary, yes. The MPI inside the container must also link against the
> IB libraries (also within the container).
>
> Hopefully that helps!
>
> On Wed, Feb 22, 2017 at 10:58 AM, Chih-Song Kuo <chihs...@gmail.com
> <javascript:_e(%7B%7D,'cvml','chihs...@gmail.com');>> wrote:
>
>> Hi Greg,
>>
>> That reply was very prompt! Anyways, my answer follows.
>>
>> > * Make sure the MPI inside the container is properly linking against
>> the IB libraries
>> Did that mean that I need to install IB libraries (like Mellanox OFED)
>> into the container? I guess not?
>> But apparently openib is missing in the container.
>> [me@cn03 ompi-rhel6-host]$ ompi_info | grep openib
>>                  MCA btl: openib (MCA v2.1.0, API v3.0.0, Component
>> v3.0.0)
>>
>> Singularity.container-centos6-demo.img> ompi_info | grep openib
>> Singularity.container-centos6-demo.img>
>> Singularity.container-centos6-demo.img> ls /etc/infiniband/openib.conf
>> ls: cannot access /etc/infiniband/openib.conf: No such file or directory
>>
>> > * Make sure that the MPI configuration is configured to use MPI
>> I think you meant "to use IB" instead.
>> But still, did you mean that OpenMPI should be "configured"
>> "--with-verbs"? (Did you do so or you never had my problem?)
>> I did not use this flag when compiling Open MPI either on the host or in
>> the container.
>> > * Make sure you have Singularity configured properly to share the
>> devices properly, tmp, and you are *NOT* using the IPC or PID namespaces
>> Can you provide more hint on how that can be done?
>>
>> Thank you once again for your time.
>> Chih-Song
>>
>> On Wednesday, February 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer
>> wrote:
>>>
>>> There are various things that *could* go wrong, and usage of containers
>>> (any of the technologies) actually introduce complexities in kernel and
>>> user space alignment that normally we don't consider. For that reason, =
my
>>> first suspicion is that your IB fabric is not being properly utilized b=
y
>>> the MPI within the container. That could be due to anything from build
>>> errors within the container to IB library/kmod API misalignment.
>>>
>>> Things I would look at and check:
>>>
>>> * Make sure the MPI inside the container is properly linking against th=
e
>>> IB libraries
>>> * Make sure that the IB libraries inside the container are compatible
>>> with the host kernel
>>> * Make sure that the MPI configuration is configured to use MPI
>>> * Make sure you have Singularity configured properly to share the
>>> devices properly, tmp, and you are *NOT* using the IPC or PID namespace=
s
>>>
>>> Hope that helps.
>>>
>>> Greg
>>>
>>> On Wed, Feb 22, 2017 at 10:19 AM, Chih-Song Kuo <chi...@gmail.com>
>>> wrote:
>>>
>>>> Hello,
>>>>
>>>> This is again Chih-Song from Fujitsu. I decided to make another post t=
o
>>>> share my experience of performance impact with two kernel benchmarks: =
High
>>>> Performance Linpack (HPL) and the OSU MPI benchmark suit.
>>>>
>>>> Overall, there was no noticeable performance difference for benchmarks
>>>> running on a single node. But for benchmarks running across nodes, I d=
id
>>>> observe some difference, which was against to the claim of Singularity=
.
>>>> Have anybody done any similar exercise? What are your findings? Can yo=
u
>>>> suspect whether I was doing anything wrong?
>>>>
>>>> Host configuration:
>>>> 2x Intel E5-2680v2 (Ivybridge)
>>>> 64GB memory
>>>> Mellanox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch=
)
>>>> RHEL 6.7
>>>> OpenMPI development master branch (8.2.17)
>>>> Intel MKL 2017.0 community edition
>>>> gcc 4.4
>>>>
>>>> Container:
>>>> Centos 6 and 7 both tested without noticeable performance difference
>>>> OpenMPI development master branch (8.2.17)
>>>> Intel MKL 2017.0 community edition
>>>> gcc 4.4.7 (Centos-6), gcc 4.8.5 (Centos-7)
>>>>
>>>> Benchmarks:
>>>> 1. LINPACK 2.2
>>>> 2. OSU 5.3.2
>>>>
>>>> <LINPACK>
>>>> Single node. N=3D40000, P=3D5, Q=3D4
>>>> Container: 368 GFlops
>>>> Host: 368 GFLOPS
>>>> #A single node has 2x Intel E5-2680v2. So we are expecting 2 x 10 x 8 =
*
>>>> 2.8 =3D 448 GFlops
>>>> Efficiency =3D 368 / 448 =3D 82%. Not bad (given the small N (matrix s=
ize)
>>>> and that gcc instead of icc was used, and the executable was dynamical=
ly
>>>> linked -- by purpose)
>>>>
>>>> Dual-node, N=3D60000, P=3D8, Q=3D5
>>>> Container: 702 GFLOPS
>>>> Host:737 GFLOPS
>>>> There is roughly 5% of performance degradation with the container.
>>>>
>>>> <OSU-P2P-Bandwidth>
>>>> The container only saw 50-65% of the total bandwidth.
>>>>
>>>> Container:
>>>> Msg size(bytes) BW (MB/s)
>>>> 65536                2142.28
>>>> 131072               2363.45
>>>> 262144               1705.79
>>>> 524288               1592.56
>>>> 1048576              1721.88
>>>> 2097152              1557.42
>>>> 4194304              1655.90
>>>>
>>>> Host:
>>>> Msg size(bytes) BW (MB/s)
>>>> 65536                3722.32
>>>> 131072               3751.33
>>>> 262144               3771.13
>>>> 524288               3774.33
>>>> 1048576              3781.43
>>>> 2097152              3775.00
>>>> 4194304              3773.68
>>>>
>>>> <OSU-P2P-Latency>
>>>> Here the container was significantly slower.
>>>>
>>>> Container:
>>>> Msg size(bytes)  Latency (us)
>>>> 0                      31.59
>>>> 1                      31.86
>>>> 2                      31.83
>>>>
>>>> Host:
>>>> Msg size(bytes)  Latency (us)
>>>> 0                       1.55
>>>> 1                       1.63
>>>> 2                       1.63
>>>>
>>>> Note 1: Run-to-run variation of performance was much smaller than the
>>>> difference on the host and in the container.
>>>> Note 2: When Singularity was used, I could not instruct mpirun to use
>>>> the ofed by specifying "--mca btl openib,self,vader" in the mpirun
>>>> parameter list. Doing so would give me an error message stating that t=
he
>>>> openib component is missing. However, from the bandwidth measured abov=
e,
>>>> the container did seem to be able to use InfiniBand, otherwise the
>>>> bandwidth would not be so high (the nodes only had InfiniBand and 1G
>>>> Ethernet). Maybe container was using IPoIB? I did not check that yet.
>>>>
>>>> Reference: How the benchmarks were executed:
>>>> mpirun -n 20 -hostfile hostfile singularity exec
>>>> /home/chih/containers/container-centos6-demo.img xhpl
>>>> mpirun -n 2 -hostfile hostfile singularity exec
>>>> /home/chih/containers/container-centos6-demo.img osu_bw
>>>> mpirun -n 2 -hostfile hostfile singularity exec
>>>> /home/chih/containers/container-centos6-demo.img osu_latency
>>>>
>>>> Chih-Song
>>>>
>>>> --
>>>> You received this message because you are subscribed to the Google
>>>> Groups "singularity" group.
>>>> To unsubscribe from this group and stop receiving emails from it, send
>>>> an email to singu...@lbl.gov.
>>>>
>>>
>>>
>>>
>>> --
>>> Gregory M. Kurtzer
>>> HPC Systems Architect and Technology Developer
>>> Lawrence Berkeley National Laboratory HPCS
>>> University of California Berkeley Research IT
>>> Singularity Linux Containers (http://singularity.lbl.gov/)
>>> Warewulf Cluster Management (http://warewulf.lbl.gov/)
>>> GitHub: https://github.com/gmkurtzer, Twitter: https://twitt
>>> er.com/gmkurtzer
>>>
>> --
>> You received this message because you are subscribed to the Google Group=
s
>> "singularity" group.
>> To unsubscribe from this group and stop receiving emails from it, send a=
n
>> email to singu...@lbl.gov
>> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>>
>
>
>
> --
> Gregory M. Kurtzer
> HPC Systems Architect and Technology Developer
> Lawrence Berkeley National Laboratory HPCS
> University of California Berkeley Research IT
> Singularity Linux Containers (http://singularity.lbl.gov/)
> Warewulf Cluster Management (http://warewulf.lbl.gov/)
> GitHub: https://github.com/gmkurtzer, Twitter: https://
> twitter.com/gmkurtzer
>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov
> <javascript:_e(%7B%7D,'cvml','singularity%...@lbl.gov');>.
>


--=20
Chih-Song Kuo =E9=83=AD=E7=9F=A5=E9=A0=8C
Senior Sales Consultant - HPC Benchmark Specialist at Fujitsu
M.Sc. RWTH with distinction in Software Systems Engineering with HPC focus
B.Sc. NTHU in Computer Science, B.S.M. NTHU in Quantitative Finance
Tel:  +49-177-88949-28; +49-241-88949-155; +886-2-26629518

--001a113f75f457714d0549274628
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Greg,<div><br></div><div>The problem is that I=C2=A0don&#39;t feel I=C2=
=A0have IB libraries inside my container. How can I=C2=A0check that? Or did=
 you simply=C2=A0install the ofed into the container?=C2=A0</div><div><br>C=
hiu-Song</div><div><br>On Thursday, February 23, 2017, Gregory M. Kurtzer &=
lt;<a href=3D"mailto:gmku...@lbl.gov">gmku...@lbl.gov</a>&gt; wrote:<br><bl=
ockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #=
ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Chih-Song,<div><br></div><d=
iv>Haha, every now and then I get lucky when a new mail comes in and is at =
the top of my mbox and I have a moment.</div><div><br></div><div>In summary=
, yes. The MPI inside the container must also link against the IB libraries=
 (also within the container).</div><div><br></div><div>Hopefully that helps=
!</div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">On W=
ed, Feb 22, 2017 at 10:58 AM, Chih-Song Kuo <span dir=3D"ltr">&lt;<a href=
=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;chih...@gmail.com&#39;);" targ=
et=3D"_blank">chihs...@gmail.com</a>&gt;</span> wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex"><div dir=3D"ltr">Hi Greg,<br><br>That reply was very prompt! =
Anyways, my answer follows.<span><br><br>&gt; * Make sure the MPI inside th=
e container is properly linking against the IB libraries<br></span>Did that=
 mean that I need to install IB libraries (like Mellanox OFED) into the con=
tainer? I guess not?<br>But apparently openib is missing in the container.<=
br>[me@cn03 ompi-rhel6-host]$ ompi_info | grep openib<br>=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 MCA btl: openib (MCA v2.1.0, API v3.0.0, Component v3.0.0)<br><br>Singu=
larity.container-centos6-<wbr>demo.img&gt; ompi_info | grep openib<br>Singu=
larity.container-centos6-<wbr>demo.img&gt;<br>Singularity.container-centos6=
-<wbr>demo.img&gt; ls /etc/infiniband/openib.conf<br>ls: cannot access /etc=
/infiniband/openib.conf: No such file or directory<span><br><br>&gt; * Make=
 sure that the MPI configuration is configured to use MPI<br></span>I think=
 you meant &quot;to use IB&quot; instead. <br>But still, did you mean that =
OpenMPI should be &quot;configured&quot; &quot;--with-verbs&quot;? (Did you=
 do so or you never had my problem?)<br>I did not use this flag when compil=
ing Open MPI either on the host or in the container.<span><br>&gt; * Make s=
ure you have Singularity configured properly to share the=20
devices properly, tmp, and you are *NOT* using the IPC or PID namespaces<br=
></span>Can you provide more hint on how that can be done?<br><br>Thank you=
 once again for your time.<br>Chih-Song<span><br><br>On Wednesday, February=
 22, 2017 at 7:33:22 PM UTC+1, Gregory M. Kurtzer wrote:</span><blockquote =
class=3D"gmail_quote" style=3D"margin:0;margin-left:0.8ex;border-left:1px #=
ccc solid;padding-left:1ex"><span><div dir=3D"ltr">There are various things=
 that *could* go wrong, and usage of containers (any of the technologies) a=
ctually introduce complexities in kernel and user space alignment that norm=
ally we don&#39;t consider. For that reason, my first suspicion is that you=
r IB fabric is not being properly utilized by the MPI within the container.=
 That could be due to anything from build errors within the container to IB=
 library/kmod API misalignment.<div><br></div><div>Things I would look at a=
nd check:</div><div><br></div><div>* Make sure the MPI inside the container=
 is properly linking against the IB libraries</div><div>* Make sure that th=
e IB libraries inside the container are compatible with the host kernel</di=
v><div>* Make sure that the MPI configuration is configured to use MPI</div=
><div>* Make sure you have Singularity configured properly to share the dev=
ices properly, tmp, and you are *NOT* using the IPC or PID namespaces</div>=
<div><br></div><div>Hope that helps.</div><div><br></div><div>Greg</div></d=
iv></span><div><br><div class=3D"gmail_quote"><div><div>On Wed, Feb 22, 201=
7 at 10:19 AM, Chih-Song Kuo <span dir=3D"ltr">&lt;<a rel=3D"nofollow">chi.=
..@gmail.com</a>&gt;</span> wrote:<br></div></div><blockquote class=3D"gmai=
l_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left=
:1ex"><div><div><div dir=3D"ltr">Hello,<br><br>This is again Chih-Song from=
 Fujitsu. I decided to make another post to share my experience of performa=
nce impact with two kernel benchmarks: High Performance Linpack (HPL) and t=
he OSU MPI benchmark suit.<br><br>Overall, there was no noticeable performa=
nce difference for benchmarks running on a single node. But for benchmarks =
running across nodes, I did observe some difference, which was against to t=
he claim of Singularity. Have anybody done any similar exercise? What are y=
our findings? Can you suspect whether I was doing anything wrong?<br><br>Ho=
st configuration:<br>2x Intel E5-2680v2 (Ivybridge)<br>64GB memory<br>Mella=
nox ConnectX-3 FDR adapter (but connects to a Mellanox QDR switch)<br>RHEL =
6.7<br>OpenMPI development master branch (8.2.17)<br>Intel MKL 2017.0 commu=
nity edition<br>gcc 4.4<br><br>Container:<br>Centos 6 and 7 both tested wit=
hout noticeable performance difference<br>OpenMPI development master branch=
 (8.2.17)<br>Intel MKL 2017.0 community edition<br>gcc 4.4.7 (Centos-6), gc=
c 4.8.5 (Centos-7)<br><br>Benchmarks:<br>1. LINPACK 2.2<br>2. OSU 5.3.2<br>=
<br>&lt;LINPACK&gt;<br>Single node. N=3D40000, P=3D5, Q=3D4<br>Container: 3=
68 GFlops<br>Host: 368 GFLOPS<br>#A single node has 2x Intel E5-2680v2. So =
we are expecting 2 x 10 x 8 * 2.8 =3D 448 GFlops<br>Efficiency =3D 368 / 44=
8 =3D 82%. Not bad (given the small N (matrix size) and that gcc instead of=
 icc was used, and the executable was dynamically linked -- by purpose)<br>=
<br>Dual-node, N=3D60000, P=3D8, Q=3D5<br>Container: 702 GFLOPS<br>Host:737=
 GFLOPS<br>There is roughly 5% of performance degradation with the containe=
r.<br><br>&lt;OSU-P2P-Bandwidth&gt;<br>The container only saw 50-65% of the=
 total bandwidth.<br><br>Container: <br>Msg size(bytes) BW (MB/s)<br>65536=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0 2142.28<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 2363.45<br>262144=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1705.=
79<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 1592.56<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1721.88<br>2097152=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 1557.=
42<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1655.90<br><br>Host:<br>Msg size(bytes) BW (MB/s)<br>655=
36=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 3722.32<br>131072=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3751.33<br>262144=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
71.13<br>524288=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0 3774.33<br>1048576=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 3781.43<br>2097152=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 37=
75.00<br>4194304=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0 3773.68<br><br>&lt;OSU-P2P-Latency&gt;<br>Here the co=
ntainer was significantly slower.<br><br>Container: <br>Msg size(bytes)=C2=
=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31=
.59<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.86<br>2=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0 31.83<br><br>Host:<br>Msg =
size(bytes)=C2=A0 Latency (us)<br>0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0=C2=A0 1.55<br>1=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0=C2=A0 1.63<br>2=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=
=C2=A0 1.63<br><br>Note 1: Run-to-run variation of performance was much sma=
ller than the difference on the host and in the container.<br>Note 2: When =
Singularity was used, I could not instruct mpirun to use the ofed by specif=
ying &quot;--mca btl openib,self,vader&quot; in the mpirun parameter list. =
Doing so would give me an error message stating that the openib component i=
s missing. However, from the bandwidth measured above, the container did se=
em to be able to use InfiniBand, otherwise the bandwidth would not be so hi=
gh (the nodes only had InfiniBand and 1G Ethernet). Maybe container was usi=
ng IPoIB? I did not check that yet. <br><br>Reference: How the benchmarks w=
ere executed:<br>mpirun -n 20 -hostfile hostfile singularity exec /home/chi=
h/containers/containe<wbr>r-centos6-demo.img xhpl<br>mpirun -n 2 -hostfile =
hostfile singularity exec /home/chih/containers/containe<wbr>r-centos6-demo=
.img osu_bw<br>mpirun -n 2 -hostfile hostfile singularity exec /home/chih/c=
ontainers/containe<wbr>r-centos6-demo.img osu_latency<span><font color=3D"#=
888888"><br><br>Chih-Song<br><br></font></span></div></div></div><span><fon=
t color=3D"#888888"><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br></div></div>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a rel=3D"nofollow">singu...@lbl.gov</a>.<br>
</font></span></blockquote></div><span><br><br clear=3D"all"><div><br></div=
>-- <br><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><=
div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=3D"ltr"><div>Gregory M. Kurt=
zer</div><div>HPC Systems Architect and Technology Developer</div><div>Lawr=
ence Berkeley National Laboratory HPCS<br>University of California Berkeley=
 Research IT<br>Singularity Linux Containers=C2=A0(<a href=3D"http://singul=
arity.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://singularity<wbr>.=
lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a href=3D"http:/=
/warewulf.lbl.gov/" rel=3D"nofollow" target=3D"_blank">http://warewulf.lb<w=
br>l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtz=
er" rel=3D"nofollow" target=3D"_blank">https://github.com/gmk<wbr>urtzer</a=
>,=C2=A0<span style=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"ht=
tps://twitter.com/gmkurtzer" style=3D"font-size:12.8px" rel=3D"nofollow" ta=
rget=3D"_blank">https://twitt<wbr>er.com/gmkurtzer</a></div></div></div></d=
iv></div></div></div></div></div></div></div>
</span></div>
</blockquote></div><div><div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singularity+unsubscribe@lbl.go<wbr>v</a>.=
<br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
<div data-smartmail=3D"gmail_signature"><div dir=3D"ltr"><div><div dir=3D"l=
tr"><div><div dir=3D"ltr"><div><div dir=3D"ltr"><div dir=3D"ltr"><div dir=
=3D"ltr"><div>Gregory M. Kurtzer</div><div>HPC Systems Architect and Techno=
logy Developer</div><div>Lawrence Berkeley National Laboratory HPCS<br>Univ=
ersity of California Berkeley Research IT<br>Singularity Linux Containers=
=C2=A0(<a href=3D"http://singularity.lbl.gov/" target=3D"_blank">http://<wb=
r>singularity.lbl.gov/</a>)</div><div>Warewulf Cluster Management=C2=A0(<a =
href=3D"http://warewulf.lbl.gov/" target=3D"_blank">http://warewulf.<wbr>lb=
l.gov/</a>)</div><div>GitHub:=C2=A0<a href=3D"https://github.com/gmkurtzer"=
 target=3D"_blank">https://github.com/<wbr>gmkurtzer</a>,=C2=A0<span style=
=3D"font-size:12.8px">Twitter:=C2=A0</span><a href=3D"https://twitter.com/g=
mkurtzer" style=3D"font-size:12.8px" target=3D"_blank">https://<wbr>twitter=
.com/gmkurtzer</a></div></div></div></div></div></div></div></div></div></d=
iv></div>
</div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"javascript:_e(%7B%7D,&#39;cvml&#39;,&#39;singularity...@=
lbl.gov&#39;);" target=3D"_blank">singularity+unsubscribe@lbl.<wbr>gov</a>.=
<br>
</blockquote></div><br><br>-- <br><div dir=3D"ltr"><span><div><div dir=3D"l=
tr"><div>Chih-Song Kuo =E9=83=AD=E7=9F=A5=E9=A0=8C<br>Senior Sales Consulta=
nt - HPC Benchmark Specialist at Fujitsu<br>M.Sc. RWTH with distinction in =
Software Systems Engineering with HPC focus<br>B.Sc. NTHU in Computer Scien=
ce, B.S.M. NTHU in Quantitative Finance<br>Tel: =C2=A0+49-177-88949-28; +49=
-241-88949-155; +886-2-26629518</div></div></div></span></div><br>

--001a113f75f457714d0549274628--
