X-Received: by 2002:a65:5187:: with SMTP id h7-v6mr291948pgq.57.1527058556416;
        Tue, 22 May 2018 23:55:56 -0700 (PDT)
X-BeenThere: singularity@lbl.gov
Received: by 2002:a17:902:344:: with SMTP id 62-v6ls7164295pld.4.gmail; Tue,
 22 May 2018 23:55:55 -0700 (PDT)
X-Received: by 2002:a17:902:8494:: with SMTP id c20-v6mr1750637plo.66.1527058555172;
        Tue, 22 May 2018 23:55:55 -0700 (PDT)
ARC-Seal: i=1; a=rsa-sha256; t=1527058555; cv=none;
        d=google.com; s=arc-20160816;
        b=i7nCADNsUhX6s5j6MEzVKSIlWdNmc4LAxqkCznfhohcfhg2OQlkisHWthFG9Pu6Pl9
         2J6j//tGjJdkQu52DLSFVJnr3f11WXkxiBndWycOV8UeLdkaX4SCpi/i+i96aKGc00Gf
         7MU3EtcOZ8f6iswIl7anTpaTfWUoH8U164kOl5hkXbKtOSqFeXNnIQ/kArgBoJXofj9q
         dsnAnll2Bi1SdC5eB5vzjZ6iGK/bnOcGqcdWH7zvO+lU+IF7bKF6k9tSoNQXTaYasWao
         AUQSOKk15+83lApAh5DX6BGCdfAby8D7eFdXyV8ovydv1w/Gfhw3oXWyAmQ0kTutI+or
         gCGA==
ARC-Message-Signature: i=1; a=rsa-sha256; c=relaxed/relaxed; d=google.com; s=arc-20160816;
        h=to:subject:message-id:date:from:references:in-reply-to:mime-version
         :dkim-signature:arc-authentication-results;
        bh=QytmFaxL2lQeGWSFg1wIJbraYz3CLLiDeTIpe3ohg4g=;
        b=v9JuthILJpjiNOYd1W7ilAnzZDybFWyazWzhkI1c2ce0aI/E4EsoyWRuPaCQOpuIOP
         zB8JpcQV2DdlN0RMGuIREThqMYVBDOBUu5eEzKARjDc/kSnmqeEZADuG/qoiGL51qJ9c
         3/yQkt566DC1TFKGFJtJ7x6d+wlTy+5RTGjJoBLcj0ysIqJ2MDtyQr0GsTRwzHuKeUCv
         N+xHX2CqNgQAMb8Cc0yF2/VK3am59b9PvKx/KffKN+4UlxXVY+RTnpzFCoFmBBMKNXZO
         2nxxUOroahZ2Oi2BeaQ2woNTR/Suy5R5roGLGpvxmrf1BZPlC/9V6yJXjxA734pQ2IWy
         OhKg==
ARC-Authentication-Results: i=1; mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=OG/RWblb;
       spf=pass (google.com: domain of vict...@gmail.com designates 74.125.82.45 as permitted sender) smtp.mailfrom=vict...@gmail.com
Return-Path: <vict...@gmail.com>
Received: from fe3.lbl.gov (fe3.lbl.gov. [128.3.41.68])
        by mx.google.com with ESMTP id c2-v6si18160469plb.77.2018.05.22.23.55.54
        for <singu...@lbl.gov>;
        Tue, 22 May 2018 23:55:55 -0700 (PDT)
Received-SPF: pass (google.com: domain of vict...@gmail.com designates 74.125.82.45 as permitted sender) client-ip=74.125.82.45;
Authentication-Results: mx.google.com;
       dkim=pass head...@gmail.com header.s=20161025 header.b=OG/RWblb;
       spf=pass (google.com: domain of vict...@gmail.com designates 74.125.82.45 as permitted sender) smtp.mailfrom=vict...@gmail.com
X-Ironport-SBRS: 2.8
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: =?us-ascii?q?A2G6AQAXDwVbgC1SfUpcHAEBAQQBAQoBA?=
 =?us-ascii?q?YMZgQ19KINxBoEdglCEF4x2gXmBD5M3FIEOAxgXIQMjAQwJgUmCL0YCgiMhNBg?=
 =?us-ascii?q?BAgEBAQEBAQIBAQIQAQEJDQkIJiUMQg4BgWQFAgMCHgUESykCATABAQEBAQEBA?=
 =?us-ascii?q?QEBAQEBAQEBAQEBAQEUAgwBIBcSAhgBAQEDARoBCB0BDQ4eAwELBgMCCwYDAQE?=
 =?us-ascii?q?BARULAQkCAiEBAQ4DAQUBCwkIDgcEARoCBIMDKIE+AQMNCAUKjkyQAjyLBYF/B?=
 =?us-ascii?q?QEXgnAFg04KGSYNVFeBdAIGEogjgVQ/gQ+CXy6CTysXAQECgRkKCAESASsUDRK?=
 =?us-ascii?q?CQYJUAodmiTWGdhcsCYVqhW+Cf4E3g2+HWYlkSoQJgjgwgQQcbC5xcBU7MYICA?=
 =?us-ascii?q?QEBDQmBZySDaIF/iFU9MAEPiCWCbUeBcAEB?=
X-IronPort-AV: E=Sophos;i="5.49,432,1520924400"; 
   d="scan'208,217";a="116520663"
Received: from mail-wm0-f45.google.com ([74.125.82.45])
  by fe3.lbl.gov with ESMTP; 22 May 2018 23:55:51 -0700
Received: by mail-wm0-f45.google.com with SMTP id t11-v6so5930629wmt.0
        for <singu...@lbl.gov>; Tue, 22 May 2018 23:55:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20161025;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to;
        bh=QytmFaxL2lQeGWSFg1wIJbraYz3CLLiDeTIpe3ohg4g=;
        b=OG/RWblb062EET4OZidUM3CH2QCUUMI7RT7thWoqhqmqUouLyQ1/Bom38YRZl64xkB
         mDfFyDgFIBpcFbtyxp4mk853Sv0W3RFGBlgALMg42tDTaIgGn3RDhezBlNUxTt3Zj8eq
         UdJO6dbSfB0F2JCG0r9r3bfN3QYFzMYnjWiYXBnHtLCb/fyVU6nDj2X+/laXoLVyuvas
         Vyf00bWVhFGu5eY1nO39Xf6xsOylTIBZiE1EjELGFPTLwzXgc3r05K8AADIDxLjTyMs8
         dU+EeDEf5kH0YZ7lUIjDF4+9etSlEKXSU2SKYgh2cSjuwjhf1ncedzxG38P76huJIFqi
         vOiw==
X-Gm-Message-State: ALKqPweRdodAomedO5eWKjdCMzmQCrufg8XmLyymAR4KIO4K9TwDNrO0
	zzWzR7B9nnox9ePOR7uHAS0230nKFwIid1ynGZg=
X-Received: by 2002:a2e:2a45:: with SMTP id q66-v6mr909956ljq.40.1527058550503;
 Tue, 22 May 2018 23:55:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 2002:a19:8fd9:0:0:0:0:0 with HTTP; Tue, 22 May 2018 23:55:49
 -0700 (PDT)
In-Reply-To: <CAB6eJZLupe=P+5awRJjV0=V4uNWuPcwTevLELZpMsRR5uJVStA@mail.gmail.com>
References: <167671ff-657d-4560-a35f-87091223fe29@lbl.gov> <B58197C146EC324AA00A2A07DC082602C2CBA0C3@XMAIL-MBX-BT1.AD.UCSD.EDU>
 <CAB6eJZ+6PDjs3POSQPNoLdaMys5d7iYDYcUeQFpMyzJ6DEP31w@mail.gmail.com>
 <CAGfAqt_SyMw8CqJxb8DjbnfTfsAj__eXOrJvKPCzJzsEuQcnvg@mail.gmail.com>
 <CA+Wz_FzkYf0HX_yND-TKQBZiog7UZ-Uh_NMpsQfvbpcyebtgLA@mail.gmail.com>
 <CAB6eJZL+F7DwMcebA6DC+QWHqqsnnqEfJ2uqR2D-6bas6DmnYw@mail.gmail.com>
 <CA+Wz_FznUJ9kWSy7qneZsVEYqemqnm6eq0ASo+AoD49VNqU=zw@mail.gmail.com> <CAB6eJZLupe=P+5awRJjV0=V4uNWuPcwTevLELZpMsRR5uJVStA@mail.gmail.com>
From: victor sv <vict...@gmail.com>
Date: Wed, 23 May 2018 08:55:49 +0200
Message-ID: <CA+Wz_FyZdYe65GfUp2qApunch=dvV9HzAohStJPqAi4+1j2nUQ@mail.gmail.com>
Subject: Re: [Singularity] Running an mpi program with mvapitch
To: singularity@lbl.gov
Content-Type: multipart/alternative; boundary="000000000000743bcf056cda063d"

--000000000000743bcf056cda063d
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

Hi George,

it's strange. The file that is not finding is "srun" not "mpi-pi.o". Is
this something related with Slurm workload manager? is srun installed
inside the container?

Best,
V=C3=ADctor

2018-05-21 17:35 GMT+02:00 George Zaki <georg...@gmail.com>:

> Thanks Victor,
>
> Here is the results with the path of the executable:
>
> Singularity swift-hypervisor-horovod-mvapich.simg:~/mpi-examples/mpi-exam=
ple>
> mpiexec -n 1 ./mpi-pi.o
>
> [mpiexec@cn3112] HYDU_create_process (utils/launch/launch.c:75): execvp
> error on file srun (No such file or directory)
>
>
>
> On Mon, May 21, 2018 at 9:18 AM victor sv <vict...@gmail.com> wrote:
>
>> Hi George,
>>
>> please check that you are calling the right program. Is the executable
>> path in the PATH environment variable? If not you have the prepend the p=
ath
>> to call the executable.
>>
>> Take a look to this:
>>
>> https://stackoverflow.com/questions/47472153/mpi-mpirun-
>> execvp-error-no-such-file-or-directory
>>
>> BR,
>> V=C3=ADctor.
>>
>> 2018-05-21 14:58 GMT+02:00 George Zaki <georg...@gmail.com>:
>>
>>> Hi Jason,
>>>
>>> I was not able to run the program even without singularity when I use
>>> mvapitch. I am in contact with our system admin
>>>
>>> Here is the  output I got when I run mpiexec within singularity:
>>>
>>> mpiexec -n 1 mpi-pi.o
>>>
>>> [mpiexec@cn3137] HYDU_create_process (utils/launch/launch.c:75): execvp
>>> error on file srun (No such file or directory)
>>>
>>> Best regards,
>>>
>>> George.
>>>
>>> On Mon, May 21, 2018 at 3:04 AM victor sv <vict...@gmail.com> wrote:
>>>
>>>> Hi George,
>>>>
>>>> not any experience with mvapitch. I think the compiler version has no
>>>> effect here.
>>>>
>>>> Jason solution and is a good starting point to check if the container
>>>> MPI works in a single node (not in several nodes).
>>>>
>>>> To run the hybrid MPI approach you should take into account that both
>>>> version and vendor of MPI and PMI must match. Can you check if PMI
>>>> libraries match?
>>>>
>>>> BR,
>>>> V=C3=ADctor.
>>>>
>>>> 2018-05-18 20:14 GMT+02:00 Jason Stover <jason...@gmail.com>:
>>>>
>>>>> Hi George,
>>>>>
>>>>>   Can you run it from inside the container? For example:
>>>>>
>>>>>     singularity exec
>>>>> /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpiexec -n =
1
>>>>> mpi-pi.o
>>>>>
>>>>> -J
>>>>>
>>>>>
>>>>> On Fri, May 18, 2018 at 12:56 PM, George Zaki <georg...@gmail.com>
>>>>> wrote:
>>>>> > Thanks Marty
>>>>> >
>>>>> > Below are the values I got, any obvious mismatch? I git this workin=
g
>>>>> fine
>>>>> > with OpenMPI.
>>>>> >
>>>>> > Here is also what I try to run:
>>>>> >
>>>>> > singularity exec /data/zakigf/candle/swift-
>>>>> hypervisor-horovod-mvapich.simg
>>>>> > mpicc mpi-pi.c -o  mpi-pi.o
>>>>> >
>>>>> > [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec
>>>>> > /data/zakigf/candle/swift-hypervisor-horovod-mvapich.simg mpi-pi.o
>>>>> >
>>>>> > Then I kill after no response:
>>>>> >
>>>>> > ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested
>>>>> >
>>>>> > [mpiexec@cn2360] Press Ctrl-C again to force abort
>>>>> >
>>>>> > [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write
>>>>> error (Bad
>>>>> > file descriptor)
>>>>> >
>>>>> > [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal
>>>>> (pm/pmiserv/pmiserv_cb.c:169):
>>>>> > unable to write data to proxy
>>>>> >
>>>>> > [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79): unable
>>>>> to send
>>>>> > signal downstream
>>>>> >
>>>>> > [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event
>>>>> > (tools/demux/demux_poll.c:76): callback returned error status
>>>>> >
>>>>> > [mpiexec@cn2360] HYD_pmci_wait_for_completion
>>>>> > (pm/pmiserv/pmiserv_pmci.c:198): error waiting for event
>>>>> >
>>>>> > [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager
>>>>> error
>>>>> > waiting for completion
>>>>> >
>>>>> >
>>>>> > Now about the versions:
>>>>> >
>>>>> > Singularity: Invoking an interactive shell within container...
>>>>> >
>>>>> >
>>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> mpiexec
>>>>> --version
>>>>> >
>>>>> > HYDRA build details:
>>>>> >
>>>>> >     Version:                                 3.1.4
>>>>> >
>>>>> >     Release Date:                            Wed Sep  7 14:33:43 ED=
T
>>>>> 2016
>>>>> >
>>>>> >     CC:                              gcc
>>>>> >
>>>>> >     CXX:                             g++
>>>>> >
>>>>> >     F77:                             gfortran
>>>>> >
>>>>> >     F90:                             gfortran
>>>>> >
>>>>> >     Configure options:
>>>>>  '--disable-option-checking'
>>>>> > '--prefix=3DNONE' '--cache-file=3D/dev/null' '--srcdir=3D.' 'CC=3Dg=
cc'
>>>>> 'CFLAGS=3D
>>>>> > -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/lib
>>>>> -Wl,-rpath,/lib
>>>>> > -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-libmad -libumad
>>>>> -libverbs -ldl
>>>>> > -lrt -lm -lpthread ' 'CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/inclu=
de
>>>>> > -I/tmp/mvapich2-2.2/src/mpl/include -I/tmp/mvapich2-2.2/src/
>>>>> openpa/src
>>>>> > -I/tmp/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>>>> > -I/tmp/mvapich2-2.2/src/mpi/romio/include -I/include -I/include
>>>>> -I/include
>>>>> > -I/include'
>>>>> >
>>>>> >     Process Manager:                         pmi
>>>>> >
>>>>> >     Launchers available:                     ssh rsh fork slurm ll
>>>>> lsf sge
>>>>> > manual persist
>>>>> >
>>>>> >     Topology libraries available:            hwloc
>>>>> >
>>>>> >     Resource management kernels available:   user slurm ll lsf sge
>>>>> pbs
>>>>> > cobalt
>>>>> >
>>>>> >     Checkpointing libraries available:
>>>>> >
>>>>> >     Demux engines available:                 poll select
>>>>> >
>>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> gcc --version
>>>>> >
>>>>> > gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>>>> >
>>>>> > Copyright (C) 2015 Free Software Foundation, Inc.
>>>>> >
>>>>> > This is free software; see the source for copying conditions.  Ther=
e
>>>>> is NO
>>>>> >
>>>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>>> PURPOSE.
>>>>> >
>>>>> >
>>>>> > Singularity swift-hypervisor-horovod-mvapich.simg:~> exit
>>>>> >
>>>>> > exit
>>>>> >
>>>>> >
>>>>> > [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0
>>>>> >
>>>>> > [+] Loading mvapich2 2.2 for GCC 5.3.0
>>>>> >
>>>>> > [zakigf@cn2360 ~]$ gcc --version
>>>>> >
>>>>> > gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)
>>>>> >
>>>>> > Copyright (C) 2010 Free Software Foundation, Inc.
>>>>> >
>>>>> > This is free software; see the source for copying conditions.  Ther=
e
>>>>> is NO
>>>>> >
>>>>> > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>>> PURPOSE.
>>>>> >
>>>>> >
>>>>> > [zakigf@cn2360 ~]$ mpiexec --version
>>>>> >
>>>>> > HYDRA build details:
>>>>> >
>>>>> >     Version:                                 3.1.4
>>>>> >
>>>>> >     Release Date:                            Wed Sep  7 14:33:43 ED=
T
>>>>> 2016
>>>>> >
>>>>> >     CC:                              /usr/local/GCC/5.3.0/bin/gcc
>>>>> >
>>>>> >     CXX:                             /usr/local/GCC/5.3.0/bin/g++
>>>>> >
>>>>> >     F77:                             /usr/local/GCC/5.3.0/bin/
>>>>> gfortran
>>>>> >
>>>>> >     F90:                             /usr/local/GCC/5.3.0/bin/
>>>>> gfortran
>>>>> >
>>>>> >     Configure options:
>>>>>  '--disable-option-checking'
>>>>> > '--prefix=3D/usr/local/MVAPICH2/2.2/gcc-5.3.0'
>>>>> > '--with-slurm-lib=3D/usr/local/slurm/lib'
>>>>> > '--with-slurm-include=3D/usr/local/slurm/include'
>>>>> '--enable-debug=3Dnone'
>>>>> > '--enable-timing=3Druntime' 'CC=3D/usr/local/GCC/5.3.0/bin/gcc'
>>>>> > 'CXX=3D/usr/local/GCC/5.3.0/bin/g++' 'FC=3D/usr/local/GCC/5.3.0/bin=
/
>>>>> gfortran'
>>>>> > 'F77=3D/usr/local/GCC/5.3.0/bin/gfortran' '--cache-file=3D/dev/null=
'
>>>>> > '--srcdir=3D.' 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/l=
ib
>>>>> -L/lib
>>>>> > -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib'
>>>>> 'LIBS=3D-libmad
>>>>> > -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpl/include
>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src
>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/openpa/src -D_REENTRANT
>>>>> > -I/usr/local/src/mvapich2/mvapich2-2.2/src/mpi/romio/include
>>>>> -I/include
>>>>> > -I/include -I/include -I/include'
>>>>> >
>>>>> >     Process Manager:                         pmi
>>>>> >
>>>>> >     Launchers available:                     ssh rsh fork slurm ll
>>>>> lsf sge
>>>>> > manual persist
>>>>> >
>>>>> >     Topology libraries available:            hwloc
>>>>> >
>>>>> >     Resource management kernels available:   user slurm ll lsf sge
>>>>> pbs
>>>>> > cobalt
>>>>> >
>>>>> >     Checkpointing libraries available:
>>>>> >
>>>>> >     Demux engines available:                 poll select
>>>>> >
>>>>> >
>>>>> > On Fri, May 18, 2018 at 1:31 PM Kandes, Martin <mka...@sdsc.edu>
>>>>> wrote:
>>>>> >>
>>>>> >> Hi George,
>>>>> >>
>>>>> >> I run with different gcc compiler versions inside and outside my M=
PI
>>>>> >> containers. So I would be surprised if that is the issue here. I'm
>>>>> not sure
>>>>> >> I have a good recommendation of where to start debugging your
>>>>> problem. But I
>>>>> >> might start by double checking the MPI versions match inside and
>>>>> outside the
>>>>> >> container. e.g. see [1].
>>>>> >>
>>>>> >> Marty
>>>>> >>
>>>>> >> [1]
>>>>> >>
>>>>> >> [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1
>>>>> >> --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash
>>>>> >> srun: job 16364303 queued and waiting for resources
>>>>> >> srun: job 16364303 has been allocated resources
>>>>> >> [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/
>>>>> >> [mkandes@comet-14-06 16364303]$ cp
>>>>> >> /oasis/scratch/comet/mkandes/temp_project/singularity/
>>>>> images/ubuntu-mvapich2.img
>>>>> >> ./
>>>>> >> [mkandes@comet-14-06 16364303]$ module purge
>>>>> >> [mkandes@comet-14-06 16364303]$ module load gnu
>>>>> >> [mkandes@comet-14-06 16364303]$ module load mvapich2_ib
>>>>> >> [mkandes@comet-14-06 16364303]$ module list
>>>>> >> Currently Loaded Modulefiles:
>>>>> >>   1) gnu/4.9.2         2) mvapich2_ib/2.1
>>>>> >> [mkandes@comet-14-06 16364303]$ module load singularity
>>>>> >> [mkandes@comet-14-06 16364303]$ gcc --version
>>>>> >> gcc (GCC) 4.9.2
>>>>> >> Copyright (C) 2014 Free Software Foundation, Inc.
>>>>> >> This is free software; see the source for copying conditions.
>>>>> There is NO
>>>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>>> >> PURPOSE.
>>>>> >>
>>>>> >> [mkandes@comet-14-06 16364303]$ mpirun --version
>>>>> >> HYDRA build details:
>>>>> >>     Version:                                 3.1.4
>>>>> >>     Release Date:                            Thu Apr  2 17:15:15
>>>>> EDT 2015
>>>>> >>     CC:                              gcc  -fPIC -O3
>>>>> >>     CXX:                             g++  -fPIC -O3
>>>>> >>     F77:                             gfortran -fPIC -O3
>>>>> >>     F90:                             gfortran -fPIC -O3
>>>>> >>     Configure options:
>>>>>  '--disable-option-checking'
>>>>> >> '--prefix=3D/opt/mvapich2/gnu/ib' '--enable-shared'
>>>>> '--enable-sharedlibs=3Dgcc'
>>>>> >> '--with-hwloc' '--enable-f77' '--enable-fc' '--enable-hybrid'
>>>>> >> '--with-ib-include=3D/usr/include/infiniband'
>>>>> '--with-ib-libpath=3D/usr/lib64'
>>>>> >> '--enable-fast=3DO3'
>>>>> >> '--with-limic2=3D/state/partition1/git/mpi-roll/BUILD/
>>>>> sdsc-mvapich2_gnu_ib-2.1/../..//cache/build-limic'
>>>>> >> '--with-slurm=3D/usr/lib64/slurm' '--with-file-system=3Dlustre'
>>>>> 'CC=3Dgcc'
>>>>> >> 'CFLAGS=3D-fPIC -O3 -O3' 'CXX=3Dg++' 'CXXFLAGS=3D-fPIC -O3 -O3'
>>>>> 'FC=3Dgfortran'
>>>>> >> 'FCFLAGS=3D-fPIC -O3 -O3' 'F77=3Dgfortran' 'FFLAGS=3D-L/usr/lib64 =
-L/lib
>>>>> -L/lib
>>>>> >> -fPIC -O3 -O3' '--cache-file=3D/dev/null' '--srcdir=3D.'
>>>>> 'LDFLAGS=3D-L/usr/lib64
>>>>> >> -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib
>>>>> -L/usr/lib64
>>>>> >> -L/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/../..//cache/build-limic/lib
>>>>> >> -L/lib -L/lib' 'LIBS=3D-libmad -lrdmacm -libumad -libverbs -ldl -l=
rt
>>>>> -llimic2
>>>>> >> -lm -lpthread ' 'CPPFLAGS=3D-I/usr/include/infiniband
>>>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/mvapich2-2.1/src/mpl/include
>>>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/mvapich2-2.1/src/mpl/include
>>>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/mvapich2-2.1/src/openpa/src
>>>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/mvapich2-2.1/src/openpa/src
>>>>> >> -D_REENTRANT
>>>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/mvapich2-2.1/src/mpi/romio/include
>>>>> >> -I/include -I/include -I/usr/include/infiniband
>>>>> >> -I/state/partition1/git/mpi-roll/BUILD/sdsc-mvapich2_gnu_
>>>>> ib-2.1/../..//cache/build-limic/include
>>>>> >> -I/include -I/include'
>>>>> >>     Process Manager:                         pmi
>>>>> >>     Launchers available:                     ssh rsh fork slurm ll
>>>>> lsf sge
>>>>> >> manual persist
>>>>> >>     Topology libraries available:            hwloc
>>>>> >>     Resource management kernels available:   user slurm ll lsf sge
>>>>> pbs
>>>>> >> cobalt
>>>>> >>     Checkpointing libraries available:
>>>>> >>     Demux engines available:                 poll select
>>>>> >> [mkandes@comet-14-06 16364303]$ singularity shell
>>>>> ubuntu-mvapich2.img
>>>>> >> Singularity: Invoking an interactive shell within container...
>>>>> >>
>>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> gcc
>>>>> --version
>>>>> >> gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
>>>>> >> Copyright (C) 2015 Free Software Foundation, Inc.
>>>>> >> This is free software; see the source for copying conditions.
>>>>> There is NO
>>>>> >> warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR
>>>>> >> PURPOSE.
>>>>> >>
>>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303> mpirun
>>>>> >> --version
>>>>> >> HYDRA build details:
>>>>> >>     Version:                                 3.1.4
>>>>> >>     Release Date:                            Thu Apr  2 17:15:15
>>>>> EDT 2015
>>>>> >>     CC:                              gcc
>>>>> >>     CXX:                             g++
>>>>> >>     F77:                             gfortran
>>>>> >>     F90:                             gfortran
>>>>> >>     Configure options:
>>>>>  '--disable-option-checking'
>>>>> >> '--prefix=3D/opt/mvapich2' '--cache-file=3D/dev/null' '--srcdir=3D=
.'
>>>>> 'CC=3Dgcc'
>>>>> >> 'CFLAGS=3D -DNDEBUG -DNVALGRIND -O2' 'LDFLAGS=3D-L/lib -L/lib -L/l=
ib
>>>>> >> -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib' 'LIBS=3D-lib=
mad
>>>>> -lrdmacm
>>>>> >> -libumad -libverbs -ldl -lrt -lm -lpthread ' 'CPPFLAGS=3D
>>>>> >> -I/tmp/mvapich2-2.1/src/mpl/include -I/tmp/mvapich2-2.1/src/mpl/
>>>>> include
>>>>> >> -I/tmp/mvapich2-2.1/src/openpa/src -I/tmp/mvapich2-2.1/src/
>>>>> openpa/src
>>>>> >> -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/romio/include -I/include
>>>>> -I/include
>>>>> >> -I/include -I/include'
>>>>> >>     Process Manager:                         pmi
>>>>> >>     Launchers available:                     ssh rsh fork slurm ll
>>>>> lsf sge
>>>>> >> manual persist
>>>>> >>     Topology libraries available:            hwloc
>>>>> >>     Resource management kernels available:   user slurm ll lsf sge
>>>>> pbs
>>>>> >> cobalt
>>>>> >>     Checkpointing libraries available:
>>>>> >>     Demux engines available:                 poll select
>>>>> >> Singularity ubuntu-mvapich2.img:/scratch/mkandes/16364303>
>>>>> >>
>>>>> >>
>>>>> >> ________________________________
>>>>> >> From: George Zaki [georg...@gmail.com]
>>>>> >> Sent: Friday, May 18, 2018 6:48 AM
>>>>> >> To: singularity
>>>>> >> Subject: [Singularity] Running an mpi program with mvapitch
>>>>> >>
>>>>> >> Hi singularity team,
>>>>> >>
>>>>> >>
>>>>> >> I would like to run an MPI program in a singularity container. The
>>>>> program
>>>>> >> is compiled using mvapicth2.2 using a gcc version 5.4.
>>>>> >>
>>>>> >>
>>>>> >> I can see that my cluster has a compiled version of mvapitch2.2
>>>>> with gcc
>>>>> >> 5.3
>>>>> >>
>>>>> >>
>>>>> >> When I run:
>>>>> >>
>>>>> >>
>>>>> >> mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o
>>>>> >>
>>>>> >> the call does not return.
>>>>> >>
>>>>> >>
>>>>> >>
>>>>> >> Does the gcc version has to be exactly the same? I tried the switc=
h
>>>>> the
>>>>> >> compiler in this image:
>>>>> >>
>>>>> >>
>>>>> >> BootStrap: docker
>>>>> >> From: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04
>>>>> >>
>>>>> >>
>>>>> >> However when gcc 5.3 is used the mvapitch does not build correctly=
.
>>>>> >>
>>>>> >>
>>>>> >> If that's the problem, Is there a preferred method of switching gc=
c
>>>>> >> version in this container singularity container?
>>>>> >>
>>>>> >>
>>>>> >> Thanks,
>>>>> >> George
>>>>> >>
>>>>> >> --
>>>>> >> You received this message because you are subscribed to the Google
>>>>> Groups
>>>>> >> "singularity" group.
>>>>> >> To unsubscribe from this group and stop receiving emails from it,
>>>>> send an
>>>>> >> email to singu...@lbl.gov.
>>>>> >>
>>>>> >> --
>>>>> >> You received this message because you are subscribed to a topic in
>>>>> the
>>>>> >> Google Groups "singularity" group.
>>>>> >> To unsubscribe from this topic, visit
>>>>> >> https://groups.google.com/a/lbl.gov/d/topic/singularity/
>>>>> A6I5mZxnmFU/unsubscribe.
>>>>> >> To unsubscribe from this group and all its topics, send an email t=
o
>>>>> >> singu...@lbl.gov.
>>>>> >
>>>>> > --
>>>>> > You received this message because you are subscribed to the Google
>>>>> Groups
>>>>> > "singularity" group.
>>>>> > To unsubscribe from this group and stop receiving emails from it,
>>>>> send an
>>>>> > email to singu...@lbl.gov.
>>>>>
>>>>> --
>>>>> You received this message because you are subscribed to the Google
>>>>> Groups "singularity" group.
>>>>> To unsubscribe from this group and stop receiving emails from it, sen=
d
>>>>> an email to singu...@lbl.gov.
>>>>>
>>>>
>>>> --
>>>> You received this message because you are subscribed to a topic in the
>>>> Google Groups "singularity" group.
>>>> To unsubscribe from this topic, visit https://groups.google.com/a/
>>>> lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe.
>>>> To unsubscribe from this group and all its topics, send an email to
>>>> singu...@lbl.gov.
>>>>
>>> --
>>> You received this message because you are subscribed to the Google
>>> Groups "singularity" group.
>>> To unsubscribe from this group and stop receiving emails from it, send
>>> an email to singu...@lbl.gov.
>>>
>>
>> --
>> You received this message because you are subscribed to a topic in the
>> Google Groups "singularity" group.
>> To unsubscribe from this topic, visit https://groups.google.com/a/
>> lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe.
>> To unsubscribe from this group and all its topics, send an email to
>> singu...@lbl.gov.
>>
> --
> You received this message because you are subscribed to the Google Groups
> "singularity" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to singu...@lbl.gov.
>

--000000000000743bcf056cda063d
Content-Type: text/html; charset="UTF-8"
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi George,<div><br></div><div>it&#39;s strange. The file t=
hat is not finding is &quot;srun&quot; not &quot;mpi-pi.o&quot;. Is this so=
mething related with Slurm workload manager? is srun installed inside the c=
ontainer?</div><div><br></div><div>Best,</div><div>V=C3=ADctor</div></div><=
div class=3D"gmail_extra"><br><div class=3D"gmail_quote">2018-05-21 17:35 G=
MT+02:00 George Zaki <span dir=3D"ltr">&lt;<a href=3D"mailto:georg...@gmail=
.com" target=3D"_blank">georg...@gmail.com</a>&gt;</span>:<br><blockquote c=
lass=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;=
padding-left:1ex"><div dir=3D"ltr">Thanks Victor,=C2=A0<div><br></div><div>=
Here is the results with the path of the executable:</div><div><br></div><d=
iv>





<p class=3D"m_351347644437966094inbox-inbox-p1"><span class=3D"m_3513476444=
37966094inbox-inbox-s1">Singularity swift-hypervisor-horovod-<wbr>mvapich.s=
img:~/mpi-examples/<wbr>mpi-example&gt; mpiexec -n 1 ./mpi-pi.o<span class=
=3D"m_351347644437966094inbox-inbox-Apple-converted-space">=C2=A0</span></s=
pan></p>
<p class=3D"m_351347644437966094inbox-inbox-p1"><span class=3D"m_3513476444=
37966094inbox-inbox-s1">[mpiexec@cn3112] HYDU_create_process (utils/launch/=
launch.c:75): execvp error on file srun (No such file or directory)</span><=
/p><p class=3D"m_351347644437966094inbox-inbox-p1"><span class=3D"m_3513476=
44437966094inbox-inbox-s1"><br></span></p></div></div><div class=3D"HOEnZb"=
><div class=3D"h5"><br><div class=3D"gmail_quote"><div dir=3D"ltr">On Mon, =
May 21, 2018 at 9:18 AM victor sv &lt;<a href=3D"mailto:vict...@gmail.com" =
target=3D"_blank">vict...@gmail.com</a>&gt; wrote:<br></div><blockquote cla=
ss=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;pa=
dding-left:1ex"><div dir=3D"ltr">Hi George,=C2=A0<div><br></div><div>please=
 check that you are calling the right program. Is the executable path in th=
e PATH environment variable? If not you have the prepend the path to call t=
he executable.</div><div><br></div><div>Take a look to this:</div><div><br>=
</div><div><a href=3D"https://stackoverflow.com/questions/47472153/mpi-mpir=
un-execvp-error-no-such-file-or-directory" target=3D"_blank">https://stacko=
verflow.com/<wbr>questions/47472153/mpi-mpirun-<wbr>execvp-error-no-such-fi=
le-or-<wbr>directory</a><br></div><div><br></div><div>BR,</div><div>V=C3=AD=
ctor.</div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote">=
2018-05-21 14:58 GMT+02:00 George Zaki <span dir=3D"ltr">&lt;<a href=3D"mai=
lto:georg...@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt;</span>=
:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-le=
ft:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr">Hi Jason,=C2=A0<div><b=
r></div><div>I was not able to run the program even without singularity whe=
n I use mvapitch. I am in contact with our system admin</div><div><br></div=
><div>Here is the=C2=A0 output I got when I run mpiexec within singularity:=
=C2=A0</div><div><br></div><div>mpiexec -n 1 mpi-pi.o<span class=3D"m_35134=
7644437966094m_7541958181832643763m_-6547978355576408365inbox-inbox-Apple-c=
onverted-space">=C2=A0</span><br></div><div>
<p class=3D"m_351347644437966094m_7541958181832643763m_-6547978355576408365=
inbox-inbox-p1"><span class=3D"m_351347644437966094m_7541958181832643763m_-=
6547978355576408365inbox-inbox-s1">[mpiexec@cn3137] HYDU_create_process (ut=
ils/launch/launch.c:75): execvp error on file srun (No such file or directo=
ry)</span></p><p class=3D"m_351347644437966094m_7541958181832643763m_-65479=
78355576408365inbox-inbox-p1">Best regards,</p><p class=3D"m_35134764443796=
6094m_7541958181832643763m_-6547978355576408365inbox-inbox-p1">George.</p><=
/div></div><div class=3D"m_351347644437966094m_7541958181832643763HOEnZb"><=
div class=3D"m_351347644437966094m_7541958181832643763h5"><br><div class=3D=
"gmail_quote"><div dir=3D"ltr">On Mon, May 21, 2018 at 3:04 AM victor sv &l=
t;<a href=3D"mailto:vict...@gmail.com" target=3D"_blank">vict...@gmail.com<=
/a>&gt; wrote:<br></div><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex"><div dir=3D"ltr"><di=
v><div><div><div>Hi George,<br><br></div>not any experience with mvapitch. =
I think the compiler version has no effect here. <br><br>Jason solution and=
 is a good starting point to check if the container MPI works in a single n=
ode (not in several nodes).<br><br></div>To run the hybrid MPI approach you=
 should take into account that both version and vendor of MPI and PMI must =
match. Can you check if PMI libraries match?<br><br></div>BR,<br></div>V=C3=
=ADctor.<br></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote"=
>2018-05-18 20:14 GMT+02:00 Jason Stover <span dir=3D"ltr">&lt;<a href=3D"m=
ailto:jason...@gmail.com" target=3D"_blank">jason...@gmail.com</a>&gt;</spa=
n>:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-=
left:1px #ccc solid;padding-left:1ex">Hi George,<br>
<br>
=C2=A0 Can you run it from inside the container? For example:<br>
<br>
=C2=A0 =C2=A0 singularity exec<br>
/data/zakigf/candle/swift-<wbr>hypervisor-horovod-mvapich.<wbr>simg mpiexec=
 -n 1<br>
mpi-pi.o<br>
<span class=3D"m_351347644437966094m_7541958181832643763m_-6547978355576408=
365m_-6591493537311087044HOEnZb"><font color=3D"#888888"><br>
-J<br>
</font></span><div class=3D"m_351347644437966094m_7541958181832643763m_-654=
7978355576408365m_-6591493537311087044HOEnZb"><div class=3D"m_3513476444379=
66094m_7541958181832643763m_-6547978355576408365m_-6591493537311087044h5"><=
br>
<br>
On Fri, May 18, 2018 at 12:56 PM, George Zaki &lt;<a href=3D"mailto:georg..=
.@gmail.com" target=3D"_blank">georg...@gmail.com</a>&gt; wrote:<br>
&gt; Thanks Marty<br>
&gt;<br>
&gt; Below are the values I got, any obvious mismatch? I git this working f=
ine<br>
&gt; with OpenMPI.<br>
&gt;<br>
&gt; Here is also what I try to run:<br>
&gt;<br>
&gt; singularity exec /data/zakigf/candle/swift-<wbr>hypervisor-horovod-mva=
pich.<wbr>simg<br>
&gt; mpicc mpi-pi.c -o=C2=A0 mpi-pi.o<br>
&gt;<br>
&gt; [zakigf@cn2360 mpi-example]$ mpiexec -n 1 singularity exec<br>
&gt; /data/zakigf/candle/swift-<wbr>hypervisor-horovod-mvapich.<wbr>simg mp=
i-pi.o<br>
&gt;<br>
&gt; Then I kill after no response:<br>
&gt;<br>
&gt; ^C[mpiexec@cn2360] Sending Ctrl-C to processes as requested<br>
&gt;<br>
&gt; [mpiexec@cn2360] Press Ctrl-C again to force abort<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDU_sock_write (utils/sock/sock.c:286): write error =
(Bad<br>
&gt; file descriptor)<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmcd_pmiserv_send_signal (pm/pmiserv/pmiserv_cb.c=
:169):<br>
&gt; unable to write data to proxy<br>
&gt;<br>
&gt; [mpiexec@cn2360] ui_cmd_cb (pm/pmiserv/pmiserv_pmci.c:79)<wbr>: unable=
 to send<br>
&gt; signal downstream<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYDT_dmxu_poll_wait_for_event<br>
&gt; (tools/demux/demux_poll.c:76): callback returned error status<br>
&gt;<br>
&gt; [mpiexec@cn2360] HYD_pmci_wait_for_completion<br>
&gt; (pm/pmiserv/pmiserv_pmci.c:<wbr>198): error waiting for event<br>
&gt;<br>
&gt; [mpiexec@cn2360] main (ui/mpich/mpiexec.c:344): process manager error<=
br>
&gt; waiting for completion<br>
&gt;<br>
&gt;<br>
&gt; Now about the versions:<br>
&gt;<br>
&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; mpiexec -=
-version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3DNONE&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--sr=
cdir=3D.&#39; &#39;CC=3Dgcc&#39; &#39;CFLAGS=3D<br>
&gt; -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib -L/lib -L/lib -Wl,=
-rpath,/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -libumad=
 -libverbs -ldl<br>
&gt; -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D -I/tmp/mvapich2-2.2/src/mpl/=
<wbr>include<br>
&gt; -I/tmp/mvapich2-2.2/src/mpl/<wbr>include -I/tmp/mvapich2-2.2/src/<wbr>=
openpa/src<br>
&gt; -I/tmp/mvapich2-2.2/src/<wbr>openpa/src -D_REENTRANT<br>
&gt; -I/tmp/mvapich2-2.2/src/mpi/<wbr>romio/include -I/include -I/include -=
I/include<br>
&gt; -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; gcc --ver=
sion<br>
&gt;<br>
&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;<br>
&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; Singularity swift-hypervisor-horovod-<wbr>mvapich.simg:~&gt; exit<br>
&gt;<br>
&gt; exit<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ ml mvapich2/2.2/gcc-5.3.0<br>
&gt;<br>
&gt; [+] Loading mvapich2 2.2 for GCC 5.3.0<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ gcc --version<br>
&gt;<br>
&gt; gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-17)<br>
&gt;<br>
&gt; Copyright (C) 2010 Free Software Foundation, Inc.<br>
&gt;<br>
&gt; This is free software; see the source for copying conditions.=C2=A0 Th=
ere is NO<br>
&gt;<br>
&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PUR=
POSE.<br>
&gt;<br>
&gt;<br>
&gt; [zakigf@cn2360 ~]$ mpiexec --version<br>
&gt;<br>
&gt; HYDRA build details:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A03.1.4<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Wed Sep=C2=A0 7=
 14:33:43 EDT 2016<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 /usr/local/GCC/5.3=
.0/bin/gcc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/g++<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/<wbr>gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0/usr/local/GCC/5=
.3.0/bin/<wbr>gfortran<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-option-c=
hecking&#39;<br>
&gt; &#39;--prefix=3D/usr/local/MVAPICH2/<wbr>2.2/gcc-5.3.0&#39;<br>
&gt; &#39;--with-slurm-lib=3D/usr/local/<wbr>slurm/lib&#39;<br>
&gt; &#39;--with-slurm-include=3D/usr/<wbr>local/slurm/include&#39; &#39;--=
enable-debug=3Dnone&#39;<br>
&gt; &#39;--enable-timing=3Druntime&#39; &#39;CC=3D/usr/local/GCC/5.3.0/bin=
/<wbr>gcc&#39;<br>
&gt; &#39;CXX=3D/usr/local/GCC/5.3.0/bin/<wbr>g++&#39; &#39;FC=3D/usr/local=
/GCC/5.3.0/bin/<wbr>gfortran&#39;<br>
&gt; &#39;F77=3D/usr/local/GCC/5.3.0/bin/<wbr>gfortran&#39; &#39;--cache-fi=
le=3D/dev/null&#39;<br>
&gt; &#39;--srcdir=3D.&#39; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#=
39;LDFLAGS=3D-L/lib -L/lib<br>
&gt; -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;=
LIBS=3D-libmad<br>
&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpl/include<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/openpa/src<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/openpa/src -D_REENTRAN=
T<br>
&gt; -I/usr/local/src/mvapich2/<wbr>mvapich2-2.2/src/mpi/romio/<wbr>include=
 -I/include<br>
&gt; -I/include -I/include -I/include&#39;<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf sge<=
br>
&gt; manual persist<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =C2=A0=
user slurm ll lsf sge pbs<br>
&gt; cobalt<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;<br>
&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;<br>
&gt;<br>
&gt; On Fri, May 18, 2018 at 1:31 PM Kandes, Martin &lt;<a href=3D"mailto:m=
ka...@sdsc.edu" target=3D"_blank">mka...@sdsc.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Hi George,<br>
&gt;&gt;<br>
&gt;&gt; I run with different gcc compiler versions inside and outside my M=
PI<br>
&gt;&gt; containers. So I would be surprised if that is the issue here. I&#=
39;m not sure<br>
&gt;&gt; I have a good recommendation of where to start debugging your prob=
lem. But I<br>
&gt;&gt; might start by double checking the MPI versions match inside and o=
utside the<br>
&gt;&gt; container. e.g. see [1].<br>
&gt;&gt;<br>
&gt;&gt; Marty<br>
&gt;&gt;<br>
&gt;&gt; [1]<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-ln3 ~]$ srun --partition=3Ddebug --pty --nodes=3D1<=
br>
&gt;&gt; --ntasks-per-node=3D24 -t 00:30:00 --wait=3D0 --export=3DALL /bin/=
bash<br>
&gt;&gt; srun: job 16364303 queued and waiting for resources<br>
&gt;&gt; srun: job 16364303 has been allocated resources<br>
&gt;&gt; [mkandes@comet-14-06 ~]$ cd /scratch/mkandes/16364303/<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ cp<br>
&gt;&gt; /oasis/scratch/comet/mkandes/<wbr>temp_project/singularity/<wbr>im=
ages/ubuntu-mvapich2.img<br>
&gt;&gt; ./<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module purge<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load gnu<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load mvapich2_ib<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module list<br>
&gt;&gt; Currently Loaded Modulefiles:<br>
&gt;&gt;=C2=A0 =C2=A01) gnu/4.9.2=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A02) mvapi=
ch2_ib/2.1<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ module load singularity<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ gcc --version<br>
&gt;&gt; gcc (GCC) 4.9.2<br>
&gt;&gt; Copyright (C) 2014 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ mpirun --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc=C2=A0 -f=
PIC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++=C2=A0 -fP=
IC -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran -fPI=
C -O3<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2/gnu/<wbr>ib&#39; &#39;--enable-share=
d&#39; &#39;--enable-sharedlibs=3Dgcc&#39;<br>
&gt;&gt; &#39;--with-hwloc&#39; &#39;--enable-f77&#39; &#39;--enable-fc&#39=
; &#39;--enable-hybrid&#39;<br>
&gt;&gt; &#39;--with-ib-include=3D/usr/<wbr>include/infiniband&#39; &#39;--=
with-ib-libpath=3D/usr/lib64&#39;<br>
&gt;&gt; &#39;--enable-fast=3DO3&#39;<br>
&gt;&gt; &#39;--with-limic2=3D/state/<wbr>partition1/git/mpi-roll/BUILD/<wb=
r>sdsc-mvapich2_gnu_ib-2.1/../..<wbr>//cache/build-limic&#39;<br>
&gt;&gt; &#39;--with-slurm=3D/usr/lib64/<wbr>slurm&#39; &#39;--with-file-sy=
stem=3Dlustre&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D-fPIC -O3 -O3&#39; &#39;CXX=3Dg++&#39; &#39;CXXFLAGS=
=3D-fPIC -O3 -O3&#39; &#39;FC=3Dgfortran&#39;<br>
&gt;&gt; &#39;FCFLAGS=3D-fPIC -O3 -O3&#39; &#39;F77=3Dgfortran&#39; &#39;FF=
LAGS=3D-L/usr/lib64 -L/lib -L/lib<br>
&gt;&gt; -fPIC -O3 -O3&#39; &#39;--cache-file=3D/dev/null&#39; &#39;--srcdi=
r=3D.&#39; &#39;LDFLAGS=3D-L/usr/lib64<br>
&gt;&gt; -L/lib -L/lib -L/lib -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/usr=
/lib64<br>
&gt;&gt; -L/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/../..//cache/build-<wbr>limic/lib<br>
&gt;&gt; -L/lib -L/lib&#39; &#39;LIBS=3D-libmad -lrdmacm -libumad -libverbs=
 -ldl -lrt -llimic2<br>
&gt;&gt; -lm -lpthread &#39; &#39;CPPFLAGS=3D-I/usr/include/<wbr>infiniband=
<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpl/<wbr>include<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpl/<wbr>include<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/<wbr>openpa/src<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/<wbr>openpa/src<br>
&gt;&gt; -D_REENTRANT<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/mvapich2-2.1/src/mpi/<wbr>romio/include<br>
&gt;&gt; -I/include -I/include -I/usr/include/infiniband<br>
&gt;&gt; -I/state/partition1/git/mpi-<wbr>roll/BUILD/sdsc-mvapich2_gnu_<wbr=
>ib-2.1/../..//cache/build-<wbr>limic/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; [mkandes@comet-14-06 16364303]$ singularity shell ubuntu-mvapich2.=
img<br>
&gt;&gt; Singularity: Invoking an interactive shell within container...<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
 gcc --version<br>
&gt;&gt; gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>
&gt;&gt; Copyright (C) 2015 Free Software Foundation, Inc.<br>
&gt;&gt; This is free software; see the source for copying conditions.=C2=
=A0 There is NO<br>
&gt;&gt; warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR=
<br>
&gt;&gt; PURPOSE.<br>
&gt;&gt;<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
 mpirun<br>
&gt;&gt; --version<br>
&gt;&gt; HYDRA build details:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Version:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A03.1.4<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Release Date:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 Thu Apr=C2=
=A0 2 17:15:15 EDT 2015<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CC:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 gcc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0CXX:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0g++<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F77:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0F90:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0gfortran<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Configure options:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0&#39;--disable-optio=
n-checking&#39;<br>
&gt;&gt; &#39;--prefix=3D/opt/mvapich2&#39; &#39;--cache-file=3D/dev/null&#=
39; &#39;--srcdir=3D.&#39; &#39;CC=3Dgcc&#39;<br>
&gt;&gt; &#39;CFLAGS=3D -DNDEBUG -DNVALGRIND -O2&#39; &#39;LDFLAGS=3D-L/lib=
 -L/lib -L/lib<br>
&gt;&gt; -Wl,-rpath,/lib -L/lib -Wl,-rpath,/lib -L/lib -L/lib&#39; &#39;LIB=
S=3D-libmad -lrdmacm<br>
&gt;&gt; -libumad -libverbs -ldl -lrt -lm -lpthread &#39; &#39;CPPFLAGS=3D<=
br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/mpl/<wbr>include -I/tmp/mvapich2-2.1/src/m=
pl/<wbr>include<br>
&gt;&gt; -I/tmp/mvapich2-2.1/src/<wbr>openpa/src -I/tmp/mvapich2-2.1/src/<w=
br>openpa/src<br>
&gt;&gt; -D_REENTRANT -I/tmp/mvapich2-2.1/src/mpi/<wbr>romio/include -I/inc=
lude -I/include<br>
&gt;&gt; -I/include -I/include&#39;<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Process Manager:=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0pmi<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Launchers available:=C2=A0 =C2=A0 =C2=A0 =C2=A0=
 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0ssh rsh fork slurm ll lsf =
sge<br>
&gt;&gt; manual persist<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Topology libraries available:=C2=A0 =C2=A0 =C2=
=A0 =C2=A0 =C2=A0 =C2=A0 hwloc<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Resource management kernels available:=C2=A0 =
=C2=A0user slurm ll lsf sge pbs<br>
&gt;&gt; cobalt<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Checkpointing libraries available:<br>
&gt;&gt;=C2=A0 =C2=A0 =C2=A0Demux engines available:=C2=A0 =C2=A0 =C2=A0 =
=C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0 =C2=A0poll select<br>
&gt;&gt; Singularity ubuntu-mvapich2.img:/scratch/<wbr>mkandes/16364303&gt;=
<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; ______________________________<wbr>__<br>
&gt;&gt; From: George Zaki [<a href=3D"mailto:georg...@gmail.com" target=3D=
"_blank">georg...@gmail.com</a>]<br>
&gt;&gt; Sent: Friday, May 18, 2018 6:48 AM<br>
&gt;&gt; To: singularity<br>
&gt;&gt; Subject: [Singularity] Running an mpi program with mvapitch<br>
&gt;&gt;<br>
&gt;&gt; Hi singularity team,<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I would like to run an MPI program in a singularity container. The=
 program<br>
&gt;&gt; is compiled using mvapicth2.2 using a gcc version 5.4.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; I can see that my cluster has a compiled version of mvapitch2.2 wi=
th gcc<br>
&gt;&gt; 5.3<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; When I run:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; mpiexe -n 1 singularity exec /path/to/sing/image ./mpi-pi.o<br>
&gt;&gt;<br>
&gt;&gt; the call does not return.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Does the gcc version has to be exactly the same? I tried the switc=
h the<br>
&gt;&gt; compiler in this image:<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; BootStrap: docker<br>
&gt;&gt; From: nvidia/cuda:8.0-cudnn6-devel-<wbr>ubuntu16.04<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; However when gcc 5.3 is used the mvapitch does not build correctly=
.<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; If that&#39;s the problem, Is there a preferred method of switchin=
g gcc<br>
&gt;&gt; version in this container singularity container?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thanks,<br>
&gt;&gt; George<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to the Google=
 Groups<br>
&gt;&gt; &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this group and stop receiving emails from it, =
send an<br>
&gt;&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_bla=
nk">singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;&gt;<br>
&gt;&gt; --<br>
&gt;&gt; You received this message because you are subscribed to a topic in=
 the<br>
&gt;&gt; Google Groups &quot;singularity&quot; group.<br>
&gt;&gt; To unsubscribe from this topic, visit<br>
&gt;&gt; <a href=3D"https://groups.google.com/a/lbl.gov/d/topic/singularity=
/A6I5mZxnmFU/unsubscribe" rel=3D"noreferrer" target=3D"_blank">https://grou=
ps.google.com/a/<wbr>lbl.gov/d/topic/singularity/<wbr>A6I5mZxnmFU/unsubscri=
be</a>.<br>
&gt;&gt; To unsubscribe from this group and all its topics, send an email t=
o<br>
&gt;&gt; <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singu=
larity+unsubscribe@lbl.<wbr>gov</a>.<br>
&gt;<br>
&gt; --<br>
&gt; You received this message because you are subscribed to the Google Gro=
ups<br>
&gt; &quot;singularity&quot; group.<br>
&gt; To unsubscribe from this group and stop receiving emails from it, send=
 an<br>
&gt; email to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">=
singularity+unsubscribe@lbl.<wbr>gov</a>.<br>
<br>
-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singularity%...@lbl.gov" target=3D"_blank">singul=
arity+unsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/<wbr>lbl.gov/d/topic/singularity/<wbr>A6I5mZxnmFU=
/unsubscribe</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscribe@lbl.=
<wbr>gov</a>.<br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

<p></p>

-- <br>
You received this message because you are subscribed to a topic in the Goog=
le Groups &quot;singularity&quot; group.<br>
To unsubscribe from this topic, visit <a href=3D"https://groups.google.com/=
a/lbl.gov/d/topic/singularity/A6I5mZxnmFU/unsubscribe" target=3D"_blank">ht=
tps://groups.google.com/a/<wbr>lbl.gov/d/topic/singularity/<wbr>A6I5mZxnmFU=
/unsubscribe</a>.<br>
To unsubscribe from this group and all its topics, send an email to <a href=
=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+unsubscribe@lbl.=
<wbr>gov</a>.<br>
</blockquote></div>

<p></p>

-- <br>
You received this message because you are subscribed to the Google Groups &=
quot;singularity&quot; group.<br>
To unsubscribe from this group and stop receiving emails from it, send an e=
mail to <a href=3D"mailto:singu...@lbl.gov" target=3D"_blank">singularity+u=
nsubscribe@lbl.<wbr>gov</a>.<br>
</div></div></blockquote></div><br></div>

--000000000000743bcf056cda063d--
